{"0":{"Abstract":"In see-through systems an observer watches a (background) scene partially occluded by a display. In this display, usually positioned close to the observer, a region of the background scene is shown, yielding the sensation that the display is transparent. To achieve the transparency effect, it is very important to compensate the parallax error and other distortions caused by the image acquisition system. In this paper a detailed study of a video see-through methodology with parallax correction is performed. In a system composed by two cameras-one directed to the user and another to the background scene-and a display, the relative position between the user, the display and the scene is estimated using a feature detection algorithm and the parallax error is compensated assuming a planar scene model. The application of the proposed methodology on Driver Assistance Systems (DAS) is proposed. A theoretical assessment of the algorithm shows that although approximations are proposed to simplify the methodology and reduce the computational cost, such as the planar scene model and fixed working distance, on some practical situations their effects can be neglected without noticeable impact on the perceptual quality of the solution.","Authors":"R. A. Borsoi; G. H. Costa","DOI":"10.1109\/TVCG.2017.2705184","Keywords":"See-through;virtual transparency;DAS;parallax;augmented reality;user-perspective rendering;dual-view","Keywords_Processed":"see through;dual view;das;augment reality;parallax;virtual transparency;user perspective render","Title":"On the Performance and Implementation of Parallax Free Video See-Through Displays","Labels":null,"Keyword_Vector":[0.1292683262,0.3112111118,0.0320906643,-0.0241342374,0.0523840692,-0.0792716553,0.007992199,0.0433988337,0.0297311261,-0.0322391816,-0.0026471453,-0.0398549027,0.0341501051,-0.0114223976,0.0095072919,-0.0048396064,0.0328832265,0.1547988191,-0.075171259,0.0507900328,-0.0182348344,-0.1372500012,-0.0779056334,0.0762101],"Abstract_Vector":[0.2077394835,0.2129944512,0.1079251227,0.1206546936,-0.0087735717,-0.006363738,-0.0058005994,0.0606058037,-0.0826957666,0.0071747766,-0.0112051433,0.2065785052,0.1100116311,-0.2130769913,-0.1124899412,0.0608094033,-0.0960052923,-0.0149542745,0.0624227539,0.0617862821,-0.0176853534,-0.0998507826,-0.006988696,0.0106418672,0.02998913,0.1374865005,0.029566934,0.0910544215,-0.1161263877,0.0363206448,0.0465858475,0.0391256333,-0.004619187,0.0423593315,0.0228771805,0.0760235973,-0.0316858868,-0.0508756849,0.0603118537,-0.0166754462,0.0914770265,-0.0080040766,0.0267159021,0.0055175345,0.1366160376,0.002831082,0.0457258825,-0.100559861,-0.1200604779,0.0245458929,0.0539738366,0.0405504986,0.1065067349,-0.0599343051,0.0658765649,-0.0595634026,0.0176096846,-0.0058367431,0.0162628551,-0.0006025267,-0.0901462553,0.0970732518,-0.0516751119,-0.0555681927,0.0059629289,0.0365767473,-0.1122335229,0.169733206,0.061794324,-0.0267662132,0.0198390338,-0.0965795503,-0.0670027958,0.012236165,-0.0364250806,0.0268718996]},"1":{"Abstract":"Flow fields are usually visualized relative to a global observer, i.e., a single frame of reference. However, often no global frame can depict all flow features equally well. Likewise, objective criteria for detecting features such as vortices often use either a global reference frame, or compute a separate frame for each point in space and time. We propose the first general framework that enables choosing a smooth trade-off between these two extremes. Using global optimization to minimize specific differential geometric properties, we compute a time-dependent observer velocity field that describes the motion of a continuous field of observers adapted to the input flow. This requires developing the novel notion of an observed time derivative. While individual observers are restricted to rigid motions, overall we compute an approximate Killing field, corresponding to almost-rigid motion. This enables continuous transitions between different observers. Instead of focusing only on flow features, we furthermore develop a novel general notion of visualizing how all observers jointly perceive the input field. This in fact requires introducing the concept of an observation time, with respect to which a visualization is computed. We develop the corresponding notions of observed stream, path, streak, and time lines. For efficiency, these characteristic curves can be computed using standard approaches, by first transforming the input field accordingly. Finally, we prove that the input flow perceived by the observer field is objective. This makes derived flow features, such as vortices, objective as well.","Authors":"M. Hadwiger; M. Mlejnek; T. Theu\u00dfl; P. Rautek","DOI":"10.1109\/TVCG.2018.2864839","Keywords":"Flow visualization;observer frames of reference;Killing vector fields;infinitesimal isometries;Lie derivatives;objectivity","Keywords_Processed":"objectivity;kill vector field;observer frame of reference;flow visualization;lie derivative;infinitesimal isometry","Title":"Time-Dependent Flow seen through Approximate Observer Killing Fields","Labels":null,"Keyword_Vector":[0.115027252,-0.0415685805,-0.0962963075,0.0059526489,-0.0625372426,-0.0773891776,0.0551509455,-0.028776297,-0.0334580419,0.0321236212,0.1772917053,0.0958355252,-0.0199528451,-0.0456045367,-0.020868456,0.01831971,-0.0579976327,-0.1566633457,-0.006099812,-0.0144034291,-0.0816628809,-0.1274910593,0.0645711227,0.0875128148],"Abstract_Vector":[0.2309196573,0.1485571542,-0.1221838795,0.0284830972,-0.0355960212,0.0159498249,0.233980147,-0.243505441,-0.030912234,-0.0647066127,0.0566898657,-0.0192654111,0.2155663755,-0.0958566187,0.1043042418,0.0142669564,0.0719430375,0.0685984115,-0.0002061322,-0.0420556579,-0.0828994515,0.0552922303,-0.0645006168,-0.1118550989,0.0069440554,0.1116179084,-0.1285139923,0.1105236479,0.0051970168,0.1220776991,0.0043115861,0.0750200842,-0.0370597662,-0.0030240469,-0.0129702574,-0.0242077766,0.0275095121,0.0622129705,0.0294466605,0.0004030481,-0.056105932,-0.0213668669,0.0376813614,0.001769053,0.0061842108,0.0145277453,0.0926222303,-0.0663393577,0.0750943342,-0.0369390141,-0.005550944,0.0507762067,-0.0020625916,-0.0038236193,0.0522739981,0.0432448248,-0.0086498297,0.0756680846,-0.0810032906,-0.059016634,-0.0279808001,-0.043908981,0.0166045782,-0.0815192449,0.0243719189,0.0628788004,-0.0243729087,-0.0211921228,0.0410870458,0.0017142616,-0.0206420882,0.0185415665,-0.0436400627,-0.0466502406,0.0143639886,-0.0289750184]},"2":{"Abstract":"Kinetic approaches, i.e., methods based on the lattice Boltzmann equations, have long been recognized as an appealing alternative for solving incompressible Navier-Stokes equations in computational fluid dynamics. However, such approaches have not been widely adopted in graphics mainly due to the underlying inaccuracy, instability and inflexibility. In this paper, we try to tackle these problems in order to make kinetic approaches practical for graphical applications. To achieve more accurate and stable simulations, we propose to employ the non-orthogonal central-moment-relaxation model, where we develop a novel adaptive relaxation method to retain both stability and accuracy in turbulent flows. To achieve flexibility, we propose a novel continuous-scale formulation that enables samples at arbitrary resolutions to easily communicate with each other in a more continuous sense and with loose geometrical constraints, which allows efficient and adaptive sample construction to better match the physical scale. Such a capability directly leads to an automatic sample construction which generates static and dynamic scales at initialization and during simulation, respectively. This effectively makes our method suitable for simulating turbulent flows with arbitrary geometrical boundaries. Our simulation results with applications to smoke animations show the benefits of our method, with comparisons for justification and verification.","Authors":"W. Li; K. Bai; X. Liu","DOI":"10.1109\/TVCG.2018.2859931","Keywords":"multi-resolution fluid simulation;lattice Boltzmann model;adaptive refinement","Keywords_Processed":"adaptive refinement;multi resolution fluid simulation;lattice boltzmann model","Title":"Continuous-Scale Kinetic Fluid Simulation","Labels":null,"Keyword_Vector":[0.0515748933,0.0241841032,0.0405149747,0.0403591904,0.026188102,-0.1199137865,0.0625280192,0.0052247096,-0.1267243084,-0.1024164112,0.0264293813,0.1500796895,-0.022432436,0.0625925853,0.0374694997,-0.2124230533,0.1277390151,0.0629072283,0.1048008604,0.2219895408,-0.109823264,0.2416968643,-0.1438139202,0.06530239],"Abstract_Vector":[0.2008465471,0.1012986903,-0.1192979255,-0.0411368757,0.023525909,0.0265099928,0.0945422029,-0.0309882853,-0.0490805039,-0.1666332088,-0.0675669269,-0.0037682683,0.05089744,0.1996616464,-0.0333483675,0.1130522405,0.0091254042,0.0732653658,-0.0647673569,0.0387054378,0.0427002699,-0.0463795207,0.0414451858,0.0102043403,-0.0258426057,0.0427213247,-0.0256430726,0.0245832202,0.0777687444,-0.083261502,-0.1347179986,-0.0254126843,0.0484137645,-0.0391405,0.0657182386,-0.0189277967,0.0417577045,-0.0383643641,-0.0283283056,-0.0500006765,0.0530899032,0.0646704439,0.0516375411,-0.0641941347,0.0741080256,0.0262722741,0.0014329619,0.0363917709,-0.0026356169,-0.0188629988,0.0302640292,-0.0372848476,0.0405175226,0.0477909639,0.061653913,-0.0612885686,0.018854118,0.0279819089,0.0087179736,0.0009479444,-0.1893780735,0.1189286641,-0.0612090717,-0.042395295,-0.0404659648,0.10836689,0.0490158509,0.100743777,-0.1796683039,0.0390899496,0.0398185776,0.1385056604,0.081425683,0.0659230748,-0.0739159185,0.0732682296]},"3":{"Abstract":"Multiple time series are a set of multiple quantitative variables occurring at the same interval. They are present in many domains such as medicine, finance, and manufacturing for analytical purposes. In recent years, streamgraph visualization (evolved from ThemeRiver) has been widely used for representing temporal evolution patterns in multiple time series. However, streamgraph as well as ThemeRiver suffer from scalability problems when dealing with several time series. To solve this problem, multiple time series can be organized into a hierarchical structure where individual time series are grouped hierarchically according to their proximity. In this paper, we present a new streamgraph-based approach to convey the hierarchical structure of multiple time series to facilitate the exploration and comparisons of temporal evolution. Based on a focus+context technique, our method allows time series exploration at different granularities (e.g., from overview to details). To illustrate our approach, two usage examples are presented.","Authors":"E. Cuenca; A. Sallaberry; F. Y. Wang; P. Poncelet","DOI":"10.1109\/TVCG.2018.2796591","Keywords":"Streamgraph;stacked graph;time series;aggregation;multiresolution visualization;overview+detail;focus+context;fisheye","Keywords_Processed":"stack graph;fisheye;focus context;streamgraph;aggregation;time series;multiresolution visualization;overview detail","Title":"MultiStream: A Multiresolution Streamgraph Approach to Explore Hierarchical Time Series","Labels":null,"Keyword_Vector":[0.1545024124,-0.017381092,-0.0438160029,0.0218930621,-0.1153040771,-0.0873746985,0.0927508961,0.0484803647,0.0261125691,0.2956674817,-0.1159284601,-0.090350727,-0.2375725266,-0.0954687863,0.1199135365,-0.0318958813,0.1254102988,0.0594241127,0.0361032442,-0.1008766446,-0.0622975997,0.0827787447,0.0226096305,0.0328945856],"Abstract_Vector":[0.2111628975,-0.0577775061,-0.0805580466,-0.0433098695,0.0116903486,-0.0783191062,0.105774521,-0.0225078514,-0.0099269753,-0.0285689113,-0.022308202,-0.0442499876,-0.0276955555,0.0402508785,0.1478932525,0.0017180863,-0.0044635017,-0.1665676876,-0.0816239436,0.1111889912,-0.1735662758,-0.1257786641,0.033749111,-0.0842440463,0.1297985648,0.0427500055,0.0181324631,-0.0263616557,0.0182405745,0.0656402977,0.0111749932,-0.0614534455,0.1202511151,-0.0608214954,-0.008420827,0.1207016544,0.0123253218,0.0986647014,0.0130577436,0.0263554403,0.0088741425,-0.0658136745,-0.1161171135,0.1052702384,-0.0248052004,0.0310520667,-0.1010264811,-0.0777918339,0.0267804118,-0.1301137898,-0.0658552578,0.0021558395,-0.1279188893,-0.003079266,0.0545907793,0.014584996,-0.117987729,0.0317329656,0.1300844742,0.0259288404,0.0001203714,-0.0696450268,0.0550282694,0.0711559671,-0.0163855399,0.047703489,-0.0199770575,0.1297280063,-0.0380329522,-0.0517611192,-0.0211727536,-0.1095147019,-0.0439654799,-0.0140783562,0.0403677996,-0.0903071979]},"4":{"Abstract":"Ensemble sensitivity analysis (ESA) has been established in the atmospheric sciences as a correlation-based approach to determine the sensitivity of a scalar forecast quantity computed by a numerical weather prediction model to changes in another model variable at a different model state. Its applications include determining the origin of forecast errors and placing targeted observations to improve future forecasts. We - a team of visualization scientists and meteorologists - present a visual analysis framework to improve upon current practice of ESA. We support the user in selecting regions to compute a meaningful target forecast quantity by embedding correlation-based grid-point clustering to obtain statistically coherent regions. The evolution of sensitivity features computed via ESA are then traced through time, by integrating a quantitative measure of feature matching into optical-flow-based feature assignment, and displayed by means of a swipe-path showing the geo-spatial evolution of the sensitivities. Visualization of the internal correlation structure of computed features guides the user towards those features robustly predicting a certain weather event. We demonstrate the use of our method by application to real-world 2D and 3D cases that occurred during the 2016 NAWDEX field campaign, showing the interactive generation of hypothesis chains to explore how atmospheric processes sensitive to each other are interrelated.","Authors":"A. Kumpf; M. Rautenhaus; M. Riemer; R. Westermann","DOI":"10.1109\/TVCG.2018.2864901","Keywords":"Correlation;clustering;tracking;ensemble visualization","Keywords_Processed":"cluster;correlation;tracking;ensemble visualization","Title":"Visual Analysis of the Temporal Evolution of Ensemble Forecast Sensitivities","Labels":null,"Keyword_Vector":[0.2776158079,-0.1448051704,-0.1725791445,-0.2065291772,0.0628114021,-0.1585090898,-0.2427312584,-0.1586235248,0.0605066584,-0.059414627,-0.1050133748,0.137618185,-0.017335855,-0.1292939373,-0.0483811817,0.0967454165,0.0770697789,-0.0281439356,-0.1180757056,-0.04633433,0.0433631833,0.0376297347,-0.1481739791,-0.1830568834],"Abstract_Vector":[0.2700100516,-0.0136296035,-0.125779773,0.1982941211,0.0308071943,-0.0465191126,0.0303059198,-0.1193113035,0.0699809592,0.1491640616,-0.0601116045,-0.0997562837,0.2676780795,-0.0192317819,-0.0755522009,0.0115109591,-0.0434414344,-0.0522536692,0.0351774242,0.0433104419,-0.0972678155,0.1057862977,0.0218538268,0.0373771782,-0.0544140153,0.0492561582,0.0406752019,-0.1128500519,-0.0929778323,0.0266863079,0.0215164359,0.0788199775,0.0538169515,-0.0699160327,-0.0091525253,0.0360657025,0.0086808957,-0.0730546498,-0.0262090474,-0.1166817631,-0.036024182,-0.0028288462,0.0129552358,0.0309536588,0.0132307716,0.0070830689,0.0298761456,-0.0271199794,-0.0428879834,0.0134460803,-0.0059822289,0.0028486919,0.0056283501,-0.0646627057,-0.0058218438,-0.0810263935,-0.0288509786,0.0450655825,-0.0216091862,-0.0961674323,-0.0129118254,-0.024615031,-0.0848843829,0.0992948839,0.1356897882,0.0293106896,0.0184006687,-0.0141424489,0.0335192074,0.0050403121,0.0764938141,-0.048880581,0.0478507752,-0.0362645212,0.0316245622,-0.1165970671]},"5":{"Abstract":"Many algorithms for scientific visualization and image analysis are rooted in the world of continuous scalar, vector, and tensor fields, but are programmed in low-level languages and libraries that obscure their mathematical foundations. Diderot is a parallel domain-specific language that is designed to bridge this semantic gap by providing the programmer with a high-level, mathematical programming notation that allows direct expression of mathematical concepts in code. Furthermore, Diderot provides parallel performance that takes advantage of modern multicore processors and GPUs. The high-level notation allows a concise and natural expression of the algorithms and the parallelism allows efficient execution on real-world datasets.","Authors":"G. Kindlmann; C. Chiw; N. Seltzer; L. Samuels; J. Reppy","DOI":"10.1109\/TVCG.2015.2467449","Keywords":"Domain specific language;portable parallel programming;scientific visualization;tensor fields;Domain specific language;portable parallel programming;scientific visualization;tensor fields","Keywords_Processed":"domain specific language;tensor field;portable parallel programming;scientific visualization","Title":"Diderot: a Domain-Specific Language for Portable Parallel Scientific Visualization and Image Analysis","Labels":null,"Keyword_Vector":[0.1376687673,-0.0629312123,-0.0951753501,-0.0045936132,0.0120450789,0.012335098,0.0519216679,0.0507370073,-0.08402784,0.0015927226,0.0134514169,0.0301620087,0.0249062672,-0.0306459555,-0.0961467821,0.0871750059,-0.1197261387,-0.137228731,0.0530777098,-0.0709766121,-0.1010770076,-0.0642054607,-0.1537670346,0.0323376619],"Abstract_Vector":[0.1594935072,-0.0011051997,-0.030612426,0.0137322734,0.0616645433,0.0333555246,0.0316054853,-0.0324480447,-0.0945419574,-0.0328873677,-0.0032477908,-0.03218723,-0.0377729378,0.0672622308,0.0442876882,-0.1001822008,0.1130631537,-0.0601094105,0.2066725957,0.0224862268,0.1930307716,-0.0431333613,-0.04355736,-0.0141275136,0.1439139951,0.0702142047,0.0096796332,0.001535282,0.025786812,0.009926792,-0.0030137956,0.0426255385,-0.0573570713,-0.0340539968,-0.1579869445,0.0629270271,-0.1301871823,-0.0031366953,0.0961966737,-0.0379344301,-0.1139755327,-0.0283929627,0.1184593534,0.0968009128,-0.0785517329,0.0479922863,0.0084664822,0.1614914185,-0.1386978452,0.0515244632,-0.0538122814,-0.0700035981,0.011581244,-0.0446993288,-0.0258411648,0.0852337464,-0.1407970194,0.0063009239,0.0117126703,0.020060419,0.0341865927,0.0055461413,-0.0707314642,-0.0740890369,0.0155236057,0.0338372443,-0.0254419212,0.0295027543,-0.0522614203,-0.0393234963,0.056882802,0.0088724382,-0.1611107011,-0.0178362479,-0.0957257107,0.0418639605]},"6":{"Abstract":"Generating visualizations at the size of a word creates dense information representations often called sparklines. The integration of word-sized graphics into text could avoid additional cognitive load caused by splitting the readers' attention between figures and text. In scientific publications, these graphics make statements easier to understand and verify because additional quantitative information is available where needed. In this work, we perform a literature review to find out how researchers have already applied such word-sized representations. Illustrating the versatility of the approach, we leverage these representations for reporting empirical and bibliographic data in three application examples. For interactive Web-based publications, we explore levels of interactivity and discuss interaction patterns to link visualization and text. We finally call the visualization community to be a pioneer in exploring new visualization-enriched and interactive publication formats.","Authors":"F. Beck; D. Weiskopf","DOI":"10.1109\/TVCG.2017.2674958","Keywords":"Sparklines;word-sized graphics;literature survey;text and visualization;interactive documents;scientific publishing","Keywords_Processed":"sparkline;literature survey;interactive document;word sized graphic;text and visualization;scientific publishing","Title":"Word-Sized Graphics for Scientific Texts","Labels":null,"Keyword_Vector":[0.1203346953,-0.0334213035,-0.014027416,-0.0069605794,-0.0241053977,-0.0392030278,0.049936921,-0.0562299151,0.001296804,0.0221657023,-0.0604989382,-0.0949092613,-0.0008615628,0.1673642284,-0.039618946,0.0395387016,-0.0053464905,0.0016442961,0.1256407434,-0.0330794351,-0.1179370504,-0.1208303126,0.0702489129,-0.2581453765],"Abstract_Vector":[0.1935873809,-0.0889429069,0.0756188473,-0.0798078973,0.0690090539,0.10728552,0.0207885538,-0.0453824099,-0.0661790996,-0.0711250275,-0.0063160957,-0.0183932857,-0.1031190725,-0.0728837717,0.0227844851,0.0404441565,0.0484745536,0.0134699123,-0.0698743191,0.1103837698,-0.1108916075,0.0461002354,0.1071373822,0.1246629774,-0.1152142253,-0.0181648618,0.1135104961,-0.0118531417,-0.0203452015,-0.030352048,0.0023524059,0.0707798611,0.0549982214,0.1273020677,-0.1455404137,0.1604538626,-0.0636834604,0.083860239,0.0050276817,0.0039195956,-0.125920842,-0.0464036886,0.0326348618,0.029956028,0.0459730417,0.0541578918,0.1119073745,0.1265739568,-0.0332481711,-0.0258556006,0.0130910897,-0.0853772336,-0.0402569144,0.0387375588,-0.0364231468,-0.0623202459,-0.1132012283,0.0119653724,-0.0931001497,-0.1381562787,-0.0546582379,-0.0402395552,0.044651798,-0.121150124,-0.0721849608,-0.0533775364,0.0806061018,-0.0786174418,-0.0569377885,-0.1166396983,-0.0492899756,-0.0398291362,0.0487344175,-0.1031576954,-0.0430734137,-0.0636185862]},"7":{"Abstract":"We present a novel real-time approach for user-guided intrinsic decomposition of static scenes captured by an RGB-D sensor. In the first step, we acquire a three-dimensional representation of the scene using a dense volumetric reconstruction framework. The obtained reconstruction serves as a proxy to densely fuse reflectance estimates and to store user-provided constraints in three-dimensional space. User constraints, in the form of constant shading and reflectance strokes, can be placed directly on the real-world geometry using an intuitive touch-based interaction metaphor, or using interactive mouse strokes. Fusing the decomposition results and constraints in three-dimensional space allows for robust propagation of this information to novel views by re-projection. We leverage this information to improve on the decomposition quality of existing intrinsic video decomposition techniques by further constraining the ill-posed decomposition problem. In addition to improved decomposition quality, we show a variety of live augmented reality applications such as recoloring of objects, relighting of scenes and editing of material appearance.","Authors":"A. Meka; G. Fox; M. Zollh\u00f6fer; C. Richardt; C. Theobalt","DOI":"10.1109\/TVCG.2017.2734425","Keywords":"Intrinsic video decomposition;reflectance fusion;user-guided shading refinement","Keywords_Processed":"user guide shade refinement;intrinsic video decomposition;reflectance fusion","Title":"Live User-Guided Intrinsic Video for Static Scenes","Labels":null,"Keyword_Vector":[0.0426465465,0.0342058946,0.0159286344,0.0473916715,0.0121504023,-0.0579282919,0.0718794106,0.0440467282,-0.0791862883,-0.0567141345,0.0124320903,-0.0024414152,0.1069183378,0.0311122071,0.1493139736,0.0017109056,0.0313441053,0.098854415,-0.0504302885,0.0321086044,-0.0255359453,-0.01523211,-0.0780596397,-0.0155093981],"Abstract_Vector":[0.2164335617,0.1372539229,0.0018607722,0.0255968668,-0.03854969,-0.0540446453,0.0265685058,-0.0047818858,-0.082007522,-0.0061889756,0.1464146878,0.1239579528,-0.0470674161,-0.0598408362,-0.1152150828,-0.0324772083,-0.0112830619,0.0900719378,-0.0636687077,-0.0083549908,0.0519429876,-0.0430822854,0.1215303187,-0.0734313561,0.0091568098,0.0390843441,0.1433210745,0.009013985,-0.0593614252,-0.119502204,0.0579506988,0.0051453911,0.1092272139,0.1426992495,0.00638237,0.1668358391,-0.0729686384,0.027227427,0.0185778983,0.0415203848,-0.0651478577,0.1320337167,-0.0345713551,-0.0755668487,-0.0102139123,0.1494599438,-0.0008654105,0.0923472884,0.0769384958,0.0266038273,0.0884816756,0.0052522542,-0.1391189856,-0.0734325499,-0.0377746862,-0.0293723147,0.0703298859,0.052328064,0.0306581863,0.1895490545,0.0114512512,0.0518885834,-0.0281655483,0.0131629227,0.0418706203,-0.0301657428,0.052274414,0.057130157,0.0511079864,0.0512658247,0.0201133643,0.035001182,0.0987102546,0.0339338672,0.0481297081,-0.0820765977]},"8":{"Abstract":"Moving sand pictures are interesting devices that can be used to generate an infinite number of unique scenes when repeatedly being flipped over. However, little work has been done on attempting to simulate the process of picture formulation. In this paper, we present an approach capable of generating images in the style of moving sand pictures. Our system defines moving sand pictures in a few steps, such as initialization, segmentation and physical simulation, so that a variety of moving sand pictures including mountain ridges, desert, clouds and even regular patterns can be generated by either automatic or semi-automatic via interaction during initialization and segmentation. Potential applications of our approach range from advertisements, posters, post cards, packaging, to digital arts.","Authors":"M. Zhang; H. Lin; K. Zhang; J. Yu","DOI":"10.1109\/TVCG.2017.2779799","Keywords":"Moving sand picture;tessellation;segmentation;color arrangement;physical simulation","Keywords_Processed":"color arrangement;move sand picture;segmentation;physical simulation;tessellation","Title":"Computer Simulation and Generation of Moving Sand Pictures","Labels":null,"Keyword_Vector":[0.0373478622,0.0047506592,0.0040470855,-0.0052188109,0.0341951795,0.0076463696,0.0527542357,-0.008054771,-0.0408397094,-0.0573232009,0.0557422005,0.0106645687,-0.0620523009,0.0571821072,-0.0616097157,0.0004984936,-0.0241303051,0.1270184759,0.1032258451,0.0310786894,-0.0751758213,0.1795517425,-0.1536742861,0.0626499532],"Abstract_Vector":[0.1104895942,0.055096047,0.0057663009,-0.0214833831,0.032351972,-0.0114517287,0.0322078217,0.0276483353,-0.0604081437,-0.0339620754,0.0326566158,-0.0003067548,-0.009393469,0.0654580217,-0.039964618,0.0795001972,0.0361374411,-0.0068349995,-0.0455333735,-0.0138619173,0.0082911393,0.0184869119,0.0319223989,0.0049452542,0.0167075737,-0.0174897128,-0.0063355424,0.0540089359,0.0206371768,-0.0134766694,0.0439121325,-0.0526357322,0.073662648,0.0405802555,0.045621805,0.0513596149,0.0837648807,-0.066794186,0.095924716,-0.1618471227,0.0531653586,-0.0082500605,-0.0208027868,-0.0400315574,0.0143501788,-0.0850067086,-0.1004154525,0.0899216515,-0.0076791023,0.1225812504,0.0783090855,-0.1306806194,0.0800609004,0.1032045896,-0.1182529405,-0.052714955,0.083045742,0.0826080398,0.1127822997,-0.1019328125,-0.1089164842,0.1143991752,-0.0562600474,-0.103406267,0.0133807914,0.0000828476,0.0392118033,0.1360109732,-0.0505469051,0.0458094741,-0.008771562,0.0167083307,-0.0309142048,0.0538380636,0.1143653973,-0.0841695324]},"9":{"Abstract":"Analyzing large, multivariate graphs is an important problem in many domains, yet such graphs are challenging to visualize. In this paper, we introduce a novel, scalable, tree-table multivariate graph visualization technique, which makes many tasks related to multivariate graph analysis easier to achieve. The core principle we follow is to selectively query for nodes or subgraphs of interest and visualize these subgraphs as a spanning tree of the graph. The tree is laid out linearly, which enables us to juxtapose the nodes with a table visualization where diverse attributes can be shown. We also use this table as an adjacency matrix, so that the resulting technique is a hybrid node-link\/adjacency matrix technique. We implement this concept in Juniper and complement it with a set of interaction techniques that enable analysts to dynamically grow, restructure, and aggregate the tree, as well as change the layout or show paths between nodes. We demonstrate the utility of our tool in usage scenarios for different multivariate networks: a bipartite network of scholars, papers, and citation metrics and a multitype network of story characters, places, books, etc.","Authors":"C. Nobre; M. Streit; A. Lex","DOI":"10.1109\/TVCG.2018.2865149","Keywords":"Multivariate graphs;networks;tree-based graph visualization;adjacency matrix;spanning trees;visualization","Keywords_Processed":"span tree;multivariate graph;visualization;adjacency matrix;network;tree base graph visualization","Title":"Juniper: A Tree+Table Approach to Multivariate Graph Visualization","Labels":null,"Keyword_Vector":[0.2440586334,-0.0542691347,-0.1235229817,0.0921651886,-0.1149456238,-0.0509167441,0.1591651504,0.0319061179,-0.1265388181,0.2643236954,-0.2091761289,0.0186656384,0.0839637835,-0.2329399138,-0.1531124228,-0.0526892635,0.1138597538,0.210083309,0.1255605652,-0.0772930108,0.2576880984,-0.1146708738,-0.0019702059,-0.0800068867],"Abstract_Vector":[0.2342517484,-0.1473832106,0.016662026,-0.0061369164,0.0507327437,-0.1348013625,0.2713668204,0.1831394956,0.2237908404,-0.0022400873,-0.1413166984,0.020915038,-0.1835149655,-0.0279472992,-0.0572914174,-0.010138652,0.1494474096,0.1009725348,-0.0461365404,0.0119342199,0.0081101843,-0.0266988788,-0.0589553307,0.0742164853,0.0417637399,0.0315584115,-0.1340148816,0.0130782809,-0.2351213335,0.1130684488,0.0374894407,0.0702726963,0.019972775,0.0974955395,0.0040683306,-0.0824892104,0.0364318652,0.0826832208,0.0828868208,0.0835594326,0.064647445,0.055251011,0.0601782296,-0.1063552357,-0.0587102427,-0.0485358565,0.009682867,-0.0278890942,0.1209327186,0.0688163665,-0.0323898219,0.0197633395,-0.0224896446,0.0439162032,-0.0189993933,0.031393205,0.0199935669,-0.0338770531,-0.1233755505,0.0287019302,-0.0274802234,-0.0497804475,-0.0033917223,0.0427587022,-0.0470646731,0.0227154675,-0.0038986861,0.0424390326,0.0116214435,0.0510241966,0.0188719606,0.0045215938,-0.0107775081,0.0022831111,-0.0171264179,0.0287116257]},"10":{"Abstract":"We propose a computer generated integral photography (CGIP) method that employs a lens based rendering (LBR) algorithm for super-multiview displays to achieve higher frame rates and better image quality without pixel resampling or view interpolation. The algorithm can utilize both fixed and programmable graphics pipelines to accelerate CGIP rendering and inter-perspective antialiasing. Two hardware prototypes were fabricated with two high-resolution liquid crystal displays and micro-lens arrays (MLA). Qualitative and quantitative experiments were performed to evaluate the feasibility of the proposed algorithm. To the best of our knowledge, the proposed LBR method outperforms state-of-the-art CGIP algorithms relative to rendering speed and image quality with our super-multiview hardware configurations. A demonstration experiment was also conducted to reveal the interactivity of a super-multiview display utilizing the proposed algorithm.","Authors":"G. Chen; C. Ma; Z. Fan; X. Cui; H. Liao","DOI":"10.1109\/TVCG.2017.2756634","Keywords":"Lens based rendering;integral photography;super-multiview display;GPU","Keywords_Processed":"gpu;lens base render;integral photography;super multiview display","Title":"Real-Time Lens Based Rendering Algorithm for Super-Multiview Integral Photography without Image Resampling","Labels":null,"Keyword_Vector":[0.0482735829,0.068829635,-0.0097072429,0.0471085998,0.0163267716,-0.0416125511,0.0692760549,0.0024722531,-0.0573662722,0.0369021217,-0.0765433298,0.0255501927,0.0165956177,-0.0702420059,0.0459298967,0.0139924603,-0.0480869896,0.0372057692,0.0271457105,-0.0189195914,0.0685259964,-0.0551741495,0.0421886101,-0.061492258],"Abstract_Vector":[0.1665967504,0.14587045,0.0009623043,0.0853611903,-0.0071534641,0.0529799106,-0.0181079634,0.1680649563,-0.0511726563,-0.0699736361,-0.053760644,0.0436054743,-0.0461876332,-0.0494657724,0.0243325174,-0.0109970431,-0.0490690749,0.0029575281,0.1916041437,-0.1166142631,-0.0602166308,-0.0819110486,0.0892012592,0.0818410273,0.0272253813,0.045473828,-0.169958544,0.0469470344,0.0555318205,0.1039453159,-0.116561912,0.0394784665,0.0083218199,-0.1335616905,-0.0405474878,0.124516438,-0.1461031458,-0.0814727223,-0.0776250548,-0.0795098152,0.0084907598,0.0087552498,-0.0165806136,0.0260707315,0.0197538434,0.0727720624,-0.0102277702,-0.0406103699,-0.0420469911,0.0630511479,-0.0141111909,0.0512823868,0.0416650471,-0.028424065,0.0791553947,0.0555751084,-0.0394440441,0.1129064618,-0.0313871167,-0.0720124277,-0.1059369258,-0.0096639402,-0.038440027,-0.0021713814,0.020184486,-0.0198251977,0.0103654247,0.1167767285,-0.0657961727,-0.0420986503,-0.0163624385,0.0052185176,0.0804402598,-0.0142896278,0.0604639411,0.1476754304]},"11":{"Abstract":"Social media data with geotags can be used to track people's movements in their daily lives. By providing both rich text and movement information, visual analysis on social media data can be both interesting and challenging. In contrast to traditional movement data, the sparseness and irregularity of social media data increase the difficulty of extracting movement patterns. To facilitate the understanding of people's movements, we present an interactive visual analytics system to support the exploration of sparsely sampled trajectory data from social media. We propose a heuristic model to reduce the uncertainty caused by the nature of social media data. In the proposed system, users can filter and select reliable data from each derived movement category, based on the guidance of uncertainty model and interactive selection tools. By iteratively analyzing filtered movements, users can explore the semantics of movements, including the transportation methods, frequent visiting sequences and keyword descriptions. We provide two cases to demonstrate how our system can help users to explore the movement patterns.","Authors":"S. Chen; X. Yuan; Z. Wang; C. Guo; J. Liang; Z. Wang; X. Zhang; J. Zhang","DOI":"10.1109\/TVCG.2015.2467619","Keywords":"Spatial temporal visual analytics;Geo-tagged social media;Sparsely sampling;Uncertainty;Movement;Spatial temporal visual analytics;Geo-tagged social media;Sparsely sampling;Uncertainty;Movement","Keywords_Processed":"spatial temporal visual analytic;uncertainty;movement;sparsely sample;geo tag social medium","Title":"Interactive Visual Discovering of Movement Patterns from Sparsely Sampled Geo-tagged Social Media Data","Labels":null,"Keyword_Vector":[0.1594571879,-0.0249715769,0.1394831583,0.111219781,-0.1123576598,-0.0291907795,-0.2023195707,-0.0524637499,0.0487976854,-0.2271591001,-0.0392979598,-0.1019760858,-0.1056212061,-0.0891455911,-0.0196469363,-0.0053878152,-0.1184521978,0.0973904765,0.1493966089,0.08673559,0.0592468488,-0.1968706495,0.0479190226,0.047312706],"Abstract_Vector":[0.2419311521,-0.1427749448,-0.0036065792,-0.0323749999,-0.0781592061,-0.0514740087,-0.0811335492,-0.0935296187,-0.0073868972,0.0632154254,-0.002712883,0.1901888275,-0.0626685397,0.0058846763,0.1281654153,0.1481870852,-0.0758955736,-0.0545081886,-0.0942477114,-0.133238949,-0.0026538183,0.0378094508,0.0069290906,0.0198023246,-0.0218255182,-0.1605421137,0.1367221734,0.041278004,0.0478743832,0.0904333489,0.0064446756,-0.1358489671,-0.0453437435,-0.0206023281,-0.1424523524,0.0053355532,-0.1408911955,-0.1158221493,0.044918985,-0.0885820595,0.0969131007,-0.1107409922,-0.005988925,-0.0022711942,0.0174122812,0.0805304355,0.0335514032,0.0579890567,0.0961292523,-0.0260624162,-0.1427126402,0.0485407441,0.134509039,-0.0096700545,-0.0408494857,-0.0157763386,0.0857858306,-0.1428809624,-0.17154127,0.0769039291,-0.0086568172,0.0102137799,-0.0506048848,-0.1274878237,0.0043340698,-0.1937292047,-0.102610024,-0.0022954085,0.1057347165,-0.021966544,-0.0113195952,0.018691963,-0.0490396896,0.0640603301,-0.0278808251,0.0266146586]},"12":{"Abstract":"CoDDA (Copula-based Distribution Driven Analysis) is a flexible framework for large-scale multivariate datasets. A common strategy to deal with large-scale scientific simulation data is to partition the simulation domain and create statistical data summaries. Instead of storing the high-resolution raw data from the simulation, storing the compact statistical data summaries results in reduced storage overhead and alleviated I\/O bottleneck. Such summaries, often represented in the form of statistical probability distributions, can serve various post-hoc analysis and visualization tasks. However, for multivariate simulation data using standard multivariate distributions for creating data summaries is not feasible. They are either storage inefficient or are computationally expensive to be estimated in simulation time (in situ) for large number of variables. In this work, using copula functions, we propose a flexible multivariate distribution-based data modeling and analysis framework that offers significant data reduction and can be used in an in situ environment. The framework also facilitates in storing the associated spatial information along with the multivariate distributions in an efficient representation. Using the proposed multivariate data summaries, we perform various multivariate post-hoc analyses like query-driven visualization and sampling-based visualization. We evaluate our proposed method on multiple real-world multivariate scientific datasets. To demonstrate the efficacy of our framework in an in situ environment, we apply it on a large-scale flow simulation.","Authors":"S. Hazarika; S. Dutta; H. Shen; J. Chen","DOI":"10.1109\/TVCG.2018.2864801","Keywords":"In situ processing;Distribution-based;Multivariate;Query-driven;Copula","Keywords_Processed":"in situ processing;distribution base;query drive;multivariate;copula","Title":"CoDDA: A Flexible Copula-based Distribution Driven Analysis Framework for Large-Scale Multivariate Data","Labels":null,"Keyword_Vector":[0.0615084457,-0.012816133,-0.0200703641,0.028205532,0.0530005744,-0.020828306,0.0562998523,-0.0102940376,-0.0910690166,0.0179424459,-0.026381638,0.0327992953,0.0234495662,-0.023229517,-0.0794718436,0.020018446,0.0317908216,0.1351023696,0.0837029408,-0.0359381644,0.0539528924,-0.1371973071,-0.0351458417,0.0420852553],"Abstract_Vector":[0.290616046,-0.1673504056,-0.1111101262,-0.0775272412,-0.1082939077,0.0852593689,0.1033777279,0.1030681492,-0.0248318535,-0.0332478857,-0.1702687617,0.122855427,0.0754350768,0.1926108917,-0.1623764546,-0.0465631489,-0.0022936781,-0.0128276462,-0.0398439179,0.0131243952,0.164832175,-0.0875589169,0.0848849912,0.0770469231,-0.0625266115,0.1305564865,-0.0818232858,-0.0747948888,-0.0715693546,0.1035192686,0.1142896748,-0.1113771922,0.054167991,0.0092144311,-0.0860527867,-0.0532991817,0.0954057493,0.0345178604,-0.0886722817,0.0257734998,0.0130338121,0.0098024817,-0.0108861259,-0.1235254302,-0.0180623303,0.076270728,-0.0144928509,0.0250843804,-0.0792368384,0.0287244067,0.0202258961,0.0448657298,-0.0392152426,-0.0100189426,0.0483953526,0.0386769095,0.049298583,-0.0136079024,-0.0620873394,-0.0364001918,0.0622417852,-0.0549941525,0.0340079222,-0.0482522992,-0.1098142542,0.0697613348,-0.0163677669,0.0109271251,0.0200794621,-0.1139827812,-0.0217018055,-0.0051826654,0.0630355948,0.0038858187,-0.0228221087,-0.0559209497]},"13":{"Abstract":"Reconstructed building models using stereo-based methods inevitably suffer from noise, leading to the lack of regularity which is characterized by straightness of structural linear features and smoothness of homogeneous regions. We leverage the structural linear features embedded in the mesh to construct a novel surface scaffold structure for model regularization. The regularization comprises two iterative stages: (1) the linear features are semi-automatically proposed from images by exploiting photometric and geometric clues jointly; (2) the scaffold topology represented by spatial relations among the linear features is optimized according to data fidelity and topological rules, then the mesh is refined by adjusting itself to the consolidated scaffold. Our method has two advantages. First, the proposed scaffold representation is able to concisely describe semantic building structures. Second, the scaffold structure is embedded in the mesh, which can preserve the mesh connectivity and avoid stitching or intersecting surfaces in challenging cases. We demonstrate that our method can enhance structural characteristics and suppress irregularities in the building models robustly in some challenging datasets. Moreover, the regularization can significantly improve the results of general applications such as simplification and non-photorealistic rendering.","Authors":"J. Wang; T. Fang; Q. Su; S. Zhu; J. Liu; S. Cai; C. Tai; L. Quan","DOI":"10.1109\/TVCG.2015.2461163","Keywords":"Modeling packages;Reconstruction.;Modeling packages;reconstruction","Keywords_Processed":"reconstruction;modeling package","Title":"Image-Based Building Regularization Using Structural Linear Features","Labels":null,"Keyword_Vector":[0.0344692925,0.006541163,-0.0041300997,0.0632181525,0.0137214951,-0.1037519702,0.0571689677,-0.1167359737,-0.1061678818,-0.1484754686,-0.0400071858,-0.0048849599,-0.0004838246,0.0491560908,-0.0031941072,0.0121083523,0.083656926,-0.0090643765,0.034967557,0.0605476427,0.0787711266,-0.0269827582,0.1597108262,0.0683186467],"Abstract_Vector":[0.1927434542,0.1178399135,-0.1733595415,0.0060556256,0.1214641804,0.0148590774,-0.0131763894,-0.0012272115,0.0491294379,0.0820198235,-0.1453857202,-0.0699102841,0.1044474415,-0.0847292222,0.0602832404,-0.0412257189,-0.0823034321,0.0283224811,-0.0104365286,-0.1289839916,0.0001079192,0.0812877193,-0.0283937487,0.0565613478,-0.0157187684,-0.0048138628,0.1693787972,-0.0860186243,-0.0012612432,-0.1025624195,0.0536008613,0.0337148824,-0.0435938898,-0.0077140105,0.121485457,0.0274931085,-0.0111594864,-0.002528737,0.0602519659,-0.0687708378,-0.1587284788,-0.001438151,-0.0422532511,-0.0683594996,0.0020768674,0.0269810498,-0.1526063995,-0.0686516594,0.0273991191,-0.022052702,-0.1222857785,-0.036976769,0.0255115043,0.0283830081,0.0548951598,0.0638577342,0.0002387025,0.040702184,-0.0090749352,-0.0402269571,0.0062628049,-0.0471658401,-0.0233905017,0.0247616305,-0.0865899198,-0.0672151056,0.0139254893,-0.0136880203,0.0894846087,-0.1155829425,0.0193861604,0.0159182726,-0.0078306306,-0.0273820011,-0.0145982428,0.0229920026]},"14":{"Abstract":"Multi-view deep neural network is perhaps the most successful approach in 3D shape classification. However, the fusion of multi-view features based on max or average pooling lacks a view selection mechanism, limiting its application in, e.g., multi-view active object recognition by a robot. This paper presents VERAM, a view-enhanced recurrent attention model capable of actively selecting a sequence of views for highly accurate 3D shape classification. VERAM addresses an important issue commonly found in existing attention-based models, i.e., the unbalanced training of the subnetworks corresponding to next view estimation and shape classification. The classification subnetwork is easily overfitted while the view estimation one is usually poorly trained, leading to a suboptimal classification performance. This is surmounted by three essential view-enhancement strategies: 1) enhancing the information flow of gradient backpropagation for the view estimation subnetwork, 2) devising a highly informative reward function for the reinforcement training of view estimation and 3) formulating a novel loss function that explicitly circumvents view duplication. Taking grayscale image as input and AlexNet as CNN architecture, VERAM with 9 views achieves instance-level and class-level accuracy of 95.5 and 95.3 percent on ModelNet10, 93.7 and 92.1 percent on ModelNet40, both are the state-of-the-art performance under the same number of views.","Authors":"S. Chen; L. Zheng; Y. Zhang; Z. Sun; K. Xu","DOI":"10.1109\/TVCG.2018.2866793","Keywords":"3D shape classification;multi-view 3D shape recognition;visual attention model;recurrent neural network;reinforcement learning;convolutional neural network","Keywords_Processed":"multi view 3d shape recognition;convolutional neural network;visual attention model;reinforcement learning;recurrent neural network;3d shape classification","Title":"VERAM: View-Enhanced Recurrent Attention Model for 3D Shape Classification","Labels":null,"Keyword_Vector":[0.1500480863,0.0160011256,0.1854108148,0.2171531914,-0.0985380001,-0.0724956757,0.0006087603,0.0035503689,-0.119752916,0.0599488999,-0.1247332916,0.0451999644,-0.0657404805,0.0754960393,-0.0354888361,0.1066721225,0.2833528239,-0.1772653385,-0.1644421731,0.2120716797,0.1997329434,-0.0522102066,-0.0331548215,0.1150290208],"Abstract_Vector":[0.1812278414,0.1058960284,0.006208204,0.0044260128,0.1232350435,-0.0558659148,-0.0202194169,0.0066163492,0.1019146016,-0.0212772548,0.0554346307,-0.03999137,0.0435077932,-0.0837801983,-0.0276427564,-0.1797034025,-0.0575096795,-0.1110406246,0.0105668946,0.0413693594,0.0822445005,-0.0314094478,0.0997195281,-0.0049554136,-0.0018871566,-0.0696454609,-0.1258796382,-0.0507303808,0.3238489637,-0.0656583978,-0.0638670944,-0.0190970342,0.0531862359,0.0657183918,-0.1223887716,-0.0007736535,0.0236053672,0.0009581099,0.0132926577,0.0057388562,-0.0391450861,0.1008875602,0.1398900025,-0.1264993162,0.0354623329,0.0455805667,0.0004462151,0.1294995035,0.0049152638,0.032430336,-0.028846419,0.0018439467,-0.0426759218,-0.0479350408,-0.0257248426,-0.0159942178,0.005171541,-0.0540880482,-0.1167345198,0.0376327788,0.0101448853,0.0627819984,-0.0807001249,-0.0551130677,0.1085936967,-0.0297018018,-0.0304811368,-0.0598626705,-0.0457831408,-0.0421317592,-0.0579861362,0.0484334776,0.088721599,-0.0758079919,-0.0594577036,0.0350943556]},"15":{"Abstract":"In this article, we investigate methods for suggesting the interactivity of online visualizations embedded with text. We first assess the need for such methods by conducting three initial experiments on Amazon's Mechanical Turk. We then present a design space for Suggested Interactivity (i. e., visual cues used as perceived affordances-SI), based on a survey of 382 HTML5 and visualization websites. Finally, we assess the effectiveness of three SI cues we designed for suggesting the interactivity of bar charts embedded with text. Our results show that only one cue (SI3) was successful in inciting participants to interact with the visualizations, and we hypothesize this is because this particular cue provided feedforward.","Authors":"J. Boy; L. Eveillard; F. Detienne; J. Fekete","DOI":"10.1109\/TVCG.2015.2467201","Keywords":"Suggested interactivity;perceived affordances;information visualization for the people;online visualization;Suggested interactivity;perceived affordances;information visualization for the people;online visualization","Keywords_Processed":"suggested interactivity;information visualization for the people;online visualization;perceive affordance","Title":"Suggested Interactivity: Seeking Perceived Affordances for Information Visualization","Labels":null,"Keyword_Vector":[0.201287635,-0.0652803495,-0.17698892,-0.0351354622,-0.0848531894,-0.0340505518,-0.0159708993,0.0194419443,0.0093831494,0.0479955016,0.1269136939,-0.1457602182,0.1207848761,0.0266580352,-0.0719653404,-0.0464757484,-0.0047326982,-0.0671382549,-0.060063491,-0.0312883647,-0.028803994,0.0363023718,0.0051833727,0.0413933234],"Abstract_Vector":[0.1594601111,-0.0098177432,0.0982724236,-0.0265791323,0.064327716,0.1488840189,-0.0339987678,-0.0366067575,0.006534748,0.0092953229,-0.0063283266,-0.1305484511,-0.0089426972,-0.0301683383,0.0470823353,0.0197879305,0.0617859214,0.0540466955,0.0266854212,0.0303559382,-0.086228799,0.006308155,0.0041583696,-0.0077360295,-0.0721910991,-0.1102904863,-0.0142411225,-0.0550926289,-0.0687376696,-0.0361261258,0.0272199271,0.0311598617,0.0541394992,0.0845844202,-0.0580529986,0.210910677,-0.1334769223,0.0347404715,-0.187636761,0.0772386177,0.0141080772,-0.1000284252,0.0087433715,-0.0875957245,0.0852178219,0.0120070605,0.1166968472,0.0633371324,0.0310543765,-0.1432016689,0.0661893169,-0.0177443414,-0.1199833876,0.1064420415,-0.057320109,0.0430334946,-0.0187543775,-0.0105225048,-0.0757515222,-0.1603835976,0.0916920422,0.08589959,0.076222497,0.0475578149,0.0115355276,0.0583296328,0.015265496,0.0925193328,0.1145951602,-0.0555132537,0.020371454,0.1180132279,0.0144031308,0.0610506515,0.0024515855,0.1000914991]},"16":{"Abstract":"Handheld scanning using commodity depth cameras provides a flexible and low-cost manner to get 3D models. The existing methods scan a target by densely fusing all the captured depth images, yet most frames are redundant. The jittering frames inevitably embedded in handheld scanning process will cause feature blurring on the reconstructed model and even trigger the scan failure (i.e., camera tracking losing). To address these problems, in this paper, we propose a novel sparse-sequence fusion (SSF) algorithm for handheld scanning using commodity depth cameras. It first extracts related measurements for analyzing camera motion. Then based on these measurements, we progressively construct a supporting subset for the captured depth image sequence to decrease the data redundancy and the interference from jittering frames. Since SSF will reveal the intrinsic heavy noise of the original depth images, our method introduces a refinement process to eliminate the raw noise and recover geometric features for the depth images selected into the supporting subset. We finally obtain the fused result by integrating the refined depth images into the truncated signed distance field (TSDF) of the target. Multiple comparison experiments are conducted and the results verify the feasibility and validity of SSF for handheld scanning with a commodity depth camera.","Authors":"L. Yang; Q. Yan; Y. Fu; C. Xiao","DOI":"10.1109\/TVCG.2017.2657766","Keywords":"Depth image refinement;handheld scanning;sparse-sequence fusion;surface reconstruction;supporting subset","Keywords_Processed":"support subset;depth image refinement;sparse sequence fusion;handheld scanning;surface reconstruction","Title":"Surface Reconstruction via Fusing Sparse-Sequence of Depth Images","Labels":null,"Keyword_Vector":[0.0394907919,0.0094238205,0.0082991781,0.1117383839,0.0558337609,-0.1082836032,0.1425155023,-0.1042486717,-0.0768785528,-0.0553757625,0.0176596808,-0.0185894229,0.0626013174,0.0298848461,0.1281162185,0.0261937625,-0.0820452429,0.0665377899,-0.0698931857,0.0872334801,0.0339084881,0.0976288948,-0.0470643696,0.0000948618],"Abstract_Vector":[0.1909054835,0.2064147221,-0.0620984895,-0.0012088493,0.0625106537,-0.0208380909,-0.1068260288,0.0091809322,-0.0234747793,0.0934977398,-0.1186020005,0.0074709399,0.0887376783,-0.1736887252,0.0308601738,0.0543325917,0.0537837948,-0.0995032792,0.0947173164,-0.1027382187,0.0646812843,-0.0970532853,-0.021921251,-0.0350307977,-0.1187676768,-0.0274087218,-0.0803927942,0.0495473089,-0.0221859945,0.0546039576,0.0165927094,-0.0034457675,0.0273236509,0.0196437963,-0.066475483,0.0694439726,-0.0106395622,0.0422795692,-0.0920324636,-0.132516925,-0.0449253587,-0.0155231277,-0.0242272804,-0.0034789735,0.0684792296,0.0064196314,-0.093420135,0.0237776776,0.1328029894,-0.0309943775,-0.042650228,0.016086882,0.0950517341,0.08135439,0.041629873,0.081074149,0.0364559263,0.0827734499,0.0574202933,0.0438078871,-0.0409914642,-0.0379234406,0.1584876726,-0.013109855,-0.0965867645,0.0242848557,0.0138091744,-0.0127694975,0.0208844894,0.0038618618,0.0096493506,0.138321232,0.0426311191,-0.0685593771,0.0189012094,0.0182553526]},"17":{"Abstract":"Generative models bear promising implications to learn data representations in an unsupervised fashion with deep learning. Generative Adversarial Nets (GAN) is one of the most popular frameworks in this arena. Despite the promising results from different types of GANs, in-depth understanding on the adversarial training process of the models remains a challenge to domain experts. The complexity and the potential long-time training process of the models make it hard to evaluate, interpret, and optimize them. In this work, guided by practical needs from domain experts, we design and develop a visual analytics system, GANViz, aiming to help experts understand the adversarial process of GANs in-depth. Specifically, GANViz evaluates the model performance of two subnetworks of GANs, provides evidence and interpretations of the models' performance, and empowers comparative analysis with the evidence. Through our case studies with two real-world datasets, we demonstrate that GANViz can provide useful insight into helping domain experts understand, interpret, evaluate, and potentially improve GAN models.","Authors":"J. Wang; L. Gou; H. Yang; H. Shen","DOI":"10.1109\/TVCG.2018.2816223","Keywords":"Generative adversarial nets;deep learning;model interpretation;visual analytics","Keywords_Processed":"deep learning;model interpretation;generative adversarial net;visual analytic","Title":"GANViz: A Visual Analytics Approach to Understand the Adversarial Game","Labels":null,"Keyword_Vector":[0.1338361088,-0.0278196661,0.3184870549,0.2095397574,-0.2143781581,-0.030755529,-0.1495331246,-0.018438441,-0.094380858,-0.0624796208,0.0020787531,0.0227133572,-0.003668182,0.0362067713,-0.1362729369,-0.0491006609,0.1284520489,-0.0353343375,-0.1359802985,0.0016222721,-0.0547526122,0.0842023409,-0.1238080681,-0.0855345524],"Abstract_Vector":[0.2007203602,-0.0290553485,0.0241293426,-0.0640267584,0.1898628646,-0.1307824179,-0.1827004496,-0.0623838284,0.0571060781,0.0742035723,0.0130947251,0.0422529877,0.1006628917,0.0470865258,-0.0145890982,0.0408084661,-0.044180246,0.0033780556,0.143568176,0.014853077,-0.0052846245,0.001901849,-0.0487827559,-0.0212565895,0.024815367,0.0478500831,-0.0200202429,0.0499155781,0.0362981346,-0.037183733,0.05519297,-0.0376960661,-0.0125228873,0.0334374277,-0.0554314155,-0.0708785423,-0.0433828737,-0.0930601399,-0.0642017367,0.1201863627,-0.0078922196,0.0719342399,-0.0242738758,0.0880506762,0.0116310703,-0.0817355032,-0.0328051096,0.0576564374,0.0953893445,-0.0523980847,-0.0310445552,0.0057114002,-0.0011600348,-0.1054311921,-0.0539466971,-0.1313019187,-0.02850974,-0.0362607069,0.009243745,-0.0177337277,-0.0216822046,-0.0444035009,-0.0391762251,0.0084417679,-0.1614976271,0.0363005919,-0.0413959758,0.01995439,-0.0268720018,0.023218365,-0.0465408295,0.0814961601,0.0414688001,0.0198901992,-0.0660325372,0.0625034363]},"18":{"Abstract":"Prewriting is the process of generating and organizing ideas before drafting a document. Although often overlooked by novice writers and writing tool developers, prewriting is a critical process that improves the quality of a final document. To better understand current prewriting practices, we first conducted interviews with writing learners and experts. Based on the learners' needs and experts' recommendations, we then designed and developed InkPlanner, a novel pen and touch visualization tool that allows writers to utilize visual diagramming for ideation during prewriting. InkPlanner further allows writers to sort their ideas into a logical and sequential narrative by using a novel widget- NarrativeLine. Using a NarrativeLine, InkPlanner can automatically generate a document outline to guide later drafting exercises. Inkplanner is powered by machine-generated semantic and structural suggestions that are curated from various texts. To qualitatively review the tool and understand how writers use InkPlanner for prewriting, two writing experts were interviewed and a user study was conducted with university students. The results demonstrated that InkPlanner encouraged writers to generate more diverse ideas and also enabled them to think more strategically about how to organize their ideas for later drafting.","Authors":"Z. Lu; M. Fan; Y. Wang; J. Zhao; M. Annett; D. Wigdor","DOI":"10.1109\/TVCG.2018.2864887","Keywords":"Writing;prewriting;diagraming;content and structure recommendation;pen and touch interfaces","Keywords_Processed":"diagram;content and structure recommendation;writing;pen and touch interface;prewrit","Title":"InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming","Labels":null,"Keyword_Vector":[0.0454784699,0.004643499,-0.0021291669,-0.0020589647,0.0045597656,-0.068494171,0.0264591927,0.0758244974,-0.0034456871,0.0357410205,0.0660585417,-0.0051350134,0.1060104189,0.0529440525,0.0443692354,-0.0731555088,0.0516363332,0.0336122348,0.0069324153,-0.0586003,-0.0258896536,-0.1192106395,0.0275230603,0.0211785368],"Abstract_Vector":[0.116543493,-0.0391094549,0.0505705056,-0.0263423022,0.066538493,-0.0298772896,-0.0358270218,-0.020611421,-0.0610571053,-0.0037684824,0.0495989995,-0.043230897,-0.0134299056,-0.0284062059,-0.0028825823,0.0753301316,0.0369390373,0.029434178,0.0064707207,-0.0499929465,0.0272825682,0.0156717879,0.05687465,0.0333872389,0.0466968332,0.0168370748,-0.0105177456,0.0103674903,0.0256896677,-0.0613445766,-0.0246261466,0.0536954649,0.0308199788,0.0307876452,0.0742692542,0.0281856943,0.0721833986,-0.0418931564,-0.0849844508,-0.0571821636,0.0400350589,0.038991935,-0.0603644182,0.0420264301,-0.0873461212,-0.054414612,0.0097962278,0.0512139149,0.0653961125,-0.0019894858,-0.0427981206,0.0013636089,-0.0997487811,-0.1250551897,-0.0329516339,-0.0359301992,-0.1019814618,0.0385747169,0.0295316133,-0.1698855321,-0.0522661801,-0.0490827038,0.0713727935,-0.1245949453,-0.1472338238,-0.0817473034,0.0125458192,0.0990189375,-0.0354234245,-0.0277208667,0.1518292162,-0.0884377029,0.076725566,-0.0715465431,-0.0230117812,0.0484969698]},"19":{"Abstract":"Visualizations are nowadays appearing in popular media and are used everyday in the workplace. This democratisation of visualization challenges educators to develop effective learning strategies, in order to train the next generation of creative visualization specialists. There is high demand for skilled individuals who can analyse a problem, consider alternative designs, develop new visualizations, and be creative and innovative. Our three-stage framework, leads the learner through a series of tasks, each designed to develop different skills necessary for coming up with creative, innovative, effective, and purposeful visualizations. For that, we get the learners to create an explanatory visualization of an algorithm of their choice. By making an algorithm choice, and by following an active-learning and project-based strategy, the learners take ownership of a particular visualization challenge. They become enthusiastic to develop good results and learn different creative skills on their learning journey.","Authors":"J. C. Roberts; P. D. Ritsos; J. R. Jackson; C. Headleand","DOI":"10.1109\/TVCG.2017.2745878","Keywords":"Explanatory visualization;Information Visualization;Teaching visualization;Learning Support","Keywords_Processed":"teach visualization;explanatory visualization;information visualization;learn support","Title":"The Explanatory Visualization Framework: An Active Learning Framework for Teaching Creative Computing Using Explanatory Visualizations","Labels":null,"Keyword_Vector":[0.316559539,-0.1004592686,-0.2723743528,-0.0289494021,-0.133024702,-0.0446931299,-0.0258649997,0.0103304898,0.0145583892,0.0787246165,0.1024186989,-0.1762500675,0.0961644829,0.0634792434,-0.0292581002,-0.0592346208,-0.0357444281,-0.0685594165,-0.03296282,-0.0159100914,0.0017021974,0.0887306828,-0.0003615659,0.0293305889],"Abstract_Vector":[0.1809690124,-0.0622229727,0.0867197313,-0.0513349902,0.1080847243,0.0908145467,-0.0453683849,-0.0303018942,-0.0736981158,-0.0438733432,0.0007938876,-0.0751600165,-0.0060555063,-0.0356833775,0.0205915918,-0.016168693,0.0969675037,0.0500669898,0.0251416402,0.029758839,0.0011514581,0.0353045603,-0.0005564239,-0.0016275602,-0.0325503427,0.0854672653,-0.0060118796,-0.0449973748,0.0345600467,0.0299078873,-0.0518588785,0.0141563258,0.0555285851,-0.0585010872,-0.0408242586,0.0457325255,-0.0022733746,-0.1186947083,0.0763630978,0.0674768076,0.1313345833,0.0655559666,-0.004512801,0.0856738223,-0.0698861095,0.1017510345,0.0427706017,-0.0217186961,0.1192587588,-0.151220327,0.0384040581,0.0227018398,-0.1119846647,0.0505892035,0.0308707226,-0.0365050804,0.126543838,0.0337902378,0.0198344485,0.0124365238,-0.1108274129,0.0858132764,0.0448396046,-0.0195803402,-0.1975791997,-0.0216943122,-0.1577783548,-0.0820697892,-0.1562586525,-0.1059726376,-0.0518092385,-0.0800097371,-0.024479266,-0.0545394522,0.135529554,0.2034775794]},"20":{"Abstract":"Paper documents such as passports, visas and banknotes are frequently checked by inspection of security elements. In particular, optically variable devices such as holograms are important, but difficult to inspect. Augmented Reality can provide all relevant information on standard mobile devices. However, hologram verification on mobiles still takes long and provides lower accuracy than inspection by human individuals using appropriate reference information. We aim to address these drawbacks by automatic matching combined with a special parametrization of an efficient goal-oriented user interface which supports constrained navigation. We first evaluate a series of similarity measures for matching hologram patches to provide a sound basis for automatic decisions. Then a re-parametrized user interface is proposed based on observations of typical user behavior during document capture. These measures help to reduce capture time to approximately 15 s with better decisions regarding the evaluated samples than what can be achieved by untrained users.","Authors":"A. D. Hartl; C. Arth; J. Grubert; D. Schmalstieg","DOI":"10.1109\/TVCG.2015.2498612","Keywords":"Document inspection;holograms;augmented reality;user interfaces;mobile devices;Document inspection;holograms;augmented reality;user interfaces;mobile devices","Keywords_Processed":"document inspection;user interface;augment reality;hologram;mobile device","Title":"Efficient Verification of Holograms Using Mobile Augmented Reality","Labels":null,"Keyword_Vector":[0.1376339706,0.2426590966,0.0176057289,-0.0307956006,0.0044689897,-0.0732636052,-0.0209647662,0.1483320261,0.0246424229,-0.0247652042,0.0728532054,0.0202505448,0.1512557699,0.0584957954,0.1228583863,-0.0719396237,0.07778542,0.1636624413,0.0030251173,0.0230528527,-0.1149080965,-0.1911435228,-0.0558137649,-0.0057751155],"Abstract_Vector":[0.1976306419,-0.0206222227,0.0595130809,-0.0261286652,-0.01679385,-0.0845502092,-0.0265373469,-0.0018358495,0.0387667895,-0.0911666487,0.1254014273,-0.0207590309,0.0500237885,0.0751855704,-0.0644282311,0.0225411737,-0.130291703,-0.0504210528,-0.1099750969,-0.0267381723,0.0068243349,-0.0072729463,0.1150624373,-0.0662346247,0.0841477248,-0.0327855432,-0.0428161237,0.0997454115,-0.0410151173,0.0791592166,-0.0020139172,0.0835905636,-0.0231629635,-0.1267961297,0.0164312019,0.1844730526,-0.0071224125,0.0524948079,0.0216816167,-0.1453627531,0.0794206575,0.1199062283,0.0012574248,-0.0022012306,-0.1869994267,0.0046840572,-0.0231623379,0.1190785315,0.0654451284,0.0678222713,-0.008195283,-0.0376099665,-0.0373638393,-0.0228740817,-0.0169763323,-0.1386306066,-0.0329474596,-0.1547345941,0.1221840596,-0.0070810164,0.0682993134,0.0783034093,0.1873207048,0.0162132822,0.0045554885,0.1119065897,-0.08145145,-0.0357061597,-0.1362906436,0.0847543352,0.0433396297,-0.0165002862,0.028586409,-0.0034760101,0.0136452588,0.0240946247]},"21":{"Abstract":"We introduce a set of integrated interaction techniques to interpret and interrogate dimensionality-reduced data. Projection techniques generally aim to make a high-dimensional information space visible in form of a planar layout. However, the meaning of the resulting data projections can be hard to grasp. It is seldom clear why elements are placed far apart or close together and the inevitable approximation errors of any projection technique are not exposed to the viewer. Previous research on dimensionality reduction focuses on the efficient generation of data projections, interactive customisation of the model, and comparison of different projection techniques. There has been only little research on how the visualization resulting from data projection is interacted with. We contribute the concept of probing as an integrated approach to interpreting the meaning and quality of visualizations and propose a set of interactive methods to examine dimensionality-reduced data as well as the projection itself. The methods let viewers see approximation errors, question the positioning of elements, compare them to each other, and visualize the influence of data dimensions on the projection space. We created a web-based system implementing these methods, and report on findings from an evaluation with data analysts using the prototype to examine multidimensional datasets.","Authors":"J. Stahnke; M. D\u00f6rk; B. M\u00fcller; A. Thom","DOI":"10.1109\/TVCG.2015.2467717","Keywords":"Information visualization;interactivity;dimensionality reduction;multidimensional scaling;Information visualization;interactivity;dimensionality reduction;multidimensional scaling","Keywords_Processed":"interactivity;multidimensional scaling;information visualization;dimensionality reduction","Title":"Probing Projections: Interaction Techniques for Interpreting Arrangements and Errors of Dimensionality Reductions","Labels":null,"Keyword_Vector":[0.1949346357,-0.0951775436,-0.1141500005,-0.1076081705,-0.0049944273,-0.039304765,-0.0065157472,0.0613918984,-0.0338521436,0.1212319539,0.1927262974,-0.1928653741,0.1845213931,0.0591573508,-0.1833768868,0.0458834935,0.0829494688,-0.0562706705,-0.0679433309,0.0283581061,-0.0534218693,-0.0013481519,0.013880598,0.0479810043],"Abstract_Vector":[0.2729078986,-0.0776650326,-0.0346697192,-0.1222756751,-0.1299645149,0.137341256,-0.1007651733,0.0988662884,-0.0351242289,0.0283122622,-0.033313771,0.1400752052,-0.0400815928,-0.0044903997,-0.0061144763,-0.0871456148,-0.0832766171,0.1867839772,0.030943317,-0.0302178657,-0.0422333908,-0.0752161931,-0.1682093177,-0.0951214845,-0.131334981,-0.0799876603,0.1154218942,0.0525704444,-0.0005880007,-0.0048319266,0.0079312696,0.1845863192,-0.0398948502,0.046818175,-0.0333648766,0.038236836,0.1238686287,0.0014139564,0.137971511,0.0282953404,0.084169946,0.086026444,-0.0423885356,-0.0165892521,0.1263301947,0.0679412167,0.0484601547,-0.0049965817,0.0254694872,0.0806181411,-0.0153906146,0.0761125203,-0.0578528216,-0.0085223563,0.010765294,-0.069280314,-0.1015309844,-0.0231981113,0.0416726207,-0.0040799572,-0.0896352517,-0.0499053749,-0.0130655537,-0.036138571,0.0932943428,-0.0117011454,0.040891567,0.0531745745,-0.0597204549,0.0382562115,-0.0613942976,-0.0190312565,-0.0179519428,-0.0598522669,-0.1084070384,0.0601094903]},"22":{"Abstract":"Interactive ranking techniques have substantially promoted analysts' ability in making judicious and informed decisions effectively based on multiple criteria. However, the existing techniques cannot satisfactorily support the analysis tasks involved in ranking large-scale spatial alternatives, such as selecting optimal locations for chain stores, where the complex spatial contexts involved are essential to the decision-making process. Limitations observed in the prior attempts of integrating rankings with spatial contexts motivate us to develop a context-integrated visual ranking technique. Based on a set of generic design requirements we summarized by collaborating with domain experts, we propose SRVis, a novel spatial ranking visualization technique that supports efficient spatial multi-criteria decision-making processes by addressing three major challenges in the aforementioned context integration, namely, a) the presentation of spatial rankings and contexts, b) the scalability of rankings' visual representations, and c) the analysis of context-integrated spatial rankings. Specifically, we encode massive rankings and their cause with scalable matrix-based visualizations and stacked bar charts based on a novel two-phase optimization framework that minimizes the information loss, and the flexible spatial filtering and intuitive comparative analysis are adopted to enable the in-depth evaluation of the rankings and assist users in selecting the best spatial alternative. The effectiveness of the proposed technique has been evaluated and demonstrated with an empirical study of optimization methods, two case studies, and expert interviews.","Authors":"D. Weng; R. Chen; Z. Deng; F. Wu; J. Chen; Y. Wu","DOI":"10.1109\/TVCG.2018.2865126","Keywords":"Spatial ranking;visualization","Keywords_Processed":"visualization;spatial ranking","Title":"SRVis: Towards Better Spatial Integration in Ranking Visualization","Labels":null,"Keyword_Vector":[0.2495716815,-0.014645108,-0.1194135589,0.0620211948,-0.0687966156,-0.0609487759,-0.0881914224,0.1444106019,0.0937006794,-0.1763043883,0.0748576262,-0.2569867795,-0.0852083443,-0.0782904127,0.1149147698,-0.077837275,-0.1446847428,0.0067863813,0.0969881821,0.0556814832,0.2009699556,-0.0442158338,-0.0752534411,-0.0680864453],"Abstract_Vector":[0.2497828464,-0.1063660633,0.0002643035,0.001202517,0.0417528862,-0.0265220787,-0.0151167517,-0.0455388084,0.0214546977,-0.0152102774,-0.0319533376,0.0759748185,0.0232861579,0.0379749923,-0.0935637921,0.0408346857,0.0536857022,0.0012555881,0.0695487574,0.0222504688,-0.0278842066,-0.0098571252,-0.0333022011,0.0725959353,0.0338104473,-0.1486960831,-0.0726285606,0.1424268297,0.0174291264,-0.0698218793,-0.026547779,0.1650120693,0.0760966683,-0.079984422,0.0943850261,0.0426752684,0.0260337536,0.0307959011,-0.1547073472,0.066032766,-0.0125102336,0.0880576637,-0.0147795132,-0.0244243087,-0.1224484191,-0.0159947924,-0.1091150676,-0.086563646,0.0418314288,-0.1477640975,0.0335315211,0.1091778622,0.0035137042,0.0017588673,0.2610026495,-0.0400838616,0.0681625911,-0.0054424722,0.0596409584,-0.0731305687,0.1595360574,0.144333905,-0.0833730474,0.0353663369,-0.0377244361,-0.1492565182,0.105498255,-0.0779543867,-0.0459803274,-0.1046743643,-0.0027151575,-0.008264647,-0.1494809062,-0.0175579083,-0.071485466,-0.2016470856]},"23":{"Abstract":"This paper introduces dynamic composite physicalizations, a new class of physical visualizations that use collections of self-propelled objects to represent data. Dynamic composite physicalizations can be used both to give physical form to well-known interactive visualization techniques, and to explore new visualizations and interaction paradigms. We first propose a design space characterizing composite physicalizations based on previous work in the fields of Information Visualization and Human Computer Interaction. We illustrate dynamic composite physicalizations in two scenarios demonstrating potential benefits for collaboration and decision making, as well as new opportunities for physical interaction. We then describe our implementation using wheeled micro-robots capable of locating themselves and sensing user input, before discussing limitations and opportunities for future work.","Authors":"M. Le Goc; C. Perin; S. Follmer; J. Fekete; P. Dragicevic","DOI":"10.1109\/TVCG.2018.2865159","Keywords":"information visualization;data physicalization;tangible user interfaces","Keywords_Processed":"data physicalization;information visualization;tangible user interface","Title":"Dynamic Composite Data Physicalization Using Wheeled Micro-Robots","Labels":null,"Keyword_Vector":[0.2439418589,-0.0223432696,-0.1022756504,-0.0201679224,-0.0447681131,-0.0925017146,0.0082892904,0.1864850553,0.0905985068,0.0078094763,0.2819939652,-0.1326611607,0.2483417011,0.1324366094,0.0870734772,0.0753174467,0.1421007815,0.1822414563,-0.0469630287,-0.0480800145,-0.047289519,-0.0666661109,0.038883314,0.0873831892],"Abstract_Vector":[0.2271576967,-0.0027879803,0.1180445095,-0.0746046742,0.0453820278,0.0796213734,0.031837409,-0.0584714967,0.013103838,-0.0126970445,0.0095164357,-0.0793870078,-0.0416334776,0.135622011,0.0011213863,-0.0147106602,0.1106562863,0.0278501111,-0.2069864013,0.0189492701,0.0020014021,-0.0086366049,-0.0664468708,0.0384601729,-0.0984118958,-0.0914661118,0.0525750087,0.0737047182,0.0351897746,0.0992054456,0.0070943298,0.0007169825,0.0739179037,0.1158087247,0.0581049483,0.1127905085,-0.0151318937,-0.0250089545,0.0313270101,0.0444511674,-0.1101510081,0.0747594338,0.0399918005,0.0475812636,0.0179506226,-0.0718776096,0.0292958457,-0.0805139787,-0.216402983,0.0123684292,-0.0511212466,0.0853755946,0.1526321493,-0.1239340755,0.0153291083,-0.0497361338,0.0662257017,0.1153226865,0.0140660623,-0.0666476892,-0.0806648694,0.0151205935,0.0982607637,-0.108349221,0.0031009449,0.0033941,0.0018175175,-0.07714441,-0.0314734956,0.0900475362,0.0037072048,0.1069318501,0.0114681151,0.0874667471,0.0000146241,0.0141673931]},"24":{"Abstract":"We propose a new shape analysis approach based on the non-local analysis of local shape variations. Our method relies on a novel description of shape variations, called Local Probing Field (LPF), which describes how a local probing operator transforms a pattern onto the shape. By carefully optimizing the position and orientation of each descriptor, we are able to capture shape similarities and gather them into a geometrically relevant dictionary over which the shape decomposes sparsely. This new representation permits to handle shapes with mixed intrinsic dimensionality (e.g., shapes containing both surfaces and curves) and to encode various shape features such as boundaries. Our shape representation has several potential applications; here we demonstrate its efficiency for shape resampling and point set denoising for both synthetic and real data.","Authors":"J. Digne; S. Valette; R. Chaine","DOI":"10.1109\/TVCG.2017.2719024","Keywords":"Shape similarity;local shape descriptor;point set denoising and resampling","Keywords_Processed":"local shape descriptor;shape similarity;point set denoising and resampling","Title":"Sparse Geometric Representation Through Local Shape Probing","Labels":null,"Keyword_Vector":[0.0174219776,-0.0000435225,0.0204391026,0.0545227101,0.0010648011,-0.0660068686,0.0553552196,-0.0353869711,-0.0343421193,0.0244967228,-0.0359952578,0.002556526,-0.0962382004,0.0691825998,-0.0171389863,0.1103967043,0.1036702416,-0.1123794641,-0.1019884959,0.1238650277,0.0851762145,-0.1361875423,0.109766955,0.2682440433],"Abstract_Vector":[0.1870566087,0.1079078218,-0.2164654932,-0.0408687447,-0.0653228031,0.0398124229,-0.0943404277,-0.1339004914,0.0555434709,0.0424493887,0.0642591731,-0.1113982833,-0.214513038,-0.0372998807,0.0009892725,-0.2909258294,-0.2227300948,-0.1790707607,-0.1864517004,-0.0316959378,-0.0206959369,-0.0189758522,0.1510935225,0.1175987273,-0.0231613446,0.0010481747,-0.1018727073,0.1150509933,0.0203575645,-0.016507684,0.059423213,-0.0082478257,-0.0166629431,0.0761204388,0.0477892667,-0.1010087507,0.0329817367,-0.0387410068,0.0380159071,0.0545550103,-0.0053028398,0.0944916329,0.1033007802,0.0478129657,0.0071946971,0.0060492429,0.1068141675,-0.0414463088,0.02333463,-0.1295421941,-0.0054566531,-0.0287922358,0.0433883795,-0.022268485,0.0187993835,-0.0871827533,0.0339380568,0.0845396487,-0.0881620545,0.0085963429,-0.0157461559,0.0424425179,-0.0687776639,-0.0414313207,-0.0031692618,0.0044696054,0.1001432502,-0.0239853961,-0.0227445812,0.0047837606,-0.0067023579,-0.0508632216,-0.0833071312,-0.0447084667,-0.0065269081,0.0467015256]},"25":{"Abstract":"This paper presents a novel immersive system called MR360 that provides interactive mixed reality (MR) experiences using a conventional low dynamic range (LDR) 360\u00b0 panoramic video (360-video) shown in head mounted displays (HMDs). MR360 seamlessly composites 3D virtual objects into a live 360-video using the input panoramic video as the lighting source to illuminate the virtual objects. Image based lighting (IBL) is perceptually optimized to provide fast and believable results using the LDR 360-video as the lighting source. Regions of most salient lights in the input panoramic video are detected to optimize the number of lights used to cast perceptible shadows. Then, the areas of the detected lights adjust the penumbra of the shadow to provide realistic soft shadows. Finally, our real-time differential rendering synthesizes illumination of the virtual 3D objects into the 360-video. MR360 provides the illusion of interacting with objects in a video, which are actually 3D virtual objects seamlessly composited into the background of the 360-video. MR360 was implemented in a commercial game engine and tested using various 360-videos. Since our MR360 pipeline does not require any pre-computation, it can synthesize an interactive MR scene using a live 360-video stream while providing realistic high performance rendering suitable for HMDs.","Authors":"T. Rhee; L. Petikam; B. Allen; A. Chalmers","DOI":"10.1109\/TVCG.2017.2657178","Keywords":"Mixed reality rendering;image based lighting;image based shadowing;360\u00b0 panoramic video","Keywords_Processed":"image base lighting;mixed reality render;image base shadowing;360 panoramic video","Title":"MR360: Mixed Reality Rendering for 360\u00b0 Panoramic Videos","Labels":null,"Keyword_Vector":[0.1082368648,0.1575126497,0.0081733957,0.1249004742,0.1025561945,-0.1599815487,0.2061693352,-0.1435093167,-0.0764781609,0.0090166365,-0.067315341,0.0025873416,0.1435197862,-0.0355196008,-0.0337340835,0.0077096921,-0.1779726598,0.2920713653,-0.1322461187,-0.0257314238,0.0992524894,-0.0308302866,-0.0249913801,0.0353769489],"Abstract_Vector":[0.1560423392,0.2465922331,0.134471969,0.0547439747,-0.0592707016,-0.0322528804,0.0084824412,0.0218848412,-0.156564051,0.1055353512,0.0341312362,0.1468737946,-0.0063806449,-0.0621814855,-0.1048154061,-0.0422251857,0.0791774198,-0.0275727252,-0.1648799465,0.2170736508,0.0529739324,0.0388430385,0.0179171531,0.0349589715,0.1378794352,0.0483090206,0.0184225127,-0.0281458768,0.0166139009,-0.0100877656,-0.0735528165,-0.0357447028,0.0075684383,-0.0173701401,0.030081198,-0.046997882,-0.0292533369,-0.0187575996,-0.0142085703,0.0560694565,-0.123609226,-0.0132064771,0.001144097,0.0693085834,-0.0380960914,-0.0324920148,0.1099557717,-0.0910956595,0.1390372218,0.0389981871,0.0243524269,0.0741722097,0.0059305449,0.0101051899,0.0077285173,-0.0112271328,-0.1268966781,-0.1524075199,-0.0978871275,-0.0880912888,0.0500637608,-0.0749419437,-0.0260928804,0.1149148003,-0.0359894998,0.0127838818,0.0071852596,-0.0134215672,0.1058770813,0.086931492,-0.0849316118,0.084901776,0.0329220887,0.1353120333,0.01136084,0.0341791515]},"26":{"Abstract":"Today molecular simulations produce complex data sets capturing the interactions of molecules in detail. Due to the complexity of this time-varying data, advanced visualization techniques are required to support its visual analysis. Current molecular visualization techniques utilize ambient occlusion as a global illumination approximation to improve spatial comprehension. Besides these shadow-like effects, interreflections are also known to improve the spatial comprehension of complex geometric structures. Unfortunately, the inherent computational complexity of interreflections would forbid interactive exploration, which is mandatory in many scenarios dealing with static and time-varying data. In this paper, we introduce a novel analytic approach for capturing interreflections of molecular structures in real-time. By exploiting the knowledge of the underlying space filling representations, we are able to reduce the required parameters and can thus apply symbolic regression to obtain an analytic expression for interreflections. We show how to obtain the data required for the symbolic regression analysis, and how to exploit our analytic solution to enhance interactive molecular visualizations.","Authors":"R. Sk\u00e5nberg; P. V\u00e1zquez; V. Guallar; T. Ropinski","DOI":"10.1109\/TVCG.2015.2467293","Keywords":"Molecular visualization;diffuse interreflections;ambient occlusion;Molecular visualization;diffuse interreflections;ambient occlusion","Keywords_Processed":"molecular visualization;ambient occlusion;diffuse interreflection","Title":"Real-Time Molecular Visualization Supporting Diffuse Interreflections and Ambient Occlusion","Labels":null,"Keyword_Vector":[0.1106567713,-0.0348808965,-0.0999802849,0.005012332,-0.0486471507,-0.0235065846,-0.0148377163,-0.0038223916,-0.0001375959,0.0338480977,-0.0159908213,-0.0939060306,-0.0273625453,0.0188900362,0.0355577925,-0.0541676252,-0.000568786,-0.0333883705,0.0834725479,0.0513468044,-0.0332953046,0.1125688628,-0.0398192661,0.0157734686],"Abstract_Vector":[0.2397268651,-0.0405284642,-0.0398480377,-0.0011933993,-0.0273724037,-0.0084183864,0.0172793111,0.0244705229,-0.0995079874,-0.0435185529,-0.1425954237,0.0461207095,-0.0241534665,-0.0100074477,-0.0167509234,0.0126993597,-0.0629260267,-0.0091103771,-0.0499779828,-0.0126591307,-0.0511940474,0.0934042945,-0.0222398165,0.0105009288,-0.0382418648,0.046851078,0.0279939728,0.0552903351,0.0034635545,-0.0103795332,0.0490721345,0.0241449806,-0.0402622054,-0.0184789668,0.0104754631,0.0616536008,0.0248562747,0.106763779,-0.0796849939,0.1172751196,-0.0300355109,-0.0018857559,0.0666487407,0.0955592497,-0.0230017003,-0.0193208751,0.0151312061,0.0403518167,0.1226741317,0.0009612173,-0.0281547083,-0.0827332976,-0.0333962819,0.0705982312,0.1471389415,0.0596888397,-0.101956065,0.0523321333,0.1735693804,-0.0181805541,-0.0040830722,-0.0922978095,-0.1690861801,-0.0983889835,-0.0275234812,0.0426631624,-0.0598424595,-0.0919571195,0.0566593664,0.0593182355,-0.0587995973,-0.0335419427,0.0313987305,0.1224489445,-0.1117750664,-0.1075893294]},"27":{"Abstract":"Animated transitions can be effective in explaining and exploring a small number of visualizations where there are drastic changes in the scene over a short interval of time. This is especially true if data elements cannot be visually distinguished by other means. Current research in animated transitions has mainly focused on linear transitions (all elements follow straight line paths) or enhancing coordinated motion through bundling of linear trajectories. In this paper, we introduce animated transition design, a technique to build smooth, non-linear transitions for clustered data with either minimal or no user involvement. The technique is flexible and simple to implement, and has the additional advantage that it explicitly enhances coordinated motion and can avoid crowding, which are both important factors to support object tracking in a scene. We investigate its usability, provide preliminary evidence for the effectiveness of this technique through metric evaluations and user study and discuss limitations and future directions.","Authors":"Y. Wang; D. Archambault; C. E. Scheidegger; H. Qu","DOI":"10.1109\/TVCG.2017.2750689","Keywords":"Information visualization;animated transitions;vector field design","Keywords_Processed":"animate transition;information visualization;vector field design","Title":"A Vector Field Design Approach to Animated Transitions","Labels":null,"Keyword_Vector":[0.2254103985,-0.0676625479,-0.1607562228,-0.0319713315,-0.1561795763,0.0682400064,0.2390765857,0.0545784348,0.0011482919,-0.0796322889,0.144756338,-0.0477960427,0.0699477664,0.0242160209,-0.1965995072,0.0607299659,-0.0700352132,-0.185520622,-0.0831425087,-0.0828100917,-0.1239815774,-0.1212314171,-0.0642904541,0.1892545946],"Abstract_Vector":[0.2331732393,0.0113958162,0.0540537573,0.0014501595,-0.1431954982,-0.035092771,0.0121072852,-0.033715764,0.0043859581,-0.0035527416,-0.0315577234,-0.0727008122,0.0473449838,-0.1430409241,-0.0864209978,-0.008279791,-0.0124063741,0.0206703814,-0.0516290728,-0.0010850106,-0.206594415,-0.0514519339,-0.0947646695,-0.1423783215,0.0425906802,-0.0385456211,0.1514187572,0.1083269813,-0.0436618885,-0.0585200048,0.0536611239,0.0432526594,-0.0805706119,0.0184264655,-0.0770578168,-0.0714220195,0.0025624591,0.0416263277,0.0212555895,0.1082019411,-0.0991142695,-0.0749528599,-0.0420041504,-0.1337287542,-0.1089837636,0.1395611289,-0.0780926399,-0.0917773467,-0.0930542067,0.1099186347,-0.0654831117,-0.0538075018,0.0579100099,0.0937185906,-0.188748827,0.0837616832,-0.0849044558,-0.0251487651,0.0623263888,0.0252079147,0.0296381251,0.121934624,-0.1262780556,-0.0633005744,-0.091032109,0.0039413716,0.0117946748,-0.076714163,-0.056736756,-0.0065290817,-0.0400519566,0.0558266694,-0.098321597,-0.1059592381,0.132475735,0.0419381853]},"28":{"Abstract":"Before-and-after image pairs show how entities in a given region have evolved over a specific period of time. Satellite images are a major source of such data, that capture how natural phenomena or human activity impact a geographical area. These images are used both for data analysis and to illustrate the resulting findings to diverse audiences. The simple techniques used to display them, including juxtaposing, swapping and monolithic blending, often fail to convey the underlying phenomenon in a meaningful manner. We introduce Baia, a framework to create advanced animated transitions, called animation plans, between before-and-after images. Baia relies on a pixel-based transition model that gives authors much expressive power, while keeping animations for common types of changes easy to create thanks to predefined animation primitives. We describe our model, the associated animation editor, and report on two user studies. In the first study, advanced transitions enabled by Baia were compared to monolithic blending, and perceived as more realistic and better at focusing viewer's attention on a region of interest than the latter. The second study aimed at gathering feedback about the usability of Baia's animation editor.","Authors":"M. Lobo; C. Appert; E. Pietriga","DOI":"10.1109\/TVCG.2018.2796557","Keywords":"Animation;blending;staging;remote sensing images","Keywords_Processed":"blend;remote sensing image;animation;stag","Title":"Animation Plans for Before-and-After Satellite Images","Labels":null,"Keyword_Vector":[0.0226459702,0.0084508935,-0.0013582329,0.1193716498,0.1044238502,-0.0822761305,0.148419418,-0.1486618712,-0.0863736031,-0.039445259,-0.0082512699,-0.0766526209,0.108250132,-0.066078419,0.0772910237,0.0217804488,-0.0790427055,0.0537489831,-0.1045041221,-0.0065540567,-0.0413681,0.0987848898,0.0298387716,-0.0100026936],"Abstract_Vector":[0.2123005091,0.0734292619,-0.0111247212,-0.0414583904,0.0131105231,0.0010308853,-0.0349393375,0.0068473868,-0.0133232038,-0.0665772029,0.009648535,0.0016400079,-0.1099718477,-0.032987803,-0.1272194148,0.0279008602,0.0875047529,-0.0534545264,0.0266465761,-0.0623093298,-0.1634196552,-0.0795770869,0.0304270047,0.0169490405,-0.0027477962,0.0331539079,0.1348220766,0.001890255,-0.1064284273,-0.0240283742,-0.1390116093,-0.0723424019,-0.158077614,-0.0866352937,-0.2061715651,-0.1171242908,0.0670384706,0.0851249546,-0.1148822383,-0.0033927565,-0.107990324,-0.0869695748,-0.0383824508,-0.1371493234,-0.0676608823,0.0007683283,-0.1410804267,-0.1415647758,-0.1131085597,-0.0252805492,-0.0172685141,-0.0349736535,-0.0201073015,-0.0063955737,-0.0272042848,-0.108221897,-0.0118829024,0.008205072,0.0601999712,-0.0302253647,-0.1090695575,-0.0297443547,-0.0061541013,-0.1116887619,-0.0195497718,0.0268284682,0.0250932941,-0.0576262009,0.053928888,0.0187250866,0.0287367951,-0.0121079215,-0.0526025508,-0.1419963131,0.0237980181,0.0425196008]},"29":{"Abstract":"While nothing can be more vivid, immediate and real than our own sensorial experiences, emerging virtual reality technologies are playing with the possibility of being able to share someone else's sensory reality. The Painter Project is a virtual environment where users see a video from a painter's point of view in tandem with a tracked rendering of their own hand while they paint on a physical canvas. The end result is an experiment in superimposition of one experiential reality on top of another, hopefully opening a new window into an artist's creative process. This explorative study tested this virtual environment on stimulating empathy and creativity. The findings indicate potential for this technology as a new expert-novice mentorship simulation.","Authors":"L. J. Gerry","DOI":"10.1109\/TVCG.2017.2657239","Keywords":"Embodied simulations;virtual environments;mixed reality;creativity;empathy;painting","Keywords_Processed":"embody simulation;creativity;empathy;virtual environment;mixed reality;paint","Title":"Paint with Me: Stimulating Creativity and Empathy While Painting with a Painter in Virtual Reality","Labels":null,"Keyword_Vector":[0.1252771584,0.3209049261,0.0007873204,-0.0459900857,0.061926804,-0.0626501693,-0.0115090002,0.0378642551,0.0382522783,-0.0381611841,-0.0284758813,0.091689179,0.0419405878,0.0726073692,-0.1539428319,-0.1114428752,0.0033895219,0.0707553291,0.0786605956,0.0442211259,-0.1017879031,0.1294826857,-0.0145835493,0.108161502],"Abstract_Vector":[0.1288337679,0.1229483005,0.2540335102,0.0181137876,-0.0968684022,-0.0474128152,0.0028160107,-0.0421884964,-0.0110530423,0.0429270053,0.0482579794,0.0000969002,-0.0077543676,0.162439457,0.0043974919,-0.0414633856,-0.0236052566,0.0047100108,-0.0067335645,0.0571863179,0.1055471005,0.0436410391,0.0436826709,0.0390134732,0.0104625059,0.1085675957,0.0397671999,-0.0719734142,0.1067511741,0.0405541392,0.0234169326,0.0136321055,0.0420891311,-0.0047030438,0.0067385831,-0.0084335398,0.0670311529,-0.1215087075,-0.0271996905,0.0122560766,-0.1110028921,0.0212797088,0.036439364,0.0398077991,0.1000365413,-0.0027622826,-0.1005350636,-0.1418769058,0.157052661,0.0873253462,-0.029282558,0.0190355118,-0.0435190699,0.0141057395,0.0117951344,-0.048469501,0.0928137667,0.0377559225,-0.0099440208,-0.1278428433,0.0198111077,0.0663055275,0.0199626396,0.0024892842,-0.1924549157,-0.0625084516,0.0518695945,-0.0184964002,0.0977461172,-0.0137996587,-0.0132329665,-0.1021925866,-0.0212400505,-0.0704492296,-0.0837567823,0.135433779]},"30":{"Abstract":"Redirected walking (RDW) promises to allow near-natural walking in an infinitely large virtual environment (VE) by subtle manipulations of the virtual camera. Previous experiments analyzed the human sensitivity to RDW manipulations by focusing on the worst-case scenario, in which users walk perfectly straight ahead in the VE, whereas they are redirected on a circular path in the real world. The results showed that a physical radius of at least 22 meters is required for undetectable RDW. However, users do not always walk exactly straight in a VE. So far, it has not been investigated how much a physical path can be bent in situations in which users walk a virtual curved path instead of a straight one. Such curved walking paths can be often observed, for example, when users walk on virtual trails, through bent corridors, or when circling around obstacles. In such situations the question is not, whether or not the physical path can be bent, but how much the bending of the physical path may vary from the bending of the virtual path. In this article, we analyze this question and present redirection by means of bending gains that describe the discrepancy between the bending of curved paths in the real and virtual environment. Furthermore, we report the psychophysical experiments in which we analyzed the human sensitivity to these gains. The results reveal encouragingly wider detection thresholds than for straightforward walking. Based on our findings, we discuss the potential of curved walking and present a first approach to leverage bent paths in a way that can provide undetectable RDW manipulations even in room-scale VR.","Authors":"E. Langbehn; P. Lubos; G. Bruder; F. Steinicke","DOI":"10.1109\/TVCG.2017.2657220","Keywords":"Virtual reality;redirected walking;room-scale;bending gains","Keywords_Processed":"redirect walking;room scale;virtual reality;bend gain","Title":"Bending the Curve: Sensitivity to Bending of Curved Paths and Application in Room-Scale VR","Labels":null,"Keyword_Vector":[0.0904517732,0.2660022906,0.0089279632,-0.0704527357,0.0681048848,-0.0293121327,-0.0235635686,-0.0235815435,0.0462157446,0.0181202302,0.0046496821,-0.0036644508,-0.0104889746,0.0118434393,-0.1422239088,0.0055688205,0.0126424241,0.0348322917,-0.0018010317,0.0342035126,-0.0215556569,0.0122243054,-0.0134490878,0.0900719687],"Abstract_Vector":[0.1446452048,0.0693773816,0.1958729624,0.0515458941,-0.103736035,-0.0745735466,0.041501249,-0.0271357539,0.1029345085,0.0422948112,0.0095364995,-0.0233775285,0.0026220805,0.1731304468,0.0711517889,0.0101028993,-0.0316926392,-0.0128859911,-0.030943219,-0.0575841156,-0.0573613074,0.0818147653,-0.0146916721,-0.0961772268,0.1022111469,0.0468624345,0.0853832439,-0.0625569494,-0.0498795671,0.1246203771,0.0665353965,0.1517101604,0.0758122149,0.0522600205,-0.0336288167,-0.0218317571,0.0602163213,-0.0283829558,-0.0966838586,-0.0660442455,-0.171708924,0.0692521887,0.0519085365,-0.0931652583,0.0974238343,0.0878664404,0.0164429615,-0.0623952212,-0.0771838674,-0.0325329331,0.0723440501,-0.0862673109,0.0752060527,0.0198657302,0.0258971846,0.0556897248,-0.0207992142,0.0596992749,0.0834849934,0.0015392336,-0.0027757106,-0.0570521296,-0.0958771422,-0.0741773093,0.0303794864,0.0338928262,-0.0616247369,-0.0613147532,0.0341994287,0.1302910793,0.1067040297,0.0432105554,0.0839364739,0.0254299795,-0.0079517147,0.1045685467]},"31":{"Abstract":"This work introduces a tool for interactive exploration and visualization using MetaTracts. MetaTracts is a novel method for extraction and visualization of individual fiber bundles and weaving patterns from X-ray computed tomography (XCT) scans of endless carbon fiber reinforced polymers (CFRPs). It is designed specifically to handle XCT scans of low resolutions where the individual fibers are barely visible, which makes extraction of fiber bundles a challenging problem. The proposed workflow is used to analyze unit cells of CFRP materials integrating a recurring weaving pattern. First, a coarse version of integral curves is used to trace sections of the individual fiber bundles in the woven CFRP materials. We call these sections MetaTracts. In the second step, these extracted fiber bundle sections are clustered using a two-step approach: first by orientation, then by proximity. The tool can generate volumetric representations as well as surface models of the extracted fiber bundles to be exported for further analysis. In addition a custom interactive tool for exploration and visual analysis of MetaTracts is designed. We evaluate the proposed workflow on a number of real world datasets and demonstrate that MetaTracts effectively and robustly identifies and extracts fiber bundles.","Authors":"A. Bhattacharya; J. Weissenb\u00f6ck; R. Wenger; A. Amirkhanov; J. Kastner; C. Heinzl","DOI":"10.1109\/TVCG.2016.2582158","Keywords":"MetaTracts;fiber bundle extraction;analysis and visualization;carbon fiber reinforced polymers;X-ray computed tomography;interactive visual exploration and analysis","Keywords_Processed":"carbon fiber reinforce polymer;analysis and visualization;metatract;ray compute tomography;interactive visual exploration and analysis;fiber bundle extraction","Title":"Interactive Exploration and Visualization Using MetaTracts extracted from Carbon Fiber Reinforced Composites","Labels":null,"Keyword_Vector":[0.2071953661,-0.1184658427,0.1462729263,-0.0661178658,0.0641819245,-0.0732008724,0.0908961443,-0.0214694483,0.0889422075,0.0684550677,-0.0858975198,0.0171202939,-0.0024064606,0.191561962,0.0552484981,-0.1221836355,-0.0693473435,-0.1549849807,0.0737194349,-0.0295714803,-0.0910341978,-0.1412772776,-0.0873259578,-0.0575556669],"Abstract_Vector":[0.1530478465,-0.0198009988,-0.059340259,0.0027681709,0.0650653329,-0.0045892432,0.0195043511,-0.03257915,-0.0947536011,0.0156297972,0.028710126,-0.0326248003,-0.0661657365,-0.003291684,0.0516055113,0.0435595404,-0.0286896857,0.0394878591,0.0371609763,0.069054617,0.0704425601,0.049683011,-0.0116441844,-0.0628903815,-0.0111288451,-0.0115610883,0.0046288678,-0.0293697474,-0.0619049374,0.0172307899,0.0271496195,-0.0593973903,0.0371912258,0.0454544957,0.0179603881,0.0354589412,0.0153896727,-0.0608319806,0.0242808058,-0.084951942,0.0308066853,-0.0150391689,-0.1418847554,-0.0014429855,-0.0468696579,-0.0320820652,-0.047086845,0.0635996098,0.0602523957,0.0108353111,-0.0180993083,-0.0544536231,-0.0210079207,-0.0983975867,0.066669177,0.0664940915,0.0779578423,0.1510332883,-0.0605362528,-0.1019117257,-0.0177025395,0.0685948693,0.0385987603,-0.0032214618,-0.0005745169,0.0057001341,0.074086942,-0.0709107663,0.0308271498,-0.1151634786,0.0378726752,-0.0524392592,-0.0707670527,0.2142411572,-0.0428026866,0.0565515462]},"32":{"Abstract":"The mobility and ubiquity of mobile head-mounted displays make them a promising platform for telepresence research as they allow for spontaneous and remote use cases not possible with stationary hardware. In this work we present a system that provides immersive telepresence and remote collaboration on mobile and wearable devices by building a live spherical panoramic representation of a user's environment that can be viewed in real time by a remote user who can independently choose the viewing direction. The remote user can then interact with this environment as if they were actually there through intuitive gesture-based interaction. Each user can obtain independent views within this environment by rotating their device, and their current field of view is shared to allow for simple coordination of viewpoints. We present several different approaches to create this shared live environment and discuss their implementation details, individual challenges, and performance on modern mobile hardware; by doing so we provide key insights into the design and implementation of next generation mobile telepresence systems, guiding future research in this domain. The results of a preliminary user study confirm the ability of our system to induce the desired sense of presence in its users.","Authors":"J. Young; T. Langlotz; M. Cook; S. Mills; H. Regenbrecht","DOI":"10.1109\/TVCG.2019.2898737","Keywords":"Telepresence;Remote Collaboration;CSCW","Keywords_Processed":"remote collaboration;cscw;telepresence","Title":"Immersive Telepresence and Remote Collaboration using Mobile and Wearable Devices","Labels":null,"Keyword_Vector":[0.0105492387,0.0000352849,-0.0111784122,0.0318513404,0.0131999697,-0.0046811144,0.0156255412,0.0035499481,-0.0151568818,0.0147129008,-0.0229567244,-0.0002493063,0.0290095855,-0.0139670215,0.0350108231,-0.0238743145,-0.0094526185,-0.00128592,-0.0239230117,-0.026457875,-0.0666400682,0.0069546431,0.0139891453,0.0081171659],"Abstract_Vector":[0.2299947484,0.0166387193,0.2080739076,0.0128950205,0.0587659401,-0.09058896,0.0559484884,0.0489494914,-0.0364989151,-0.1344987092,0.2386764112,0.0139268061,0.1113615111,0.0003880589,-0.0872678747,-0.1357867365,-0.1937185913,-0.1047722418,-0.0407859633,-0.1003237377,0.0176932768,-0.0533554708,0.0070241147,-0.0223396044,-0.0167943313,-0.0716595998,0.0334189377,-0.0914171784,0.0533441602,0.100738819,-0.0238977441,0.0121972517,0.0786926967,-0.1347238436,-0.0139204658,0.0717618137,-0.0312784594,-0.0337339453,0.071787973,0.0565164438,-0.1143957973,0.0355435489,-0.0322438714,-0.0405929022,-0.045800423,-0.104346261,-0.0487111574,0.0519909019,0.0147495003,0.0664258729,-0.1059619652,0.0591792446,-0.0359215835,0.0693571213,-0.0899486479,-0.0407778667,-0.0530544323,-0.0143124304,-0.009237082,-0.0390521334,0.0080105895,0.0459037604,-0.0223071784,-0.0034441644,-0.0255131693,0.0762780228,-0.0226596859,0.0189395223,0.018758858,0.0632092755,-0.1119918994,-0.0121501933,0.0120668691,0.0639183646,0.0387461817,0.0015176182]},"33":{"Abstract":"This paper presents Abstractocyte, a system for the visual analysis of astrocytes and their relation to neurons, in nanoscale volumes of brain tissue. Astrocytes are glial cells, i.e., non-neuronal cells that support neurons and the nervous system. The study of astrocytes has immense potential for understanding brain function. However, their complex and widely-branching structure requires high-resolution electron microscopy imaging and makes visualization and analysis challenging. Furthermore, the structure and function of astrocytes is very different from neurons, and therefore requires the development of new visualization and analysis tools. With Abstractocyte, biologists can explore the morphology of astrocytes using various visual abstraction levels, while simultaneously analyzing neighboring neurons and their connectivity. We define a novel, conceptual 2D abstraction space for jointly visualizing astrocytes and neurons. Neuroscientists can choose a specific joint visualization as a point in this space. Interactively moving this point allows them to smoothly transition between different abstraction levels in an intuitive manner. In contrast to simply switching between different visualizations, this preserves the visual context and correlations throughout the transition. Users can smoothly navigate from concrete, highly-detailed 3D views to simplified and abstracted 2D views. In addition to investigating astrocytes, neurons, and their relationships, we enable the interactive analysis of the distribution of glycogen, which is of high importance to neuroscientists. We describe the design of Abstractocyte, and present three case studies in which neuroscientists have successfully used our system to assess astrocytic coverage of synapses, glycogen distribution in relation to synapses, and astrocytic-mitochondria coverage.","Authors":"H. Mohammed; A. K. Al-Awami; J. Beyer; C. Cali; P. Magistretti; H. Pfister; M. Hadwiger","DOI":"10.1109\/TVCG.2017.2744278","Keywords":"Connectomics;Neuroscience;Data Abstraction;Interactive 3D Visualization","Keywords_Processed":"neuroscience;interactive 3d visualization;data abstraction;connectomic","Title":"Abstractocyte: A Visual Tool for Exploring Nanoscale Astroglial Cells","Labels":null,"Keyword_Vector":[0.2170647825,-0.0471049255,-0.0539721789,0.0690213894,0.0532161412,-0.0925480333,0.034114553,-0.0053312902,0.0905344113,0.0172306377,0.0481698606,-0.1174236195,-0.1195298611,0.279769763,0.027891149,0.2276302562,0.0366148649,-0.0850000158,0.0547096343,0.0256100539,0.0968050661,0.0549526387,0.1308850361,-0.1397941057],"Abstract_Vector":[0.2004119556,-0.0787046533,0.0070836505,0.0369407311,0.0580257488,0.0008988296,0.0612631673,-0.0121454057,-0.088119115,-0.0141040109,0.0087507955,-0.0631902383,0.0261059979,-0.0073043232,-0.0475424161,-0.100608243,0.0453514806,-0.0818958482,0.0117381571,-0.002692775,0.0071293468,0.1272155759,-0.0359768854,0.0357101041,0.054459127,-0.0532614626,-0.0136440541,0.0037273688,0.0766725068,-0.0500277961,0.0085613075,-0.0233499729,-0.0363881122,0.0807536342,-0.0533523107,-0.0620561039,0.1254823441,-0.0005100514,0.0058523685,0.0234845373,-0.0046642397,-0.0254095862,-0.064883129,-0.067164533,0.0693783205,0.095845228,-0.0662558766,0.02195498,-0.0206822777,0.0951570158,0.072339106,-0.0226469788,-0.0770780528,0.0301686444,-0.0240483802,0.1967040693,0.049257265,-0.0798421125,0.08079433,-0.0798283948,0.0086855,0.2195313243,-0.1152659998,-0.035832878,0.0976025199,0.0326600245,0.0006052719,-0.0078282624,0.1265475353,-0.1229692864,-0.0599907748,0.0224454022,-0.0773450339,-0.0735043044,0.0724215081,0.0838169014]},"34":{"Abstract":"The use of scatterplots is an important method for multivariate data visualization. The point distribution on the scatterplot, along with variable values represented by each point, can help analyze underlying patterns in data. However, determining the multivariate data variation on a scatterplot generated using projection methods, such as multidimensional scaling, is difficult. Furthermore, the point distribution becomes unclear when the data scale is large and clutter problems occur. These conditions can significantly decrease the usability of scatterplots on multivariate data analysis. In this study, we present a cluster-based visual abstraction method to enhance the visualization of multivariate scatterplots. Our method leverages an adapted multilabel clustering method to provide abstractions of high quality for scatterplots. An image-based method is used to deal with large scale data problem. Furthermore, a suite of glyphs is designed to visualize the data at different levels of detail and support data exploration. The view coordination between the glyph-based visualization and the table lens can effectively enhance the multivariate data analysis. Through numerical evaluations for data abstraction quality, case studies and a user study, we demonstrate the effectiveness and usability of the proposed techniques for multivariate data analysis on scatterplots.","Authors":"H. Liao; Y. Wu; L. Chen; W. Chen","DOI":"10.1109\/TVCG.2017.2754480","Keywords":"Data abstraction;scatterplot;glyph visualization;multilabel optimization","Keywords_Processed":"glyph visualization;multilabel optimization;scatterplot;datum abstraction","Title":"Cluster-Based Visual Abstraction for Multivariate Scatterplots","Labels":null,"Keyword_Vector":[0.2209000499,-0.1007905803,-0.0584175047,-0.0113446564,0.0792791862,0.1240650702,0.0197986056,0.0517685913,-0.1832565214,-0.050078348,0.0634531357,0.0240415651,-0.1629491291,0.0204315197,-0.0690214304,0.0369148022,-0.0924587916,0.0264917425,0.0445155238,-0.061943814,0.0728468182,-0.0311477834,0.0410865236,0.0237245452],"Abstract_Vector":[0.3559660852,-0.2183809632,-0.1496308427,-0.0743603838,-0.2098175976,0.1361409028,0.0522073161,0.1675356522,0.0524028879,0.1365437571,-0.0814958075,-0.0335772723,0.0890668142,0.0297174862,-0.1519452249,-0.1090235796,0.0199579243,-0.0361497151,-0.0404972728,-0.0633424046,0.2139363027,-0.1062962826,0.0868838443,-0.0283146572,0.0041775584,0.0139512994,-0.0471055286,-0.0193817304,-0.0548233866,0.0792515566,0.0379482124,0.0114654146,0.0337150758,0.0569929754,-0.0894762523,-0.1536725139,-0.0152596615,0.0263632684,-0.0837574654,0.0488702074,0.0923844801,-0.1114431515,-0.0325663019,-0.0475960366,0.0794912156,0.0296968731,-0.0933096709,-0.0634378898,0.0144636971,0.0653212031,0.0076426002,0.012119181,-0.1233561655,-0.0478785376,-0.0649249671,0.0631938293,0.0514513054,-0.0132247996,0.0152434388,-0.0571823344,0.0238898129,-0.0101844152,0.020486877,-0.0229259562,-0.0560900287,-0.0630258435,-0.010812093,0.0662764352,-0.0824549938,0.0341288797,-0.0630219013,0.0471662141,0.0195124249,-0.0031142127,0.007554211,0.0206314776]},"35":{"Abstract":"We present a novel algorithm to generate virtual acoustic effects in captured 3D models of real-world scenes for multimodal augmented reality. We leverage recent advances in 3D scene reconstruction in order to automatically compute acoustic material properties. Our technique consists of a two-step procedure that first applies a convolutional neural network (CNN) to estimate the acoustic material properties, including frequency-dependent absorption coefficients, that are used for interactive sound propagation. In the second step, an iterative optimization algorithm is used to adjust the materials determined by the CNN until a virtual acoustic simulation converges to measured acoustic impulse responses. We have applied our algorithm to many reconstructed real-world indoor scenes and evaluated its fidelity for augmented reality applications.","Authors":"C. Schissler; C. Loftin; D. Manocha","DOI":"10.1109\/TVCG.2017.2666150","Keywords":"Sound propagation;material optimization;recognition","Keywords_Processed":"sound propagation;material optimization;recognition","Title":"Acoustic Classification and Optimization for Multi-Modal Rendering of Real-World Scenes","Labels":null,"Keyword_Vector":[0.017570969,-0.0104054134,-0.0056991894,0.0406771107,0.0258595022,0.011841054,0.0041036716,-0.025832658,-0.031390494,-0.0281891069,-0.0014896598,0.0214840165,-0.0929479372,0.0320770827,-0.0065287611,0.0157929406,0.0135697961,0.0107404697,-0.0144654572,0.0396524032,0.0218932082,-0.0846895803,0.0033795973,0.0724578354],"Abstract_Vector":[0.1765528865,0.1708512429,0.0643124658,0.03650783,0.0004104269,-0.0661211866,0.035079189,0.0122748138,-0.030589938,0.0649804342,-0.0486156085,0.0837025025,-0.0496659213,-0.0138320332,-0.0362955854,-0.0181554559,-0.0407216169,0.1085890086,0.0414012984,0.1781400516,0.0725408763,0.0447428726,0.0994117397,-0.035187238,0.0815219204,0.2005539654,0.1093911889,0.0648646997,-0.0243374486,-0.0046628268,-0.0913960882,-0.0110658219,0.0684903781,0.0778009141,0.0441443807,0.0911559561,-0.0417495537,-0.0661445941,0.0404065944,-0.0825500093,0.0893841942,0.0008895011,-0.0001248277,-0.0637127998,0.0131891422,0.1170287494,-0.1233169327,0.1138093928,-0.0139667931,0.0255919294,0.0329419408,-0.0981789464,-0.1159056525,-0.0406938487,0.0721991684,-0.0506785527,0.0551813291,0.1303361605,-0.1038433515,0.0945368738,0.1758104324,0.0800286947,0.0347074754,-0.0551641097,-0.0150621711,0.0169049964,0.0093953651,0.0105135961,0.0893437375,0.0144764826,0.1656402227,0.0357164229,-0.0699765785,-0.0229257932,0.0031986984,-0.108889288]},"36":{"Abstract":"We present DrawFromDrawings, an interactive drawing system that provides users with visual feedback for assistance in 2D drawing using a database of sketch images. Following the traditional imitation and emulation training from art education, DrawFromDrawings enables users to retrieve and refer to a sketch image stored in a database and provides them with various novel strokes as suggestive or deformation feedback. Given regions of interest (ROIs) in the user and reference sketches, DrawFromDrawings detects as-long-as-possible (ALAP) stroke segments and the correspondences between user and reference sketches that are the key to computing seamless interpolations. The stroke-level interpolations are parametrized with the user strokes, the reference strokes, and new strokes created by warping the reference strokes based on the user and reference ROI shapes, and the user study indicated that the interpolation could produce various reasonable strokes varying in shapes and complexity. DrawFromDrawings allows users to either replace their strokes with interpolated strokes (deformation feedback) or overlays interpolated strokes onto their strokes (suggestive feedback). The other user studies on the feedback modes indicated that the suggestive feedback enabled drawers to develop and render their ideas using their own stroke style, whereas the deformation feedback enabled them to finish the sketch composition quickly.","Authors":"Y. Matsui; T. Shiratori; K. Aizawa","DOI":"10.1109\/TVCG.2016.2554113","Keywords":"interactive drawing;2D shape interpolation","Keywords_Processed":"2d shape interpolation;interactive drawing","Title":"DrawFromDrawings: 2D Drawing Assistance via Stroke Interpolation with a Sketch Database","Labels":null,"Keyword_Vector":[0.0545528245,-0.0227600531,0.0156203104,0.0469706193,-0.0116902632,-0.0915561567,0.1226260331,-0.0935296551,-0.0524789333,0.0426987143,-0.0749705087,-0.0698424785,-0.1641616879,0.2729918682,-0.0057065807,0.0609394135,0.0550990603,-0.0855470886,-0.1261457593,0.1186837276,0.0295600143,-0.1077409606,0.1090873564,0.1350469025],"Abstract_Vector":[0.1429394284,0.0533761228,-0.008625035,0.0051558142,0.007888397,-0.0551518575,0.0432236761,-0.082077625,-0.0231895952,-0.0193270855,0.2768466103,0.0342413896,-0.0853109557,0.0269935556,-0.0587368933,-0.0056904337,-0.0115506568,0.0185918958,-0.0357162268,-0.230683705,-0.0830397647,0.0664852601,0.188079771,-0.1163133783,-0.1187078445,0.0417820511,-0.1029007592,-0.0079875743,-0.1578864144,-0.176480972,0.038729349,0.0002073487,0.0155538507,0.0804839986,-0.0253469335,0.0017331774,-0.0430103718,-0.0908289532,-0.1068339965,0.0458012306,-0.043484382,0.0727768274,0.1436357645,-0.0249694849,-0.0288600286,0.1112168542,-0.0912682262,0.0291445565,0.0459526481,0.0232585339,0.0365213777,0.1162190087,0.0700258429,0.0006755414,-0.0831389104,0.0303102963,0.0394602115,0.0244534448,0.0639845186,0.1153221914,0.0512933039,-0.0286372149,-0.0013795222,0.0345210195,0.0196938574,0.0216564725,-0.0974764002,0.0728502265,0.0039856922,-0.0988975124,0.1105947884,0.0005646907,0.041092506,0.036806269,-0.0299569131,-0.0543624047]},"37":{"Abstract":"Ego-network, which represents relationships between a specific individual, i.e., the ego, and people connected to it, i.e., alters, is a critical target to study in social network analysis. Evolutionary patterns of ego-networks along time provide huge insights to many domains such as sociology, anthropology, and psychology. However, the analysis of dynamic ego-networks remains challenging due to its complicated time-varying graph structures, for example: alters come and leave, ties grow stronger and fade away, and alter communities merge and split. Most of the existing dynamic graph visualization techniques mainly focus on topological changes of the entire network, which is not adequate for egocentric analytical tasks. In this paper, we present egoSlider, a visual analysis system for exploring and comparing dynamic ego-networks. egoSlider provides a holistic picture of the data through multiple interactively coordinated views, revealing ego-network evolutionary patterns at three different layers: a macroscopic level for summarizing the entire ego-network data, a mesoscopic level for overviewing specific individuals' ego-network evolutions, and a microscopic level for displaying detailed temporal information of egos and their alters. We demonstrate the effectiveness of egoSlider with a usage scenario with the DBLP publication records. Also, a controlled user study indicates that in general egoSlider outperforms a baseline visualization of dynamic networks for completing egocentric analytical tasks.","Authors":"Y. Wu; N. Pitipornvivat; J. Zhao; S. Yang; G. Huang; H. Qu","DOI":"10.1109\/TVCG.2015.2468151","Keywords":"Egocentric network;dynamic graph;network visualization;glyph-based design;visual analytics;Egocentric network;dynamic graph;network visualization;glyph-based design;visual analytics","Keywords_Processed":"dynamic graph;visual analytic;glyph base design;egocentric network;network visualization","Title":"egoSlider: Visual Analysis of Egocentric Network Evolution","Labels":null,"Keyword_Vector":[0.3107651002,-0.0650667957,0.1010049564,0.1845535138,-0.2978146301,0.0412395424,0.219081217,0.0409324214,-0.0423748137,0.1121959287,-0.1822283165,0.093004944,0.0252335424,-0.1874825913,-0.2867141894,0.0258276178,0.0237065841,0.0918492612,0.1462814096,-0.0151046768,0.1731986993,-0.0605467931,0.0223225305,-0.0285095374],"Abstract_Vector":[0.182490888,-0.1119746759,0.0454290891,-0.0226113571,0.0887010317,-0.1617926113,0.2052843646,0.0484546432,0.1092546732,0.0558637941,-0.0811762314,-0.0239341123,-0.1990784359,-0.1178757531,0.0518550659,-0.0167552001,-0.0294400509,0.0337033723,-0.0210603005,0.1059848465,-0.0158115673,0.0816472016,-0.0209573159,-0.135704418,-0.0603179498,0.1039634295,-0.0833699492,0.0084967038,-0.0172999622,-0.015257074,0.007303052,-0.1006566845,-0.0758983186,-0.0518062122,-0.0600844391,0.0401198591,-0.0560022346,-0.0234370233,0.1057816837,0.0018265037,0.0029330628,0.0231756275,0.0456466847,-0.0017524864,0.0363515231,-0.1464693979,-0.0457559764,0.0877216611,-0.0998782421,-0.0190930333,0.0211511869,0.0636297416,0.0178266702,0.0021777697,-0.0215915112,0.0184078691,-0.0101515687,-0.0280342869,0.0184381065,-0.0293656347,0.0030073083,0.0391927287,-0.0844043918,-0.0985961195,0.0486354701,-0.0606741049,0.075982133,0.0447399766,-0.017488625,0.0242913739,-0.0636770876,0.0588887345,0.0345300967,-0.0301816655,0.0343206555,0.0288007028]},"38":{"Abstract":"3D object temporal trackers estimate the 3D rotation and 3D translation of a rigid object by propagating the transformation from one frame to the next. To confront this task, algorithms either learn the transformation between two consecutive frames or optimize an energy function to align the object to the scene. The motivation behind our approach stems from a consideration on the nature of learners and optimizers. Throughout the evaluation of different types of objects and working conditions, we observe their complementary nature - on one hand, learners are more robust when undergoing challenging scenarios, while optimizers are prone to tracking failures due to the entrapment at local minima; on the other, optimizers can converge to a better accuracy and minimize jitter. Therefore, we propose to bridge the gap between learners and optimizers to attain a robust and accurate RGB-D temporal tracker that runs at approximately 2 ms per frame using one CPU core. Our work is highly suitable for Augmented Reality (AR), Mixed Reality (MR) and Virtual Reality (VR) applications due to its robustness, accuracy, efficiency and low latency. Aiming at stepping beyond the simple scenarios used by current systems, often constrained by having a single object in the absence of clutter, averting to touch the object to prevent close-range partial occlusion or selecting brightly colored objects to easily segment them individually, we demonstrate the capacity to handle challenging cases under clutter, partial occlusion and varying lighting conditions.","Authors":"D. J. Tan; N. Navab; F. Tombari","DOI":"10.1109\/TVCG.2017.2734539","Keywords":"3D Tracking;Random Forest;6D Pose Estimation","Keywords_Processed":"random forest;6d pose estimation;3d tracking","Title":"Looking Beyond the Simple Scenarios: Combining Learners and Optimizers in 3D Temporal Tracking","Labels":null,"Keyword_Vector":[0.0580212611,0.0403168986,0.0165582521,0.0767706983,0.0360737218,-0.0963562027,-0.040052698,-0.0154095404,0.0178112152,-0.0104352788,0.0128549107,0.0849575761,-0.1099499488,0.0792949805,-0.0621029654,0.1368196977,0.0758928056,-0.1629676885,-0.1459426674,-0.0924621714,0.2242605097,0.0734635957,-0.1289914275,-0.1592638144],"Abstract_Vector":[0.1926724586,0.2205459606,0.1363598323,0.0289157085,-0.0536308698,0.000162013,-0.0218819249,-0.0084574369,-0.0236109442,0.1863722398,-0.0671133408,-0.0310561659,-0.0109239376,0.0006867686,0.0879294106,0.0237780571,0.1719910352,-0.0225190568,-0.0825800248,-0.0171759292,0.0555296641,-0.0120562695,-0.0814164515,0.0277848883,0.0274938391,0.0720094554,-0.1034186526,0.0212308447,0.0521884889,-0.0786254689,-0.0192743771,-0.0253867588,0.0277892492,-0.052279999,0.037983436,-0.032293747,0.0300209398,0.0257589179,0.0364025907,-0.0211558169,0.1087832025,0.1425084042,0.0308221368,0.050334277,-0.1758396933,0.0580597548,0.1114616171,0.0696679949,-0.0105852409,0.0205397246,-0.020249757,0.0077880192,-0.1429800676,0.0580777605,-0.0470424523,-0.108154592,0.0236288917,0.0418302718,0.0651740897,0.0943038357,0.0220779908,0.150199989,0.0489813932,0.0017377416,-0.0771825785,-0.1733226936,-0.0904016436,-0.0250087586,-0.00107278,-0.0713555812,-0.0218257438,0.0585714219,0.1723198573,-0.0121883343,0.0722402503,0.0274912843]},"39":{"Abstract":"We present a novel method for posing and animating botanical tree models interactively in real time. Unlike other state of the art methods which tend to produce trees that are overly flexible, bending and deforming as if they were underwater plants, our approach allows for arbitrarily high stiffness while still maintaining real-time frame rates without spurious artifacts, even on quite large trees with over ten thousand branches. This is accomplished by using an articulated rigid body model with as-stiff-as-desired rotational springs in conjunction with our newly proposed simulation technique, which is motivated both by position based dynamics and the typical O(N) algorithms for articulated rigid bodies. The efficiency of our algorithm allows us to pose and animate trees with millions of branches or alternatively simulate a small forest comprised of many highly detailed trees. Even using only a single CPU core, we can simulate ten thousand branches in real time while still maintaining quite crisp user interactivity. This has allowed us to incorporate our framework into a commodity game engine to run interactively even on a low-budget tablet. We show that our method is amenable to the incorporation of a large variety of desirable effects such as wind, leaves, fictitious forces, collisions, fracture, etc.","Authors":"E. Quigley; Y. Yu; J. Huang; W. Lin; R. Fedkiw","DOI":"10.1109\/TVCG.2017.2661308","Keywords":"Computer graphics;physically-based modeling;botanical tree","Keywords_Processed":"botanical tree;computer graphic;physically base modeling","Title":"Real-Time Interactive Tree Animation","Labels":null,"Keyword_Vector":[0.0759950374,0.0190140006,-0.0195469058,0.1008140867,0.0071795622,-0.1113034941,0.1365142112,-0.1216638111,-0.1589270563,-0.0695646376,-0.072086705,0.0135853548,0.0778550639,-0.0230445769,-0.1030477742,0.0014036841,0.1055883647,0.0675196169,0.231709119,-0.0819499192,0.1862148752,-0.1510457656,0.0688868506,-0.0298140442],"Abstract_Vector":[0.1898994266,0.1066157702,-0.0629984552,-0.0080471998,0.0183152802,-0.137048927,-0.045772802,0.0848639469,0.0866541379,-0.1051223457,-0.0987838471,0.0082329252,-0.0241337255,0.1209639854,0.0432598086,0.0434425354,0.0568025175,0.0179172347,0.0305309618,-0.0442218543,-0.058429036,0.0131928248,-0.0812813251,0.0438185562,0.1013101462,0.0428419112,-0.1878057281,0.0581862988,-0.1367949146,0.1571439806,-0.0997807519,0.0217675201,0.1221895425,0.1031243135,0.104145915,0.0310941609,-0.0716928228,0.1109957138,0.2086710599,0.0534497052,-0.0519454846,0.0180995328,0.0644333503,-0.109199663,-0.0125534134,0.1414297632,-0.01467312,-0.0495725045,-0.0199068461,0.0516793494,-0.0129840535,0.0069733418,-0.0636396902,-0.0137112644,-0.1104902495,0.0458405691,-0.1093349361,-0.1076037027,-0.0217695024,-0.0234132315,0.0125629319,0.0611771088,-0.0509236097,0.0103660745,-0.0973517822,0.0404674591,0.0056606253,0.123619209,0.087785675,-0.0285410119,0.0115307061,-0.044134463,0.1131650973,-0.0282676493,0.1131944775,0.007872713]},"40":{"Abstract":"When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.","Authors":"W. Chen; F. Guo; D. Han; J. Pan; X. Nie; J. Xia; X. Zhang","DOI":"10.1109\/TVCG.2018.2865139","Keywords":"Large Network Exploration;Structure-Based Exploration;Suggestive Exploration","Keywords_Processed":"large network exploration;structure based exploration;suggestive exploration","Title":"Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks","Labels":null,"Keyword_Vector":[0.0720576848,-0.0435890471,0.0524378357,0.0482958401,0.0182691974,-0.0194703491,0.0501839176,0.0653680821,-0.0344432772,0.2215662809,-0.149442485,-0.0169448297,-0.063300253,-0.0694996966,0.0331492755,-0.1100136843,0.1234284008,-0.0942880797,0.0846636363,0.1226057995,-0.2101914597,-0.2113344606,-0.1181756037,0.0312093631],"Abstract_Vector":[0.2998834957,-0.132440181,0.0131571249,-0.0080748863,0.1653121097,-0.277861614,0.2849416344,0.150966298,0.1497783955,0.0052087007,-0.0197187069,-0.0442007052,-0.2150490462,-0.0864953362,0.0091828023,-0.0269400626,-0.0907670301,0.0297677997,-0.0937849228,0.0510932709,0.0451428558,0.2076541232,-0.0387274426,-0.1760963407,-0.1391134349,0.1069777573,-0.0542324668,-0.0092846071,-0.0020266125,-0.1195363578,0.0256222445,-0.0437571925,-0.0340401089,-0.0122313894,-0.0025897131,0.0457282725,-0.089418215,0.055240048,0.008465639,-0.0511401276,-0.0155034102,0.0029799267,-0.118150266,0.0160884379,0.0670041433,-0.0251477689,0.0731877449,-0.0260040548,-0.0072244786,-0.0278516579,-0.1155573745,0.0177924935,0.0418616406,0.022701994,0.0310027735,-0.0099471519,0.0064923993,0.0139050798,0.0190996307,-0.0342590554,-0.0150268696,0.000608686,-0.0065587047,0.0408650493,-0.017221639,-0.002253811,-0.0074885693,0.0697127884,0.0107738074,0.0041532866,-0.0334544621,-0.0267818071,-0.0143492982,-0.0250504862,0.0123826929,0.0294505009]},"41":{"Abstract":"We propose a visual marker embedding method for the pose estimation of a projection surface to correctly map projected images onto the surface. Assuming that the surface is fabricated by a full-color or multi-material three-dimensional (3D) printer, we propose to automatically embed visual markers on the surface with mechanical accuracy. The appearance of the marker is designed such that the marker is detected by infrared cameras even when printed on a non-planar surface while its appearance can be diminished by the projection to be as imperceptible as possible to human observers. The marker placement is optimized using a genetic algorithm to maximize the number of valid viewpoints from which the pose of the object can be estimated correctly using a stereo camera system. We also propose a radiometric compensation technique to quickly diminish the marker appearance. Experimental results confirm that the pose of projection objects are correctly estimated while the appearance of the markers was diminished to an imperceptible level. At the same time, we confirmed the limitations of the current method; only one object can be handled, and pose estimation is not performed at interactive frame rates. Finally, we demonstrate the proposed technique to show that it works successfully for various surface shapes and target textures.","Authors":"H. Asayama; D. Iwai; K. Sato","DOI":"10.1109\/TVCG.2017.2657634","Keywords":"Digital fabrication;spatial augmented reality;projection mapping;diminished reality;marker-based tracking","Keywords_Processed":"digital fabrication;diminish reality;marker base tracking;projection mapping;spatial augment reality","Title":"Fabricating Diminishable Visual Markers for Geometric Registration in Projection Mapping","Labels":null,"Keyword_Vector":[0.1632688337,0.2942065214,0.0073450019,-0.0403465052,0.1107105864,-0.123505543,-0.012211833,0.0277250241,0.0801398527,-0.0860480164,0.0478155742,-0.0783903065,-0.114295512,-0.088917282,-0.0978299421,-0.0693899747,-0.0382100015,0.1043894924,0.0121876155,-0.0205192034,0.1848440484,-0.1197909602,-0.199826996,-0.1300496566],"Abstract_Vector":[0.1990868996,0.2182631295,-0.0836748818,-0.0827209231,-0.0348060843,0.1304501078,-0.0882375684,-0.0170724085,0.0490902394,0.1080617646,0.0564398196,0.0375778502,-0.1623154475,-0.0691861143,-0.0145736991,-0.0568365408,-0.0091178223,0.0991205381,0.0486365746,0.0444479893,0.005341046,-0.0119853058,-0.1019157375,0.0149264302,-0.1028549094,-0.1454733355,-0.0677121269,-0.0812088375,0.059902637,0.0931549838,-0.0091724364,0.0696449363,0.1089318868,-0.0711384398,-0.0151582889,0.0082157279,0.0645903443,0.0418472176,0.0580914312,0.0196819263,0.0239338256,0.110343604,-0.0494906849,-0.0934528994,-0.0282221883,0.0965901888,0.1365207239,0.0968304019,-0.1045648702,0.036310165,-0.0551256107,-0.0052801,-0.0167014832,-0.0487072321,-0.0254892716,0.0989317911,0.029891494,0.0749259047,0.1069045942,0.00287345,-0.0306843445,-0.0817973188,-0.0010428949,0.0947112411,-0.0281339024,-0.0010466185,0.0097810407,0.0711537607,0.1195291418,-0.0110265215,-0.0264387022,0.0002281658,-0.04577177,0.090433233,0.0741538659,-0.0814316397]},"42":{"Abstract":"In this paper, we present a novel method on surface partition from the perspective of approximation theory. Different from previous shape proxies, the ellipsoidal variance proxy is proposed to penalize the partition results falling into disconnected parts. On its support, the Principle Component Analysis (PCA) based energy is developed for asymptotic cluster aspect ratio and size control. We provide the theoretical explanation on how the minimization of the PCA-based energy leads to the optimal asymptotic behavior for approximation. Moreover, we show the partitions on densely sampled triangular meshes converge to the theoretic expectations. To evaluate the effectiveness of surface approximation, polygonal\/triangular surface remeshing results are generated. The experimental results demonstrate the high approximation quality of our method.","Authors":"Y. Cai; X. Guo; Y. Liu; W. Wang; W. Mao; Z. Zhong","DOI":"10.1109\/TVCG.2016.2623779","Keywords":"Surface approximation;approximation theory;principal component analysis;optimal asymptotic behavior;surface remeshing","Keywords_Processed":"principal component analysis;surface remesh;surface approximation;optimal asymptotic behavior;approximation theory","Title":"Surface Approximation via Asymptotic Optimal Geometric Partition","Labels":null,"Keyword_Vector":[0.0643696717,-0.0306763334,0.0546362882,-0.0414878537,0.0720077456,-0.0557555767,0.0908670755,-0.0188131748,0.0213363086,0.0054672533,0.0085373655,0.0782801029,0.0777298119,0.062824922,0.0430611249,-0.0573379577,-0.0775325527,-0.0687584574,0.0908612868,-0.0113011211,0.0084591384,-0.0345042592,-0.0666951219,-0.0043984578],"Abstract_Vector":[0.1652303584,0.0774689547,-0.1274429034,0.0348759598,-0.0107101304,0.0770012462,-0.0526546625,0.001959585,-0.0106058758,0.0141665092,0.0204447044,-0.0856638909,-0.0876282045,0.027708006,-0.0073003559,-0.0583474227,-0.1041425781,0.2221554338,0.0464186299,-0.0015903671,0.0199140069,-0.0618982842,0.0499176741,0.0861138846,0.0135945154,-0.053919264,0.0084527704,-0.0447191257,-0.0422266138,-0.0324983991,0.1224944737,0.0822415538,-0.0680537731,-0.0661301533,0.0888035285,0.0877411264,0.0228474702,-0.0572483282,-0.0294776727,0.0004438464,0.0140710464,0.0703979803,-0.1210453289,-0.0889812511,0.046719876,-0.1266928965,-0.0369388354,0.1067617007,0.0406666923,-0.0414460124,0.0052494123,-0.0124335777,-0.0298235754,-0.0464915141,0.1000881364,0.0006779614,-0.0535927168,-0.1391453323,-0.0525706586,0.0950662869,-0.1084011387,-0.0175113236,-0.1314832893,0.076640346,-0.1562269863,-0.0087417055,-0.1647451828,0.1238621605,-0.0731261967,0.033308114,-0.1340319773,-0.146275946,-0.0553695939,-0.0047491564,0.0539722383,-0.0209296793]},"43":{"Abstract":"We propose a novel 360\u00b0 scene representation for converting real scenes into stereoscopic 3D virtual reality content with head-motion parallax. Our image-based scene representation enables efficient synthesis of novel views with six degrees-of-freedom (6-DoF) by fusing motion fields at two scales: (1) disparity motion fields carry implicit depth information and are robustly estimated from multiple laterally displaced auxiliary viewpoints, and (2) pairwise motion fields enable real-time flow-based blending, which improves the visual fidelity of results by minimizing ghosting and view transition artifacts. Based on our scene representation, we present an end-to-end system that captures real scenes with a robotic camera arm, processes the recorded data, and finally renders the scene in a head-mounted display in real time (more than 40 Hz). Our approach is the first to support head-motion parallax when viewing real 360\u00b0 scenes. We demonstrate compelling results that illustrate the enhanced visual experience - and hence sense of immersion-achieved with our approach compared to widely-used stereoscopic panoramas.","Authors":"B. Luo; F. Xu; C. Richardt; J. Yong","DOI":"10.1109\/TVCG.2018.2794071","Keywords":"360\u00b0 scene capture;scene representation;head-motion parallax;6 degrees-of-freedom (6-DoF);image-based rendering","Keywords_Processed":"scene representation;360 scene capture;head motion parallax;image base rendering;degree of freedom dof","Title":"Parallax360: Stereoscopic 360\u00b0 Scene Representation for Head-Motion Parallax","Labels":null,"Keyword_Vector":[0.0690044967,0.0459196231,0.002917868,0.1830919459,0.1391821238,-0.0815233516,0.1582656442,-0.1415418587,-0.0249761804,0.021312201,-0.0753122418,-0.0436546302,0.0793260688,-0.1345033943,-0.0012014454,-0.0265267002,-0.1600401313,0.1607820178,-0.2083254816,-0.0430995928,-0.0059171557,0.0563191556,0.0916208624,-0.0609574841],"Abstract_Vector":[0.2618133537,0.268321436,0.0892199019,0.0044348019,-0.1435824471,-0.117306746,0.1268037776,-0.0913600613,-0.1197504342,-0.091772889,-0.000520926,0.1465909146,0.112264741,-0.2355834051,-0.135673689,0.0012988535,-0.0070337043,-0.0597489215,-0.001310183,0.1224837929,0.0734044307,-0.0874000533,0.0376322536,-0.0510171197,-0.0212828216,0.0599858343,0.0758305234,0.1852681365,-0.0354465236,-0.0510493306,0.1184885891,0.0015553236,0.1262772952,0.0837616267,-0.0053385295,-0.013565836,-0.0410237392,-0.0193542085,0.0169035426,0.0117832523,0.00773113,-0.0251541976,-0.0944602437,-0.0060987774,0.0064378137,-0.0006649086,-0.089328058,-0.0196343217,-0.0563197322,0.0217951796,-0.0679759491,-0.0190596301,0.0098864064,-0.0724691557,-0.0421163817,0.0295901964,-0.0637335109,-0.0225861257,-0.0526726531,-0.046034881,-0.0369402572,0.0655259908,-0.0190089072,0.0303077578,-0.0323665499,0.0429122328,0.0236788743,-0.0096748122,-0.0163388467,0.0101657192,0.0380771031,0.0057794115,-0.0121947002,0.0120308847,-0.0162608153,0.0003154761]},"44":{"Abstract":"Modern virtual reality simulations require a constant high-frame rate from the rendering engine. They may also require very low latency and stereo images. Previous rendering engines for virtual reality applications have exploited spatial and temporal coherence by using image-warping to re-use previous frames or to render a stereo pair at lower cost than running the full render pipeline twice. However these previous approaches have shown artifacts or have not scaled well with image size. We present a new image-warping algorithm that has several novel contributions: an adaptive grid generation algorithm for proxy geometry for image warping; a low-pass hole-filling algorithm to address un-occlusion; and support for transparent surfaces by efficiently ray casting transparent fragments stored in per-pixel linked lists of an A-Buffer. We evaluate our algorithm with a variety of challenging test cases. The results show that it achieves better quality image-warping than state-of-the-art techniques and that it can support transparent surfaces effectively. Finally, we show that our algorithm can achieve image warping at rates suitable for practical use in a variety of applications on modern virtual reality equipment.","Authors":"A. Schollmeyer; S. Schneegans; S. Beck; A. Steed; B. Froehlich","DOI":"10.1109\/TVCG.2017.2657078","Keywords":"Image warping;stereoscopic rendering;transparency warping;A-buffer ray casting;image warping strategies;surface estimation quadtree","Keywords_Processed":"stereoscopic rendering;transparency warping;image warping strategy;buffer ray casting;image warping;surface estimation quadtree","Title":"Efficient Hybrid Image Warping for High Frame-Rate Stereoscopic Rendering","Labels":null,"Keyword_Vector":[0.0497670997,-0.0021804832,0.0139745587,0.1608058981,0.1102689746,-0.0879747009,0.1463437411,-0.1384923525,-0.0058973838,0.0472534141,-0.0064396063,0.053005807,0.0556610276,0.1429251525,0.0447092866,0.0765936427,-0.2330253206,0.1382275773,-0.1481671615,0.0634133534,0.0077529059,-0.0177349768,-0.0750740698,-0.0017135953],"Abstract_Vector":[0.2464651974,0.2747039757,-0.0018776959,0.0865740053,-0.0077480283,0.1047394891,-0.0090404015,0.1999271532,-0.1562318342,0.0507083711,-0.0923194909,0.0168261322,-0.1670355785,0.1330327316,0.0460584945,0.0267190941,0.1098719859,-0.0108169481,0.2043143012,-0.080349384,-0.0543387352,0.0310625232,0.1042169663,0.0683464087,0.004127217,0.0631485171,-0.0850825413,0.0643050587,0.0369407829,0.0696771361,0.0247658591,0.072769332,-0.0515839845,-0.1110205682,0.0134956961,0.0710191708,-0.0750125533,-0.0296086186,-0.0817056521,-0.1232664048,-0.0179083485,0.0171295547,-0.0031646016,0.0307801882,-0.1112659106,0.0721446526,-0.0634217428,-0.0278839473,0.0594314298,0.0729331261,-0.0472268708,-0.0161265858,0.0063663152,-0.0765522728,-0.0202865255,0.0110330839,0.0375862872,0.0125993171,0.0225330145,0.0106559019,-0.0022682463,0.0184435176,-0.0479708788,-0.047826779,-0.028932248,0.0708971021,-0.0356839033,0.0331071694,0.0824664701,0.0770364542,-0.0503439981,-0.0398275843,0.0015543106,-0.0080848127,0.0415744778,0.0080132211]},"45":{"Abstract":"Despite the widely recognized importance of symmetric second order tensor fields in medicine and engineering, the visualization of data uncertainty in tensor fields is still in its infancy. A recently proposed tensorial normal distribution, involving a fourth order covariance tensor, provides a mathematical description of how different aspects of the tensor field, such as trace, anisotropy, or orientation, vary and covary at each point. However, this wealth of information is far too rich for a human analyst to take in at a single glance, and no suitable visualization tools are available. We propose a novel approach that facilitates visual analysis of tensor covariance at multiple levels of detail. We start with a visual abstraction that uses slice views and direct volume rendering to indicate large-scale changes in the covariance structure, and locations with high overall variance. We then provide tools for interactive exploration, making it possible to drill down into different types of variability, such as in shape or orientation. Finally, we allow the analyst to focus on specific locations of the field, and provide tensor glyph animations and overlays that intuitively depict confidence intervals at those points. Our system is demonstrated by investigating the effects of measurement noise on diffusion tensor MRI, and by analyzing two ensembles of stress tensor fields from solid mechanics.","Authors":"A. Abbasloo; V. Wiens; M. Hermann; T. Schultz","DOI":"10.1109\/TVCG.2015.2467031","Keywords":"Uncertainty visualization;tensor visualization;direct volume rendering;interaction;glyph based visualization;Uncertainty visualization;tensor visualization;direct volume rendering;interaction;glyph based visualization","Keywords_Processed":"direct volume rendering;tensor visualization;glyph base visualization;uncertainty visualization;interaction","Title":"Visualizing Tensor Normal Distributions at Multiple Levels of Detail","Labels":null,"Keyword_Vector":[0.4573231593,-0.1319587601,-0.3639350019,0.36822412,0.1163453839,0.1380591968,-0.0899300905,-0.0101460824,0.0409826427,-0.0372245138,-0.0751974631,0.1479538188,0.0412056585,-0.0450013714,-0.0019024334,-0.0223286095,-0.0720075327,-0.0229705808,0.1021439306,-0.1301741817,0.034878793,-0.0725831095,-0.0666560749,0.0992584716],"Abstract_Vector":[0.2396975153,-0.0338591162,-0.0769923006,0.0931893806,0.0488142862,0.1291002943,0.1193379979,-0.2067129121,-0.0529397178,-0.1597843544,0.0714547052,-0.0576544647,-0.1094752073,0.0308389743,0.1527558686,-0.211797535,-0.0140147934,-0.2099301645,0.166869892,0.0418783567,0.3074034402,0.0652410966,-0.1929000411,-0.063936663,0.1580145547,-0.073909606,0.1152225022,0.1301942227,-0.1954731545,-0.0399334984,-0.0865650005,-0.0193443667,-0.0151241409,0.0134804226,-0.096081595,-0.10139153,-0.0349753023,0.052766408,-0.0986727707,-0.0518803559,0.0201210607,0.0033838415,0.0146118613,0.0424927548,0.0261449127,0.0371499157,0.042614022,-0.0181334148,0.0232604087,0.0567268282,0.0212766335,0.0405472222,0.0543687141,-0.0551802921,-0.0182637588,-0.0541249299,-0.0818422945,-0.04087803,0.0696166733,0.022865434,-0.0198746597,0.0196349035,0.0561342287,-0.0165189918,-0.0214513113,-0.0289115877,0.03076539,0.0743574728,-0.0022045494,0.006554247,-0.0407388133,-0.0454458489,0.0570586802,0.0421402967,0.0374620353,0.0161369368]},"46":{"Abstract":"We present a visualization application that enables effective interactive visual analysis of large-scale 3D histopathology, that is, high-resolution 3D microscopy data of human tissue. Clinical work flows and research based on pathology have, until now, largely been dominated by 2D imaging. As we will show in the paper, studying volumetric histology data will open up novel and useful opportunities for both research and clinical practice. Our starting point is the current lack of appropriate visualization tools in histopathology, which has been a limiting factor in the uptake of digital pathology. Visualization of 3D histology data does pose difficult challenges in several aspects. The full-color datasets are dense and large in scale, on the order of 100,000 \u00d7 100,000 \u00d7 100 voxels. This entails serious demands on both rendering performance and user experience design. Despite this, our developed application supports interactive study of 3D histology datasets at native resolution. Our application is based on tailoring and tuning of existing methods, system integration work, as well as a careful study of domain specific demands emanating from a close participatory design process with domain experts as team members. Results from a user evaluation employing the tool demonstrate a strong agreement among the 14 participating pathologists that 3D histopathology will be a valuable and enabling tool for their work.","Authors":"M. Falk; A. Ynnerman; D. Treanor; C. Lundstr\u00f6m","DOI":"10.1109\/TVCG.2018.2864816","Keywords":"Histology;Pathology;Volume Rendering;Expert Evaluation","Keywords_Processed":"expert evaluation;pathology;histology;volume rendering","Title":"Interactive Visualization of 3D Histopathology in Native Resolution","Labels":null,"Keyword_Vector":[0.1035958064,-0.0411500994,-0.0626052563,0.2827852854,0.1930464395,0.3000927903,-0.0081857409,-0.0938844689,0.1705717171,0.0726960139,0.0449433956,0.0451149398,0.0581293906,-0.0267923305,0.0606276697,-0.0794789847,0.0589208483,0.0384274366,-0.0321258792,0.0556549656,0.0196204724,0.0002723252,-0.0024696893,0.015072735],"Abstract_Vector":[0.2810138349,-0.0591308418,0.0360848141,-0.0438624186,0.0535607324,0.0617221592,0.0010431088,0.0021228469,-0.0968782839,0.0226410728,0.0668640146,-0.0299475604,0.0121663555,-0.0370161968,-0.0015777799,0.0443603226,-0.0200355702,-0.0169236624,0.0579053219,0.0082556585,0.0096857352,0.0819409137,-0.0517455506,0.0711826353,-0.0489496837,-0.0093070923,-0.0409834448,-0.0410936305,0.1120839686,-0.0972383482,-0.0285465589,-0.0796477881,0.094749133,-0.0146529897,0.0275732154,-0.1932904062,0.035910819,-0.0569877689,0.033905277,0.0487340948,-0.0053744119,0.059834729,-0.1186637745,-0.0405548826,-0.010248485,0.0621849019,0.0255818114,-0.0331555515,-0.0191308356,0.0304595565,0.0961521508,0.0087947065,-0.025226622,-0.0149659819,0.0564044275,-0.0462512569,-0.052448461,-0.0275007674,-0.0745519255,0.0160766077,0.0428346294,0.0215171212,-0.0029766734,0.052318621,-0.0285859391,0.0197951088,-0.0593626586,0.0822453793,-0.0971017227,0.0310371382,0.0338199066,0.0477047783,-0.0292277012,0.0644414181,-0.0388012755,0.0033510851]},"47":{"Abstract":"The exploration and analysis of scientific literature collections is an important task for effective knowledge management. Past interest in such document sets has spurred the development of numerous visualization approaches for their interactive analysis. They either focus on the textual content of publications, or on document metadata including authors and citations. Previously presented approaches for citation analysis aim primarily at the visualization of the structure of citation networks and their exploration. We extend the state-of-the-art by presenting an approach for the interactive visual analysis of the contents of scientific documents, and combine it with a new and flexible technique to analyze their citations. This technique facilitates user-steered aggregation of citations which are linked to the content of the citing publications using a highly interactive visualization approach. Through enriching the approach with additional interactive views of other important aspects of the data, we support the exploration of the dataset over time and enable users to analyze citation patterns, spot trends, and track long-term developments. We demonstrate the strengths of our approach through a use case and discuss it based on expert user feedback.","Authors":"F. Heimerl; Q. Han; S. Koch; T. Ertl","DOI":"10.1109\/TVCG.2015.2467621","Keywords":"scientific literature;visual document analysis;visual citation analysis;streamgraph;clustering;scientific literature;visual document analysis;visual citation analysis;streamgraph;clustering","Keywords_Processed":"cluster;streamgraph;visual citation analysis;visual document analysis;scientific literature","Title":"CiteRivers: Visual Analytics of Citation Patterns","Labels":null,"Keyword_Vector":[0.2224396997,-0.1448074839,0.3699855523,-0.126787031,0.0519882906,-0.057074538,-0.0005465074,-0.0275839929,0.1758933303,-0.0006013024,-0.1325402082,0.0349507582,0.1230229988,0.0594644891,-0.001840927,-0.0781682844,-0.0334409601,-0.0642707746,0.0605625414,-0.0180330342,-0.0642408011,-0.1411523331,0.0118983958,-0.1580148698],"Abstract_Vector":[0.2575618835,-0.1404360513,0.0303925414,-0.0102534714,0.0636346039,-0.0795365376,0.083873552,-0.0027827124,-0.0581455542,-0.0211257272,0.0154866522,-0.0288827751,-0.1054379669,-0.0430958947,-0.0038314174,0.036371577,-0.0280754183,-0.0508391318,-0.1078745057,0.0275183503,0.0150151971,0.0396459435,0.0793434316,-0.0421447536,-0.1135767534,0.0440849312,0.0343543526,0.0408809976,0.0390002029,0.0254155916,0.0487674459,0.1279259413,0.0438937093,-0.019564852,-0.0976772749,0.0679059399,0.0622002209,0.149328618,-0.0047035827,-0.0777323151,0.0197627935,-0.0308137423,-0.013296603,0.0086658815,-0.0080289091,0.0173207102,0.1074275907,0.0247388534,0.013754522,0.0318798659,-0.0637801256,-0.0192754817,-0.030596013,0.0039685578,-0.0927852682,-0.0513710366,-0.0891456796,-0.0232606028,0.0430693275,-0.0878387535,-0.0407279441,-0.005008382,-0.0488948497,0.0757477474,-0.0254295866,-0.1105722744,0.0110919394,-0.1039393411,-0.0811830412,-0.0753304802,0.081846446,-0.1194249704,0.0314430144,0.103444652,-0.0395164222,-0.003895668]},"48":{"Abstract":"We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopic viewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of the stereoscopic and monoscopic displays. We show how this mapping supports interactive data exploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users' movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.","Authors":"D. L\u00f3pez; L. Oehlberg; C. Doger; T. Isenberg","DOI":"10.1109\/TVCG.2015.2440233","Keywords":"Visualization of 3D data;human-computer interaction;expert interaction;direct-touch input;mobile displays;stereoscopic environments;VR;AR;conceptual model of interaction;interaction reference frame mapping;observational study;Visualization of 3D data;human-computer interaction;expert interaction;direct-touch input;mobile displays;stereoscopic environments;VR;AR;conceptual model of interaction;interaction reference frame mapping;observational study","Keywords_Processed":"direct touch input;mobile display;visualization of 3d datum;observational study;ar;stereoscopic environment;expert interaction;interaction reference frame mapping;conceptual model of interaction;human computer interaction;vr","Title":"Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration","Labels":null,"Keyword_Vector":[0.2444838543,0.0730773576,-0.0222627196,0.1701414954,0.0990548759,-0.0043861047,-0.0601169491,0.3245533946,-0.1968994663,-0.0663475637,-0.0455027299,0.2149666938,-0.0155360575,0.0316976112,0.2366742561,0.0904441645,-0.053192927,-0.1429315657,0.0072404802,-0.0357382837,0.016682271,0.0620658731,0.0499992181,0.0064073691],"Abstract_Vector":[0.2334541013,-0.0549515525,0.1469993034,0.0168174633,0.0062973667,-0.0474858555,0.0509711953,0.0135903073,-0.0541223012,-0.0402655814,0.1044172781,0.0018749526,-0.0010689527,-0.0351786446,-0.0837350737,0.0636568965,-0.135045811,-0.0446313998,-0.0305287548,-0.0464278412,-0.0223436029,-0.0267203086,-0.1165245299,0.0650395284,-0.1017232584,-0.0110630157,0.0301142222,0.0584312993,0.0849094476,-0.0178523807,0.0841670069,-0.0953478774,-0.0434158666,0.113166107,-0.0184055685,-0.0152473798,-0.1570734947,-0.1347215559,0.0416603478,0.0023718364,0.008328585,-0.0243502664,-0.1214478133,-0.0621918109,-0.0865614911,-0.0037959051,-0.0358379646,0.0046685371,0.0882462753,0.0466997296,-0.1243571357,0.0242239125,0.0434289288,-0.0875456584,0.0396208728,0.0576962749,0.016806395,-0.0035061006,0.1701797043,0.0482533327,-0.1274220097,-0.0649721589,0.0341019321,0.0152262666,0.0518547946,-0.0679395086,-0.0435656761,-0.0649725643,0.0970811426,-0.009159994,-0.0331009829,0.0219308998,0.0965026087,-0.0900668622,0.0600772116,-0.0311834738]},"49":{"Abstract":"Virtual reality often uses motion tracking to incorporate physical hand movements into interaction techniques for selection and manipulation of virtual objects. To increase realism and allow direct hand interaction, real-world physical objects can be aligned with virtual objects to provide tactile feedback and physical grasping. However, unless a physical space is custom configured to match a specific virtual reality experience, the ability to perfectly match the physical and virtual objects is limited. Our research addresses this challenge by studying methods that allow one physical object to be mapped to multiple virtual objects that can exist at different virtual locations in an egocentric reference frame. We study two such techniques: one that introduces a static translational offset between the virtual and physical hand before a reaching action, and one that dynamically interpolates the position of the virtual hand during a reaching motion. We conducted two experiments to assess how the two methods affect reaching effectiveness, comfort, and ability to adapt to the remapping techniques when reaching for objects with different types of mismatches between physical and virtual locations. We also present a case study to demonstrate how the hand remapping techniques could be used in an immersive game application to support realistic hand interaction while optimizing usability. Overall, the translational technique performed better than the interpolated reach technique and was more robust for situations with larger mismatches between virtual and physical objects.","Authors":"D. T. Han; M. Suhail; E. D. Ragan","DOI":"10.1109\/TVCG.2018.2794659","Keywords":"Virtual reality;3D interaction;passive haptics;hand interaction;remapped reach;3D object selection","Keywords_Processed":"3d interaction;3d object selection;virtual reality;passive haptic;hand interaction;remapp reach","Title":"Evaluating Remapped Physical Reach for Hand Interactions with Passive Haptics in Virtual Reality","Labels":null,"Keyword_Vector":[0.175359106,0.3328933862,-0.008821029,0.1042814027,0.1501451466,-0.0683386226,-0.069847461,0.1919495403,-0.0279090603,-0.0090690372,-0.078233104,0.1669071539,-0.0067160251,0.1279027551,0.1067877265,0.2058990238,0.0678986013,-0.1780177358,0.0747692125,-0.0025319638,0.1588531822,0.0661395163,0.0792819202,0.0259752701],"Abstract_Vector":[0.2265421444,0.2189503663,0.3059660295,-0.0247972852,-0.2104102389,-0.0632468543,0.0381414601,-0.0990308324,0.1022602864,0.1514731793,-0.0737998318,-0.0857200388,-0.0852154904,0.2275667554,0.1000771852,0.0034376753,0.1667404955,0.0643521908,-0.0925758992,-0.0113639335,0.0564274171,0.0440598936,-0.1296929139,0.0508160739,-0.014388762,-0.0165869508,0.0618453796,-0.022987398,0.0926745052,-0.001288008,0.0765983216,-0.0115227314,0.0678824871,0.0584963276,-0.0159579104,0.0299003756,0.0179606819,-0.1051452099,-0.0756122948,0.0048868234,-0.0718354573,0.0182127734,0.1161655777,0.0359678445,-0.0236162491,0.0259381586,-0.0243913243,-0.081810953,0.0415693488,0.0489947728,-0.0594584767,0.0078882223,-0.0343516512,-0.0402252011,0.041979084,-0.0403571446,-0.059519183,0.0401067706,0.0349249839,0.0099113468,0.0530284759,0.1185644732,0.0233321747,0.0306335349,0.0201332629,-0.0085860599,0.0357009814,0.0455425079,-0.0123998633,0.0622480834,0.0001683428,-0.0004786931,-0.0118443208,-0.0153371702,-0.0109658923,0.003546983]},"50":{"Abstract":"Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.","Authors":"A. Srinivasan; S. M. Drucker; A. Endert; J. Stasko","DOI":"10.1109\/TVCG.2018.2865145","Keywords":"Natural Language Generation;Mixed-initiative Interaction;Visualization Recommendation;Data-driven Communication","Keywords_Processed":"natural language generation;mixed initiative interaction;visualization recommendation;datum drive communication","Title":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication","Labels":null,"Keyword_Vector":[0.2156736833,-0.0454246277,-0.0188748394,0.0018469542,0.1308268112,-0.0330800073,-0.0641940717,0.2193043042,-0.1752313237,-0.0337821567,0.0287555922,0.0351992754,0.0835260513,0.1457142188,0.039039556,-0.0338239403,-0.0328055828,0.1038387457,-0.0008050097,-0.2362873484,-0.0251349885,0.082195247,0.0714536464,0.1089481973],"Abstract_Vector":[0.2845436849,-0.1582936499,0.0874461366,-0.084805846,0.0331570681,0.1837606798,-0.043520837,-0.0277169626,-0.1436039245,-0.0022364898,0.0369961254,-0.0515757891,0.0297667941,-0.0882195522,0.010042415,-0.035561311,0.0066356987,0.1143525843,-0.0977035217,-0.0773495705,0.0111292614,0.1170599646,-0.0598754597,-0.0228222767,-0.0996275418,0.0538203336,0.0566275926,-0.1028354368,-0.0233323102,0.0439683293,-0.0319288483,-0.0154963971,0.0529204616,-0.048152302,-0.0318596853,-0.0293518472,-0.0783978173,-0.0025632889,0.0293947219,-0.0688876443,0.0399809268,-0.0265545211,0.0822873114,-0.0123829062,0.0364805001,-0.0397905586,0.0159668727,-0.0393158295,0.0946798212,-0.0435365336,-0.0215137747,-0.1197664053,0.0384612011,-0.0468729101,-0.0336753321,0.0224954866,-0.0305567283,-0.0313751945,0.043988879,-0.0759432928,0.0677248631,-0.0211686854,-0.0548856691,-0.1397426394,-0.00238701,-0.0124237512,-0.1121785579,0.069376125,-0.0454439666,-0.0971896015,-0.0018397785,0.0874815922,0.0139900758,0.0129062641,0.0685344704,0.0210027435]},"51":{"Abstract":"Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification, and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization, and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice, we propose olfactory marks, the substrate in which they exist, and their olfactory channels that are available to designers. To exemplify this idea, we present viScent: A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction, as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart, and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general visualization beyond the examples in this paper.","Authors":"B. Patnaik; A. Batch; N. Elmqvist","DOI":"10.1109\/TVCG.2018.2865237","Keywords":"Olfaction;smell;scent;olfactory display;immersive analytics;immersion","Keywords_Processed":"smell;olfaction;immersion;immersive analytic;olfactory display;scent","Title":"Information Olfactation: Harnessing Scent to Convey Data","Labels":null,"Keyword_Vector":[0.077365348,0.084958054,0.050907803,0.0755477195,-0.0584084215,-0.0010624459,-0.068913907,0.0845342597,-0.0111498088,0.009243587,-0.0722388157,0.0356244473,0.0209940399,-0.0719509143,-0.0982926319,0.0159172026,-0.1790834478,-0.0515581349,-0.0592425291,0.0713215055,-0.1238996292,0.0770718795,0.1316079682,-0.1084352421],"Abstract_Vector":[0.2001992855,-0.0445304544,0.1617128017,0.052811093,0.0533607041,0.1455881985,0.0942606121,-0.0221779307,-0.051331835,-0.0468655876,-0.016114525,-0.0372327606,0.0371711455,-0.1106215244,0.0175500683,-0.0175630699,0.0172407398,-0.0102527249,-0.0173578453,-0.0251484232,-0.0614434792,-0.007629362,0.0266556439,0.1517930242,-0.0027066189,-0.0151298029,0.0819394796,-0.0029435148,-0.0507223625,0.0384142489,-0.0507019222,0.0406790415,0.1223865741,0.0410127468,0.0384729276,-0.0697700215,-0.0826396206,-0.0531601671,0.0204392705,-0.0627323651,0.0553822033,0.0096989306,0.0950362481,0.0202858065,0.0579227329,-0.0205288426,0.0715353404,0.0311132496,-0.1317129427,0.0727640368,0.0085694659,0.1351070853,-0.0232340122,0.0223200338,0.0109008961,0.0406531078,0.0806875199,-0.047760767,0.0162112881,0.1775853092,0.0538460926,-0.1191356811,-0.0018200923,-0.0851455087,-0.0736748295,0.031425994,0.0835494862,0.0661108784,0.0351795292,-0.0687159767,-0.0691865782,0.036043259,-0.0460196783,-0.0083843195,-0.0406590614,-0.0380872017]},"52":{"Abstract":"A myriad of design rules for what constitutes a \u201cgood\u201d colormap can be found in the literature. Some common rules include order, uniformity, and high discriminative power. However, the meaning of many of these terms is often ambiguous or open to interpretation. At times, different authors may use the same term to describe different concepts or the same rule is described by varying nomenclature. These ambiguities stand in the way of collaborative work, the design of experiments to assess the characteristics of colormaps, and automated colormap generation. In this paper, we review current and historical guidelines for colormap design. We propose a specified taxonomy and provide unambiguous mathematical definitions for the most common design rules.","Authors":"R. Bujack; T. L. Turton; F. Samsel; C. Ware; D. H. Rogers; J. Ahrens","DOI":"10.1109\/TVCG.2017.2743978","Keywords":"colormap;survey;taxonomy;order;uniformity;discriminative power;smoothness;monotonicity;linearity;speed","Keywords_Processed":"linearity;order;smoothness;colormap;survey;taxonomy;discriminative power;uniformity;speed;monotonicity","Title":"The Good, the Bad, and the Ugly: A Theoretical Framework for the Assessment of Continuous Colormaps","Labels":null,"Keyword_Vector":[0.004753815,-0.000997718,-0.0028149423,0.0093559076,0.0074619759,0.0112096576,0.002951175,-0.0111047615,0.0100827913,0.0120873099,0.0044071188,-0.0116637923,0.0043527889,0.0156686621,-0.006234187,0.0058910334,-0.0043281781,0.0126718381,-0.0059124584,0.0044993563,-0.0191524177,-0.003255878,-0.0000429913,-0.0426817271],"Abstract_Vector":[0.1216655358,-0.0213015497,0.0401379683,-0.0350832678,0.043000927,0.0536646174,-0.0296006464,-0.0420282604,-0.0211129239,-0.021960266,-0.0088986735,-0.0486043947,0.0190022733,0.0155506,0.0300316782,-0.0076792297,0.0230335578,0.0055786939,0.0338259769,0.0052557542,-0.0301646702,-0.0220363666,0.0425020495,-0.0339224568,0.1642627904,0.0360132428,0.0243097847,-0.0418404121,-0.0364008116,-0.032954663,-0.0036042479,-0.060185907,-0.0723678904,-0.0372742681,-0.0428555568,0.0764852012,0.0744648592,0.0258546063,-0.0070809892,0.0571252076,0.0106948856,-0.0059660224,-0.002334407,0.0043076418,-0.0373741569,0.0915529273,0.0512017914,0.061411379,-0.095793614,0.041738415,-0.150983858,0.0746704803,-0.0233024025,0.0390611634,-0.002412727,0.008703464,0.060673814,0.0842557069,-0.0115515278,-0.0690324765,-0.0873401203,-0.044864113,-0.0064031605,0.0887990479,-0.0748403142,-0.1301782214,0.0413391917,-0.0915258435,0.048325932,0.1360356116,0.1470493659,-0.1290114935,0.0433002027,-0.0461245982,-0.1143256192,0.1879103211]},"53":{"Abstract":"Collecting sensor data results in large temporal data sets which need to be visualized, analyzed, and presented. One-dimensional time-series charts are used, but these present problems when screen resolution is small in comparison to the data. This can result in severe over-plotting, giving rise for the requirement to provide effective rendering and methods to allow interaction with the detailed data. Common solutions can be categorized as multi-scale representations, frequency based, and lens based interaction techniques. In this paper, we comparatively evaluate existing methods, such as Stack Zoom [15] and ChronoLenses [38], giving a graphical overview of each and classifying their ability to explore and interact with data. We propose new visualizations and other extensions to the existing approaches. We undertake and report an empirical study and a field study using these techniques.","Authors":"J. Walker; R. Borgo; M. W. Jones","DOI":"10.1109\/TVCG.2015.2467751","Keywords":"Time-series Exploration;Focus+Context;Lens;Interaction Techniques;Time-series Exploration;Focus+Context;Lens;Interaction Techniques","Keywords_Processed":"time series exploration;focus context;lens;interaction techniques","Title":"TimeNotes: A Study on Effective Chart Visualization and Interaction Techniques for Time-Series Data","Labels":null,"Keyword_Vector":[0.1383698217,0.0132990846,0.0243253847,0.0788840715,-0.0168299338,-0.1026461111,0.0641603843,0.1817555914,0.0043289871,0.2747840085,-0.1312984732,-0.0370317071,-0.3033278,-0.0658893342,0.2970528793,-0.0227344419,0.1536558637,-0.0608073535,0.077285404,-0.1014723156,-0.1950128121,0.0790716741,-0.0150275303,0.123723227],"Abstract_Vector":[0.3289110635,-0.0926396414,-0.0197215639,-0.0882983784,-0.1068654367,0.0677648812,0.0280503963,0.0700907842,-0.0377776364,-0.0664329538,-0.02737655,0.0058288912,-0.0318197337,0.0268128686,0.0835076905,0.0146999777,-0.05020879,0.0366744568,-0.0668262392,0.0620041576,-0.0831216525,-0.1510448605,-0.0135288006,-0.076386566,0.0337472114,-0.0325864551,-0.0293506902,-0.0186583858,0.0889269837,-0.0538143363,-0.0742323497,0.0086819513,0.0869054039,0.1218894419,-0.0161261993,0.0480101507,-0.0944891541,0.0279288207,-0.0211193077,0.0185765433,-0.1702653013,-0.1221130049,-0.1397623631,0.0202610764,-0.0234630306,0.0234854909,-0.0343024079,-0.1134631455,-0.0156149768,-0.1265910604,0.0192077251,0.097433928,-0.0635183076,0.0082171822,0.0959058162,-0.0012344641,0.0946651353,-0.0831926847,0.0296049075,0.0078655021,0.0233828076,-0.0223165833,0.0576850719,0.0388132237,0.0251792385,0.0409267535,-0.0260251459,0.0522634149,-0.0605128749,-0.0093207491,0.1017323249,0.0290930009,0.1311424954,-0.036048355,-0.0384402047,0.0285288029]},"54":{"Abstract":"With the broad range of motion capture devices available on the market, it is now commonplace to directly control the limb movement of an avatar during immersion in a virtual environment. Here, we study how the subjective experience of embodying a full-body controlled avatar is influenced by motor alteration and self-contact mismatches. Self-contact is in particular a strong source of passive haptic feedback and we assume it to bring a clear benefit in terms of embodiment. For evaluating this hypothesis, we experimentally manipulate self-contacts and the virtual hand displacement relatively to the body. We introduce these body posture transformations to experimentally reproduce the imperfect or incorrect mapping between real and virtual bodies, with the goal of quantifying the limits of acceptance for distorted mapping on the reported body ownership and agency. We first describe how we exploit egocentric coordinate representations to perform a motion capture ensuring that real and virtual hands coincide whenever the real hand is in contact with the body. Then, we present a pilot study that focuses on quantifying our sensitivity to visuo-tactile mismatches. The results are then used to design our main study with two factors, offset (for self-contact) and amplitude (for movement amplification). Our main result shows that subjects' embodiment remains important, even when an artificially amplified movement of the hand was performed, but provided that correct self-contacts are ensured.","Authors":"S. Bovet; H. G. Debarba; B. Herbelin; E. Molla; R. Boulic","DOI":"10.1109\/TVCG.2018.2794658","Keywords":"Virtual Reality;Avatar;Embodiment;Agency;Body Ownership;Self-contact","Keywords_Processed":"agency;avatar;virtual reality;body ownership;self contact;embodiment","Title":"The Critical Role of Self-Contact for Embodiment in Virtual Reality","Labels":null,"Keyword_Vector":[0.0835113812,0.279702376,0.0061587697,-0.0612435898,0.0707987602,-0.0428395149,0.0018212938,-0.0576530926,0.0567866357,-0.0369649729,-0.0047531992,-0.0579810161,-0.0037358359,-0.0685311964,-0.0872638684,-0.0661485324,0.0683036462,-0.0252667221,-0.0196232294,-0.0736049333,-0.0597450461,0.0090187327,0.0267791537,0.0291785344],"Abstract_Vector":[0.1452569396,0.1307562612,0.1528525101,-0.1238995505,-0.250933626,-0.1169703627,-0.0451559927,-0.1456129278,0.1199971787,-0.0579144115,-0.0839970818,-0.1006223571,-0.0729712951,0.0365901558,-0.0501048776,0.0752649433,0.0008124512,-0.0363451133,0.0762903299,-0.0731854178,0.0418637287,0.1263624313,-0.0138668382,0.0348208428,-0.0344326994,-0.1279189467,0.0754010195,-0.0054856977,0.0491180908,0.110033917,0.12600642,-0.1905290298,-0.1074235744,0.0297583228,0.2494943601,0.0422075372,-0.1223045976,-0.0346726508,0.0687903396,-0.0683274512,0.0029079675,-0.1134444453,0.1015085163,-0.0512191665,-0.0898573607,0.038020646,-0.039068993,-0.0295118437,0.0006711373,-0.0429647735,0.0810879747,0.0742879614,-0.1701115583,-0.1267871147,-0.0071961106,0.0173453894,-0.0461437284,-0.0673958811,0.003698905,-0.0464593995,-0.0937768498,-0.0457084615,0.0252896368,-0.0172891448,0.0134249372,-0.0031939591,0.0423264049,0.0392013645,0.000674747,-0.0123920597,-0.0050377285,0.0117835538,0.0016136346,0.0199606968,-0.1144377555,0.0276390367]},"55":{"Abstract":"Convolutional neural networks can successfully perform many computer vision tasks on images. For visualization, how do CNNs perform when applied to graphical perception tasks? We investigate this question by reproducing Cleveland and McGill's seminal 1984 experiments, which measured human perception efficiency of different visual encodings and defined elementary perceptual tasks for visualization. We measure the graphical perceptual capabilities of four network architectures on five different visualization tasks and compare to existing and new human performance baselines. While under limited circumstances CNNs are able to meet or outperform human task performance, we find that CNNs are not currently a good model for human graphical perception. We present the results of these experiments to foster the understanding of how CNNs succeed and fail when applied to data visualizations.","Authors":"D. Haehn; J. Tompkin; H. Pfister","DOI":"10.1109\/TVCG.2018.2865138","Keywords":"Machine Perception;Graphical Perception;Deep Learning;Convolutional Neural Networks","Keywords_Processed":"machine perception;graphical perception;deep learning;convolutional neural networks","Title":"Evaluating \u2018Graphical Perception\u2019 with CNNs","Labels":null,"Keyword_Vector":[0.1339833711,0.1374775249,0.2024929896,0.0824476841,-0.1808115607,0.2797238029,-0.1472625544,-0.266523722,-0.2160126,0.0919325307,0.116969613,0.0307651981,0.0559430994,0.1018634355,0.1248002449,-0.0100848215,0.154657538,-0.1113124995,-0.1104399022,-0.0728262648,0.0942755135,0.0129800996,-0.1815012914,-0.0633282333],"Abstract_Vector":[0.19652353,-0.0251217959,0.1350724398,-0.0220513625,0.1202423921,0.0652839256,-0.0223983893,0.0259987491,0.1737606588,0.0465523743,-0.0697760157,-0.1048938388,-0.0643490256,-0.0435701847,0.0238495509,-0.0717917187,-0.0921596559,0.0959033204,0.0969816147,0.1766334573,-0.0529271135,0.0577530691,0.2094150563,-0.1732862151,0.0149565389,0.0537132521,0.0053334197,0.150715732,0.0087641186,0.0310519096,-0.1553132232,-0.1120084142,-0.0114663194,-0.0525895407,0.0166996751,-0.0466835247,0.0125548708,0.0319602162,0.0689832998,-0.0985370525,0.1030212541,-0.0747629571,0.107113252,-0.0755723378,0.0774616589,0.0221664899,-0.0286920904,0.0649225115,-0.0800500396,0.0480307728,0.0557073955,-0.0373869359,-0.0057317474,0.0235331856,0.1253303258,-0.0126846946,0.0400560794,-0.0076503766,-0.0239230683,-0.0239870207,0.0960862254,-0.0474653817,0.1036220999,-0.0552815268,-0.0038949981,0.0359104025,-0.1309936547,-0.0350748155,0.0328656755,0.0168266693,0.07422347,0.0217731618,-0.0161878856,0.0761155708,0.0191583714,-0.0699414468]},"56":{"Abstract":"In this paper we present a novel Smoothed Particle Hydrodynamics (SPH) method for the efficient and stable simulation of incompressible fluids. The most efficient SPH-based approaches enforce incompressibility either on position or velocity level. However, the continuity equation for incompressible flow demands to maintain a constant density and a divergence-free velocity field. We propose a combination of two novel implicit pressure solvers enforcing both a low volume compression as well as a divergence-free velocity field. While a compression-free fluid is essential for realistic physical behavior, a divergence-free velocity field drastically reduces the number of required solver iterations and increases the stability of the simulation significantly. Thanks to the improved stability, our method can handle larger time steps than previous approaches. This results in a substantial performance gain since the computationally expensive neighborhood search has to be performed less frequently. Moreover, we introduce a third optional implicit solver to simulate highly viscous fluids which seamlessly integrates into our solver framework. Our implicit viscosity solver produces realistic results while introducing almost no numerical damping. We demonstrate the efficiency, robustness and scalability of our method in a variety of complex simulations including scenarios with millions of turbulent particles or highly viscous materials.","Authors":"J. Bender; D. Koschier","DOI":"10.1109\/TVCG.2016.2578335","Keywords":"Fluid simulation;smoothed particle hydrodynamics;divergence-free fluids;incompressibility;viscous fluids;implicit integration","Keywords_Processed":"incompressibility;fluid simulation;divergence free fluid;viscous fluid;smooth particle hydrodynamic;implicit integration","Title":"Divergence-Free SPH for Incompressible and Viscous Fluids","Labels":null,"Keyword_Vector":[0.0165653396,0.0117850103,-0.0072507635,0.0186587729,0.0156605412,-0.0777675534,0.0684948149,-0.0621222273,-0.0474846083,-0.0997301274,0.0052982253,0.1070830076,-0.0041750756,0.0696403694,-0.0615976011,-0.1935886845,0.0476050671,0.0588874216,0.2633444913,0.2266125452,-0.0410380549,0.2242689561,-0.1634847572,0.0676601301],"Abstract_Vector":[0.1658027401,0.1031985445,-0.1165697898,-0.0073261304,-0.0045240289,-0.016558435,0.1042991226,-0.0257817483,-0.0619534514,-0.1851213493,-0.0754469529,-0.0189064359,0.0510274206,0.2493592782,0.0156544098,0.1252033689,-0.0868958001,0.0652044673,-0.0784476304,0.0827988616,0.0051322503,-0.1385295781,-0.0903541985,0.0341192093,0.0169919198,0.0231865583,-0.0645043918,0.1171933999,0.0384720392,-0.1555799368,-0.0528777349,0.0022476716,-0.0282085779,-0.0560672916,0.0097548177,-0.0321199049,0.0530196897,-0.0514204036,0.0058427307,-0.0663683699,-0.0112102535,0.0329331416,0.0962926863,-0.1269827046,0.1888405849,0.0100557527,0.1322781503,-0.0161553103,0.0284662635,-0.0037668015,-0.01321246,-0.1401329786,-0.0247248079,-0.0168598065,-0.0650546173,0.1047697432,0.0757874235,-0.0235654554,-0.0657289879,-0.0833257409,0.0201260693,-0.0345140942,0.041423267,-0.0228459548,-0.0043923576,-0.0934794919,-0.0508635447,-0.0364194353,-0.0288469633,0.0751463634,-0.0571263261,0.053595967,0.000954081,0.0475376342,0.09768188,-0.0584365389]},"57":{"Abstract":"Previous perceptual research and human factors studies have identified several effective methods for texturing 3D surfaces to ensure that their curvature is accurately perceived by viewers. However, most of these studies examined the application of these techniques to static surfaces. This paper explores the effectiveness of applying these techniques to dynamically changing surfaces. When these surfaces change shape, common texturing methods, such as grids and contours, induce a range of different motion cues, which can draw attention and provide information about the size, shape, and rate of change. A human factors study was conducted to evaluate the relative effectiveness of these methods when applied to dynamically changing pseudo-terrain surfaces. The results indicate that, while no technique is most effective for all cases, contour lines generally perform best, and that the pseudo-contour lines induced by banded color scales convey the same benefits.","Authors":"T. Butkiewicz; A. H. Stevens","DOI":"10.1109\/TVCG.2015.2467962","Keywords":"Structured textures;terrain;deformation;dynamic surfaces;Structured textures;terrain;deformation;dynamic surfaces","Keywords_Processed":"deformation;terrain;structured texture;dynamic surface","Title":"Effectiveness of Structured Textures on Dynamically Changing Terrain-like Surfaces","Labels":null,"Keyword_Vector":[0.0381732266,-0.0009729682,0.0228602704,0.0382379028,0.0109887895,-0.0718579757,0.1337408219,-0.0770653695,-0.0027100931,0.0390850158,0.0509687193,0.1302588895,-0.011120636,0.0715134519,0.0253835973,0.0058557392,-0.1402236242,0.0013288184,0.093142376,0.045600131,0.0485179086,0.0170198391,-0.0254420746,-0.0423210449],"Abstract_Vector":[0.2270613107,0.1409950439,-0.069618251,-0.0487306515,-0.048439341,0.1454895279,0.0227721471,-0.0327635449,0.1061814735,0.0451238535,0.0639636205,-0.1053239578,-0.182780424,-0.017304748,0.0327598078,-0.0136599024,-0.1123004002,0.0921030825,0.1065143392,0.1138896239,-0.112310852,0.0296713806,-0.006642787,0.0189868546,-0.1598311346,-0.193427366,-0.0012874321,-0.0810750826,-0.0329420034,-0.0104346628,0.0349271226,0.0655559256,0.0674822719,-0.1205875761,0.0234942119,-0.0871260908,-0.005947126,0.0080183755,-0.0221597456,0.0782888528,0.0115575564,-0.064916055,-0.0892546993,-0.1249860342,-0.0620216955,-0.0328719286,-0.0132426774,-0.0559853913,-0.0261018308,0.0249638113,-0.006814751,0.0001670343,-0.0508139298,0.0568563268,-0.0427523888,-0.1017858062,0.0208609334,-0.109356142,-0.0890896819,0.1012349799,0.0999959611,0.0230120518,-0.1064179587,-0.0565651189,-0.0058759524,0.1452784899,0.0469181431,-0.010547769,-0.0837740325,0.0782303578,0.019218472,0.0003300886,-0.0777905861,-0.0048294874,0.1180600826,0.0424156548]},"58":{"Abstract":"We present a formal approach to the visual analysis of recirculation in flows by introducing recirculation surfaces for 3D unsteady flow fields. Recirculation surfaces are the loci where massless particle integration returns to its starting point after some variable, finite integration. We give a rigorous definition of recirculation surfaces as 2-manifolds embedded in 5D space and study their properties. Based on this we construct an algorithm for their extraction, which searches for intersections of a recirculation surface with lines defined in 3D. This reduces the problem to a repeated search for critical points in 3D vector fields. We provide a uniform sampling of the search space paired with a surface reconstruction and visualize results. This way, we present the first algorithm for a comprehensive feature extraction in the 5D flow map of a 3D flow. The problem of finding isolated closed orbits in steady vector fields occurs as a special case of recirculation surfaces. This includes isolated closed orbits with saddle behavior. We show recirculation surfaces for a number of artificial and real flow data sets.","Authors":"T. Wilde; C. R\u00f6ssi; H. Theisel","DOI":"10.1109\/TVCG.2018.2864813","Keywords":"Flow visualization;recirculation;unsteady flow","Keywords_Processed":"recirculation;flow visualization;unsteady flow","Title":"Recirculation Surfaces for Flow Visualization","Labels":null,"Keyword_Vector":[0.1648879303,-0.0626632124,-0.1299736777,-0.0357692558,-0.1087459626,-0.1702432245,0.0141379666,-0.1115633161,0.0315946682,0.0767147852,0.4188660954,0.2966870255,-0.0829773222,-0.14151139,0.0721357696,-0.0885927689,0.0447750424,-0.0341354884,0.0006748574,0.0945944804,-0.0840164029,-0.0328917609,0.26360562,-0.1386916131],"Abstract_Vector":[0.1758845634,0.1129367076,-0.1534521225,0.0354865863,0.0239395039,0.1220039784,0.1810975915,-0.1607512126,-0.0107359743,0.0580416448,0.1899346738,0.0713011017,0.0406532743,0.0215193244,0.0671842661,-0.0322329419,-0.0327684578,0.1783338128,0.0366466624,0.1012163243,-0.0160445389,0.0432859239,-0.0567055107,0.105228071,-0.0129075389,-0.0417081333,-0.0859247456,-0.1470678665,0.1040203815,0.0288828064,0.0331191848,-0.0028221025,-0.0243517911,-0.06118414,0.0116149895,-0.0631302989,-0.0666095163,0.0909782336,-0.0215594681,-0.0307956425,0.0567206724,-0.0622124975,-0.085356346,-0.0506825801,-0.0452176364,-0.0533014483,-0.0074021727,-0.0369940799,-0.0540220663,0.0460616663,0.0345158836,-0.1172417775,-0.0811474001,-0.0559924748,-0.0221929724,0.0308243914,-0.0076554913,0.029666399,0.0018511746,-0.0101367625,0.0056595555,-0.0162480255,0.074901909,0.0639060837,0.0249483082,-0.0037461528,-0.1007761077,0.0307856845,0.0369458549,0.1467015363,0.0473536444,-0.0124429144,-0.0244689092,-0.0389653827,-0.0169558513,0.0262021608]},"59":{"Abstract":"This paper presents a declarative grammar for conveniently and effectively specifying advanced volume visualizations. Existing methods for creating volume visualizations either lack the flexibility to specify sophisticated visualizations or are difficult to use for those unfamiliar with volume rendering implementation and parameterization. Our design provides the ability to quickly create expressive visualizations without knowledge of the volume rendering implementation. It attempts to capture aspects of those difficult but powerful methods while remaining flexible and easy to use. As a proof of concept, our current implementation of the grammar allows users to combine multiple data variables in various ways and define transfer functions for diverse input data. The grammar also has the ability to describe advanced shading effects and create animations. We demonstrate the power and flexibility of our approach using multiple practical volume visualizations.","Authors":"M. Shih; C. Rozhon; K. Ma","DOI":"10.1109\/TVCG.2018.2864841","Keywords":"Volume visualization;direct volume rendering;declarative specification;multivariate\/multimodal volume data;animation","Keywords_Processed":"multivariate multimodal volume datum;animation;volume visualization;declarative specification;direct volume rendering","Title":"A Declarative Grammar of Flexible Volume Visualization Pipelines","Labels":null,"Keyword_Vector":[0.2148864483,-0.0958046328,-0.1256560845,0.4002378126,0.4181378098,0.3076158905,-0.0493260927,-0.0818364853,0.0744918391,0.0419090849,0.069739578,0.0146210761,-0.0156357408,-0.1077682661,-0.0565834773,-0.1375594223,0.1160662664,0.0125694522,0.0169476871,-0.0100165958,-0.0371832399,-0.0543080283,0.0325305001,0.0288017202],"Abstract_Vector":[0.2412244787,-0.037522567,0.03895496,-0.0716676151,0.0589373,0.0819449305,-0.0259614666,0.0861864542,-0.1453553383,-0.2001429434,-0.0281925585,-0.1097021359,0.0110966558,0.0684266544,-0.0636935137,-0.0558293561,0.1294754429,-0.0458940976,-0.1031483516,-0.0790180399,-0.0069446626,0.0746751638,0.1061812383,0.0340493443,-0.0290680335,-0.0631797786,0.0058306611,-0.0634215352,-0.1214026173,0.0517412938,-0.1139066274,-0.0798396907,-0.0097372265,-0.0721641099,-0.1031057726,-0.0334327756,0.1371694269,0.007454138,0.0176760697,0.0152263927,0.0027361142,0.0221373333,-0.0658654154,0.093079419,0.0494942585,-0.0475502688,-0.0587121363,-0.0582655687,-0.0187842651,-0.043851313,-0.0106270188,0.127331659,0.0000323557,0.0391450448,0.0226835564,0.0167048688,-0.1267858673,0.0433342607,0.0212400078,0.0683505881,0.0158754217,-0.0358751921,0.0387240229,0.0551736171,0.0063465869,-0.0106311999,-0.0952459752,0.0111525548,0.0178906915,0.0838465373,0.1150601927,0.0964306374,-0.0553838549,-0.0969597522,0.0006589919,-0.0750866349]},"60":{"Abstract":"Visualization tools are often specialized for specific tasks, which turns the user's analytical workflow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components-the building blocks of this model-can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch using JavaScript or Python source code. To realize our model, we introduce Vistrates, a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present several Vistrates use cases that span the full range of the classic \u201canytime\u201d and \u201canywhere\u201d motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices..","Authors":"S. K. Badam; A. Mathisen; R. R\u00e4dle; C. N. Klokmose; N. Elmqvist","DOI":"10.1109\/TVCG.2018.2865144","Keywords":"Components;literate computing;development;exploration;dissemination;collaboration;heterogeneous devices","Keywords_Processed":"literate computing;dissemination;component;exploration;collaboration;heterogeneous device;development","Title":"Vistrates: A Component Model for Ubiquitous Analytics","Labels":null,"Keyword_Vector":[0.0379991543,0.0018355203,0.0050101375,0.0238401086,0.0424491408,-0.0033503922,0.0120558439,0.0558021195,-0.0165047109,0.0961137932,-0.0661576367,0.0286716164,0.0099780464,-0.0169052816,0.0894928341,-0.0746178828,0.0315516565,-0.0679208777,0.0338255128,0.0356853249,-0.221602655,-0.1326992182,-0.0549342834,0.0335528971],"Abstract_Vector":[0.2355001107,-0.0747388817,0.1258071589,-0.0689990881,0.212041768,0.0211269852,-0.0883883738,-0.0095241852,-0.0869983797,-0.0314215358,0.034302714,-0.1138687242,0.0820789877,0.0200938253,-0.0888919986,-0.0175764084,-0.0222487674,0.044649789,0.0161682486,-0.0060369618,0.0073927087,-0.0797272515,-0.0257326951,-0.0121439881,0.0149098179,0.044646905,0.0264975365,-0.0702788498,-0.0037662044,0.1177715482,0.0808655508,0.0125680941,-0.101386682,0.0342325527,0.0584039681,0.0091350544,0.0169602317,-0.008909463,0.0292125248,-0.0135143085,-0.002288342,-0.0244383935,0.0911800761,-0.0238266742,-0.0752672902,-0.0830330963,-0.1160798299,-0.0088630831,-0.0252986083,-0.0246257698,-0.1090266456,0.0269655723,0.0376739073,-0.0252961562,0.0819588583,-0.0311317437,-0.15262885,-0.0059198622,-0.1506815363,0.0629708614,-0.00403841,0.0547510977,0.0763051021,-0.0270089217,-0.0261004362,-0.0643034907,0.0365896944,0.0689330818,0.1012212842,0.0630254191,-0.1219633777,-0.073424419,-0.0162586193,0.1610331208,0.09425817,-0.049863005]},"61":{"Abstract":"Shading is a tedious process for artists involved in 2D cartoon and manga production given the volume of contents that the artists have to prepare regularly over tight schedule. While we can automate shading production with the presence of geometry, it is impractical for artists to model the geometry for every single drawing. In this work, we aim to automate shading generation by analyzing the local shapes, connections, and spatial arrangement of wrinkle strokes in a clean line drawing. By this, artists can focus more on the design rather than the tedious manual editing work, and experiment with different shading effects under different conditions. To achieve this, we have made three key technical contributions. First, we model five perceptual cues by exploring relevant psychological principles to estimate the local depth profile around strokes. Second, we formulate stroke interpretation as a global optimization model that simultaneously balances different interpretations suggested by the perceptual cues and minimizes the interpretation discrepancy. Lastly, we develop a wrinkle-aware inflation method to generate a height field for the surface to support the shading region computation. In particular, we enable the generation of two commonly-used shading styles: 3D-like soft shading and manga-style flat shading.","Authors":"P. K. Jayaraman; C. Fu; J. Zheng; X. Liu; T. Wong","DOI":"10.1109\/TVCG.2017.2705182","Keywords":"Shading;perception;inflation;manga;cartoon","Keywords_Processed":"shade;inflation;cartoon;perception;manga","Title":"Globally Consistent Wrinkle-Aware Shading of Line Drawings","Labels":null,"Keyword_Vector":[0.0504085577,0.1002316734,0.0431827616,-0.030342402,-0.0318461379,0.1371159546,0.0030783601,-0.1421663008,-0.0910015297,0.0545680199,0.056056137,-0.0142284708,0.0354762701,0.0456280751,0.140497399,0.0550226637,-0.0318539147,0.0043449614,0.009510644,-0.0173199016,0.0350537794,0.0112062924,-0.0620582254,-0.0474863173],"Abstract_Vector":[0.16427297,0.1107428445,-0.0267519089,-0.0065648696,0.1111854717,0.0387530851,-0.0137062677,-0.0517458471,0.0079948318,-0.0007049474,0.1040788406,-0.0024775544,-0.0363788823,0.0258782477,-0.0035008403,-0.0241301818,-0.0439142259,0.0620360761,0.0862393427,-0.1027379057,-0.135425901,0.0791285841,0.1330843984,-0.0369937503,0.0191073308,-0.0073049338,-0.0993165583,-0.019204872,-0.1998279175,-0.1226769155,0.0758379681,-0.0956226716,0.0281782917,0.0593786035,-0.0666200848,-0.0001730912,0.0740941289,-0.0015687866,-0.1243642235,0.0819721432,0.013284829,0.0676013632,0.0804472874,0.0869604644,0.0958685248,0.0098196193,-0.0392477997,0.0557599264,0.1228131972,-0.0287367709,-0.0857232382,0.2810729401,-0.05263979,0.0292932541,-0.1669752965,-0.1247671904,0.0454960225,0.0040072116,-0.0126995399,-0.0369046959,-0.0163453311,0.0942882061,0.0613567719,0.0459419555,0.0795920885,-0.0692282346,-0.0133035565,-0.0516109125,0.0314358019,-0.0028257283,0.1620229658,0.0050498755,-0.0566999565,0.0372741496,-0.0127473638,0.0370618289]},"62":{"Abstract":"In this paper, we present a novel pairwise-force smoothed particle hydrodynamics (PF-SPH) model to enable simulation of various interactions at interfaces in real time. Realistic capture of interactions at interfaces is a challenging problem for SPH-based simulations, especially for scenarios involving multiple interactions at different interfaces. Our PF-SPH model can readily handle multiple types of interactions simultaneously in a single simulation; its basis is to use a larger support radius than that used in standard SPH. We adopt a novel anisotropic filtering term to further improve the performance of interaction forces. The proposed model is stable; furthermore, it avoids the particle clustering problem which commonly occurs at the free surface. We show how our model can be used to capture various interactions. We also consider the close connection between droplets and bubbles, and show how to animate bubbles rising in liquid as well as bubbles in air. Our method is versatile, physically plausible and easy-to-implement. Examples are provided to demonstrate the capabilities and effectiveness of our approach.","Authors":"T. Yang; R. R. Martin; M. C. Lin; J. Chang; S. Hu","DOI":"10.1109\/TVCG.2017.2706289","Keywords":"Smoothed particle hydrodynamics (SPH);pairwise force;surface tension;bubble animation;fluid simulation","Keywords_Processed":"smoothed particle hydrodynamic sph;bubble animation;fluid simulation;surface tension;pairwise force","Title":"Pairwise Force SPH Model for Real-Time Multi-Interaction Applications","Labels":null,"Keyword_Vector":[0.032346616,0.0144867652,-0.0101327442,0.070157128,0.0826054051,-0.1063092141,0.1219405899,-0.1147540109,-0.0900192989,-0.1083338959,0.045974351,0.065022065,0.049398367,0.043010608,-0.0042277762,-0.1899322577,0.020679355,-0.0380335409,0.2992741282,0.1642410084,-0.0126883164,0.2201297504,-0.1228510025,0.0424191731],"Abstract_Vector":[0.2304988526,0.0946888703,-0.0891223215,-0.0538919049,0.0665725839,-0.1080889918,0.0174298022,0.0195001304,0.0034850507,-0.1396928531,-0.0582636057,-0.0622902536,0.0207008219,0.2832338244,-0.0836156952,0.1411273327,-0.125298325,0.0716515212,-0.1708435488,0.1235552088,-0.0011077139,-0.1930691869,-0.1052835228,0.0313735715,-0.0723764414,-0.0913502667,-0.0378204085,0.0405508414,-0.0476686162,-0.0859720222,-0.0328502609,-0.0766155425,-0.0516227372,-0.0390288331,-0.0405850542,0.0249067676,-0.147232515,-0.028721438,0.0076612325,-0.0679711642,0.0228199509,0.0073982591,0.013811915,0.1199863979,0.0406010505,0.0530856957,0.0067781637,-0.0872685487,0.0631393744,0.0712666471,-0.0465069014,-0.0273364846,-0.0300011299,0.0020919572,-0.0689158908,0.0852233649,-0.0104004967,0.0528516512,0.0134036609,0.0711512296,0.0141756245,-0.0481470482,0.0282500198,-0.014395006,-0.0265114054,0.0651614935,-0.0438071286,-0.1082503346,0.0615309784,0.0267814017,0.008214546,-0.0571274659,0.0300318116,-0.0239448136,-0.017121609,-0.0582287275]},"63":{"Abstract":"Details-on-demand is a crucial feature in the visual information-seeking process but is often only implemented in highly constrained settings. The most common solution, hover queries (i.e., tooltips), are fast and expressive but are usually limited to single mark (e.g., a bar in a bar chart). `Queries' to retrieve details for more complex sets of objects (e.g., comparisons between pairs of elements, averages across multiple items, trend lines, etc.) are difficult for end-users to invoke explicitly. Further, the output of these queries require complex annotations and overlays which need to be displayed and dismissed on demand to avoid clutter. In this work we introduce SmartCues, a library to support details-on-demand through dynamically computed overlays. For end-users, SmartCues provides multitouch interactions to construct complex queries for a variety of details. For designers, SmartCues offers an interaction library that can be used out-of-the-box, and can be extended for new charts and detail types. We demonstrate how SmartCues can be implemented across a wide array of visualization types and, through a lab study, show that end users can effectively use SmartCues.","Authors":"H. Subramonyam; E. Adar","DOI":"10.1109\/TVCG.2018.2865231","Keywords":"Graphical overlays;details-on-demand;graph comprehension","Keywords_Processed":"graph comprehension;graphical overlay;detail on demand","Title":"SmartCues: A Multitouch Query Approach for Details-on-Demand through Dynamically Computed Overlays","Labels":null,"Keyword_Vector":[0.0526016223,0.0097951646,0.0075965403,0.0042229118,-0.0787521613,0.0718641847,0.0230837309,-0.0541087906,-0.0855512479,0.1713250238,-0.0665330392,0.0063902185,-0.0146697063,-0.0558418738,0.0552367541,-0.0351578975,0.0056052174,0.1092046622,0.0417283821,-0.079374796,0.0539969609,-0.0521927244,-0.0184634534,-0.0151381115],"Abstract_Vector":[0.1812959387,0.0005882739,0.0446331371,0.0099348713,0.0154525341,-0.008130602,0.0010609402,0.0255880417,-0.0117289984,-0.0806908339,0.0241746508,-0.0538796279,0.0667728769,-0.0121201409,0.0120220174,-0.009545469,0.0922905694,-0.0683567536,-0.1077143884,-0.0018589701,-0.0797362522,-0.070164324,-0.0538860394,-0.0109564555,-0.0041465716,-0.0457961349,-0.0542087271,-0.013886308,-0.0390778984,-0.0715968063,0.0814705821,0.032649051,0.0243577883,0.0075537477,0.0151642938,0.0058921905,-0.0312631064,0.0043102415,-0.0999842061,0.1093811763,-0.0576250173,-0.0428839551,-0.0474549805,0.0568991485,-0.0083774524,-0.0820559133,0.0906090947,-0.0515867812,-0.0990493672,0.1915645134,0.0321244024,-0.1046135913,-0.0739767189,-0.0035736087,-0.1062933161,-0.0416312767,0.0014254786,0.0180935029,0.0052493226,0.1474509714,0.1063359208,-0.0409756979,0.0384949481,-0.1080462863,-0.0628520082,-0.1008053242,-0.0330464184,0.1177898947,0.1014603697,0.0206275048,-0.060408245,-0.0686942577,-0.004577569,-0.0602518585,-0.121261967,-0.0177914323]},"64":{"Abstract":"Traditional methods in graphics to simulate liquid-air dynamics under different scenarios usually employ separate approaches with sophisticated interface tracking\/reconstruction techniques. In this paper, we propose a novel unified approach which is easy and effective to produce a variety of liquid-air interface phenomena. These phenomena, such as complex surface splashes, bubble interactions, as well as surface tension effects, can co-exist in one single simulation, and are created within the same computational framework. Such a framework is unique in that it is free from any complicated interface tracking\/reconstruction procedures. Our approach is developed from the two-phase lattice Boltzmann method with the mean field model, which provides a unified framework for interface dynamics but is numerically unstable under turbulent conditions. Considering the drawbacks of the existing approaches, we propose techniques to suppress oscillations for significant stability enhancement, as well as derive a new subgrid-scale model to further improve stability, faithfully preserving liquid-air interface details without excessive diffusion by taking into account the density variation. The whole framework is highly parallel, enabling very efficient implementation. Comparisons with the related approaches show superiority on stable simulations with detail preservation and multiphase phenomena simultaneously involved. A set of animation results demonstrate the effectiveness of our method.","Authors":"Y. Guo; X. Liu; X. Xu","DOI":"10.1109\/TVCG.2016.2532335","Keywords":"Flow simulation;two-phase lattice Boltzmann method;interface flow","Keywords_Processed":"two phase lattice boltzmann method;flow simulation;interface flow","Title":"A Unified Detail-Preserving Liquid Simulation by Two-Phase Lattice Boltzmann Modeling","Labels":null,"Keyword_Vector":[0.0815196381,0.0032073761,-0.0440018523,-0.0182299517,-0.0430432277,-0.220037225,0.0533908832,-0.0739780278,0.0174410381,0.0318128718,0.420607888,0.3371767464,-0.0487615984,-0.0518081457,0.0812665926,-0.1680506661,0.092401453,0.0597517958,0.1021047436,0.1257890353,-0.1108528447,0.0453409991,0.1386253207,-0.0525219916],"Abstract_Vector":[0.2565064083,0.1037894949,-0.135566882,-0.0388198496,0.0538369498,-0.0352994036,0.0806247215,-0.0322037869,0.0211107367,-0.161670042,-0.0388329681,-0.0499161232,0.0226747919,0.2403409995,-0.0440475235,0.1353466273,-0.0895126719,0.0790326821,-0.0967816385,0.0630124682,0.036078637,-0.2078272369,-0.1038678616,0.0234960851,-0.1335408835,-0.016663674,-0.018082557,0.0065999406,-0.015793177,-0.027097699,-0.1093701077,-0.0656213502,-0.0585523127,-0.1527859544,-0.0274880782,0.0418481357,0.0012530167,0.0676135512,0.0363011126,-0.0659472964,-0.0112622764,0.0355677037,0.0068859227,0.0737369055,0.0202165913,-0.0003982455,0.0171168423,-0.0505482818,-0.0054145087,-0.0293890866,0.0865239983,0.0595207224,-0.0472041272,0.071123326,-0.110036338,-0.0994259074,0.002443094,0.0053311932,0.0699395473,0.0924423656,-0.0239048094,0.1133533272,-0.1013704179,-0.0379324664,-0.0391004666,-0.0086512742,0.0394697473,-0.0237318168,-0.0281410095,-0.075658921,-0.0030256445,0.0089075828,0.0206819648,-0.0827933311,-0.0389259739,0.0268245779]},"65":{"Abstract":"The increasing interest for reliable generation of large scale scenes and objects has facilitated several real-time applications. Although the resolution of the new generation geometry scanners are constantly improving, the output models, are inevitably noisy, requiring sophisticated approaches that remove noise while preserving sharp features. Moreover, we no longer deal exclusively with individual shapes, but with entire scenes resulting in a sequence of 3D surfaces that are affected by noise with different characteristics due to variable environmental factors (e.g., lighting conditions, orientation of the scanning device). In this work, we introduce a novel coarse-to-fine graph spectral processing approach that exploits the fact that the sharp features reside in a low dimensional structure hidden in the noisy 3D dataset. In the coarse step, the mesh is processed in parts, using a model based Bayesian learning method that identifies the noise level in each part and the subspace where the features lie. In the feature-aware fine step, we iteratively smooth face normals and vertices, while preserving geometric features. Extensive evaluation studies carried out under a broad set of complex noise patterns verify the superiority of our approach as compared to the state-of-the-art schemes, in terms of reconstruction quality and computational complexity.","Authors":"G. Arvanitis; A. S. Lalos; K. Moustakas; N. Fakotakis","DOI":"10.1109\/TVCG.2018.2802926","Keywords":"Spectral smoothing;orthogonal iteration;spectral denoising filtering;feature extraction;level noise estimation","Keywords_Processed":"feature extraction;spectral denois filtering;spectral smoothing;level noise estimation;orthogonal iteration","Title":"Feature Preserving Mesh Denoising Based on Graph Spectral Processing","Labels":null,"Keyword_Vector":[0.0239693114,0.0038689146,0.0062800619,-0.0017808931,0.0254498116,-0.0594495639,0.0480689538,-0.0447000491,0.0126366996,0.021958416,0.0553859547,0.08682812,-0.0842762509,0.0361468986,-0.024649402,0.0107330593,-0.0510264519,-0.0052596372,-0.0570070356,-0.0996584291,-0.0060008492,-0.0039793906,-0.1524052918,-0.0373713645],"Abstract_Vector":[0.2792986642,0.1517028806,-0.1719949046,0.0152581329,0.1106122212,-0.0477268251,-0.014553088,-0.0317445083,-0.0068223907,0.1167157881,-0.1462511562,-0.026572477,0.1230509046,-0.1235260879,0.1155076619,-0.0432555612,-0.1241285346,0.0637856499,-0.1117242709,-0.0578628262,0.1071654283,0.036624972,0.0344962541,0.108506506,0.0797588687,0.0543501641,0.1264607725,-0.0961537415,-0.02930334,-0.1328136558,0.1084421951,-0.025891707,0.0858654133,-0.0103973939,0.0265113962,0.0898099353,0.0215108269,0.071371207,0.1211823573,-0.0472712318,0.0261688525,-0.0171019151,-0.0427731513,-0.0082735276,-0.1037478521,-0.0229196985,-0.0486671687,-0.1059520566,0.0531437467,0.0004268877,0.0450373963,0.0081628983,0.0986664181,0.0039156572,-0.0448284732,-0.1222892874,-0.0067446271,0.0243981657,0.1386355232,0.0080603873,0.0915176149,-0.0638541302,0.0062286879,-0.0407570125,0.0067420792,0.0058572466,0.051455466,0.0075616269,-0.0758968801,-0.0275837643,-0.0686587159,0.0696251075,0.0047698915,0.003758885,0.0133768196,0.1399855636]},"66":{"Abstract":"When motion capture data is applied to virtual characters, the applied motion often exhibits geometric and physical errors, which necessitates a cumbersome refinement process. We present a novel framework to efficiently obtain a corrected motion as well as its supporting contact information from multi-contact motion capture data. To this end, first, we present a projective dynamics-based method for optimizing character motions. By carefully defining objective functions and constraints using differential representation of motions, we develop a highly efficient motion optimizer that can create geometrically and dynamically adjusted motions given reference motion data and contact information. Second, we develop a contact optimizer that finds a set of contacts that allows the motion optimizer to generate a motion that best follows the reference motion under dynamic and geometric constraints. This is achieved by iteratively improving the hypothesis on the best set of contacts by getting feedback from the motion optimizer. We demonstrate that our method significantly improves the naturalness of a wide range of motion capture data, from walking to rolling.","Authors":"S. Lee; S. Lee","DOI":"10.1109\/TVCG.2018.2818721","Keywords":"Character animation;motion capture;motion retargeting;multi-contact","Keywords_Processed":"motion retargeting;multi contact;character animation;motion capture","Title":"Projective Motion Correction with Contact Optimization","Labels":null,"Keyword_Vector":[0.0525661379,0.1033668317,0.0392640579,0.0737687129,0.1618164197,-0.150759599,0.187358219,-0.1601116726,-0.1288743778,-0.1635430455,-0.0738072331,-0.1965800664,0.150893859,-0.3334640276,0.0998401869,-0.167163372,0.1425806668,-0.1714543098,-0.132311697,-0.1928774063,-0.1373761116,0.1212524414,0.0886608784,-0.0018023459],"Abstract_Vector":[0.199207638,0.1838654591,0.0196867376,-0.2347293585,-0.2896553015,-0.1221111621,-0.0292343082,-0.2143936052,0.0764020282,-0.2369416043,-0.1422935152,-0.0493457086,0.0619645149,-0.2063425791,-0.1064317897,0.0552486131,0.1093746122,0.0245258586,0.0472385865,-0.0319373782,0.0415168478,0.1032050506,-0.0463774774,-0.0965625481,-0.1427679738,-0.0115334406,-0.0222240593,0.0316231121,-0.048026803,0.0071053123,0.1051246745,0.0136595897,0.0439958964,-0.0613756093,0.0511465304,-0.0319722953,-0.0241059993,-0.082424446,-0.0673757698,-0.0255977187,-0.0116461468,0.0017102151,0.0028100129,0.0876913266,-0.0983156964,-0.0369586846,-0.0338027134,0.0584468619,-0.0330529288,0.0135277517,0.0079039803,-0.0130586271,-0.0379931148,-0.0111685188,-0.0042509553,0.038041683,-0.0102532692,0.0046351587,-0.1090323057,0.0156026834,-0.0361962716,0.0796921745,0.0542544818,-0.0285573581,0.034482639,-0.0046907063,-0.0081789885,-0.0464545125,-0.0714151484,-0.0017237479,0.0149513815,0.0177279198,-0.0247597152,-0.0012445077,-0.0464117694,-0.017948699]},"67":{"Abstract":"Accommodative depth cues, a wide field of view, and ever-higher resolutions all present major hardware design challenges for near-eye displays. Optimizing a design to overcome one of these challenges typically leads to a trade-off in the others. We tackle this problem by introducing an all-in-one solution - a new wide field of view, gaze-tracked near-eye display for augmented reality applications. The key component of our solution is the use of a single see-through, varifocal deformable membrane mirror for each eye reflecting a display. They are controlled by airtight cavities and change the effective focal power to present a virtual image at a target depth plane which is determined by the gaze tracker. The benefits of using the membranes include wide field of view (100\u00b0 diagonal) and fast depth switching (from 20 cm to infinity within 300 ms). Our subjective experiment verifies the prototype and demonstrates its potential benefits for near-eye see-through displays.","Authors":"D. Dunn; C. Tippets; K. Torell; P. Kellnhofer; K. Ak\u015fit; P. Didyk; K. Myszkowski; D. Luebke; H. Fuchs","DOI":"10.1109\/TVCG.2017.2657058","Keywords":"Augmented reality;displays;focus accommodation;perception;user study","Keywords_Processed":"augmented reality;display;focus accommodation;perception;user study","Title":"Wide Field Of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors","Labels":null,"Keyword_Vector":[0.2157641865,0.3768786419,0.0170361668,-0.0729276182,-0.1079411316,0.1513130877,0.0609309905,0.0654114254,-0.0505617849,0.0487335311,-0.028864123,0.0568087448,0.0013782037,-0.0144528252,0.2361819402,0.0463225652,0.0323577006,0.1033406963,-0.0999972497,0.093384924,-0.1251037451,-0.0572343497,0.0376100822,-0.1287945448],"Abstract_Vector":[0.175457087,0.1232879692,0.1809023139,0.0977144295,0.0308370385,0.0531464033,0.0541708255,0.003456809,-0.0164357151,-0.0196351629,0.0243736726,-0.020217384,0.0933012308,-0.1001434203,0.0506176165,-0.0435997938,-0.092803552,-0.1067385388,0.141965025,-0.0214978288,0.0551664642,-0.1146505622,-0.0545996584,0.0411213699,-0.0700273914,-0.0163264873,-0.1531834639,-0.0286084501,0.0893265057,0.0357383269,-0.0027297494,-0.0664985441,-0.0581092648,0.0664808823,-0.0938243879,0.1374130092,0.0626818134,0.0215761606,-0.0478173574,0.0275542507,0.0374086015,-0.0035334836,0.0574823362,0.00099901,0.2066082451,-0.078952302,-0.0988984581,-0.1389441362,-0.0357053791,-0.121774236,-0.0867183243,-0.0808783263,-0.0873642074,0.0243669015,0.0078918851,-0.0766180576,0.0788074509,-0.1001795572,-0.0980214322,0.1060468689,-0.1152306356,0.0482444126,-0.0432598496,0.0347706878,-0.0602413692,-0.0399995799,0.0602131908,-0.038268239,0.0473954473,0.0968628,0.1146023501,0.1525113822,0.0265802068,0.0243050108,-0.0258218421,-0.0113103691]},"68":{"Abstract":"In order to effectively infer correct labels from noisy crowdsourced annotations, learning-from-crowds models have introduced expert validation. However, little research has been done on facilitating the validation procedure. In this paper, we propose an interactive method to assist experts in verifying uncertain instance labels and unreliable workers. Given the instance labels and worker reliability inferred from a learning-from-crowds model, candidate instances and workers are selected for expert validation. The influence of verified results is propagated to relevant instances and workers through the learning-from-crowds model. To facilitate the validation of annotations, we have developed a confusion visualization to indicate the confusing classes for further exploration, a constrained projection method to show the uncertain labels in context, and a scatter-plot-based visualization to illustrate worker reliability. The three visualizations are tightly integrated with the learning-from-crowds model to provide an iterative and progressive environment for data validation. Two case studies were conducted that demonstrate our approach offers an efficient method for validating and improving crowdsourced annotations.","Authors":"S. Liu; C. Chen; Y. Lu; F. Ouyang; B. Wang","DOI":"10.1109\/TVCG.2018.2864843","Keywords":"Crowdsourcing;learning-from-crowds;interactive visualization;focus + context","Keywords_Processed":"focus context;interactive visualization;crowdsourcing;learn from crowd","Title":"An Interactive Method to Improve Crowdsourced Annotations","Labels":null,"Keyword_Vector":[0.1786572067,-0.0434471299,-0.1230278137,0.0163422738,-0.1071803967,-0.0731102182,0.0813402999,-0.0060363126,-0.019388288,0.2061584374,-0.1276068213,-0.1682399625,-0.1892171114,0.1997089033,0.0977836972,-0.0929407738,0.0491490673,-0.0378586713,0.03135085,-0.0111204442,-0.1206403416,0.1012236654,0.0841912326,-0.0875373614],"Abstract_Vector":[0.1800037743,-0.0303344073,-0.0002517507,-0.0387352663,0.0832819255,0.0105038638,-0.1711850333,0.0948881263,0.0964971777,-0.0980976055,0.0589570222,0.0866541361,0.1414368158,0.0350640918,0.1408712699,-0.0582162527,0.0949711137,0.0184838614,0.0248020929,0.129357937,-0.0409964325,-0.0456422577,-0.0121377577,-0.0153355837,-0.1791536852,0.0458750886,0.1202798473,0.0392879114,-0.0136656972,-0.0896748774,0.0527293515,0.0540004581,-0.0772508804,0.0000811236,0.085538775,-0.0763294376,-0.0313709109,-0.0384183504,-0.0543813205,-0.0051869025,-0.0086728773,-0.0438819708,0.0022662342,-0.0254803976,-0.0064842691,0.018472902,0.0478743492,-0.0142854978,0.1258494056,0.0152905686,0.0316879412,0.0562305498,0.0286062505,-0.0782280439,-0.0740115164,-0.0366277147,-0.0038640424,0.0560509234,0.1067790563,0.0059368199,-0.0536823887,-0.0028519774,-0.0676984717,0.1158554231,-0.0672921895,-0.0725563392,0.0825893862,-0.006438101,-0.1516240061,-0.1276828484,-0.0267377568,0.128607707,-0.058926733,0.0539776399,-0.0110931932,-0.0477874118]},"69":{"Abstract":"Occlusions are a severe bottleneck for the visualization of large and complex datasets. Conventional images only show dataset elements to which there is a direct line of sight, which significantly limits the information bandwidth of the visualization. Multiperspective visualization is a powerful approach for alleviating occlusions to show more than what is visible from a single viewpoint. However, constructing and rendering multiperspective visualizations is challenging. We present a framework for designing multiperspective focus+context visualizations with great flexibility by manipulating the underlying camera model. The focus region viewpoint is adapted to alleviate occlusions. The framework supports multiperspective visualization in three scenarios. In a first scenario, the viewpoint is altered independently for individual image regions to avoid occlusions. In a second scenario, conventional input images are connected into a multiperspective image. In a third scenario, one or several data subsets of interest (i.e., targets) are visualized where they would be seen in the absence of occluders, as the user navigates or the targets move. The multiperspective images are rendered at interactive rates, leveraging the camera model's fast projection operation. We demonstrate the framework on terrain, urban, and molecular biology geometric datasets, as well as on volume rendered density datasets.","Authors":"M. Wu; V. Popescu","DOI":"10.1109\/TVCG.2015.2443804","Keywords":"Occlusion management;camera models;multiperspective visualization;interactive visualization;focus+context;Occlusion management;camera models;multiperspective visualization;interactive visualization;focus+context","Keywords_Processed":"multiperspective visualization;focus context;interactive visualization;occlusion management;camera model","Title":"Multiperspective Focus+Context Visualization","Labels":null,"Keyword_Vector":[0.2678065059,-0.0563124665,-0.1478058717,0.0615672054,-0.1255334241,-0.1263618384,0.0762615007,-0.0031042726,-0.0617691932,0.1434191982,-0.1520533674,-0.2136360656,-0.2201936718,0.1751010342,0.1516029462,-0.1388651783,0.0870116249,-0.0657498646,-0.0166061402,0.0722498786,-0.1506249895,0.1360882198,-0.0335493688,-0.0974901797],"Abstract_Vector":[0.2435786957,0.0756013788,0.0473877853,0.0721139224,0.1047641991,0.0871872585,-0.0105984864,0.1430903519,-0.1625514469,0.043714881,-0.1077235212,0.0401413763,-0.0444857283,-0.0026265707,-0.023464926,0.0570759007,0.1110208693,-0.107135529,0.0360847433,-0.120886143,-0.0347704973,-0.0523428284,-0.0389855515,-0.1265129568,-0.2153033123,-0.0198598997,0.0488861125,0.0462383772,-0.0988087615,0.0672043353,0.0704505393,-0.0280248909,-0.0928838154,-0.0487078003,-0.0269911496,-0.1475234826,-0.0012555289,0.0976096193,0.0438703866,-0.1466115984,-0.0524437059,0.1190937881,0.0759789687,-0.0164713289,-0.0307918718,-0.0774593371,0.0546223212,0.0612311053,0.0100389751,-0.1231340325,0.1091483191,-0.06570656,-0.0612879229,0.0031559798,0.007159519,0.0362900071,0.0120917826,0.0278339469,0.1293473792,-0.0622426599,0.0732737821,-0.0536030609,-0.0201489075,-0.0617123734,0.0633905387,0.0244770341,-0.0655541054,-0.1077540725,0.0110245525,0.1105419283,-0.0627220236,-0.0301717486,0.0056944979,-0.0998616446,-0.112161713,0.0096893507]},"70":{"Abstract":"Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization's value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.","Authors":"E. Wall; M. Agnihotri; L. Matzen; K. Divis; M. Haass; A. Endert; J. Stasko","DOI":"10.1109\/TVCG.2018.2865146","Keywords":"Visualization evaluation;heuristics;value of visualization","Keywords_Processed":"value of visualization;visualization evaluation;heuristic","Title":"A Heuristic Approach to Value-Driven Evaluation of Visualizations","Labels":null,"Keyword_Vector":[0.2909650253,-0.09559565,-0.1942333627,-0.0411935285,-0.1224757887,0.1123182374,0.0031167256,-0.0631082581,0.0441212019,0.0642619602,0.0023617421,-0.0855313818,0.093354027,-0.0075271663,0.0937307459,-0.0374462386,-0.0234473471,-0.0304402512,-0.000538497,0.0266166536,0.0968407499,0.1309128816,-0.0069205029,0.0143387088],"Abstract_Vector":[0.2412505158,-0.1375768566,0.1023974672,-0.0421089163,0.1094693674,0.2091691947,-0.046719218,-0.0576324948,-0.0796426468,-0.0591612297,-0.0677036464,-0.0970367772,0.0268686476,-0.0774755368,0.0107203164,-0.0289611403,0.08015172,0.0211544579,-0.0080893749,0.0656836504,-0.0598882206,-0.0817948563,0.0047412923,-0.1191791639,0.0178030908,0.0511113874,0.0358083458,-0.0131019078,0.0206674876,0.0653584801,0.0704550799,-0.020832486,-0.0001778716,-0.0572552612,-0.0119396824,0.0422948311,-0.0968972123,-0.0261565005,0.0394208888,0.0606763129,0.1260653048,0.0735551392,0.1388985343,-0.0572540908,-0.0006883441,0.0496280136,-0.0490129574,-0.064660499,0.0101178357,-0.1155353617,0.1035400588,-0.0281190789,0.1329544437,0.0494198091,-0.0054785145,0.0775888218,-0.0714899892,-0.0588061531,-0.0205738451,-0.0373106174,-0.0696848994,0.0493115474,-0.0125872866,0.0188809651,-0.0426163788,-0.0785394525,-0.086134904,0.091747686,0.0104253133,0.0592943358,0.0183085789,0.0020334471,0.0744497365,0.1457890425,-0.0865969528,-0.0061444948]},"71":{"Abstract":"Games and experiences designed for virtual or augmented reality usually require the player to move physically to play. This poses substantial challenge for level designers because the player's physical experience in a level will need to be considered, otherwise the level may turn out to be too exhausting or not challenging enough. This paper presents a novel approach to optimize level designs by considering the physical challenge imposed upon the player in completing a level of motion-based games. A game level is represented as an assembly of chunks characterized by the exercise intensity levels they impose on players. We formulate game level synthesis as an optimization problem, where the chunks are assembled in a way to achieve an optimized level of intensity. To allow the synthesis of game levels of varying lengths, we solve the trans-dimensional optimization problem with a Reversible-jump Markov chain Monte Carlo technique. We demonstrate that our approach can be applied to generate game levels for s of motion-based virtual reality games. A user evaluation validates the effectiveness of our approach in generating levels with the desired amount of physical challenge.","Authors":"B. Xie; Y. Zhang; H. Huang; E. Ogawa; T. You; L. Yu","DOI":"10.1109\/TVCG.2018.2793618","Keywords":"Virtual Reality;Level Design;Procedural Modeling;Exergaming","Keywords_Processed":"exergam;procedural modeling;virtual reality;level design","Title":"Exercise Intensity-Driven Level Design","Labels":null,"Keyword_Vector":[0.1664638646,0.3128445996,-0.0099402738,-0.0694651506,-0.0116123411,0.0459380833,0.1552449187,-0.0511071066,0.0759588037,-0.183888036,-0.0552822247,0.0749923174,-0.0666798386,0.0535963119,-0.2292595894,0.0118715737,0.0897882282,0.0044265491,0.0204097598,-0.0338237752,-0.0241886982,-0.0063528299,0.0877474493,0.104757568],"Abstract_Vector":[0.1630307465,0.0883687329,0.0782493405,-0.092619172,-0.0449804915,-0.061831188,-0.0259088438,-0.1022847378,0.0585957072,-0.0246321501,-0.0584385936,-0.0632119501,-0.0416446583,0.1049203737,-0.0156988597,-0.0325235583,0.1766309933,0.0335631111,0.0148826928,0.0668751019,0.083846863,0.037331328,-0.031863974,-0.0461382163,0.123240999,0.0729295818,-0.0767938077,-0.0469064658,0.1555167689,-0.0590473297,0.100772645,0.0500118245,0.1894788758,0.0347804542,-0.1904274763,0.0343403612,-0.0984001733,-0.0817928039,0.0728876395,0.0917297288,-0.0059360823,0.0859953886,-0.0529013296,0.0848234871,0.0993529881,-0.0645326999,-0.1126453155,0.0598781237,0.011302745,-0.040081747,0.0540125927,-0.1427207455,0.1289898191,-0.0721637539,-0.0938881934,0.0906753789,0.0066650151,-0.0974855684,0.0765127456,-0.058243652,0.0077262631,-0.0210175109,-0.0887906107,-0.0443678089,-0.0066337687,0.018574637,0.1006496459,-0.0244341851,-0.0300427442,-0.0124735713,-0.0242839249,-0.0325645591,-0.0970826172,-0.0218567369,-0.0008848091,-0.0708882799]},"72":{"Abstract":"Effective analysis of features in time-varying data is essential in numerous scientific applications. Feature extraction and tracking are two important tasks scientists rely upon to get insights about the dynamic nature of the large scale time-varying data. However, often the complexity of the scientific phenomena only allows scientists to vaguely define their feature of interest. Furthermore, such features can have varying motion patterns and dynamic evolution over time. As a result, automatic extraction and tracking of features becomes a non-trivial task. In this work, we investigate these issues and propose a distribution driven approach which allows us to construct novel algorithms for reliable feature extraction and tracking with high confidence in the absence of accurate feature definition. We exploit two key properties of an object, motion and similarity to the target feature, and fuse the information gained from them to generate a robust feature-aware classification field at every time step. Tracking of features is done using such classified fields which enhances the accuracy and robustness of the proposed algorithm. The efficacy of our method is demonstrated by successfully applying it on several scientific data sets containing a wide range of dynamic time-varying features.","Authors":"S. Dutta; H. Shen","DOI":"10.1109\/TVCG.2015.2467436","Keywords":"Gaussian mixture model (GMM);Incremental learning;Feature extraction and tracking;Time-varying data analysis;Gaussian mixture model (GMM);Incremental learning;Feature extraction and tracking;Time-varying data analysis","Keywords_Processed":"incremental learning;time vary datum analysis;gaussian mixture model gmm;feature extraction and tracking","Title":"Distribution Driven Extraction and Tracking of Features for Time-varying Data Analysis","Labels":null,"Keyword_Vector":[0.1701565668,-0.0730367596,0.186874063,-0.0740332148,0.1172477902,-0.1016804361,0.0110064399,-0.0210628608,-0.0342852194,0.0195277632,0.0671840474,0.1348853265,-0.2737889359,0.0187916648,-0.001430495,-0.0981611157,0.1121969612,-0.0560803763,-0.1949820231,-0.2175075267,0.0887220644,0.006017482,-0.2285613955,-0.0812874821],"Abstract_Vector":[0.2905282455,0.0785077208,-0.1592718151,0.0268282128,-0.0511889342,-0.0076138343,0.0764606163,-0.1415375698,0.0040345753,0.0650697425,-0.1778243584,-0.1065399431,0.2772959026,-0.1923059105,0.2209849477,-0.026164722,-0.0641033173,0.0601949586,-0.1089706647,-0.0586496838,0.0576458874,0.0645345741,-0.0318458465,-0.0230355045,-0.004972607,0.1282454169,-0.0518116905,-0.038353783,-0.0057588433,0.0587391831,-0.0724777204,-0.1071719117,0.1283492782,-0.0192604775,-0.0780549921,0.094451194,0.074140267,0.0367263388,0.002168672,-0.052506829,-0.0984471978,-0.0226915927,0.0343739505,0.0125334136,-0.1787729385,-0.0054564232,0.0076308006,0.021996963,-0.018441413,0.0459411965,-0.0763659697,-0.051354832,-0.0160576757,-0.0405419496,-0.0026086173,-0.1020857348,0.0294485065,0.0382710546,0.0249867239,0.0330117121,-0.036387484,0.0009466341,-0.1030415213,-0.0839783884,0.0376836188,0.0176228443,0.0433928172,-0.0043503163,-0.0098703421,-0.0310559268,0.0113454965,-0.0726413027,0.0564195391,0.0420009528,-0.0118583401,-0.0183139866]},"73":{"Abstract":"Media data has been the subject of large scale analysis with applications of text mining being used to provide overviews of media themes and information flows. Such information extracted from media articles has also shown its contextual value of being integrated with other data, such as criminal records and stock market pricing. In this work, we explore linking textual media data with curated secondary textual data sources through user-guided semantic lexical matching for identifying relationships and data links. In this manner, critical information can be identified and used to annotate media timelines in order to provide a more detailed overview of events that may be driving media topics and frames. These linked events are further analyzed through an application of causality modeling to model temporal drivers between the data series. Such causal links are then annotated through automatic entity extraction which enables the analyst to explore persons, locations, and organizations that may be pertinent to the media topic of interest. To demonstrate the proposed framework, two media datasets and an armed conflict event dataset are explored.","Authors":"Y. Lu; H. Wang; S. Landis; R. Maciejewski","DOI":"10.1109\/TVCG.2017.2752166","Keywords":"Semantic similarity;media annotation;visual analytics;causality modeling;social media","Keywords_Processed":"social medium;visual analytic;medium annotation;semantic similarity;causality modeling","Title":"A Visual Analytics Framework for Identifying Topic Drivers in Media Events","Labels":null,"Keyword_Vector":[0.1099151413,-0.0270437944,0.1698373011,0.144350674,-0.0788988572,-0.0473603278,-0.088158184,-0.0515236409,0.0038869024,-0.1122296778,-0.0357463499,-0.0027122213,-0.0038755964,-0.01409119,-0.1134777908,0.0072960288,-0.0335314249,0.0066562209,0.1192250785,0.0551891315,0.0109152636,-0.1008990852,0.2179935601,0.1014928122],"Abstract_Vector":[0.211605565,-0.1103529009,-0.0399768863,-0.0936956369,-0.0189425899,-0.053264293,0.0045183375,-0.1008797952,-0.0563639722,0.0884921729,-0.0239478696,0.1202985247,-0.0643597816,-0.0076707173,0.1861934134,0.1527438303,0.002440168,-0.1170536924,-0.0691614245,-0.0420804345,-0.0443479016,-0.0392363366,0.083339036,0.0910284997,-0.0152375657,-0.0589985006,0.1239026351,-0.1143687692,-0.0794060325,0.0684034636,-0.0300724698,0.0077531654,-0.1087421887,-0.0626171277,-0.1572872704,0.0426188367,-0.140924065,-0.1215956929,0.0400270465,-0.0874720339,0.0751844605,-0.0428147856,-0.0032126495,0.0456955109,-0.0070122537,0.1047933217,0.0549113095,0.0392647189,0.0539710718,0.1010239772,0.0054071147,0.0591500294,0.0826438592,-0.1170628899,0.0612710561,-0.0044191004,0.1302918946,-0.1435278447,-0.1581415701,-0.017644727,0.0286195842,0.0755803556,-0.0143512347,0.0118488528,-0.0009194293,0.0529698915,0.0071662362,0.0679245335,0.0019270444,0.0305953888,0.0532646633,-0.0133010934,0.0997905737,-0.1248016761,-0.028002194,-0.0074376558]},"74":{"Abstract":"Introducing motion into existing static paintings is becoming a field that is gaining momentum. This effort facilitates keeping artworks current and translating them to different forms for diverse audiences. Chinese ink paintings and Japanese Sumies are well recognized in Western cultures, yet not easily practiced due to the years of training required. We are motivated to develop an interactive system for artists, non-artists, Asians, and non-Asians to enjoy the unique style of Chinese paintings. In this paper, our focus is on replacing static water flow scenes with animations. We include flow patterns, surface ripples, and water wakes which are challenging not only artistically but also algorithmically. We develop a data-driven system that procedurally computes a flow field based on stroke properties extracted from the painting, and animate water flows artistically and stylishly. Technically, our system first extracts water-flow-portraying strokes using their locations, oscillation frequencies, brush patterns, and ink densities. We construct an initial flow pattern by analyzing stroke structures, ink dispersion densities, and placement densities. We cluster extracted strokes as stroke pattern groups to further convey the spirit of the original painting. Then, the system automatically computes a flow field according to the initial flow patterns, water boundaries, and flow obstacles. Finally, our system dynamically generates and animates extracted stroke pattern groups with the constructed field for controllable smoothness and temporal coherence. The users can interactively place the extracted stroke patterns through our adapted Poisson-based composition onto other paintings for water flow animation. In conclusion, our system can visually transform a static Chinese painting to an interactive walk-through with seamless and vivid stroke-based flow animations in its original dynamic spirits without flickering artifacts.","Authors":"Y. Lai; B. Chen; K. Chen; W. Si; C. Yao; E. Zhang","DOI":"10.1109\/TVCG.2016.2622269","Keywords":"NPR;data-driven;chinese ink painting;tensor field smoothing;naiver-stokes equations","Keywords_Processed":"naiver stoke equation;chinese ink painting;npr;tensor field smoothing;datum drive","Title":"Data-Driven NPR Illustrations of Natural Flows in Chinese Painting","Labels":null,"Keyword_Vector":[0.0723437503,-0.0466175356,-0.002470381,-0.0178819572,0.0991816979,0.0319506968,0.0415508989,0.0340508978,-0.1409475251,-0.0197878344,0.0327774812,0.0510768179,-0.0739651533,-0.0107135671,-0.0822961525,0.0324461431,-0.1434758476,-0.04917093,0.0238556859,-0.1264739168,-0.0519484179,-0.0721361936,-0.0761271595,0.1360362254],"Abstract_Vector":[0.1748155461,0.0639888137,-0.118979102,-0.0006183697,-0.0200661697,-0.0152045304,0.2005999112,-0.2482552922,-0.0658303822,0.0357978553,0.3216754483,0.1294574176,0.0215171778,0.0062016447,-0.0094171043,0.0812702248,0.1073724219,0.153237064,0.0350458505,-0.1585093257,-0.1248789914,0.055408474,0.1319370166,-0.0791955108,-0.0876703797,0.0472624168,-0.069555995,-0.0180765843,-0.0751204968,-0.1766866957,-0.0724734339,-0.0940031121,0.0428779086,0.074212155,-0.0170309564,-0.0270891114,0.0275367158,0.0443286801,-0.0103667458,-0.0001404053,0.045929903,-0.0027991581,0.1980572132,0.0101999118,0.1068930319,0.0878771522,-0.1602554655,-0.0254362607,0.0052290305,0.0276910086,-0.0009049311,0.1146149024,0.0429864794,-0.0311130829,-0.0429027417,0.0938960399,0.0054771671,0.0008299914,0.081687375,0.0369767199,-0.0012857729,-0.1075686416,0.0123769505,-0.0200956644,-0.0473924663,-0.0003255,0.0016473533,0.0024245249,-0.0119689476,-0.03862889,-0.0579572539,-0.0442431398,0.0118204161,0.0204438554,0.0011541216,-0.0205627781]},"75":{"Abstract":"Urban forms at human-scale, i.e., urban environments that individuals can sense (e.g., sight, smell, and touch) in their daily lives, can provide unprecedented insights on a variety of applications, such as urban planning and environment auditing. The analysis of urban forms can help planners develop high-quality urban spaces through evidence-based design. However, such analysis is complex because of the involvement of spatial, multi-scale (i.e., city, region, and street), and multivariate (e.g., greenery and sky ratios) natures of urban forms. In addition, current methods either lack quantitative measurements or are limited to a small area. The primary contribution of this work is the design of StreetVizor, an interactive visual analytics system that helps planners leverage their domain knowledge in exploring human-scale urban forms based on street view images. Our system presents two-stage visual exploration: 1) an AOI Explorer for the visual comparison of spatial distributions and quantitative measurements in two areas-of-interest (AOIs) at city- and region-scales; 2) and a Street Explorer with a novel parallel coordinate plot for the exploration of the fine-grained details of the urban forms at the street-scale. We integrate visualization techniques with machine learning models to facilitate the detection of street view patterns. We illustrate the applicability of our approach with case studies on the real-world datasets of four cities, i.e., Hong Kong, Singapore, Greater London and New York City. Interviews with domain experts demonstrate the effectiveness of our system in facilitating various analytical tasks.","Authors":"Q. Shen; W. Zeng; Y. Ye; S. M. Arisona; S. Schubiger; R. Burkhard; H. Qu","DOI":"10.1109\/TVCG.2017.2744159","Keywords":"Urban forms;human scale;street view;visual analytics","Keywords_Processed":"human scale;urban form;visual analytic;street view","Title":"StreetVizor: Visual Exploration of Human-Scale Urban Forms Based on Street Views","Labels":null,"Keyword_Vector":[0.1340840415,-0.0110260875,0.2525928265,0.1289262338,-0.0873590819,-0.0640602073,-0.0800022831,0.0272952153,-0.067283257,-0.0391964342,0.0538088413,-0.0601001545,0.0107435434,-0.0889942568,-0.0863278039,0.0354574994,-0.1436740435,0.0142218769,-0.0299581475,0.1591740764,-0.1198812592,0.0030151573,0.0090219899,0.0245482601],"Abstract_Vector":[0.2313401481,-0.0816033037,0.0045294157,-0.0134919991,0.0645152466,-0.0544528186,0.0375046748,0.0195211724,-0.0510684145,0.0988189165,-0.0158974714,0.0699208824,0.0336851958,0.0981575578,-0.062817858,-0.0083310122,-0.0633090774,-0.1266183009,0.2201965437,-0.0293691837,0.0263525194,-0.0827861149,0.0347438851,-0.0415276512,-0.1331093617,-0.0495235915,0.1978702425,0.0405450868,0.0352809401,-0.0341546326,-0.0485894935,-0.1284350524,0.2514063686,-0.0714153523,0.1188491174,-0.0283953934,0.1094750423,0.1181880228,-0.0325199442,0.1752751577,-0.0242924956,0.1250676687,0.0355026184,-0.0412012991,-0.1065417041,-0.070964767,0.0423372225,0.0064759852,0.0052388075,-0.0630525154,0.0671066024,-0.0224390245,0.0499400487,-0.0873531681,-0.0058905945,0.0866056177,0.0516639419,-0.0167702502,-0.1025827883,0.0077391669,-0.0125438062,-0.0095770368,-0.010721642,-0.1028623614,-0.06440686,-0.0360570094,-0.0701986886,-0.0140409806,0.0356553098,-0.0261728118,0.0361391285,0.0227204204,0.0089412798,-0.0722435566,-0.0407938815,0.0958375136]},"76":{"Abstract":"Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifying relevant structures in unlabeled, high-dimensional data is nontrivial, requiring iterative experimentation with clustering parameters as well as data features and instances. The number of possible clusterings for a typical dataset is vast, and navigating in this vast space is also challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment to establish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore and navigate the large clustering space so as to improve the effectiveness of exploratory clustering analysis. We introduce Clustrophile 2, a new interactive tool for guided clustering analysis. Clustrophile 2 guides users in clustering-based exploratory analysis, adapts user feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences between clusterings. To this end, Clustrophile 2 contributes a novel feature, the Clustering Tour, to help users choose clustering parameters and assess the quality of different clustering results in relation to current analysis goals and user expectations. We evaluate Clustrophile 2 through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson's disease patients. Results suggest that Clustrophile 2 improves the speed and effectiveness of exploratory clustering analysis for both experts and non-experts.","Authors":"M. Cavallo; \u00c7. Demiralp","DOI":"10.1109\/TVCG.2018.2864477","Keywords":"Clustering tour;Guided data analysis;Exploratory data analysis;Interactive clustering analysis;Interpretability;Explainability;Visual data exploration recommendation;Dimensionality reduction;What-if analysis;Clustrophile;Unsupervised learning","Keywords_Processed":"explainability;interpretability;cluster tour;clustrophile;unsupervised learning;guide datum analysis;what if analysis;visual datum exploration recommendation;interactive cluster analysis;dimensionality reduction;exploratory data analysis","Title":"Clustrophile 2: Guided Visual Clustering Analysis","Labels":null,"Keyword_Vector":[0.2964936847,-0.2235311682,0.3424640433,-0.2307916536,0.2790549652,-0.0354595204,0.0170901118,0.0284980273,0.0740716383,0.0209621684,-0.0946161961,0.0047616775,0.0547956295,0.186275648,0.0027120957,-0.1508330928,0.0830124109,-0.0726518639,-0.0454002289,-0.0520498139,-0.0496830933,-0.1079118314,0.0504986121,-0.116979063],"Abstract_Vector":[0.3025266332,-0.2026458828,-0.0809237256,0.2209048121,-0.1857171927,-0.1819543436,-0.2080955464,0.0824236551,-0.0919403506,0.0268726155,0.1385765019,-0.1718774685,0.0554930299,-0.0803227539,0.0322680489,0.0177134889,-0.0195454704,0.2091411779,0.0121591208,0.0029108699,0.0807277238,0.043521306,-0.0237769952,-0.0167659984,-0.0281076515,-0.0277038494,0.0430218635,-0.0026362234,-0.0132770097,0.015777754,0.0435084026,-0.076509158,-0.0831500997,-0.0464849181,0.0190694496,-0.0356640063,-0.0417196201,-0.0131207361,-0.0965830462,0.0508358401,-0.0922184998,0.111538008,-0.0677097656,0.0419758999,0.1017434439,0.036859433,0.0532006847,0.0216754471,0.04272585,0.0403666436,-0.0547652735,-0.0214697141,0.0493313606,-0.0396741682,-0.0363759962,-0.0159867772,0.0532697629,0.0022226756,0.0671206008,-0.0477933083,-0.0080796562,-0.0077276496,-0.0238372173,-0.0363390278,0.0369681279,0.0450730301,0.0143403251,-0.036865619,-0.0005491602,0.0228171225,0.0057457772,0.0209388642,0.0956367493,0.008131322,0.0132257718,-0.0330023465]},"77":{"Abstract":"This paper presents the extended work on LazyNav, a head-free, eyes-free and hands-free mid-air ground navigation control model presented at the IEEE 3D User Interfaces (3DUI) 2015, in particular with a new application to the head-mounted display (HMD). Our mid-air interaction metaphor makes use of only a single pair of the remaining tracked body elements to tailor the navigation. Therefore, the user can navigate in the scene while still being able to perform other interactions with her hands and head, e.g., carrying a bag, grasping a cup of coffee, or observing the content by moving her eyes and locally rotating her head. We design several body motions for navigation by considering the use of non-critical body parts and develop assumptions about ground navigation techniques. Through the user studies, we investigate the motions that are easy to discover, easy to control, socially acceptable, accurate and not tiring. Finally, we evaluate the desired ground navigation features with a prototype application in both a large display (LD) and a HMD navigation scenarios. We highlight several recommendations for designing a particular mid-air ground navigation technique for a LD and a HMD.","Authors":"P. Punpongsanon; E. Guy; D. Iwai; K. Sato; T. Boubekeur","DOI":"10.1109\/TVCG.2016.2586071","Keywords":"3D user interface;spatial interaction;virtual reality;navigation","Keywords_Processed":"virtual reality;3d user interface;navigation;spatial interaction","Title":"Extended LazyNav: Virtual 3D Ground Navigation for Large Displays and Head-Mounted Displays","Labels":null,"Keyword_Vector":[0.2775194605,0.4287998099,-0.0227939361,0.0784509548,0.0738371542,-0.1479331587,-0.063833893,0.3084557877,0.0620521925,-0.1286074519,0.0900790621,0.0043024264,0.0474077409,0.1127238864,0.1559568051,0.0545537523,0.0808620934,0.0390108547,0.0559483018,0.0010736903,0.1849329052,-0.0987443046,0.0187530548,0.046008651],"Abstract_Vector":[0.1787183379,0.1071776614,0.1600390587,-0.005030141,-0.0406209684,-0.089232683,0.0304692084,-0.0159780419,-0.00303399,-0.0624177572,0.0242642477,-0.0483698657,0.0678386546,-0.0680382225,-0.0676204609,0.0837813494,-0.1591955911,-0.0009511636,-0.0708086576,-0.0107792659,0.0299409728,-0.022458005,-0.1344128184,0.1366246032,-0.0504787097,-0.1023250672,-0.0637483309,0.0729505691,-0.0506005047,-0.0327776792,0.0547264893,-0.1365506859,-0.0774370106,0.1186151011,0.0464811709,0.0086102906,-0.0824661939,0.070182921,0.0928788412,-0.1244235523,0.1248633686,0.0158842458,-0.0824640959,0.0573446561,-0.0582756694,0.0447672057,-0.0133532788,-0.0357247742,-0.0662925871,-0.1458279883,0.0291699999,-0.0428495846,-0.0965230359,-0.0163822999,-0.1958418989,0.0471847715,-0.029418156,-0.0429948959,-0.0172787045,-0.0695877898,0.0133685348,0.0413306688,-0.0686871002,0.1213825844,-0.0901036545,0.1059377153,0.0312201214,0.0246966707,-0.0092171092,0.0949746193,0.0122417274,-0.0861053281,0.0333531755,-0.0326428613,-0.0147247835,-0.0342182624]},"78":{"Abstract":"This paper presents a novel interactive system that provides users with virtual reality (VR) experiences, wherein users feel as if they are ascending\/descending stairs through passive haptic feedback. The passive haptic stimuli are provided by small bumps under the feet of users; these stimuli are provided to represent the edges of the stairs in the virtual environment. The visual stimuli of the stairs and shoes, provided by head-mounted displays, evoke a visuo-haptic interaction that modifies a user's perception of the floor shape. Our system enables users to experience all types of stairs, such as half-turn and spiral stairs, in a VR setting. We conducted a preliminary user study and two experiments to evaluate the proposed technique. The preliminary user study investigated the effectiveness of the basic idea associated with the proposed technique for the case of a user ascending stairs. The results demonstrated that the passive haptic feedback produced by the small bumps enhanced the user's feeling of presence and sense of ascending. We subsequently performed an experiment to investigate an improved viewpoint manipulation method and the interaction of the manipulation and haptics for both the ascending and descending cases. The experimental results demonstrated that the participants had a feeling of presence and felt a steep stair gradient under the condition of haptic feedback and viewpoint manipulation based on the characteristics of actual stair walking data. However, these results also indicated that the proposed system may not be as effective in providing a sense of descending stairs without an optimization of the haptic stimuli. We then redesigned the shape of the small bumps, and evaluated the design in a second experiment. The results indicated that the best shape to present haptic stimuli is a right triangle cross section in both the ascending and descending cases. Although it is necessary to install small protrusions in the determined direction, by using this optimized shape the users feeling of presence of the stairs and the sensation of walking up and down was enhanced.","Authors":"R. Nagao; K. Matsumoto; T. Narumi; T. Tanikawa; M. Hirose","DOI":"10.1109\/TVCG.2018.2793038","Keywords":"Virtual reality;locomotion;haptic feedback;perception;stairs;staircase","Keywords_Processed":"locomotion;haptic feedback;virtual reality;stair;staircase;perception","Title":"Ascending and Descending in Virtual Reality: Simple and Safe System Using Passive Haptics","Labels":null,"Keyword_Vector":[0.1437003869,0.437958714,0.0412649324,-0.1186056496,0.0301753646,0.1181169969,-0.0528255534,-0.1292984021,-0.0008344191,0.0779048605,0.0053275577,0.0433296148,0.0044917976,0.048272134,0.0124328233,0.0474923529,0.0450459535,-0.0582875911,0.0534998661,-0.0016385509,0.0221282511,-0.0031904617,0.028499248,0.0670216889],"Abstract_Vector":[0.1811833944,0.0554455666,0.188845213,0.0202795753,-0.1054801017,-0.0483480126,-0.0394324962,-0.0142309574,0.0714569426,0.0499168582,0.1213168753,-0.0582696577,-0.0445539504,0.1017031049,0.0433626624,-0.1211986231,-0.1404332506,-0.024802512,-0.1025260196,-0.1120105246,0.0554702185,-0.0538391102,0.0376532792,-0.0546790721,-0.0240432937,0.0121308565,-0.0210891896,-0.0157062795,-0.0865773237,-0.1295836063,-0.0281507799,0.0586442548,-0.0424336733,-0.0561658398,0.039394919,-0.0438621714,-0.0627344207,-0.0586137579,-0.0576144907,0.0701647018,-0.0483861862,-0.1153572816,-0.0353318261,-0.0889394513,-0.1224420843,-0.0118982349,0.0181259053,0.0776636017,0.0155532436,-0.0095687179,0.0294008147,0.0336757848,0.0777723572,0.1038836104,0.0868091498,0.100782407,0.0135533867,-0.0007778347,-0.0361319449,-0.0426613417,-0.163779038,-0.0437038594,-0.0847425341,0.0162765489,0.0092487928,0.0554917582,-0.0326319307,0.0610003276,-0.0295318927,0.0108692769,0.015685406,-0.0021602004,0.0224570037,0.0452568803,0.0452542907,0.0504321218]},"79":{"Abstract":"Sensemaking is described as the process of comprehension, finding meaning and gaining insight from information, producing new knowledge and informing further action. Understanding the sensemaking process allows building effective visual analytics tools to make sense of large and complex datasets. Currently, it is often a manual and time-consuming undertaking to comprehend this: researchers collect observation data, transcribe screen capture videos and think-aloud recordings, identify recurring patterns, and eventually abstract the sensemaking process into a general model. In this paper, we propose a general approach to facilitate such a qualitative analysis process, and introduce a prototype, SensePath, to demonstrate the application of this approach with a focus on browser-based online sensemaking. The approach is based on a study of a number of qualitative research sessions including observations of users performing sensemaking tasks and post hoc analyses to uncover their sensemaking processes. Based on the study results and a follow-up participatory design session with HCI researchers, we decided to focus on the transcription and coding stages of thematic analysis. SensePath automatically captures user's sensemaking actions, i.e., analytic provenance, and provides multi-linked views to support their further analysis. A number of other requirements elicited from the design session are also implemented in SensePath, such as easy integration with existing qualitative analysis workflow and non-intrusive for participants. The tool was used by an experienced HCI researcher to analyze two sensemaking sessions. The researcher found the tool intuitive and considerably reduced analysis time, allowing better understanding of the sensemaking process.","Authors":"P. H. Nguyen; K. Xu; A. Wheat; B. L. W. Wong; S. Attfield; B. Fields","DOI":"10.1109\/TVCG.2015.2467611","Keywords":"Sensemaking;analytic provenance;transcription;coding;qualitative research;timeline visualization;Sensemaking;analytic provenance;transcription;coding;qualitative research;timeline visualization","Keywords_Processed":"transcription;sensemaking;analytic provenance;cod;qualitative research;timeline visualization","Title":"SensePath: Understanding the Sensemaking Process Through Analytic Provenance","Labels":null,"Keyword_Vector":[0.1310061735,-0.0334562363,-0.0149343215,0.0434477392,-0.1021544094,0.0181768578,-0.0206290474,0.046179086,0.0312464123,-0.0440955747,0.0009795515,-0.0242238137,0.008400278,-0.0328414685,-0.0315049394,0.0440212043,-0.0036863739,-0.0064310903,0.0072270596,-0.0525733176,0.0204807091,0.1715760296,0.0491589199,-0.0326090344],"Abstract_Vector":[0.2258779008,-0.1045069208,0.0185989536,-0.0543373,0.0486527386,-0.1092500595,-0.090869348,-0.0640095808,-0.0757429761,-0.0012797299,0.0290418433,0.0134098809,-0.0185341229,-0.0199945368,-0.0488118482,0.034248042,-0.0579103048,-0.007365129,0.0390443633,0.0280725138,-0.0112445745,0.0353707489,0.0243929408,0.0002512007,0.0683332141,0.0222897595,-0.0633181275,0.0691907884,0.1368135127,-0.0072070886,0.0442715839,0.0329906629,-0.0040053097,0.0329322396,-0.0132281377,-0.0128567316,0.1136928649,0.0094207016,-0.0205328754,-0.063057604,-0.0861328087,-0.056362145,-0.036194781,0.0258034386,-0.0919342013,-0.113773991,0.1071267308,-0.0200294889,0.047017172,0.1281534751,0.0135777247,0.0201603421,-0.0257684709,0.1006839008,0.0412136929,0.0725940438,-0.0385388483,0.0305637808,-0.0498477979,0.0299848106,-0.0444704317,-0.1427933506,0.0166237368,0.0251155811,0.0356480611,0.0371260943,0.0566315069,0.0926085989,0.0826979203,-0.1012178932,0.0534093308,-0.0453715011,0.0799604803,-0.0507867562,0.0628904161,-0.0069628794]},"80":{"Abstract":"Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge & Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.","Authors":"A. Sarvghad; B. Saket; A. Endert; N. Weibel","DOI":"10.1109\/TVCG.2018.2865075","Keywords":"Data Visualization;Direct Manipulation;Embedded Merge & Split;Data Grouping;Embedded Interaction","Keywords_Processed":"data visualization;direct manipulation;datum grouping;embed merge split;embed interaction","Title":"Embedded Merge & Split: Visual Adjustment of Data Grouping","Labels":null,"Keyword_Vector":[0.1770513963,-0.0677253422,-0.0238057136,0.0302960431,0.1499758823,0.0333775295,-0.0691839193,0.1314236575,-0.0552538565,-0.0051001712,0.0196901445,0.0232745701,-0.025911991,0.0208597521,0.0867953122,0.0975175466,-0.0012218639,-0.0016943884,0.0481962708,-0.1324784978,0.0304594067,0.0904619072,0.0894206745,0.0868300702],"Abstract_Vector":[0.2218338577,-0.1188263786,0.0208159251,-0.0886015362,-0.0726989593,0.0403787164,-0.0293239697,0.0256725124,-0.0768839772,-0.0044919635,0.0097073691,0.0061952853,-0.0216237433,-0.0081234154,-0.0121896049,0.0086384708,0.0411793238,0.0378381084,-0.0822159405,-0.032472637,-0.0654205271,-0.0645935821,-0.0926937812,0.0325342751,0.056395807,-0.0254905966,-0.0049808756,0.035687731,-0.0367610811,-0.0675831253,0.0640488229,-0.0336628604,-0.0103278615,0.0791619964,0.0395788795,-0.0086722934,-0.053222346,0.0032986721,-0.0219460892,0.0170024151,0.0234969777,-0.0854864848,0.0190417976,0.0273481024,0.0634020882,0.0255631436,-0.0638503172,0.0021658666,0.0418403859,-0.0130647486,-0.1587256289,-0.0540764993,0.0031132356,-0.0931566944,0.1728423595,-0.0339092436,-0.0149578048,0.0997424239,0.0055468541,-0.0591992277,0.0975439546,-0.0002661138,0.0059813591,-0.0500754526,0.1696323972,0.0557361441,-0.0686637124,0.0468106112,0.0094332666,-0.0712039551,-0.0210372006,-0.0982885306,-0.0402151325,-0.0125504673,0.0171881268,-0.0627814449]},"81":{"Abstract":"Data ensembles are often used to infer statistics to be used for a summary display of an uncertain prediction. In a spatial context, these summary displays have the drawback that when uncertainty is encoded via a spatial spread, display glyph area increases in size with prediction uncertainty. This increase can be easily confounded with an increase in the size, strength or other attribute of the phenomenon being presented. We argue that by directly displaying a carefully chosen subset of a prediction ensemble, so that uncertainty is conveyed implicitly, such misinterpretations can be avoided. Since such a display does not require uncertainty annotation, an information channel remains available for encoding additional information about the prediction. We demonstrate these points in the context of hurricane prediction visualizations, showing how we avoid occlusion of selected ensemble elements while preserving the spatial statistics of the original ensemble, and how an explicit encoding of uncertainty can also be constructed from such a selection. We conclude with the results of a cognitive experiment demonstrating that the approach can be used to construct storm prediction displays that significantly reduce the confounding of uncertainty with storm size, and thus improve viewers' ability to estimate potential for storm damage.","Authors":"L. Liu; A. P. Boone; I. T. Ruginski; L. Padilla; M. Hegarty; S. H. Creem-Regehr; W. B. Thompson; C. Yuksel; D. H. House","DOI":"10.1109\/TVCG.2016.2607204","Keywords":"Implicit uncertainty presentation;ensembles;ensemble visualization;sampling;uncertainty;hurricane prediction","Keywords_Processed":"sample;uncertainty;implicit uncertainty presentation;hurricane prediction;ensemble visualization;ensembl","Title":"Uncertainty Visualization by Representative Sampling from Prediction Ensembles","Labels":null,"Keyword_Vector":[0.1892473155,-0.1058506634,-0.1910162603,-0.0593705776,-0.0769882674,-0.0761889709,-0.2519957587,-0.2164075488,0.0337310107,-0.2404524614,-0.2171004442,0.112358625,-0.0386471299,-0.0377843417,0.0702887606,0.074723135,0.0379624146,0.1270034325,-0.0194472046,0.0494379331,-0.1039056381,-0.1244327292,-0.0679740829,0.0759379706],"Abstract_Vector":[0.1968240894,-0.073783742,0.0790048747,0.3939304879,-0.0386434511,0.2061204288,-0.1105776953,-0.1282805413,0.197349558,-0.1450368523,-0.1141319799,0.276728271,-0.045086704,-0.0330640918,-0.0579287143,0.0888235886,-0.0036545052,-0.1451861695,-0.0442060089,-0.0086795585,-0.0209605613,0.0376245919,-0.007511566,0.0916745901,-0.0539988275,-0.0489641805,-0.0841410499,-0.03600086,-0.0214204929,-0.0502133781,-0.0017811438,0.0120679002,-0.0236313923,0.019348431,0.0322397523,0.0497883911,-0.0100546869,-0.0132686352,-0.0231614117,-0.0879651552,0.021058492,0.0929769557,-0.0722894157,-0.0073755254,0.0371989565,-0.0443759315,-0.0319303949,-0.0106217147,-0.0770054974,0.042119384,-0.090771748,0.0241130546,-0.0154382294,0.0201493097,0.0374145688,-0.0484500305,-0.0652041483,0.0217571723,0.027648165,0.0247095776,0.0388089483,-0.0581490743,0.0379349213,-0.078702243,-0.0377649757,-0.0650435847,0.0821289545,-0.0315626814,-0.0510508175,-0.0243648191,-0.0778360676,-0.0188188805,-0.0733049721,-0.0542052537,0.0217355557,-0.0568082425]},"82":{"Abstract":"We present a method for outdoor markerless motion capture with sparse handheld video cameras. In the simplest setting, it only involves two mobile phone cameras following the character. This setup can maximize the flexibilities of data capture and broaden the applications of motion capture. To solve the character pose under such challenge settings, we exploit the generative motion capture methods and propose a novel model-view consistency that considers both foreground and background in the tracking stage. The background is modeled as a deformable 2D grid, which allows us to compute the background-view consistency for sparse moving cameras. The 3D character pose is tracked with a global-local optimization through minimizing our consistency cost. A novel L1 motion regularizer is also proposed in the optimization to constrain the solution pose space. The whole process of the proposed method is simple as frame by frame video segmentation is not required. Our method outperforms several alternative methods on various examples demonstrated in the paper.","Authors":"Y. Wang; Y. Liu; X. Tong; Q. Dai; P. Tan","DOI":"10.1109\/TVCG.2017.2693151","Keywords":"Markerless motion capture;handheld video cameras;model-view consistency","Keywords_Processed":"model view consistency;handheld video camera;markerless motion capture","Title":"Outdoor Markerless Motion Capture with Sparse Handheld Video Cameras","Labels":null,"Keyword_Vector":[0.0597442232,0.0443401483,0.0541724501,0.1007615496,0.060028632,-0.152166118,0.1286748232,-0.0631551868,-0.1366107652,-0.1269378466,-0.092776257,-0.140814747,0.0361383838,-0.1698676456,0.1045949532,-0.1057755024,0.057209321,-0.0923623523,-0.2302360315,0.040465319,-0.1286566668,0.0170490482,-0.0631897868,-0.0112165309],"Abstract_Vector":[0.2353710695,0.2679445189,-0.0447095335,-0.1787953245,-0.0893287919,-0.12217301,-0.0676686598,-0.0718081234,0.036694137,-0.1736962138,-0.1075622923,-0.0029987762,0.0807123183,-0.1952011564,-0.1670750137,-0.0040643245,0.1271485371,-0.1042011937,0.0930168422,0.0123876945,0.0152107345,0.0114516997,-0.0134792505,-0.0891682271,-0.0829545435,-0.0419581582,-0.0765944038,-0.0970059508,0.0492925756,0.1135890749,-0.0490110641,0.0066187283,0.0340815982,0.0226503579,-0.0065509047,-0.0205739399,0.0021329904,-0.0005548072,0.0088070856,-0.0279510346,0.0343115873,0.0540250975,-0.1266499316,0.0356672669,0.0858799852,0.0110704107,0.1418488973,0.0090263374,0.2180814633,0.0444635808,-0.0288344751,0.050910991,0.1376825431,0.1255551665,0.1023373356,0.0355708677,0.052311008,0.0624707478,-0.0248593961,0.0791862466,0.0547553931,0.0667770082,0.0000749077,0.0287583624,-0.0015220908,-0.0201040802,-0.0216443533,-0.0407999009,0.0091147078,0.0821027827,-0.0286973357,-0.0805294599,0.0359610647,0.0055843578,0.0100893539,-0.0839439801]},"83":{"Abstract":"Much research has been done regarding how to visualize and interact with observations and attributes of high-dimensional data for exploratory data analysis. From the analyst's perceptual and cognitive perspective, current visualization approaches typically treat the observations of the high-dimensional dataset very differently from the attributes. Often, the attributes are treated as inputs (e.g., sliders), and observations as outputs (e.g., projection plots), thus emphasizing investigation of the observations. However, there are many cases in which analysts wish to investigate both the observations and the attributes of the dataset, suggesting a symmetry between how analysts think about attributes and observations. To address this, we define SIRIUS (Symmetric Interactive Representations In a Unified System), a symmetric, dual projection technique to support exploratory data analysis of high-dimensional data. We provide an example implementation of SIRIUS and demonstrate how this symmetry affords additional insights.","Authors":"M. Dowling; J. Wenskovitch; J. T. Fry; S. Leman; L. House; C. North","DOI":"10.1109\/TVCG.2018.2865047","Keywords":"Dimension reduction;semantic interaction;exploratory data analysis;observation projection;attribute projection","Keywords_Processed":"dimension reduction;attribute projection;semantic interaction;observation projection;exploratory datum analysis","Title":"SIRIUS: Dual, Symmetric, Interactive Dimension Reductions","Labels":null,"Keyword_Vector":[0.1654077209,-0.0714956055,0.1158160176,-0.1119577921,0.2268445662,-0.0276292093,-0.0192659006,0.1895796281,-0.0824455749,-0.0230761243,0.0050950506,0.0485571068,0.0496121005,0.0645027656,0.0341922283,-0.0905181442,-0.014166007,-0.0124916214,-0.0268596608,-0.0654158128,0.1005637428,-0.0241471229,0.0498572267,0.0111543441],"Abstract_Vector":[0.2002150124,-0.1531185035,-0.0150970121,-0.0868672725,-0.1183942611,0.0880892307,-0.0297595216,0.019235321,-0.0685021922,0.1256194508,0.0117958563,0.1029039724,-0.0138416404,-0.0019938242,-0.0249849915,-0.1071074243,0.0149200149,0.0671084884,0.0109383085,-0.0151284694,-0.0397724962,0.0871804579,-0.1408817027,-0.1194343028,0.030112358,-0.1027595647,0.0356475375,0.103711691,0.0268294601,0.0129836877,-0.1372910703,0.1235559117,-0.0723047914,-0.0329121184,-0.0554857372,0.0743899832,0.0097227365,0.1162842553,0.0845955199,-0.1246542694,-0.0984882147,0.2345690014,-0.0216780677,0.0170236132,0.0725286964,-0.1172788885,-0.0084996885,-0.0181458111,0.0621887182,0.0917740704,0.0210939731,-0.0103127891,-0.0461559439,-0.0195658067,-0.0261353094,-0.03686914,-0.0426279815,-0.0431314749,-0.0858243543,0.0215993488,-0.1081273876,-0.0881461365,0.0233683255,0.1801429702,0.0797799353,0.0604298573,0.0455920383,-0.0099118138,0.0487959193,-0.0488598669,0.0321234826,-0.005410025,0.1006837,-0.0371787811,-0.0224615415,0.0066843848]},"84":{"Abstract":"We present Aggregate G-Buffer Anti-Aliasing (AGAA), a new technique for efficient anti-aliased deferred rendering of complex geometry using modern graphics hardware. In geometrically complex situations where many surfaces intersect a pixel, current rendering systems shade each contributing surface at least once per pixel. As the sample density and geometric complexity increase, the shading cost becomes prohibitive for real-time rendering. Under deferred shading, so does the required framebuffer memory. Our goal is to make high per-pixel sampling rates practical for real-time applications by substantially reducing shading costs and per-pixel storage compared to traditional deferred shading. AGAA uses the rasterization pipeline to generate a compact, pre-filtered geometric representation inside each pixel. We shade this representation at a fixed rate, independent of geometric complexity. By decoupling shading rate from geometric sampling rate, the algorithm reduces the storage and bandwidth costs of a geometry buffer, and allows scaling to high visibility sampling rates for anti-aliasing. AGAA with two aggregates per-pixel generates results comparable to 32$\\times$  MSAA, but requires 54 percent less memory and is up to 2.6$\\times$  faster ( $-30$  percent memory and 1.7 $\\times$  faster for 8 $\\times$  MSAA).","Authors":"C. Crassin; M. McGuire; K. Fatahalian; A. Lefohn","DOI":"10.1109\/TVCG.2016.2586073","Keywords":"Anti-aliasing;graphics pipelines;pre-filtering;shading","Keywords_Processed":"shade;pre filtering;anti alias;graphic pipeline","Title":"Aggregate G-Buffer Anti-Aliasing -Extended Version-","Labels":null,"Keyword_Vector":[0.011263638,0.0175138985,0.0045001565,0.0149904405,0.0230311899,-0.0092233079,0.0492488114,-0.0611010407,-0.0598396525,0.0053162858,0.0012256546,-0.0352215787,0.0167949318,0.0576540429,0.0131192479,0.0844799464,0.0021076761,0.0317023825,0.0628170116,0.0096647498,-0.0116162125,-0.0408257969,-0.0773605101,-0.1034768287],"Abstract_Vector":[0.1455567495,0.1170394555,-0.0561305509,0.0220841544,0.0028164781,0.0493347474,0.0082181492,0.0788242862,-0.1289505618,-0.0735983425,-0.0699871333,0.0675829809,-0.0853909279,0.0677227817,0.0755749837,-0.0211837498,-0.0947506178,0.0574313924,0.1312339876,-0.0753703174,-0.1998100754,0.08177716,0.1308416786,0.0611493989,0.0910291192,0.0444700606,-0.0923012709,-0.0082429084,-0.0682202948,0.0916988123,0.0838864558,-0.0339598799,0.0762564748,-0.0466093121,-0.0182544898,-0.0448698414,0.0126251351,0.0791280982,-0.0154355214,0.0563734357,-0.1063126917,0.0498639319,-0.005177638,0.2093310862,-0.0420339601,0.0101736635,-0.0124821602,0.0672418115,0.0701326783,-0.0102514356,-0.1843930412,0.1374651026,-0.095118317,-0.0814062678,-0.1065673678,0.0578836366,-0.0114144526,-0.0092148854,0.0913637952,-0.0751951067,-0.0329765649,0.1164550032,0.0239761606,-0.0551513581,0.1564195325,-0.0443837775,-0.0576752442,0.0268654604,-0.0378676357,0.109072345,-0.0140066353,0.1088349643,-0.1094544466,0.0667936367,0.0220674565,0.0420733573]},"85":{"Abstract":"Package managers provide ease of access to applications by removing the time-consuming and sometimes completely prohibitive barrier of successfully building, installing, and maintaining the software for a system. A package dependency contains dependencies between all packages required to build and run the target software. Package management system developers, package maintainers, and users may consult the dependency graph when a simple listing is insufficient for their analyses. However, users working in a remote command line environment must disrupt their workflow to visualize dependency graphs in graphical programs, possibly needing to move files between devices or incur forwarding lag. Such is the case for users of Spack, an open source package management system originally developed to ease the complex builds required by supercomputing environments. To preserve the command line workflow of Spack, we develop an interactive ASCII visualization for its dependency graphs. Through interviews with Spack maintainers, we identify user goals and corresponding visual tasks for dependency graphs. We evaluate the use of our visualization through a command line-centered study, comparing it to the system's two existing approaches. We observe that despite the limitations of the ASCII representation, our visualization is preferred by participants when approached from a command line interface workflow.","Authors":"K. E. Isaacs; T. Gamblin","DOI":"10.1109\/TVCG.2018.2859974","Keywords":"Software visualization;information visualization;command line interface","Keywords_Processed":"software visualization;command line interface;information visualization","Title":"Preserving Command Line Workflow for a Package Management System Using ASCII DAG Visualization","Labels":null,"Keyword_Vector":[0.285979591,-0.0538286105,-0.2311892194,-0.0268423534,-0.101419592,-0.0998888941,-0.0112051051,0.0610697127,0.0438455536,0.0779250712,0.238649305,-0.115865992,0.1919930211,0.1001264308,-0.0208091038,-0.0521433066,0.0141470305,0.0095512662,-0.0565299795,-0.0153203562,0.0120779163,-0.0278945743,0.0031801837,0.0671048022],"Abstract_Vector":[0.1671216587,-0.0514213626,0.0817372099,0.0291447502,0.0644847706,-0.0173934098,0.1052825835,0.0083115861,-0.0128389786,-0.0212232485,0.0848490056,-0.0325374889,0.0373390189,-0.0435134385,-0.043498756,-0.0182172665,0.0095393404,0.0390145536,-0.000940863,-0.0564966042,-0.1175569424,0.0282155725,0.0398947316,0.0732831064,0.0909551475,-0.0895935863,0.0253178358,0.0601491938,0.057340741,0.0608240678,0.0420239466,0.0920354803,0.0303196731,-0.1548973797,0.0811877185,-0.1611184044,-0.0177485337,-0.1371197228,0.132116978,-0.0335191539,-0.0372836033,-0.1129137897,-0.0233972494,0.039464172,-0.0263020418,-0.1091522195,-0.0365408957,0.1792301838,0.0091544786,0.0071674247,0.028321556,-0.0235921999,0.0465236547,0.1189327274,-0.0801748705,-0.0672601741,0.0483572127,0.0149960286,0.1254047866,0.0103311294,0.0588346456,0.0425896085,0.0473591586,-0.1083281493,-0.1072235972,0.1277680973,0.0413500081,-0.0490614005,0.1052763443,0.0188217354,-0.05628744,-0.0034295567,0.0900917152,0.0680986901,-0.0711589674,0.0217946299]},"86":{"Abstract":"The Parallel Coordinates plot is a popular tool for the visualization of high-dimensional data. One of the main challenges when using parallel coordinates is occlusion and overplotting resulting from large data sets. Brushing is a popular approach to address these challenges. Since its conception, limited improvements have been made to brushing both in the form of visual design and functional interaction. We present a set of novel, smart brushing techniques that enhance the standard interactive brushing of a parallel coordinates plot. We introduce two new interaction concepts: Higher-order, sketch-based brushing, and smart, data-driven brushing. Higher-order brushes support interactive, flexible, n-dimensional pattern searches involving an arbitrary number of dimensions. Smart, data-driven brushing provides interactive, real-time guidance to the user during the brushing process based on derived meta-data. In addition, we implement a selection of novel enhancements and user options that complement the two techniques as well as enhance the exploration and analytical ability of the user. We demonstrate the utility and evaluate the results using a case study with a large, high-dimensional, real-world telecommunication data set and we report domain expert feedback from the data suppliers.","Authors":"R. C. Roberts; R. S. Laramee; G. A. Smith; P. Brookes; T. D'Cruze","DOI":"10.1109\/TVCG.2018.2808969","Keywords":"Multivariate visualization;parallel coordinates;call center;glyph;brushing;interaction techniques","Keywords_Processed":"glyph;call center;interaction technique;multivariate visualization;brush;parallel coordinate","Title":"Smart Brushing for Parallel Coordinates","Labels":null,"Keyword_Vector":[0.2182067711,-0.0716737822,-0.1097165649,0.0160094218,0.0816712947,0.0494579139,0.0281009366,0.2365720918,-0.202739201,0.0897709668,-0.0392074233,0.1192866592,0.0218824781,-0.1294312902,-0.0533405008,0.1252213181,-0.0062068627,-0.0138605194,0.0609489622,0.0854248447,0.0173263387,0.0438347324,-0.1487457142,-0.0454481209],"Abstract_Vector":[0.2983369999,-0.1344159674,-0.0138613462,-0.0964979756,-0.1300982848,-0.0013389453,-0.0146802413,0.068048169,-0.1041041449,0.0231923695,0.0375444816,0.0865344901,-0.015507193,0.0632524807,-0.0157065297,-0.0233448033,-0.0074243271,0.0688182128,0.0388441057,-0.0882009614,0.072986896,-0.0780243617,-0.0716761849,-0.1004939774,-0.0411155337,-0.0314231244,-0.0022164862,-0.0176532952,0.0410153296,-0.0823252861,-0.0748028054,-0.0238444399,-0.0073196264,0.1525292291,-0.0476835309,0.1030656746,-0.0616301763,-0.028909686,0.0685905664,-0.0090764294,-0.1676771658,0.01009432,0.0346678101,0.0976732187,-0.1148745023,0.0336298075,-0.0223587488,-0.0022132267,-0.022792577,-0.0563135281,0.0169669704,-0.0771739905,0.0198994183,0.0263978662,0.070538666,-0.0588339384,-0.0850236687,0.0008476856,-0.0592673212,0.0339663882,0.0301011717,-0.0238265286,-0.0265055292,0.09111084,-0.0313506363,-0.1483880485,0.0039673196,-0.0037635437,-0.0022524003,0.1204153471,0.0252706649,0.0429639568,-0.1181419188,-0.0192572761,0.0154606247,0.0106286268]},"87":{"Abstract":"Wall-displays allow multiple users to simultaneously view and analyze large amounts of information, such as the increasingly complex graphs present in domains like biology or social network analysis. We focus on how pairs explore graphs on a touch enabled wall-display using two techniques, both adapted for collaboration: a basic localized selection, and a propagation selection technique that uses the idea of diffusion\/transmission from an origin node. We assess in a controlled experiment the impact of selection technique on a shortest path identification task. Pairs consistently divided space even if the task is not spatially divisible, and for the basic selection technique that has a localized visual effect, it led to parallel work that negatively impacted accuracy. The large visual footprint of the propagation technique led to close coordination, improving speed and accuracy for complex graphs only. We then observed the use of propagation on additional graph topology tasks, confirming pair strategies on spatial division and coordination.","Authors":"A. Prouzeau; A. Bezerianos; O. Chapuis","DOI":"10.1109\/TVCG.2016.2592906","Keywords":"Wall-displays;multi-user interaction;graph visualization;selection techniques;co-located collaboration","Keywords_Processed":"wall display;multi user interaction;graph visualization;selection technique;co locate collaboration","Title":"Evaluating Multi-User Selection for Exploring Graph Topology on Wall-Displays","Labels":null,"Keyword_Vector":[0.2087184749,0.0639881596,-0.0525713729,0.1075073036,-0.0421236412,-0.0615196187,0.0102437056,0.2954352837,-0.1384317122,0.1047119714,-0.1423826043,0.1209175915,0.0735702019,-0.063547127,0.1729313443,0.0336261422,-0.0230856685,0.105422623,0.0122224787,-0.0712971016,-0.0400928513,0.0029636844,0.072533726,-0.0389286187],"Abstract_Vector":[0.2230462165,-0.0520938612,0.109595437,0.1431249327,0.0410059156,-0.0530589621,0.1789698732,0.0890836655,0.1648475464,-0.045035066,-0.0559247593,0.015016292,-0.1215047604,-0.1312538119,-0.0021099018,0.0532034291,-0.0477088005,0.1046692992,0.0403052696,-0.0179179807,-0.0171806322,-0.1411210235,-0.0115375371,0.1393880749,0.0607681304,-0.1176345388,0.0092806129,0.0265650544,0.0141758954,0.0178808375,-0.0432823619,0.0373350708,0.0214191549,0.105795486,-0.0879706203,-0.0268830485,0.0423390579,-0.0425570694,-0.0356110473,0.0558748202,-0.0516215339,-0.0130678405,0.0955828583,0.0073139283,-0.1084100372,-0.0056206553,0.0122441379,0.0814986174,0.0427092357,0.0152314273,0.1182878732,-0.0428658838,0.0012948008,0.0101030344,0.0419642663,-0.0116449456,0.0196337069,-0.0282766089,0.0282853396,0.0802840828,-0.0569441227,0.0438618542,-0.0176854407,-0.0557030195,-0.0110817515,-0.1184302282,0.0162479824,0.073387914,0.1354890518,0.0158304133,0.0450833248,-0.0711512237,-0.014220266,0.0179910489,-0.0559593169,-0.01545951]},"88":{"Abstract":"We introduce an interactive user-driven method to reconstruct high-relief 3D geometry from a single photo. Particularly, we consider two novel but challenging reconstruction issues: i) common non-rigid objects whose shapes are organic rather than polyhedral\/symmetric, and ii) double-sided structures, where front and back sides of some curvy object parts are revealed simultaneously on image. To address these issues, we develop a three-stage computational pipeline. First, we construct a 2.5D model from the input image by user-driven segmentation, automatic layering, and region completion, handling three common types of occlusion. Second, users can interactively mark-up slope and curvature cues on the image to guide our constrained optimization model to inflate and lift up the image layers. We provide real-time preview of the inflated geometry to allow interactive editing. Third, we stitch and optimize the inflated layers to produce a high-relief 3D model. Compared to previous work, we can generate high-relief geometry with large viewing angles, handle complex organic objects with multiple occluded regions and varying shape profiles, and reconstruct objects with double-sided structures. Lastly, we demonstrate the applicability of our method on a wide variety of input images with human, animals, flowers, etc.","Authors":"C. Yeh; S. Huang; P. K. Jayaraman; C. Fu; T. Lee","DOI":"10.1109\/TVCG.2016.2574705","Keywords":"Reconstruction;high-relief;lenticular posters;single image;folded;double-sided;object modeling;depth cues;completion;inflation","Keywords_Processed":"fold;reconstruction;object modeling;lenticular poster;completion;depth cue;inflation;double sided;single image;high relief","Title":"Interactive High-Relief Reconstruction for Organic and Double-Sided Objects from a Photo","Labels":null,"Keyword_Vector":[0.0388728562,0.0217464246,0.0060683042,0.0882017228,0.0824230365,-0.105442384,0.0866732622,-0.1311964163,-0.082318555,-0.0681797464,-0.0216813817,-0.0458256494,0.0122683511,0.0549579082,0.0423805452,0.1657385609,-0.0086283203,0.1097344542,-0.026298701,0.069085401,0.0108866834,0.0445230633,0.0567677091,-0.0034646164],"Abstract_Vector":[0.2566842894,0.2401927078,-0.0844577224,0.0247117077,0.1678675559,-0.0059218064,-0.073671896,0.093326145,0.0022802222,0.1442988251,0.0806182671,-0.0243668569,-0.0997209591,-0.000177279,0.0295035555,-0.0402613019,0.1455703041,-0.0861654753,-0.0594856529,-0.1911450203,-0.0327601037,-0.0321606547,-0.0369131396,-0.0626378815,-0.0146202811,0.0203071774,0.0008091474,0.0731402221,-0.0163061924,-0.0527248519,-0.0048544068,-0.1545520392,-0.0317803712,-0.0256107562,-0.0343377212,0.0895879116,0.0304048462,0.0135558498,-0.074574063,-0.0019745356,-0.0595539172,0.0910228525,0.0169177188,0.0240271854,0.0062496234,-0.0369289332,-0.027041673,0.0090827158,-0.0267019976,0.0632519682,0.0721426275,-0.0996012492,-0.0941102807,0.0381566706,0.0061236863,-0.0922732276,0.0250684565,-0.1387527624,-0.0796049647,-0.078042014,0.0259088329,-0.0013960433,0.0080220165,0.0497628834,0.0819660439,0.0287042911,0.0886594292,-0.0818654576,-0.0447619078,0.0589169078,-0.1305902907,-0.0616933252,-0.0043417594,0.0243270538,-0.0248875261,-0.0109063412]},"89":{"Abstract":"We present a semi-automatic approach for stream surface generation. Our approach is based on the conjecture that good seeding curves can be inferred from a set of streamlines. Given a set of densely traced streamlines over the flow field, we design a sketch-based interface that allows users to describe their perceived flow patterns through drawing simple strokes directly on top of the streamline visualization results. Based on the 2D stroke, we identify a 3D seeding curve and generate a stream surface that captures the flow pattern of streamlines at the outermost layer. Then, we remove the streamlines whose patterns are covered by the stream surface. Repeating this process, users can peel the flow by replacing the streamlines with customized surfaces layer by layer. Furthermore, we propose an optimization scheme to identify the optimal seeding curve in the neighborhood of an original seeding curve based on surface quality measures. To support interactive optimization, we design a parallel surface quality estimation strategy that estimates the quality of a seeding curve without generating the surface. Our sketch-based interface leverages an intuitive painting metaphor which most users are familiar with. We present results using multiple data sets to show the effectiveness of our approach.","Authors":"J. Tao; C. Wang","DOI":"10.1109\/TVCG.2017.2750681","Keywords":"Flow visualization;sketch-based interface;human perception;seeding curves;stream surfaces","Keywords_Processed":"stream surface;sketch base interface;seed curve;human perception;flow visualization","Title":"Semi-Automatic Generation of Stream Surfaces via Sketching","Labels":null,"Keyword_Vector":[0.2031187484,0.0563331429,-0.0502116416,0.0302310254,-0.0880225517,-0.078713169,0.059275848,-0.1364036308,-0.0987142319,0.0431388114,0.314835598,0.1338496932,0.0754373367,-0.0354290033,0.1436038873,-0.0605179641,-0.056553481,-0.0065308479,0.1156609655,0.0068964344,0.0623642346,-0.1007415213,0.0685637592,-0.0614347469],"Abstract_Vector":[0.2253245094,0.1018612546,-0.1863866968,-0.0044184715,0.0075368429,0.0986667358,0.0982887953,-0.1563639599,-0.0431868477,0.0247218956,0.3303796106,0.0313502206,-0.0612250952,0.0845329817,-0.0292296835,-0.0094561237,-0.0233650373,0.2352275589,0.0407638503,-0.0138946963,0.0026801803,-0.0226831632,0.1850081882,-0.0894444319,-0.0992027131,-0.0177669537,-0.1527894999,-0.132790266,-0.0368897853,0.0186327567,0.1478519449,0.0393045553,-0.0295563705,0.0662218094,0.0122672485,0.0313059704,-0.0216645623,-0.0276566726,0.0471409198,-0.043809884,0.1192536669,0.0087612253,-0.0580961474,0.0141850179,-0.0967397729,-0.0529490833,0.0148426969,-0.177143903,-0.0389864145,-0.0026578652,0.0426246783,-0.0769932693,0.0838869098,0.0966458946,0.0639190681,0.0694521052,-0.0628163644,-0.0658091332,0.0354438421,0.0372117393,0.0946339307,0.0034866768,0.0235853725,-0.0285490566,-0.0475359858,-0.0400499286,0.0219772444,-0.0705872714,0.0842668503,-0.0983512598,0.0094229909,-0.0270284276,0.0561401895,-0.0701825177,-0.0328435434,0.0386050044]},"90":{"Abstract":"Using synthetic videos to present a 3D scene is a common requirement for architects, designers, engineers or Cultural Heritage professionals however it is usually time consuming and, in order to obtain high quality results, the support of a film maker\/computer animation expert is necessary. We introduce an alternative approach that takes the 3D scene of interest and an example video as input, and automatically produces a video of the input scene that resembles the given video example. In other words, our algorithm allows the user to \u201creplicate\u201d an existing video, on a different 3D scene. We build on the intuition that a video sequence of a static environment is strongly characterized by its optical flow, or, in other words, that two videos are similar if their optical flows are similar. We therefore recast the problem as producing a video of the input scene whose optical flow is similar to the optical flow of the input video. Our intuition is supported by a user-study specifically designed to verify this statement. We have successfully tested our approach on several scenes and input videos, some of which are reported in the accompanying material of this paper.","Authors":"A. Baldacci; F. Ganovelli; M. Corsini; R. Scopigno","DOI":"10.1109\/TVCG.2016.2608828","Keywords":"Multimedia content production;video similarity;2D vector field comparison;computer animation","Keywords_Processed":"video similarity;multimedia content production;2d vector field comparison;computer animation","Title":"Presentation of 3D Scenes Through Video Example","Labels":null,"Keyword_Vector":[0.0625818775,-0.0034810884,-0.0009617531,0.0821596507,0.0198871876,-0.0812502007,0.165601799,-0.0995576208,-0.1661467678,-0.0664042162,0.0431169204,-0.0939856776,0.1012991351,-0.0664246514,-0.0012343486,0.0360467822,-0.0409592893,-0.2353059817,-0.0308269397,-0.0606472579,-0.0491334755,-0.1543580706,0.0142681538,0.178127425],"Abstract_Vector":[0.1911900925,0.1926566436,0.0194907594,0.0463841204,-0.0039651434,-0.0751529393,0.0654580073,-0.0518541665,-0.1447499384,0.0520342628,0.1624661108,0.1999569073,0.029840309,-0.1571756833,-0.1706578196,0.0071707094,0.12772369,0.0334177802,-0.125961018,0.233874816,-0.0054267792,-0.0178540926,0.081912445,0.0313595183,0.1411763361,0.1498673417,0.0826816386,0.0122820081,0.0445297108,-0.0483598404,-0.0956594172,-0.0181365145,-0.0061720403,0.0481403273,0.0221526474,-0.0125561672,-0.0490259841,0.0288273196,0.0748258179,0.0032527679,-0.0986428925,-0.0247343447,-0.0959121418,-0.0029116679,0.036078965,0.0804948995,0.1205576537,-0.0960899507,0.0837155466,0.0175736178,0.0420653692,0.0405041092,0.0276352839,0.043591436,0.0432631644,-0.0706432323,-0.0902034013,-0.0870374284,-0.0359936409,-0.0674951602,-0.000854069,-0.1030918463,-0.0008997237,0.045296747,-0.0013030123,0.0562613675,-0.083476894,-0.0226881257,0.0490390956,0.0725672328,-0.0460300797,0.0914412468,-0.0055079624,0.0803743305,-0.0646967132,0.0236860292]},"91":{"Abstract":"We propose Average Vector Field (AVF) integration for simulation of deformable solids in physics-based animation. Our method achieves exact energy conservation for the St. Venant-Kirchhoff material without any correction steps or extra parameters. Exact energy conservation implies that our resulting animations 1) cannot explode and 2) do not suffer from numerical damping, which are two common problems with previous numerical integration techniques. Our method produces lively motion even with large time steps as typically used in physics-based animation. Our implicit update rules can be formulated as a minimization problem and solved in a similar way as optimization-based backward Euler, with only a mild computing overhead. Our approach also supports damping and collision response models, making it easy to deploy in practical computer animation pipelines.","Authors":"J. Rojas; T. Liu; L. Kavan","DOI":"10.1109\/TVCG.2018.2851233","Keywords":"Animation;three-dimensional graphics and realism","Keywords_Processed":"animation;three dimensional graphic and realism","Title":"Average Vector Field Integration for St. Venant-Kirchhoff Deformable Models","Labels":null,"Keyword_Vector":[0.0305643886,0.016618493,0.0140541403,0.0505176737,0.1326590484,-0.0543660715,0.1277829002,-0.1381787826,-0.1705454906,-0.0954110138,0.0382833364,-0.1808642941,0.1124408445,-0.0520405887,0.0217569825,0.2355270503,0.1964463915,-0.0324069455,0.2684115896,-0.0672527548,-0.1237166036,-0.0580341418,0.0498826149,-0.2574245309],"Abstract_Vector":[0.1748008297,0.1180460644,-0.102803022,-0.0804280625,-0.0200053547,-0.0297980826,0.0019632889,-0.0430381492,-0.0050086443,-0.1609933463,-0.0418213486,-0.0040083291,-0.0413829381,0.047919874,-0.1121570963,0.0525332609,0.0066515454,0.0720514514,0.0061427536,0.0200954804,-0.0568411033,-0.0596341395,-0.0223523364,0.0648973542,0.1033306719,0.1388182717,0.0529468553,-0.0541019688,-0.0486490219,-0.0430915706,-0.1624673612,-0.000259459,-0.0360438528,-0.015260897,-0.1078474208,-0.0434282739,0.0299239805,0.0676411103,-0.0951407769,0.0390849683,0.0418557449,-0.0674227137,-0.0313391669,-0.0940752006,0.0483318367,-0.0863584346,-0.0995890568,0.0900243941,-0.0244595007,-0.1016894285,-0.1407041676,-0.1249269903,-0.0077145497,-0.0236173304,0.0463429419,-0.0558065879,0.0795734079,0.00367923,0.0809083794,0.0580815467,-0.0800928997,-0.0099168312,0.1291023727,0.151276011,0.0181511701,-0.0479618443,0.1369368997,0.100820527,0.0609528399,-0.0825813659,-0.0019821686,-0.0164251703,0.0924306377,0.052921431,-0.0166578431,-0.0638562354]},"92":{"Abstract":"Many real-world datasets are large, multivariate, and relational in nature and relevant associated decisions frequently require a simultaneous consideration of both attributes and connections. Existing visualization systems and approaches, however, often make an explicit trade-off between either affording rich exploration of individual data units and their attributes or exploration of the underlying network structure. In doing so, important analysis opportunities and insights are potentially missed. In this study, we aim to address this gap by (1) considering visualizations and interaction techniques that blend the spectrum between unit and network visualizations, (2) discussing the nature of different forms of contexts and the challenges in implementing them, and (3) demonstrating the value of our approach for visual exploration of multivariate, relational data for a real-world use case. Specifically, we demonstrate through a system called Graphicle how network structure can be layered on top of unit visualization techniques to create new opportunities for visual exploration of physician characteristics and referral data. We report on the design, implementation, and evaluation of the system and effectiveness of our blended approach.","Authors":"T. Major; R. C. Basole","DOI":"10.1109\/TVCG.2018.2865151","Keywords":"Unit visualization;network visualization;context","Keywords_Processed":"context;network visualization;unit visualization","Title":"Graphicle: Exploring Units, Networks, and Context in a Blended Visualization Approach","Labels":null,"Keyword_Vector":[0.3167279235,-0.092266438,-0.2041389061,0.0600084292,-0.2105896458,-0.0542859948,0.0262064471,0.0267370971,-0.019680453,0.2443918295,-0.1527425934,-0.1178060632,-0.0393782043,-0.0413943443,-0.0386372776,-0.0571995932,0.1118210883,-0.0409283935,0.0596769727,0.0237549337,0.0678409505,0.1043636026,0.0009351894,0.0039853638],"Abstract_Vector":[0.3180120509,-0.244208553,0.0469587937,-0.0917451131,0.0693062043,-0.0855600979,0.1723383682,0.0905878901,0.0048253899,0.0987435686,-0.0832862705,0.0288209095,-0.130745088,-0.0005076511,-0.0694545109,-0.0711203147,0.0230905665,-0.0113197689,-0.0695636166,0.055484079,0.082426193,0.1404406756,-0.0361050898,-0.0874105242,-0.0939040012,0.0411968137,-0.0631884597,0.0276567387,-0.0562132082,-0.0327667876,-0.005665102,-0.0427455916,0.0213009114,-0.0884119021,0.0296991687,0.0571290032,-0.091280097,0.1239812335,0.0290572744,0.0184296131,0.0296547744,0.1240836483,-0.0306334028,0.0275903582,0.0840044632,0.0051832961,-0.0228390879,-0.0552962528,-0.0786020715,-0.0256003128,-0.1223018651,-0.0064123459,0.004812542,0.0026278449,0.0017632042,0.0358152453,0.0105155135,0.0388192216,0.0155289792,-0.0343713093,-0.0401289046,0.001264765,0.0313994194,0.0176208364,-0.1020550474,-0.0447257038,0.007120106,-0.1385947571,0.0027263636,0.0074383753,-0.0304119287,0.0050276929,-0.1163194704,0.0360747374,-0.0350893986,-0.0071908278]},"93":{"Abstract":"In this paper, we present Gaia Sky, a free and open-source multiplatform 3D Universe system, developed since 2014 in the Data Processing and Analysis Consortium framework of ESA's Gaia mission. Gaia's data release 2 represents the largest catalog of the stars of our Galaxy, comprising 1.3 billion star positions, with parallaxes, proper motions, magnitudes, and colors. In this mission, Gaia Sky is the central tool for off-the-shelf visualization of these data, and for aiding production of outreach material. With its capabilities to effectively handle these data, to enable seamless navigation along the high dynamic range of distances, and at the same time to provide advanced visualization techniques including relativistic aberration and gravitational wave effects, currently no actively maintained cross-platform, modern, and open alternative exists.","Authors":"A. Sagrist\u00e0; S. Jordan; T. M\u00fcller; F. Sadlo","DOI":"10.1109\/TVCG.2018.2864508","Keywords":"Astronomy visualization;3D Universe software;star catalog rendering;Gaia mission","Keywords_Processed":"astronomy visualization;gaia mission;3d universe software;star catalog rendering","Title":"Gaia Sky: Navigating the Gaia Catalog","Labels":null,"Keyword_Vector":[0.1362064805,-0.0094953674,-0.1066403587,0.121123634,0.0856540309,0.0113261776,-0.0307812157,0.0044903064,0.069614938,0.0432170927,0.0359235722,-0.0180425003,0.0368164026,0.0818685106,-0.0291966053,0.0343541162,0.0380063105,-0.0785719035,-0.0515716676,0.0915744967,0.1336789755,0.0837639793,-0.0009290043,-0.042945619],"Abstract_Vector":[0.1588706274,-0.0182466678,0.0028477349,-0.0925949707,-0.0524252981,0.0707986364,-0.004697655,0.0065995793,-0.0550777103,-0.0381983016,0.0075784348,0.0145398169,0.0218277787,-0.0382118183,-0.0254732185,0.0821887652,-0.0500030055,0.0097201363,-0.0506077065,0.0317654691,-0.0014072784,0.0426817569,-0.0901202465,0.0325058363,0.0055305295,0.1116884127,-0.0248494515,0.0239502222,0.014317191,0.0540231026,-0.007432598,-0.03983374,0.0008652551,-0.0113771299,0.0304219435,-0.0985160857,0.0953391711,-0.0169331078,0.0498095843,-0.0460301521,0.0232837745,-0.0067068468,0.0487336101,0.0888807012,-0.0477538914,0.065282544,-0.1478229085,0.0656097391,-0.0694583078,-0.0856625459,-0.0135103818,0.0588427075,0.0097114234,0.0794994677,-0.0194322514,-0.0021553747,0.0055239325,0.0016038695,-0.1639884189,-0.0830013363,0.0241291677,0.0196108964,-0.0831716107,0.0303863603,0.0157610333,0.0184183653,-0.0166260723,0.0357689719,-0.0231927476,0.0367185197,0.0555559808,-0.0418675251,0.0266555003,0.1235110197,0.0214084465,-0.0157462543]},"94":{"Abstract":"Virtual Reality (VR) applications allow a user to explore a scene intuitively through a tracked head-mounted display (HMD). However, in complex scenes, occlusions make scene exploration inefficient, as the user has to navigate around occluders to gain line of sight to potential regions of interest. When a scene region proves to be of no interest, the user has to retrace their path, and such a sequential scene exploration implies significant amounts of wasted navigation. Furthermore, as the virtual world is typically much larger than the tracked physical space hosting the VR application, the intuitive one-to-one mapping between the virtual and real space has to be temporarily suspended for the user to teleport or redirect in order to conform to the physical space constraints. In this paper we introduce a method for improving VR exploration efficiency by automatically constructing a multiperspective visualization that removes occlusions. For each frame, the scene is first rendered conventionally, the z-buffer is analyzed to detect horizontal and vertical depth discontinuities, the discontinuities are used to define disocclusion portals which are 3D scene rectangles for routing rays around occluders, and the disocclusion portals are used to render a multiperpsective image that alleviates occlusions. The user controls the multiperspective disocclusion effect, deploying and retracting it with small head translations. We have quantified the VR exploration efficiency brought by our occlusion removal method in a study where participants searched for a stationary target, and chased a dynamic target. Our method showed an advantage over conventional VR exploration in terms of reducing the navigation distance, the view direction rotation, the number of redirections, and the task completion time. These advantages did not come at the cost of a reduction in depth perception or situational awareness, or of an increase in simulator sickness.","Authors":"L. Wang; J. Wu; X. Yang; V. Popescu","DOI":"10.1109\/TVCG.2019.2898782","Keywords":"VR exploration;occlusion removal;disocclusion portal;multiperspective visualization","Keywords_Processed":"occlusion removal;vr exploration;multiperspective visualization;disocclusion portal","Title":"VR Exploration Assistance through Automatic Occlusion Removal","Labels":null,"Keyword_Vector":[0.1198592828,-0.0443820657,-0.0741602906,0.0281547303,-0.010832428,-0.035869296,0.0084773451,0.0272257394,-0.0118878378,0.113171047,-0.075229387,-0.1003315459,-0.085227158,0.0194803091,0.1023375981,-0.0971340635,0.0280280785,-0.0769277964,0.0697693596,0.0773592464,-0.173580696,0.0099918644,-0.0875191658,0.010128713],"Abstract_Vector":[0.2581676484,0.2054905062,0.2474035029,0.1431276127,-0.0649096756,-0.0821869783,0.1114817707,0.1040405651,-0.1570163516,0.0757498333,-0.0416337243,0.0506202829,0.0660792098,-0.0045501264,0.0112117769,0.074093612,-0.0278350645,-0.1117871803,-0.0150820475,-0.0339241519,0.0528853906,0.0058055659,0.0585699896,-0.1273849315,-0.0752928968,0.0447717135,0.0629609366,0.0389915801,-0.0743690412,-0.0415343499,0.1028368552,0.0360924069,-0.0601976347,0.1144404084,0.0237325948,-0.0697835198,-0.0541979285,0.1366269789,0.0494103449,-0.0800578671,0.0899897883,0.0644203427,-0.0636165984,-0.0422109513,0.0066513382,-0.0065381025,0.0437692002,0.0161868173,-0.1046887221,-0.1406442341,0.0593924876,-0.1259679058,0.0099845531,-0.043082396,-0.0425206003,-0.0542164301,0.0188523961,0.1061736828,0.0826203985,-0.009056624,0.049058037,0.0157348266,-0.0528213794,0.0044543766,0.0434725685,0.042680825,-0.0566838924,0.0013231087,-0.0141575098,0.0528854181,-0.0393787418,0.0165374852,-0.0449252025,-0.066397935,-0.0304982941,0.0328757789]},"95":{"Abstract":"People often have erroneous intuitions about the results of uncertain processes, such as scientific experiments. Many uncertainty visualizations assume considerable statistical knowledge, but have been shown to prompt erroneous conclusions even when users possess this knowledge. Active learning approaches been shown to improve statistical reasoning, but are rarely applied in visualizing uncertainty in scientific reports. We present a controlled study to evaluate the impact of an interactive, graphical uncertainty prediction technique for communicating uncertainty in experiment results. Using our technique, users sketch their prediction of the uncertainty in experimental effects prior to viewing the true sampling distribution from an experiment. We find that having a user graphically predict the possible effects from experiment replications is an effective way to improve one's ability to make predictions about replications of new experiments. Additionally, visualizing uncertainty as a set of discrete outcomes, as opposed to a continuous probability distribution, can improve recall of a sampling distribution from a single experiment. Our work has implications for various applications where it is important to elicit peoples' estimates of probability distributions and to communicate uncertainty effectively.","Authors":"J. Hullman; M. Kay; Y. Kim; S. Shrestha","DOI":"10.1109\/TVCG.2017.2743898","Keywords":"Graphical prediction;interactive uncertainty visualization;replication crisis;probability distribution","Keywords_Processed":"replication crisis;probability distribution;interactive uncertainty visualization;graphical prediction","Title":"Imagining Replications: Graphical Prediction & Discrete Visualizations Improve Recall & Estimation of Effect Uncertainty","Labels":null,"Keyword_Vector":[0.1986771047,-0.0709571024,-0.1382222247,-0.0314810183,-0.1168276387,0.029252859,-0.0982498392,-0.2037807919,-0.0620132318,-0.0643960202,-0.0997385488,-0.0310538437,-0.0700905298,0.1972285331,0.1155419008,-0.0101846377,0.0056976531,0.103188669,-0.0433085883,0.0621895903,-0.0557299879,-0.1760113845,-0.0440126892,0.0490483841],"Abstract_Vector":[0.2184033473,-0.0818521133,0.1035050587,0.2697395085,0.0332262578,0.2300220345,-0.1017083397,-0.1454098689,0.2721137145,-0.166678104,-0.1093269845,0.2787890094,-0.0300807698,0.054709151,-0.0298194686,0.1043467926,-0.0337112011,0.0609610257,-0.0748737208,-0.0542057124,0.158010897,0.0473244366,0.1655254104,-0.0925944774,-0.0042549645,-0.0318450234,-0.0229969654,-0.0000686822,0.0397536633,0.0216147326,0.0178225701,-0.1258576386,-0.0527732418,0.0030916241,-0.026857917,0.0483402112,0.0816248579,0.0092996588,-0.0105810034,0.0658662854,-0.0973420775,0.0016885475,-0.1009787707,0.0380649971,-0.0455186864,0.033338416,0.0055495896,0.018280216,0.013358858,0.0005815509,-0.0524222666,-0.0564365192,-0.0071633765,0.0982552795,-0.0562323337,0.0027328785,-0.0312073444,0.1055423846,-0.0412972867,0.012119827,0.0232810656,-0.0176691995,-0.0697056963,0.0343453412,0.0212369163,0.0941987828,-0.0354322556,0.0207138725,-0.005739904,-0.0491920031,0.01389964,0.007844728,0.0377439529,-0.038975664,0.0589582578,0.0070064119]},"96":{"Abstract":"Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theater-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.","Authors":"M. Chen; K. Gaither; N. W. John; B. Mccann","DOI":"10.1109\/TVCG.2018.2865025","Keywords":"Theory of visualization;virtual environments;virtual reality;augmented reality;mixed reality;cost-benefit analysis;information theory;cognitive sciences;visualization applications;immersive analytics;four levels of visualization","Keywords_Processed":"theory of visualization;augment reality;virtual reality;visualization application;four level of visualization;information theory;cost benefit analysis;immersive analytic;virtual environment;cognitive science;mixed reality","Title":"An Information-Theoretic Approach to the Cost-benefit Analysis of Visualization in Virtual Environments","Labels":null,"Keyword_Vector":[0.3818356614,0.3720323757,-0.045404019,-0.1129919715,0.0175534877,-0.0706030115,-0.0542328704,0.0476536531,0.152735395,0.0105888869,-0.010204692,-0.0405589824,0.0925026529,0.0408982473,-0.249656564,-0.1341760698,-0.0626939657,0.0040732138,-0.0508559912,-0.0397163356,-0.0658991152,0.0526789466,0.0478543334,0.0523167728],"Abstract_Vector":[0.2129709441,-0.0885832267,0.1610875723,-0.0190060024,0.0967060335,0.1086187432,0.0141660087,-0.0926229516,-0.1525880111,-0.0002103661,-0.0500023979,-0.096212455,-0.0255734102,-0.0050837939,-0.0461794165,-0.0177594349,0.0230043335,0.0355960662,0.0596901867,0.0224770943,-0.0267262593,0.0433713391,0.0929182804,0.1255044363,-0.1078849349,0.119133536,0.0928299169,0.0460876475,-0.0292366642,0.1183324044,0.0349622527,0.1044066744,0.0498584526,-0.0162407869,0.0660880434,-0.0144517569,0.0028173104,-0.0723866166,0.0759818277,-0.0253097545,-0.0274930835,0.0098275427,0.1547495345,0.0071353464,0.0780733103,0.0165816113,-0.0343540463,0.0139860175,-0.0272087716,-0.0126928365,-0.0634793001,-0.0635236285,-0.0469477583,-0.022695216,-0.0178278637,-0.0040662387,0.1013994022,-0.0253586131,-0.059468965,0.1073264714,0.0162106025,0.0011387487,-0.0315261276,0.1889937913,-0.0099169751,-0.0140633836,-0.0169017714,0.0003064211,-0.1052923476,0.01641407,-0.0732988996,-0.0131532482,0.0079636312,-0.0826772627,0.0429672428,-0.0097637622]},"97":{"Abstract":"We propose an occlusion compensation method for optical see-through head-mounted displays (OST-HMDs) equipped with a singlelayer transmissive spatial light modulator (SLM), in particular, a liquid crystal display (LCD). Occlusion is an important depth cue for 3D perception, yet realizing it on OST-HMDs is particularly difficult due to the displays' semitransparent nature. A key component for the occlusion support is the SLM-a device that can selectively interfere with light rays passing through it. For example, an LCD is a transmissive SLM that can block or pass incoming light rays by turning pixels black or transparent. A straightforward solution places an LCD in front of an OST-HMD and drives the LCD to block light rays that could pass through rendered virtual objects at the viewpoint. This simple approach is, however, defective due to the depth mismatch between the LCD panel and the virtual objects, leading to blurred occlusion. This led existing OST-HMDs to employ dedicated hardware such as focus optics and multi-stacked SLMs. Contrary to these viable, yet complex and\/or computationally expensive solutions, we return to the single-layer LCD approach for the hardware simplicity while maintaining fine occlusion-we compensate for a degraded occlusion area by overlaying a compensation image. We compute the image based on the HMD parameters and the background scene captured by a scene camera. The evaluation demonstrates that the proposed method reduced the occlusion leak error by 61.4% and the occlusion error by 85.7%.","Authors":"Y. Itoh; T. Hamasaki; M. Sugimoto","DOI":"10.1109\/TVCG.2017.2734427","Keywords":"Occlusion support;optical see-through HMD;occlusion leak;spatial light modulator;depth cue","Keywords_Processed":"optical see through hmd;occlusion leak;depth cue;spatial light modulator;occlusion support","Title":"Occlusion Leak Compensation for Optical See-Through Displays Using a Single-Layer Transmissive Spatial Light Modulator","Labels":null,"Keyword_Vector":[0.0525930946,0.0300405061,-0.022169534,0.042738915,-0.0008546276,-0.0641455364,0.0071146735,0.023377691,0.0195092521,-0.0522436421,0.0120310291,-0.1798859766,-0.1023366729,-0.0200112723,0.1220035435,-0.0145552454,-0.0251992267,-0.0069784846,0.0667831166,0.1172108687,0.0532625327,0.022457322,-0.0824041157,-0.0510759764],"Abstract_Vector":[0.1497542383,0.2024568426,0.0986834207,0.1193089733,-0.0047017309,0.0193535753,0.0241444341,0.1259564478,-0.1551213589,0.0681683923,-0.0966403704,0.118576064,0.0431320407,-0.0636015789,-0.047099479,0.0582780119,-0.0281184175,-0.1031428155,-0.0222822165,0.029831277,0.0041824468,-0.0297152398,-0.0216194624,-0.0765570377,-0.0532019,-0.116195199,-0.1002785342,0.0119131117,-0.0625732825,0.0033082502,0.0855398952,0.0191800954,-0.1102905847,-0.0299032934,-0.0065853684,0.0129655013,-0.046716519,0.0343734095,-0.0547332832,0.0268087648,0.097898597,0.0491886428,0.1239584854,0.0407412001,-0.004113807,-0.1273804069,0.0508728496,-0.0112685443,-0.025327949,-0.0900424292,0.036852697,0.0015578774,-0.0688481264,-0.0239346385,0.0983530204,0.0112311682,-0.0213340697,-0.0510489745,0.1188891342,0.0284793555,-0.0092282139,0.0025737892,0.0062396218,0.0195598927,0.0025216519,-0.0401914547,0.0778844915,-0.0644969279,0.0530405616,0.0627623984,-0.077193917,-0.0221426321,0.0936499227,-0.0203999321,0.0650789298,0.0802693909]},"98":{"Abstract":"Virtual characters that appear almost photo-realistic have been shown to induce negative responses from viewers in traditional media, such as film and video games. This effect, described as the uncanny valley, is the reason why realism is often avoided when the aim is to create an appealing virtual character. In Virtual Reality, there have been few attempts to investigate this phenomenon and the implications of rendering virtual characters with high levels of realism on user enjoyment. In this paper, we conducted a large-scale experiment on over one thousand members of the public in order to gather information on how virtual characters are perceived in interactive virtual reality games. We were particularly interested in whether different render styles (realistic, cartoon, etc.) would directly influence appeal, or if a character's personality was the most important indicator of appeal. We used a number of perceptual metrics such as subjective ratings, proximity, and attribution bias in order to test our hypothesis. Our main result shows that affinity towards virtual characters is a complex interaction between the character's appearance and personality, and that realism is in fact a positive choice for virtual characters in virtual reality.","Authors":"K. Zibrek; E. Kokkinara; R. Mcdonnell","DOI":"10.1109\/TVCG.2018.2794638","Keywords":"Personality;virtual characters;virtual reality;perception","Keywords_Processed":"virtual reality;virtual character;personality;perception","Title":"The Effect of Realistic Appearance of Virtual Characters in Immersive Environments - Does the Character's Personality Play a Role?","Labels":null,"Keyword_Vector":[0.1932115825,0.5624437281,0.0421085065,-0.1402514252,0.064821395,0.0781000364,-0.0239014444,-0.1898724121,0.0080554891,0.0329076396,0.0143905793,-0.0164573876,0.0313879506,-0.0059640525,-0.0571476439,-0.0036093396,0.0648383818,-0.062164362,0.0456352694,-0.0765826291,-0.0006644285,0.0415130759,0.0155960973,0.1157873573],"Abstract_Vector":[0.1544072424,0.1698150826,0.2292203468,-0.0373257313,-0.2071545244,-0.0595168904,-0.0004043616,-0.0721507059,0.0511859841,-0.0277413648,-0.1168594084,-0.0333933309,-0.0988940058,0.1553537142,-0.0125513307,-0.0035238407,0.1373015429,0.0103006911,0.1203321435,0.021664862,0.0539642682,0.1503098055,0.0601842138,0.0491444787,0.0142585037,0.1018353941,0.0988438222,-0.1785119523,0.0143241554,0.0325020058,-0.0727615172,0.001159459,-0.065946586,0.0171091301,-0.0425840313,-0.0202072755,-0.0022045968,-0.0129331737,-0.008114178,-0.0077992829,0.0316590305,-0.0104334876,0.0213587818,0.0217430965,0.1114522882,-0.0384675117,0.0531503807,-0.1626335152,0.2264501009,0.081053273,0.0241145317,0.020976039,-0.0057465126,-0.0168764508,-0.0096361099,-0.0901968675,0.0200856299,-0.1126982099,0.0115171132,-0.0168582927,0.0766850279,-0.0309613829,-0.0874979921,-0.0215979233,0.0040801204,-0.0228293042,-0.0323926362,-0.0495737347,0.0274376216,0.0251118034,-0.0268199173,-0.0732884873,-0.1177891779,-0.0106273502,0.0594056894,0.0367987977]},"99":{"Abstract":"Working with prescribed velocity gradients is a promising approach to efficiently and robustly simulate highly viscous SPH fluids. Such approaches allow to explicitly and independently process shear rate, spin, and expansion rate. This can be used to, e.g., avoid interferences between pressure and viscosity solvers. Another interesting aspect is the possibility to explicitly process the vorticity, e.g., to preserve the vorticity. In this context, this paper proposes a novel variant of the prescribed-gradient idea that handles vorticity in a physically motivated way. In contrast to a less appropriate vorticity preservation that has been used in a previous approach, vorticity is diffused. The paper illustrates the utility of the vorticity diffusion. Therefore, comparisons of the proposed vorticity diffusion with vorticity preservation and additionally with vorticity damping are presented. The paper further discusses the relation between prescribed velocity gradients and prescribed velocity Laplacians which improves the intuition behind the prescribed-gradient method for highly viscous SPH fluids. Finally, the paper discusses the relation of the proposed method to a physically correct implicit viscosity formulation.","Authors":"A. Peer; M. Teschner","DOI":"10.1109\/TVCG.2016.2636144","Keywords":"Physically based modeling;fluid simulation;smoothed-particle hydrodynamics;viscosity;prescribed velocity gradients","Keywords_Processed":"viscosity;fluid simulation;physically base modeling;prescribed velocity gradient;smooth particle hydrodynamic","Title":"Prescribed Velocity Gradients for Highly Viscous SPH Fluids with Vorticity Diffusion","Labels":null,"Keyword_Vector":[0.049863282,0.0180587472,-0.0131681298,0.0641463376,0.02376548,-0.136783327,0.1293767175,-0.1253187244,-0.1007706461,-0.1107257493,-0.023888228,0.1148386033,0.0229728829,0.0780993471,-0.1190270296,-0.1996736515,0.0582846489,0.0950663174,0.3254553367,0.179737313,0.0818851537,0.1444006112,-0.0795318434,0.0856417349],"Abstract_Vector":[0.0911799332,0.0477605078,-0.0504093712,-0.0079528111,0.0324847569,-0.0087406495,0.02393341,0.0049487718,-0.008097693,-0.1091193154,-0.0606549483,-0.0488508164,0.0145956402,0.1014242463,0.009468433,0.0744888112,-0.0714405444,0.0222232372,-0.0480179153,0.0277561948,0.0214979444,-0.1122133601,-0.037556208,0.057727179,0.0222646577,0.0174344669,-0.0238634527,0.0850847351,0.0383299841,-0.1610601662,-0.0188767166,0.0500512645,0.039506403,-0.0616833493,0.0137795216,0.0007148722,0.0265780293,-0.0619471572,-0.029651964,-0.0861207789,-0.0673732812,0.0711776353,0.0112690831,-0.0542810131,0.1014635841,-0.0320220523,0.1484546064,0.0201304974,0.0466080297,0.0833586761,0.0132465585,0.0367849568,-0.0460149256,0.0428473583,-0.0963510155,0.1847624857,0.0527450861,-0.1284151244,-0.0343777207,-0.1309015495,0.032223816,-0.0618099669,0.0711499376,-0.113541143,0.0185989854,-0.2048991965,-0.0112536321,-0.0858312815,0.0431695494,0.0414827628,0.0014658069,0.0360111793,-0.0768094797,-0.0055286858,0.0621583165,-0.0808469574]},"100":{"Abstract":"Weathering effects are ubiquitous phenomena in cities. Buildings age and deteriorate over time as they interact with the environment. Pollution accumulating on facades is a particularly visible consequence of this. Even though relevant work has been done to produce impressive images of virtual urban environments including weathering effects, so far, no technique using a global approach has been proposed to deal with weathering effects. Here, we propose a technique based on a fast physically-inspired approach, that focuses on modeling the changes in appearance due to pollution soiling on an urban scale. We consider pollution effects to depend on three main factors: wind, rain and sun exposure, and we take into account three intervening steps: deposition, reaction and washing. Using a low-cost pre-computation, we evaluate the pollution distribution throughout the city. Based on this and the use of screen-space operators, our method results in an efficient approach able to generate realistic images of urban scenes by combining the intervening factors at interactive rates. In addition, the pre-computation demands a reduced amount of memory to store the resulting pollution map and, as it is independent from scene complexity, it can suit large and complex models by adapting the map resolution.","Authors":"I. Mu\u00f1oz-Pandiella; C. Bosch; N. M\u00e9rillou; G. Patow; S. M\u00e9rillou; X. Pueyo","DOI":"10.1109\/TVCG.2018.2794526","Keywords":"Weathering;appearance;interactive rendering;urban scenes;screen-space techniques;shading and texture;picture\/image generation","Keywords_Processed":"appearance;screen space technique;shade and texture;interactive rendering;urban scene;picture image generation;weather","Title":"Urban Weathering: Interactive Rendering of Polluted Cities","Labels":null,"Keyword_Vector":[0.0782446891,-0.0200404003,-0.0046565399,0.1664348607,0.1209652359,-0.0336046138,0.1351009501,-0.1046020755,-0.0167012996,0.1018854363,-0.0367938708,0.0054269339,-0.0302328384,0.1544696626,0.0175931744,0.0323808066,-0.2062944473,0.2257205817,-0.1375351827,0.0376200511,-0.0598633672,0.1182528758,-0.0027439558,-0.0865816953],"Abstract_Vector":[0.2089122479,0.1179101689,-0.0198227798,0.0329810401,0.005619332,0.0123595197,0.0094852023,0.0743568854,-0.0351951602,0.002273781,-0.0322775853,0.0916387205,-0.0052410147,0.1015537772,-0.0403110451,0.0610299027,-0.0744319209,-0.0212003407,0.1436257692,-0.0261320893,-0.0397244277,-0.0280347967,0.0064540058,-0.0169269375,-0.0333157187,0.0776172987,0.2398307399,0.0819051904,0.0028349322,0.0174542468,0.0813872279,-0.0881264816,0.1787231246,-0.0416328123,0.0325642464,-0.0333217847,0.0856383331,0.1075809577,0.0204546578,0.183551185,-0.0178892825,0.0161258737,-0.029871165,0.0122498595,-0.0809172307,-0.0192704124,0.0702676451,-0.0207205554,0.0159810112,-0.0441475631,0.0794891896,-0.010722955,-0.0881854834,-0.049822388,-0.0274517477,0.0054284544,0.0710802738,-0.0391011304,-0.0874333255,-0.0838449647,-0.0157541089,0.0369720231,0.0224133011,-0.09198409,0.0310545372,-0.0733843535,-0.044379062,-0.0416719408,0.095693454,-0.0664678686,0.0470182265,0.0059606469,0.0825712954,-0.0139862319,0.0488294167,0.0238853273]},"101":{"Abstract":"In meteorology, cluster analysis is frequently used to determine representative trends in ensemble weather predictions in a selected spatio-temporal region, e.g., to reduce a set of ensemble members to simplify and improve their analysis. Identified clusters (i.e., groups of similar members), however, can be very sensitive to small changes of the selected region, so that clustering results can be misleading and bias subsequent analyses. In this article, we - a team of visualization scientists and meteorologists-deliver visual analytics solutions to analyze the sensitivity of clustering results with respect to changes of a selected region. We propose an interactive visual interface that enables simultaneous visualization of a) the variation in composition of identified clusters (i.e., their robustness), b) the variability in cluster membership for individual ensemble members, and c) the uncertainty in the spatial locations of identified trends. We demonstrate that our solution shows meteorologists how representative a clustering result is, and with respect to which changes in the selected region it becomes unstable. Furthermore, our solution helps to identify those ensemble members which stably belong to a given cluster and can thus be considered similar. In a real-world application case we show how our approach is used to analyze the clustering behavior of different regions in a forecast of \u201cTropical Cyclone Karl\u201d, guiding the user towards the cluster robustness information required for subsequent ensemble analysis.","Authors":"A. Kumpf; B. Tost; M. Baumgart; M. Riemer; R. Westermann; M. Rautenhaus","DOI":"10.1109\/TVCG.2017.2745178","Keywords":"Uncertainty visualization;ensemble visualization;clustering;meteorology","Keywords_Processed":"meteorology;cluster;uncertainty visualization;ensemble visualization","Title":"Visualizing Confidence in Cluster-Based Ensemble Weather Forecast Analyses","Labels":null,"Keyword_Vector":[0.3647448786,-0.192063578,-0.2888995924,-0.1355961449,-0.0627477224,-0.1275497283,-0.3122245429,-0.2308870414,0.0874484093,-0.1711753991,-0.25113939,0.092556143,0.0485224517,-0.0468758052,0.0529406036,0.0610844967,0.0367158925,0.0299367283,-0.0341079913,-0.0318212044,-0.0527533273,-0.0315562336,-0.0433782692,-0.0159488539],"Abstract_Vector":[0.2483665787,-0.1615652444,-0.1274579766,0.5123669936,-0.1824519135,-0.1501879273,-0.1804381788,-0.0860407209,-0.021250488,0.0294924744,0.0476127033,-0.1744246468,-0.0474105638,-0.0014274079,-0.0935289495,0.0420963115,0.089965421,-0.0165601588,0.0403987451,0.1136690927,-0.0888294845,-0.0356003956,-0.06062509,0.0014164864,-0.0992845464,0.0464558359,0.0331959601,-0.1144485629,-0.0464551865,-0.0054741309,0.0266773981,0.0016434858,0.0447290262,-0.104430654,0.0211559499,-0.029605793,0.0107440333,0.032942095,-0.0109725169,-0.0014357338,0.066821168,0.0284470689,0.0077473708,0.0313587174,0.0837851191,0.010336752,-0.0235564565,0.0272234089,0.0055865388,-0.0068986774,-0.0314016168,0.0123383203,0.0024911487,0.0309159253,0.0154861114,-0.0616580886,0.0411352835,0.0009393972,-0.0266668572,-0.0562280607,-0.0231488901,-0.0225304713,0.0227187194,0.0086913549,0.0112455311,0.0287154202,0.0175015545,-0.0431825923,-0.0039058433,0.0246440991,0.0204738976,0.0454010885,-0.0039446507,0.0020323078,0.0204989743,-0.0052307373]},"102":{"Abstract":"We propose a novel approach to simulating the formation and evolution of stains on cloths in motion. We accurately capture the diffusion of a pigmented solution over a complex knitted or woven fabric through homogenization of its inhomogeneous and\/or anisotropic properties into bulk anisotropic diffusion tensors. Secondary effects such as absorption, adsorption and evaporation are also accounted for through physically-based modeling. Finally, the influence of the cloth motion on the shape and evolution of the stain is captured by evaluating the inertial (e.g., centrifugal and Coriolis) forces experienced by the solution. The governing equations of motion are integrated in time directly on a deforming triangle mesh discretizing the inelastic cloth for efficiency and robustness. The deformation of the cloth can be precomputed or integrated through simplified two-way coupling, by using off-the-shell cloth simulations. Finally, numerical experiments demonstrate the plausibility of our results in practical applications by reproducing the usual shape and behavior of stains on various fabrics.","Authors":"X. Wang; S. Liu; Y. Tong","DOI":"10.1109\/TVCG.2017.2789203","Keywords":"Cloth animation;inhomogeneous and anisotropic cloth material;stain formation;deforming surfaces","Keywords_Processed":"stain formation;deform surface;inhomogeneous and anisotropic cloth material;cloth animation","Title":"Stain Formation on Deforming Inelastic Cloth","Labels":null,"Keyword_Vector":[0.0162499144,0.0051107438,-0.0058102532,0.0573871457,0.054729072,-0.0682921121,0.1012342265,-0.086667459,-0.0854070025,-0.0727543389,0.0372686711,0.0000090007,0.0634649951,-0.0468935274,0.0617556537,-0.0487922102,-0.0130041919,-0.0523709515,0.162010132,-0.0122854977,0.0433834345,0.0493552188,-0.0532305624,0.0247816322],"Abstract_Vector":[0.1251777687,0.1102939886,-0.0810409017,-0.0552359661,-0.0788199177,-0.0442767674,-0.0058410061,-0.1575795711,0.0372214597,-0.1516027962,-0.0816369448,-0.0742545982,-0.015362409,-0.0175503874,-0.0018290116,-0.014439713,-0.103140695,-0.0730708301,-0.000491101,0.0628413352,0.0671455803,-0.0374762188,-0.0083376489,-0.0165386111,-0.0407971485,-0.006343921,-0.0013446497,0.1664768144,-0.1117511134,-0.0797137903,-0.0011034235,0.0367716396,0.0421662567,-0.0383930164,0.0518919446,-0.0412927018,0.0122203815,-0.1089216525,-0.0921328825,0.0180159688,0.0201620785,0.058325193,0.0422572483,0.1687112026,-0.0218057942,0.011087189,-0.0275827653,0.0311022547,-0.0400752735,-0.0053062207,-0.0056769097,-0.1113740506,-0.064629471,0.0865925381,0.2191779819,0.0376655781,0.0038460262,-0.0789107026,-0.0342859169,-0.0157365008,0.0055961778,-0.1441399614,0.0671327401,0.0212339455,0.0996203014,0.0607128568,-0.0659137325,0.0325886967,0.1034477026,-0.0431076677,-0.0061562121,0.0531372898,-0.0229663874,-0.125837351,0.0925424874,0.0056040592]},"103":{"Abstract":"In this paper, we present a novel grid encoding model for content-aware image retargeting. In contrast to previous approaches such as vertex-based and axis-aligned grid encoding models, our approach takes each horizontal\/vertical distance between two adjacent vertices as an optimization variable. Upon this difference-based encoding scheme, every vertex position of a target grid is subsequently determined after optimizing the one-dimensional values. Our quad edge-based grid model has two major advantages for image retargeting. First, the model enables a grid optimization problem to be developed in a simple quadratic program while ensuring the global convexity of objective functions. Second, due to the independency of variables, spatial regularizations can be applied in a locally adaptive manner to preserve structural components. Based on this model, we propose three quadratic objective functions. Note that, in our work, their linear combination guides a grid deformation process to obtain a visually comfortable retargeting result by preserving salient regions and structural components of an input image. Comparative evaluations have been conducted with ten existing state-of-the-art image retargeting methods, and the results show that our method built upon the quad edge-based model consistently outperforms other previous methods both on qualitative and quantitative perspectives.","Authors":"Y. Kim; H. Eun; C. Jung; C. Kim","DOI":"10.1109\/TVCG.2018.2866106","Keywords":"Image retargeting;2D grid deformation;saliency detection;line segment detection;image quality assessment","Keywords_Processed":"image retargeting;image quality assessment;2d grid deformation;line segment detection;saliency detection","Title":"A Quad Edge-Based Grid Encoding Model for Content-Aware Image Retargeting","Labels":null,"Keyword_Vector":[0.0648139303,-0.0289680704,0.0944645428,0.0518741951,0.0760537742,-0.1452151921,0.1634321768,-0.173528946,0.0661539806,0.1007834718,0.0075186813,0.0306297454,0.092534546,-0.0220387267,0.0723430045,0.1424556698,-0.2190733902,0.091320698,-0.1562736843,0.044151823,0.0203651569,0.0765307392,-0.0634827721,0.1350917377],"Abstract_Vector":[0.2338096206,0.1663609877,-0.1366733307,-0.0139615764,0.1784309437,0.0299476813,-0.0730319371,0.0710642264,0.0710661961,0.0458640456,-0.0782558712,-0.0540850493,0.0467568865,0.0309270738,-0.0615876584,0.0535265988,0.0523527925,-0.0223460453,0.1129110096,-0.0791829032,-0.0295305993,-0.0130980198,-0.0039330907,-0.0750635782,0.0286679914,0.0244982006,0.0927308089,0.0274801992,0.0593940587,-0.0287460881,0.0792226599,0.1229606975,-0.0747388774,-0.0554020541,-0.0326350379,0.019861134,-0.1031055751,-0.0794630015,-0.0303730999,-0.103471726,-0.0039006864,-0.0484739408,-0.0511237218,-0.1391293645,-0.0522889346,-0.0363165687,-0.0712078384,-0.0693077298,0.0304012976,-0.1366371371,-0.0769873865,0.069179879,0.0953060334,0.0213285641,0.0048044186,-0.1048327244,0.0130489375,-0.0127787604,-0.0901734392,-0.0821233034,0.0260135391,-0.0660597491,-0.006487122,0.0296973763,-0.0109335833,0.0217405565,0.0715228184,-0.0158918611,0.0735235144,0.0290001636,0.0328876852,-0.0030066411,0.0591204385,-0.0453302651,-0.0496753963,-0.0844212448]},"104":{"Abstract":"Manual editing of a metro map is essential because many aesthetic and readability demands in map generation cannot be achieved by using a fully automatic method. In addition, a metro map should be updated when new metro lines are developed in a city. Considering that manually designing a metro map is time-consuming and requires expert skills, we present an interactive editing system that considers human knowledge and adjusts the layout to make it consistent with user expectations. In other words, only a few stations are controlled and the remaining stations are relocated by our system. Our system supports both curvilinear and octilinear layouts when creating metro maps. It solves an optimization problem, in which even spaces, route straightness, and maximum included angles at junctions are considered to obtain a curvilinear result. The system then rotates each edge to extend either vertically, horizontally, or diagonally while approximating the station positions provided by users to generate an octilinear layout. Experimental results, quantitative and qualitative evaluations, and user studies show that our editing system is easy to use and allows even non-professionals to design a metro map.","Authors":"Y. Wang; W. Peng","DOI":"10.1109\/TVCG.2015.2430290","Keywords":"Metro Map;Interactive editing;octilinear;least squares optimization;Metro map;interactive editing;octilinear;least squares optimization","Keywords_Processed":"octilinear;interactive editing;least square optimization;metro map","Title":"Interactive Metro Map Editing","Labels":null,"Keyword_Vector":[0.0412885944,-0.0241678326,0.0020917661,0.0262397158,0.020431365,-0.0068699881,0.0452042237,-0.0437885867,-0.0308609169,0.0083168291,0.0026709267,-0.0355339922,-0.1285923286,0.1766138063,-0.0339605716,-0.0221054663,-0.0569473936,0.0064345736,0.0018208313,0.0345445726,-0.0213131787,-0.0631748556,0.1082940693,-0.0579936539],"Abstract_Vector":[0.1764755729,0.0147129282,0.0213715671,-0.0389045978,0.0412143626,0.0092963321,0.0033115261,0.0800716731,0.0235302961,-0.0474354297,0.1525428373,0.0400240605,0.0358111575,0.0095627819,-0.0655992375,0.0425487277,-0.0384182224,0.0353537845,0.0185853563,-0.1336338751,-0.075025981,0.118018812,-0.0675191941,0.0200425879,0.1197951656,0.0782950319,0.1026714495,-0.0824756029,0.0641725327,-0.016592446,0.005673849,-0.0691868517,0.117305208,-0.0913414825,0.0957748674,0.0445393425,-0.0447244715,0.0189019921,-0.0033021022,0.087033163,0.0918886607,-0.0150105594,-0.0092469144,-0.0122687206,-0.0081743182,-0.0020407722,0.1720661991,0.0208604586,0.0549395676,-0.0321272887,-0.0240431361,-0.1472218862,0.0257159779,0.054020172,-0.0344170199,0.0362541037,-0.0005897902,0.014724029,-0.0435783167,-0.0203362085,-0.1168342171,-0.0177562582,0.0505379687,-0.0474495694,0.0181091672,-0.0168274736,0.1346826255,0.0249076844,-0.0777074543,0.0354585854,0.0440865414,0.0085088928,-0.1349467606,-0.095633676,0.0766134086,-0.1098799865]},"105":{"Abstract":"Relief is an art form part way between 3D sculpture and 2D painting. We present a novel approach for generating a texture-mapped high-relief model from a single brush painting. Our aim is to extract the brushstrokes from a painting and generate the individual corresponding relief proxies rather than recovering the exact depth map from the painting, which is a tricky computer vision problem, requiring assumptions that are rarely satisfied. The relief proxies of brushstrokes are then combined together to form a 2.5D high-relief model. To extract brushstrokes from 2D paintings, we apply layer decomposition and stroke segmentation by imposing boundary constraints. The segmented brushstrokes preserve the style of the input painting. By inflation and a displacement map of each brushstroke, the features of brushstrokes are preserved by the resultant high-relief model of the painting. We demonstrate that our approach is able to produce convincing high-reliefs from a variety of paintings(with humans, animals, flowers, etc.). As a secondary application, we show how our brushstroke extraction algorithm could be used for image editing. As a result, our brushstroke extraction algorithm is specifically geared towards paintings with each brushstroke drawn very purposefully, such as Chinese paintings, Rosemailing paintings, etc.","Authors":"Y. Fu; H. Yu; C. Yeh; J. Zhang; T. Lee","DOI":"10.1109\/TVCG.2018.2860004","Keywords":"Brush painting;brushstroke;layer decomposition;displacement mapping;high-relief","Keywords_Processed":"displacement mapping;layer decomposition;brush painting;high relief;brushstroke","Title":"High Relief from Brush Painting","Labels":null,"Keyword_Vector":[0.0252859019,-0.0032784106,0.0013580457,0.000579149,0.0565500654,-0.0303337941,0.0043536048,0.015009923,-0.0330324215,-0.0113188239,0.026800092,0.0127188303,-0.0321163068,-0.0036753222,-0.0049854068,0.1277485771,0.0121752372,0.0663810478,0.0849434777,-0.0215194629,-0.0643838026,0.0473503013,-0.0765155002,-0.0332144385],"Abstract_Vector":[0.1068777318,0.0962019073,-0.0804276491,-0.0019031392,0.0709552675,0.0147347,0.0082658452,-0.0242165984,0.0102538929,0.0686970956,0.137502053,0.0529903356,0.0039383376,0.0143279359,0.0177372657,0.0496735505,0.0774473133,0.0896225189,0.0225081799,-0.1727148402,-0.0586845421,0.064458519,0.0751280593,-0.0373456192,-0.0099335398,0.0890165356,-0.0165888385,-0.0133550317,-0.0476759179,-0.1186440799,-0.0538060355,-0.1761723694,0.0448631481,0.0481873911,-0.0274841711,0.1237577337,-0.0152294342,0.0463901183,-0.0275761325,-0.0745615678,0.0391511556,0.0548488979,0.1010313203,0.0689064684,0.056431201,0.0157636495,-0.1632343818,0.0073074373,-0.0138315302,0.1444690026,0.0963062762,0.0336202572,-0.048005436,-0.0323124717,0.0269283422,0.1404636054,0.0480306599,-0.0955176909,-0.0024702792,0.0177986314,0.0049472427,-0.1298532298,-0.0028722632,0.0151268529,-0.0613689291,-0.0394586979,0.1469513509,-0.0666799475,-0.0407258137,0.0180710682,-0.2463780926,-0.0846193502,-0.029169075,0.0230624234,0.0323112828,-0.0510693219]},"106":{"Abstract":"Retrieving salient structure from textured images is an important but difficult problem in computer vision because texture, which can be irregular, anisotropic, non-uniform and complex, shares many of the same properties as structure. Observing that salient structure in a textured image should be piece-wise smooth, we present a method to retrieve such structures using an L0 minimization of a modified form of the relative total variation metric. Thanks to the characteristics shared by texture and small structures, our method is effective at retrieving structure based on scale as well. Our method outperforms state-of-art methods in texture removal as well as scale-space filtering. We also demonstrate our method's ability in other applications such as edge detection, clip art compression artifact removal, and inverse half-toning.","Authors":"Y. Sun; S. Schaefer; W. Wang","DOI":"10.1109\/TVCG.2017.2711614","Keywords":"Texture removal;scale-space filtering;image smoothing;  $L_0$      sparsity","Keywords_Processed":"sparsity;texture removal;image smoothing;scale space filtering","Title":"Image Structure Retrieval via $L_0$  Minimization","Labels":null,"Keyword_Vector":[0.0383625631,-0.0032143137,0.0105583011,0.0771330527,0.1050201064,-0.0781094529,0.10303616,-0.0879345066,-0.0269840743,0.0694794033,0.0270732741,0.027674341,-0.0117707884,0.0475252277,-0.0089522053,0.086756159,-0.1919836768,0.1794042706,-0.1229811005,0.0675090351,-0.0531706674,0.1051484814,-0.1021525759,0.0265772357],"Abstract_Vector":[0.1762391725,0.0929336944,-0.1260982841,0.0133071997,0.0787342277,-0.0109576632,0.0811695312,0.144895807,0.0628042964,-0.0108848461,-0.0710750382,-0.0587281487,-0.0844227314,0.0011574298,0.0020793622,-0.0008823773,-0.011296927,-0.0888082238,-0.0299110276,-0.0876071795,0.0289584452,0.0524673983,0.018236276,-0.0379814718,-0.0866010116,0.0198966424,0.109523343,-0.004558443,0.1557425981,-0.0207304845,-0.0626134907,0.0969741632,-0.0135525788,-0.0466345169,0.1365287824,0.0630701569,-0.0105789845,0.0614363383,-0.1626355011,0.0913950114,-0.0112282275,-0.0082678171,-0.1121802085,0.0424197351,0.0907525228,0.0137864016,-0.1092997948,-0.1898248106,-0.0354237871,0.1048358676,-0.0883011054,-0.0422381523,0.0376732319,0.1152558967,-0.0360633484,0.1395756013,0.0597964979,-0.0252292296,0.0468359221,0.1156460928,0.0085664676,0.0389548512,0.0731287845,0.0088015369,0.0057600457,0.0566346276,-0.0846918047,0.1462599946,-0.0609780244,0.0301372565,0.0021908584,-0.0682136188,-0.0793134597,0.0806245172,-0.0353201916,0.0060846296]},"107":{"Abstract":"Famous examples such as Anscombe's Quartet highlight that one of the core benefits of visualizations is allowing people to discover visual patterns that might otherwise be hidden by summary statistics. This visual inspection is particularly important in exploratory data analysis, where analysts can use visualizations such as histograms and dot plots to identify data quality issues. Yet, these visualizations are driven by parameters such as histogram bin size or mark opacity that have a great deal of impact on the final visual appearance of the chart, but are rarely optimized to make important features visible. In this paper, we show that data flaws have varying impact on the visual features of visualizations, and that the adversarial or merely uncritical setting of design parameters of visualizations can obscure the visual signatures of these flaws. Drawing on the framework of Algebraic Visualization Design, we present the results of a crowdsourced study showing that common visualization types can appear to reasonably summarize distributional data while hiding large and important flaws such as missing data and extraneous modes. We make use of these results to propose additional best practices for visualizations of distributions for data quality tasks.","Authors":"M. Correll; M. Li; G. Kindlmann; C. Scheidegger","DOI":"10.1109\/TVCG.2018.2864907","Keywords":"Graphical perception;data quality;univariate visualizations","Keywords_Processed":"graphical perception;datum quality;univariate visualization","Title":"Looks Good To Me: Visualizations As Sanity Checks","Labels":null,"Keyword_Vector":[0.2628755953,0.0166441268,0.0311095793,-0.1422151284,0.0034776522,0.2997633673,-0.0955732454,-0.2085859387,-0.2705118457,0.102732552,0.0901393204,-0.0651346826,-0.0510147593,0.0462959754,0.1636819868,-0.046213174,-0.087915956,0.0194456778,0.0015248197,-0.0825961592,0.0905610933,0.0404512709,0.0146084141,0.0528345701],"Abstract_Vector":[0.3006787649,-0.2000940717,0.0331476718,-0.0543721614,-0.0343247619,0.1921544487,-0.0827600053,-0.0301249295,-0.0862712656,0.0352040907,-0.1288172141,-0.0082168117,0.0498667397,-0.1088301442,0.0238177236,-0.0266305219,0.0518125146,0.0646655527,-0.0792786069,-0.0389382858,-0.0030057606,0.0036698783,0.0366702786,0.0160349336,-0.0524129613,0.0652265429,0.0084184427,-0.0626665338,-0.0780270675,0.0028171376,-0.0595166474,-0.0383467107,0.0144787758,0.0533684006,0.0282212592,0.0276519124,-0.0075474079,-0.0333691242,-0.0743895478,0.0570459527,0.0122524077,-0.0327034312,0.1054166175,0.0103150015,-0.0109450411,-0.0025922011,0.0771585642,0.001841025,-0.0462103867,-0.0054268854,0.0268205038,-0.1157410696,0.0300937702,-0.0904349855,0.0507705728,0.0006577655,0.0858017156,-0.1426819519,0.1182944587,0.0518114138,-0.0165346845,0.0123521526,-0.0340331639,0.0204905344,-0.0097775544,0.1173070604,-0.0455061854,-0.0830812709,0.0726490541,0.0674097191,-0.1429619209,-0.1055806651,0.0924247908,0.089647557,-0.0158226412,-0.0203645392]},"108":{"Abstract":"We propose a system to facilitate biology communication by developing a pipeline to support the instructional visualization of heterogeneous biological data on heterogeneous user-devices. Discoveries and concepts in biology are typically summarized with illustrations assembled manually from the interpretation and application of heterogenous data. The creation of such illustrations is time consuming, which makes it incompatible with frequent updates to the measured data as new discoveries are made. Illustrations are typically non-interactive, and when an illustration is updated, it still has to reach the user. Our system is designed to overcome these three obstacles. It supports the integration of heterogeneous datasets, reflecting the knowledge that is gained from different data sources in biology. After pre-processing the datasets, the system transforms them into visual representations as inspired by scientific illustrations. As opposed to traditional scientific illustration these representations are generated in real-time - they are interactive. The code generating the visualizations can be embedded in various software environments. To demonstrate this, we implemented both a desktop application and a remote-rendering server in which the pipeline is embedded. The remote-rendering server supports multi-threaded rendering and it is able to handle multiple users simultaneously. This scalability to different hardware environments, including multi-GPU setups, makes our system useful for efficient public dissemination of biological discoveries.","Authors":"P. Mindek; D. Kou\u0159il; J. Sorger; D. Toloudis; B. Lyons; G. Johnson; M. E. Gr\u00f6ller; I. Viola","DOI":"10.1109\/TVCG.2017.2744518","Keywords":"Biological visualization;remote rendering;public dissemination","Keywords_Processed":"biological visualization;public dissemination;remote rendering","Title":"Visualization Multi-Pipeline for Communicating Biology","Labels":null,"Keyword_Vector":[0.1516785478,-0.0558161532,-0.1440735358,0.1530761901,0.0682770506,0.0648886488,-0.0122579578,-0.0687787396,0.0574535753,0.0489467267,0.0046480101,-0.0472387815,0.0229393587,0.0582942679,-0.0098345144,-0.0918277431,-0.0484869395,-0.0190210515,-0.0742007313,-0.0100999274,-0.0260377496,0.0923411346,0.0494331441,-0.0103243034],"Abstract_Vector":[0.2209645814,-0.0676443335,0.0573241368,-0.0534554993,0.0163945408,0.0161886311,-0.0049474992,0.0254978509,-0.1327297839,-0.0459722444,0.0823957976,0.0530536013,0.0202860619,0.0208739342,-0.0069280275,-0.0562376332,-0.093544022,-0.0120494666,0.0010114045,-0.1289154357,-0.0073113501,0.0889882083,-0.023862108,0.0509950953,-0.0294716343,0.0433878978,0.0044281223,0.0266546377,0.007662634,0.1024363089,0.0041347187,-0.0365171488,0.1147654739,-0.0444925501,0.0547180138,-0.0143474112,-0.0457601554,-0.0153874789,0.0905204405,0.046331555,-0.1042570872,-0.1168759415,0.0715334936,-0.0355140413,0.0359620885,-0.0736093721,0.0170543721,0.1219931376,0.0497191135,-0.0145603028,-0.1122460834,-0.0802435248,-0.0474299065,0.1219959692,-0.0344543573,-0.0840685038,-0.0557376912,0.0495551546,0.0863015624,-0.0496375749,0.0229925851,0.1045398668,0.0659040423,0.1547681749,0.0209652159,-0.0253797138,0.0636018362,-0.0740500541,0.0620824026,-0.1822524569,-0.0700598263,-0.0759934227,0.0007336703,0.0481610206,-0.100616645,-0.0147173965]},"109":{"Abstract":"The original Summed Area Table (SAT) structure is designed for handling 2D rectangular data. Due to the nature of spherical functions, the SAT structure cannot handle cube maps directly. This paper proposes a new SAT structure for cube maps and develops the corresponding lookup algorithm. Our formulation starts by considering a cube map as part of an auxiliary 3D function defined in a 3D rectangular space. We interpret the 2D integration process over the cube map surface as a 3D integration over the auxiliary 3D function. One may suggest that we can create a 3D SAT for this auxiliary function, and then use the 3D SAT to achieve the 3D integration. However, it is not practical to generate or store this special 3D SAT directly. This 3D SAT has some nice properties that allow us to store it in a storage-friendly data structure, namely Summed Area Cube Map (SACM). A SACM can be stored in a standard cube map texture. The lookup algorithm of our SACM structure can be implemented efficiently on current graphics hardware. In addition, the SACM structure inherits the favorable properties of the original SAT structure.","Authors":"Y. Xiao; T. Ho; C. Leung","DOI":"10.1109\/TVCG.2017.2761869","Keywords":"Cube map;summed area table;prefiltering;environment mapping;soft shadow","Keywords_Processed":"environment mapping;prefilter;soft shadow;cube map;sum area table","Title":"Summed Area Tables for Cube Maps","Labels":null,"Keyword_Vector":[0.025364642,0.0301929175,0.0015967802,0.0131770187,0.0339712174,-0.0188301161,-0.0165984438,0.0360400673,-0.0247061353,-0.0120968424,-0.0076016122,0.04080316,-0.0452668908,0.0039330007,-0.0263030835,0.0000643807,-0.0830911448,0.0001274487,0.0033360647,0.0481886466,-0.0349321351,0.0093082698,0.012232684,-0.0574381951],"Abstract_Vector":[0.1299338319,0.0695589707,-0.0613514798,-0.0117533059,0.0416145409,0.0443146427,0.0445052323,0.0863028843,0.0128426788,-0.0005033989,0.0549927973,0.0384547819,-0.0084476382,-0.0294887102,-0.0011813043,-0.0231483347,-0.0310745376,0.0068511162,-0.0355683109,0.0005649607,-0.0315125894,0.1975710364,-0.0926485916,0.1279502367,0.0168634729,0.1011832924,0.0790754802,-0.0809349435,0.1428385319,-0.0560992947,-0.0227927464,-0.1055421952,0.07145783,-0.0334005762,0.006394189,-0.0248905034,-0.0794425016,0.0280973466,-0.1272038825,0.0168695709,0.139746144,0.1318473905,-0.1147519005,0.0747939643,-0.016147615,-0.0029932585,-0.0469141234,-0.0167313201,-0.1103145056,0.1441802635,-0.0029594505,-0.0431235296,-0.1402419953,0.1073349889,0.0644365732,0.1052233878,-0.106668767,-0.0108555834,-0.0742695371,0.0185031273,0.0168580552,0.1178827862,0.0899213611,-0.0211723418,0.0152637493,-0.0234394966,-0.1303331671,-0.0012540798,-0.080577612,-0.002381157,0.0297796663,-0.0049396018,0.0748578742,0.0058901714,-0.057185281,-0.0550918221]},"110":{"Abstract":"In the first crowdsourced visualization experiment conducted exclusively on mobile phones, we compare approaches to visualizing ranges over time on small displays. People routinely consume such data via a mobile phone, from temperatures in weather forecasting apps to sleep and blood pressure readings in personal health apps. However, we lack guidance on how to effectively visualize ranges on small displays in the context of different value retrieval and comparison tasks, or with respect to different data characteristics such as periodicity, seasonality, or the cardinality of ranges. Central to our experiment is a comparison between two ways to lay out ranges: a more conventional linear layout strikes a balance between quantitative and chronological scale resolution, while a less conventional radial layout emphasizes the cyclicality of time and may prioritize discrimination between values at its periphery. With results from 87 crowd workers, we found that while participants completed tasks more quickly with linear layouts than with radial ones, there were few differences in terms of error rate between layout conditions. We also found that participants performed similarly with both layouts in tasks that involved comparing superimposed observed and average ranges.","Authors":"M. Brehmer; B. Lee; P. Isenberg; E. K. Choe","DOI":"10.1109\/TVCG.2018.2865234","Keywords":"Evaluation;graphical perception;mobile phones;range visualization;crowdsourcing","Keywords_Processed":"graphical perception;crowdsourc;range visualization;evaluation;mobile phone","Title":"Visualizing Ranges over Time on Mobile Phones: A Task-Based Crowdsourced Evaluation","Labels":null,"Keyword_Vector":[0.2075194125,0.0605485679,0.003649778,-0.0770143581,-0.1408186412,0.317727786,-0.0324250428,-0.2126121361,-0.1060420551,0.1236712116,0.057580564,-0.0379621068,0.1163724567,0.0245594369,0.296751947,-0.0144820011,-0.0387515597,-0.0086192236,0.0374285747,-0.0040741187,0.0758328269,0.009029379,-0.0649797967,0.0074581698],"Abstract_Vector":[0.1900809302,-0.0367375907,0.0974008872,-0.0142219303,-0.0241953809,0.131949426,-0.0060502198,0.0608219366,0.0793619168,-0.0803790033,-0.0599415096,-0.0148199632,0.0618159842,-0.0701575316,0.0864372602,-0.0618686322,-0.0659738485,0.0056890501,0.0651101392,0.0534100639,-0.16420348,-0.1947997756,-0.0551998514,-0.1410059856,0.1362327479,0.0268823989,0.0059854566,-0.1194951931,-0.0214810659,-0.0739613571,0.0635614475,-0.0732161338,0.0200219146,-0.0727432738,0.1736319277,-0.0202826296,0.0180926678,0.0631882392,0.026729075,-0.0919510185,0.0589484136,0.0368020922,0.1082980716,0.0552784025,-0.0866489326,-0.0122413448,-0.0690294801,-0.0303787216,0.0970568926,0.1504762447,0.0715864079,0.0603685019,0.1702096728,0.1163318437,-0.0694856566,-0.0036388238,-0.0015441954,0.0327372803,-0.0878877903,-0.0459239951,-0.0917952147,-0.0226533949,0.0021364319,0.025198119,0.0984237503,-0.0246451863,0.0543608842,-0.0946494636,-0.0116756674,-0.0731867777,0.0328505468,0.1197983547,-0.0482030619,0.0503625465,0.0663431731,-0.1004109397]},"111":{"Abstract":"Predicting specularities in images, given the camera pose and scene geometry from SLAM, forms a challenging and open problem. It is nonetheless essential in several applications such as retexturing. A recent geometric model called JOLIMAS partially answers this problem, under the assumptions that the specularities are elliptical and the scene is planar. JOLIMAS models a moving specularity as the image of a fixed 3D quadric. We propose dual JOLIMAS, a new model which raises the planarity assumption. It uses the fact that specularities remain elliptical on convex surfaces and that every surface can be divided in convex parts. The geometry of dual JOLIMAS then uses a 3D quadric per convex surface part and light source, and predicts the specularities by a means of virtual cameras, allowing it to cope with surface's unflatness. We assessed the efficiency and precision of dual JOLIMAS on multiple synthetic and real videos with various objects and lighting conditions. We give results of a retexturing application. Further results are presented as supplementary video material.","Authors":"A. Morgand; M. Tamaazousti; A. Bartoli","DOI":"10.1109\/TVCG.2017.2734538","Keywords":"Specularity Prediction;Augmented Reality;Retexturing;Quadric;Multiple Light Sources","Keywords_Processed":"augmented reality;multiple light sources;specularity prediction;retextur;quadric","Title":"A Multiple-View Geometric Model of Specularities on Non-Planar Shapes with Application to Dynamic Retexturing","Labels":null,"Keyword_Vector":[0.0792928476,0.1834613047,0.0054983735,-0.0503657614,0.0271850975,-0.0182484529,-0.0289588373,-0.0160248627,0.0292945509,-0.0009527822,-0.0275626548,-0.0349142937,-0.0886537832,-0.0382109992,0.0303995374,0.0355349978,-0.0261389793,0.0542827045,-0.0774189303,0.1193385539,-0.1041614923,-0.0547797295,-0.1209408329,0.0249724449],"Abstract_Vector":[0.1640475283,0.2599368854,-0.0417391405,-0.0042011782,0.0993347118,0.0485442235,-0.1096008645,-0.0029504404,-0.0884328263,0.1890621297,0.0417529187,0.1713942153,-0.1165410562,-0.0653023007,-0.1088192764,-0.0279044736,-0.0162739844,0.0637665305,-0.1326063728,0.263334861,0.0126883814,-0.0003878442,-0.0469366442,0.037538757,0.0634816316,-0.0421395491,0.0154834848,-0.1465992178,-0.1239848963,0.1987366938,-0.0049935274,-0.0753543357,-0.0116058312,-0.0178303172,0.113355679,-0.0590187252,0.013622029,0.0191918611,-0.0821796324,-0.1071856879,-0.1382289583,-0.0537189004,-0.0602183009,0.0273616769,0.0132518584,-0.0159716263,-0.0289414336,0.016321801,0.0538843433,-0.0687772651,0.0420427808,-0.0331046466,0.0400295896,-0.0335806989,-0.0871308514,0.1102785425,0.0430908067,-0.056722597,0.0204730658,0.0798845375,-0.0190489941,0.0681665325,0.0082231206,-0.035196576,0.0405831026,-0.0935574788,-0.1142153365,0.0429480931,-0.0881810404,-0.0820299105,0.0566897057,0.0183860246,-0.0781393634,-0.1426119937,-0.0012862929,0.0216210876]},"112":{"Abstract":"We present a novel approach for constructing a complete 3D model for an object from a single RGBD image. Given an image of an object segmented from the background, a collection of 3D models of the same category are non-rigidly aligned with the input depth, to compute a rough initial result. A volumetric-patch-based optimization algorithm is then performed to refine the initial result to generate a 3D model that not only is globally consistent with the overall shape expected from the input image but also possesses geometric details similar to those in the input image. The optimization with a set of high-level constraints, such as visibility, surface confidence and symmetry, can achieve more robust and accurate completion over state-of-the art techniques. We demonstrate the efficiency and robustness of our approach with multiple categories of objects with various geometries and details, including busts, chairs, bikes, toys, vases and tables.","Authors":"D. Li; T. Shao; H. Wu; K. Zhou","DOI":"10.1109\/TVCG.2016.2553102","Keywords":"RGBD camera;shape completion;single RGBD image","Keywords_Processed":"rgbd camera;single rgbd image;shape completion","Title":"Shape Completion from a Single RGBD Image","Labels":null,"Keyword_Vector":[0.0223609032,0.0094100645,0.0141144376,0.0913011816,0.0452761619,-0.1042585769,0.1104545394,-0.0932600219,-0.0511864473,0.0194971981,-0.0551753868,-0.0510656785,-0.0408343909,0.0511941692,0.0419093,0.0943063791,-0.029265783,0.0672188521,-0.2006394866,0.1234126141,0.0173011617,0.0022553262,0.0251514669,0.1251619258],"Abstract_Vector":[0.2509404567,0.2885353885,-0.1087193443,0.0385578204,0.1555174157,0.0192370384,-0.1164841634,0.0557445224,0.0292815351,0.1601333906,0.0462216718,-0.025968712,-0.0886989013,-0.0142320677,-0.0013917718,-0.0393780391,0.2212538956,-0.0433825569,-0.0894080234,-0.075554606,-0.0366206327,-0.1036894128,-0.0550364933,-0.0310966113,0.0139844022,-0.0004747758,-0.0377217806,0.1089436953,0.0509639401,0.0126954907,-0.008789911,-0.0882651348,0.0123790412,-0.0332030973,-0.0801923722,-0.0068673277,-0.0331780373,0.0221045616,-0.0191412441,-0.0279613324,0.0548429731,0.0177086969,-0.0652174869,0.0249238081,-0.0052971316,-0.0477547116,0.0071450236,-0.0519461561,-0.0420050471,0.0314574239,0.047674863,-0.0467406814,-0.0423800964,0.0537251113,0.0099583096,-0.0307412406,-0.0689277106,0.014523829,-0.1187854088,-0.0386829728,0.0571534394,-0.0646375397,0.0017552832,0.0534331855,0.1371971972,-0.0255887973,-0.078364374,0.0138700467,-0.0106806202,-0.0607078342,0.0365335779,0.0010536345,0.0286037333,0.0729855023,-0.0584414793,-0.0429470295]},"113":{"Abstract":"Specularities are often problematic in computer vision since they impact the dynamic range of the image intensity. A natural approach would be to predict and discard them using computer graphics models. However, these models depend on parameters which are difficult to estimate (light sources, objects' material properties and camera). We present a geometric model called JOLIMAS: JOint LIght-MAterial Specularity, which predicts the shape of specularities. JOLIMAS is reconstructed from images of specularities observed on a planar surface. It implicitly includes light and material properties, which are intrinsic to specularities. This model was motivated by the observation that specularities have a conic shape on planar surfaces. The conic shape is obtained by projecting a fixed quadric on the planar surface. JOLIMAS thus predicts the specularity using a simple geometric approach with static parameters (object material and light source shape). It is adapted to indoor light sources such as light bulbs and fluorescent lamps. The prediction has been tested on synthetic and real sequences. It works in a multi-light context by reconstructing a quadric for each light source with special cases such as lights being switched on or off. We also used specularity prediction for dynamic retexturing and obtained convincing rendering results. Further results are presented as supplementary video material, which can be found on the Computer Society Digital Library at http:\/\/doi.ieeecomputersociety.org\/10.1109\/TVCG.2017.2677445.","Authors":"A. Morgand; M. Tamaazousti; A. Bartoli","DOI":"10.1109\/TVCG.2017.2677445","Keywords":"JOLIMAS;specular reflection;multiple light sources;phong;blinn-phong;specularity;prediction;retexturing;quadric;dual space;conic;real time","Keywords_Processed":"blinn phong;prediction;jolimas;multiple light source;retextur;dual space;specularity;phong;conic;specular reflection;quadric;real time","Title":"A Geometric Model for Specularity Prediction on Planar Surfaces with Multiple Light Sources","Labels":null,"Keyword_Vector":[0.0382230492,0.0254386642,0.0167022778,-0.0054664196,0.0307095741,-0.0470210927,0.0048597962,-0.0164075883,0.042033872,0.0287834951,0.0150198846,-0.0311943398,-0.1478448029,-0.0385472046,0.077160237,0.0458647334,0.0181197684,0.083275742,-0.072398329,0.0286899656,-0.0600841873,0.0084354253,-0.1265894469,0.0844709138],"Abstract_Vector":[0.1642782882,0.2243510228,-0.072148346,0.015256899,0.1072490817,0.0428084482,-0.1591613933,-0.0226994777,-0.0634770051,0.1692194344,0.0085543668,0.1699776446,-0.1387795766,-0.0334272852,-0.1117265641,-0.0556180422,-0.0977591106,0.013837058,-0.1956959575,0.2894347193,0.0174661737,0.0561272323,-0.0296217786,-0.0167767921,0.0691588781,-0.0959292423,-0.0764485624,-0.1306982056,-0.0978760595,0.2069433411,-0.061624042,-0.011235311,-0.0504572363,-0.0180077871,0.1323962903,-0.0506109961,0.0541277338,0.0268040585,-0.151773126,-0.0281324122,-0.1359126421,-0.0831562787,0.1130369393,0.0373168591,0.0016402807,-0.0530233025,-0.0575702953,0.1073062221,0.0312158928,-0.0866988887,0.0146834473,-0.0274433624,0.0401908184,-0.0193419323,-0.04031423,0.1026767682,0.0370799553,-0.0331449263,0.0182020341,0.0699340192,-0.0945133423,0.0212744808,-0.0037194322,-0.0944278052,0.0616970112,-0.1050867415,0.027583278,0.0182244439,-0.0960651371,-0.0620323378,0.0377085092,-0.0081548771,-0.0455154902,-0.1406458956,-0.0149893762,-0.0046771299]},"114":{"Abstract":"Visualization researchers and practitioners engaged in generating or evaluating designs are faced with the difficult problem of transforming the questions asked and actions taken by target users from domain-specific language and context into more abstract forms. Existing abstract task classifications aim to provide support for this endeavour by providing a carefully delineated suite of actions. Our experience is that this bottom-up approach is part of the challenge: low-level actions are difficult to interpret without a higher-level context of analysis goals and the analysis process. To bridge this gap, we propose a framework based on analysis reports derived from open-coding 20 design study papers published at IEEE InfoVis 2009-2015, to build on the previous work of abstractions that collectively encompass a broad variety of domains. The framework is organized in two axes illustrated by nine analysis goals. It helps situate the analysis goals by placing each goal under axes of specificity (Explore, Describe, Explain, Confirm) and number of data populations (Single, Multiple). The single-population types are Discover Observation, Describe Observation, Identify Main Cause, and Collect Evidence. The multiple-population types are Compare Entities, Explain Differences, and Evaluate Hypothesis. Each analysis goal is scoped by an input and an output and is characterized by analysis steps reported in the design study papers. We provide examples of how we and others have used the framework in a top-down approach to abstracting domain problems: visualization designers or researchers first identify the analysis goals of each unit of analysis in an analysis stream, and then encode the individual steps using existing task classifications with the context of the goal, the level of specificity, and the number of populations involved in the analysis.","Authors":"H. Lam; M. Tory; T. Munzner","DOI":"10.1109\/TVCG.2017.2744319","Keywords":"Framework;Data Analysis;Analysis Goals;Design Studies;Open Coding;Task Classifications","Keywords_Processed":"data analysis;analysis goals;open coding;task classifications;framework;design studies","Title":"Bridging from Goals to Tasks with Design Study Analysis Reports","Labels":null,"Keyword_Vector":[0.1602014904,-0.109714694,0.1671316837,-0.1325073318,0.1120102392,0.0869527315,0.2200924833,0.0581773812,0.2409095688,-0.1253515809,-0.0495017045,0.0549779957,0.0536541128,0.0624485655,0.0054028405,0.0062397619,0.0864946102,-0.073530578,-0.0306319417,-0.0288979735,-0.0066964402,-0.0158308793,0.0197329345,-0.0665051696],"Abstract_Vector":[0.2791620644,-0.1457739165,0.0134560728,-0.0308326845,0.0534722611,-0.0738760788,-0.0781938666,-0.0903472655,-0.0738931036,0.0227923853,-0.0108149585,-0.0783078643,-0.0396349373,0.0171305942,-0.065054499,-0.0163480268,0.0435346679,-0.0242152262,0.0394329677,0.0775728926,0.0163426053,-0.001348373,0.0201890445,-0.0001242392,0.1351564412,0.0294358167,-0.0947586881,0.0549220157,0.160111244,-0.0368716013,0.1368583215,0.0589602159,-0.0862907241,-0.0666494575,-0.1073365203,0.0351122132,0.1119025068,-0.0394789562,0.0693516417,-0.0546396127,-0.0985688882,0.0526546534,-0.0432373692,-0.0115436745,-0.069143938,-0.0892989313,-0.0195083313,-0.0286412007,-0.0330416266,0.0350848387,0.1645422457,0.0525688234,-0.1153025449,-0.0929882523,-0.0520526906,0.0826042005,0.0632679178,-0.075539416,0.0731703918,0.0914492831,0.0767785617,-0.108097427,-0.0266091329,-0.0255597691,0.0483282565,0.0392003042,0.0925577638,0.0030486069,-0.064826339,-0.0418009812,0.0825129824,-0.003437196,0.0022983039,-0.0146456946,-0.0539661085,0.0763178737]},"115":{"Abstract":"Large image deformations pose a challenging problem for the visualization and statistical analysis of 3D image ensembles which have a multitude of applications in biology and medicine. Simple linear interpolation in the tangent space of the ensemble introduces artifactual anatomical structures that hamper the application of targeted visual shape analysis techniques. In this work we make use of the theory of stationary velocity fields to facilitate interactive non-linear image interpolation and plausible extrapolation for high quality rendering of large deformations and devise an efficient image warping method on the GPU. This does not only improve quality of existing visualization techniques, but opens up a field of novel interactive methods for shape ensemble analysis. Taking advantage of the efficient non-linear 3D image warping, we showcase four visualizations: 1) browsing on-the-fly computed group mean shapes to learn about shape differences between specific classes, 2) interactive reformation to investigate complex morphologies in a single view, 3) likelihood volumes to gain a concise overview of variability and 4) streamline visualization to show variation in detail, specifically uncovering its component tangential to a reference surface. Evaluation on a real world dataset shows that the presented method outperforms the state-of-the-art in terms of visual quality while retaining interactive frame rates. A case study with a domain expert was performed in which the novel analysis and visualization methods are applied on standard model structures, namely skull and mandible of different rodents, to investigate and compare influence of phylogeny, diet and geography on shape. The visualizations enable for instance to distinguish (population-)normal and pathological morphology, assist in uncovering correlation to extrinsic factors and potentially support assessment of model quality.","Authors":"M. Hermann; A. C. Schunke; T. Schultz; R. Klein","DOI":"10.1109\/TVCG.2015.2467198","Keywords":"Statistical deformation model;stationary velocity fields;image warping,;interactive visual analysiimage warping,;Statistical deformation model;stationary velocity fields;image warping;interactive visual analysis","Keywords_Processed":"interactive visual analysis;stationary velocity field;interactive visual analysiimage warping;statistical deformation model;image warping","Title":"Accurate Interactive Visualization of Large Deformations and Variability in Biomedical Image Ensembles","Labels":null,"Keyword_Vector":[0.1969478691,-0.0944942417,0.2364052803,0.0742492048,-0.0013507998,-0.1403675831,0.1692399947,-0.155721736,0.0000974473,0.0111223962,-0.0701966376,0.0142373647,-0.0313594948,0.2745753348,0.0076516282,-0.0057995037,-0.2223454534,0.0391777272,-0.1009095652,0.0554919576,-0.0855096709,-0.0799847504,-0.0034581745,-0.0218766583],"Abstract_Vector":[0.3685641858,0.0654068395,-0.1505114815,0.1315330162,0.1221718292,0.0938609417,-0.0619996402,-0.0339871049,-0.0428126218,0.0526577619,-0.026171318,-0.150838859,-0.1637636284,0.0160794632,-0.0778505876,-0.1532613819,-0.0040405131,-0.1701291063,0.0600962714,-0.0294090311,-0.0884893364,-0.0132354442,0.0416237622,0.053088235,-0.1820927676,0.079970567,0.006791521,0.1205137149,0.0845781511,0.0062184961,0.0528741024,0.086674719,-0.0937625188,0.0630068813,0.0465318253,-0.0905025914,-0.0120143374,-0.0363210798,-0.0016905409,0.0045626497,-0.008366559,0.0138693317,0.0337538475,-0.0568025634,-0.0120574471,0.1181604449,-0.0205238056,-0.057328879,0.0922074435,0.0223756242,0.0286507256,-0.0822956537,0.0985226732,-0.0303159687,-0.0546473013,0.0510085644,-0.0372211591,0.0256005439,-0.0456968502,0.0405480903,0.0074608719,0.0341093153,-0.0708987861,0.0985140282,0.0302323385,0.0365941768,0.0065488373,0.0206962893,-0.0530976848,-0.0058978657,0.0115031374,-0.0065917962,0.0000250613,0.031949071,-0.0179785269,-0.0203202313]},"116":{"Abstract":"As an ensemble model that consists of many independent decision trees, random forests generate predictions by feeding the input to internal trees and summarizing their outputs. The ensemble nature of the model helps random forests outperform any individual decision tree. However, it also leads to a poor model interpretability, which significantly hinders the model from being used in fields that require transparent and explainable predictions, such as medical diagnosis and financial fraud detection. The interpretation challenges stem from the variety and complexity of the contained decision trees. Each decision tree has its unique structure and properties, such as the features used in the tree and the feature threshold in each tree node. Thus, a data input may lead to a variety of decision paths. To understand how a final prediction is achieved, it is desired to understand and compare all decision paths in the context of all tree structures, which is a huge challenge for any users. In this paper, we propose a visual analytic system aiming at interpreting random forest models and predictions. In addition to providing users with all the tree information, we summarize the decision paths in random forests, which eventually reflects the working mechanism of the model and reduces users' mental burden of interpretation. To demonstrate the effectiveness of our system, two usage scenarios and a qualitative user study are conducted.","Authors":"X. Zhao; Y. Wu; D. L. Lee; W. Cui","DOI":"10.1109\/TVCG.2018.2864475","Keywords":"Interpretable Machine Learning;Random Forests;Random Forest Visualization;Visual Analytics","Keywords_Processed":"random forest;random forest visualization;interpretable machine learning;visual analytic","Title":"iForest: Interpreting Random Forests via Visual Analytics","Labels":null,"Keyword_Vector":[0.1635149518,-0.0401590823,0.1745556783,0.1649793257,-0.1640102496,-0.0190645237,-0.1530488247,-0.0299633205,-0.0305765027,-0.0175912167,0.0447306636,0.0130919406,0.0026675536,0.0507042765,-0.1432683433,0.0150298114,0.0638681293,-0.1190209355,-0.1300979763,-0.1106811536,0.0576672312,0.1226412746,-0.1053714881,-0.1391503248],"Abstract_Vector":[0.2107847622,-0.0270340539,-0.017757692,0.1201098154,0.1546033026,-0.1548372523,-0.1039995533,0.0461018187,0.1856136196,-0.039041494,-0.0082621808,0.0893282278,0.055193603,0.0711881308,0.0072302082,0.0517655988,0.0569845585,-0.0256871288,-0.0447423263,-0.0691063553,-0.0387697139,0.1783658948,-0.1200003022,0.0331504976,0.1030156099,0.0208807039,-0.1874553871,0.071451918,-0.2455291923,0.1142776921,-0.091204474,0.1662349011,0.1192225295,0.1124252931,0.0953369519,0.034206081,-0.0452783715,0.067212234,0.1291861019,0.0443816595,0.0004189198,0.1049731517,0.0438671965,-0.1218714734,-0.0768386141,0.0106282832,-0.0584349932,-0.1161268355,0.0396622399,0.0420787892,-0.1030809223,-0.0266473266,-0.0442268152,-0.0406600241,0.0139637495,-0.0075370398,-0.0550003731,-0.1476844646,-0.0507081011,0.037149169,-0.0345498631,-0.0030264313,-0.0129908219,-0.0967506017,-0.0237384536,0.0364092337,-0.0631372344,-0.0562324405,-0.0096024504,0.0111375849,0.0433230539,-0.041272064,0.0292999428,0.0222572063,0.0243635991,0.0558281962]},"117":{"Abstract":"Between the recent popularity of virtual reality (VR) and the development of 3D, immersion has become an integral part of entertainment concepts. Head-mounted Display (HMD) devices are often used to afford users a feeling of immersion in the environment. Another technique is to project additional material surrounding the viewer, as is achieved using cave systems. As a continuation of this technique, it could be interesting to extend surrounding projection to current television or cinema screens. The idea would be to entirely fill the viewer's field of vision, thus providing them with a more complete feeling of being in the scene and part of the story. The appropriate content can be captured using large field of view (FoV) technology, using a rig of cameras for 1100 to 3600 capture, or created using computer-generated images. The FoV is, however, rather limited in its use for existing (legacy) content, achieving between 36 to 90 degrees (0) field, depending on the distance from the screen. This paper seeks to improve this FoV limitation by proposing computer vision techniques to extend such legacy content to the peripheral (extrafoveal) vision without changing the original creative intent or damaging the viewer's experience. A new methodology is also proposed for performing user tests in order to evaluate the quality of the experience and confirm that the sense of immersion has been increased. This paper thus presents: i) an algorithm to spatially extend the video based on human vision characteristics, ii) its subjective results compared to state-of-the-art techniques, iii) the protocol required to evaluate the quality of the experience (QoE), and iv) the results of the user tests.","Authors":"L. Turban; F. Urban; P. Guillotel","DOI":"10.1109\/TVCG.2016.2527649","Keywords":"Augmented video;large field of view;human vision;immersion","Keywords_Processed":"human vision;augmented video;immersion;large field of view","Title":"Extrafoveal Video Extension for an Immersive Viewing Experience","Labels":null,"Keyword_Vector":[0.0973249661,0.0694819839,0.0334541878,0.0594733239,-0.0025791252,-0.058147528,0.0594823686,0.013548914,-0.1720727733,-0.032669474,0.0381508304,-0.0265329104,0.0203498885,-0.1512316107,0.0105190634,0.0743611195,-0.207302054,-0.1809116242,-0.1247625218,0.1823186159,-0.1954936073,-0.1517685863,-0.1051568441,-0.0017846362],"Abstract_Vector":[0.2508336176,0.1501790759,0.1539974993,0.0177597926,-0.0737484354,-0.0187002462,0.0067530498,0.0283655354,0.0005162367,-0.0303879477,0.0868320622,0.042761008,-0.0306589967,-0.0584525215,-0.0696762862,-0.0464855419,-0.102029943,0.0367569244,0.0427854877,0.0772324162,0.0379089884,-0.0357066563,0.0758144581,0.0597958533,-0.0018031877,-0.0080419431,0.0490234578,0.0737457903,-0.022173867,0.0520257379,-0.1405365671,0.1222306771,-0.0210088383,-0.0019421386,-0.056719106,0.1377839202,0.0949884475,0.0538196848,0.0174843689,0.0291101878,0.0840189586,-0.031446966,-0.1100687109,0.0248136109,0.0083019403,-0.0265732269,-0.0327177199,-0.0555545026,-0.002140855,0.0618252715,-0.0359822806,0.0349902941,0.0891864873,-0.0031225562,-0.0168414964,0.1337765882,0.060466813,-0.0162968287,-0.0059124122,-0.136721597,-0.0590779393,-0.0739579821,0.058287039,-0.0389715745,-0.0113818519,0.0060536456,0.0376097873,0.0309258679,-0.1633968569,-0.005755607,-0.0354141674,-0.1041402429,0.0726829785,0.0569446648,-0.06757854,0.0706648215]},"118":{"Abstract":"Storing analytical provenance generates a knowledge base with a large potential for recalling previous results and guiding users in future analyses. However, without extensive manual creation of meta information and annotations by the users, search and retrieval of analysis states can become tedious. We present KnowledgePearls, a solution for efficient retrieval of analysis states that are structured as provenance graphs containing automatically recorded user interactions and visualizations. As a core component, we describe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of a requested analysis state. Depending on the use case, this definition may be provided explicitly by the user by formulating a search query or inferred from given reference states. We explain our approach using the example of efficient retrieval of demographic analyses by Hans Rosling and discuss our implementation for a fast look-up of previous states. Our approach is independent of the underlying visualization framework. We discuss the applicability for visualizations which are based on the declarative grammar Vega and we use a Vega-based implementation of Gapminder as guiding example. We additionally present a biomedical case study to illustrate how KnowledgePearls facilitates the exploration process by recalling states from earlier analyses.","Authors":"H. Stitz; S. Gratzl; H. Piringer; T. Zichner; M. Streit","DOI":"10.1109\/TVCG.2018.2865024","Keywords":"Visualization provenance;interaction provenance;retrieval","Keywords_Processed":"visualization provenance;retrieval;interaction provenance","Title":"KnowledgePearls: Provenance-Based Visualization Retrieval","Labels":null,"Keyword_Vector":[0.1555856743,-0.0152159071,-0.0902437601,0.0665277768,0.0017480614,-0.0463086419,-0.0487501789,0.1506517606,-0.0225645375,-0.0200079535,-0.0190037406,0.0367922853,0.0341676459,0.0187441778,0.1111959113,0.0937162241,-0.0141179356,-0.0764708267,0.0457582029,-0.167668233,0.0637523552,0.1592592478,-0.0084792271,-0.0125291442],"Abstract_Vector":[0.2688252447,-0.1051689775,0.0118132357,-0.0003829882,0.0603164736,-0.0863803639,-0.0242566919,-0.012658731,-0.0686194162,-0.0943567531,0.033490213,-0.1046236692,-0.0094439584,0.0158607535,-0.0659798014,-0.0123550447,0.0273691097,-0.0057101849,-0.1144577334,-0.0306723546,0.0263060599,0.0097734183,0.1363701856,0.0753242988,-0.0560026049,0.0457114114,0.0079673608,0.0526973404,0.0517529044,0.035083827,0.0605447445,0.1862329067,-0.0281457429,-0.1248936863,-0.0144659324,0.068033761,0.1110887662,0.0453005652,-0.0826637673,-0.0490720778,0.0062401551,-0.0249301666,-0.0235346518,0.0557371447,-0.077844027,-0.1246114847,0.0810485916,-0.0537332295,-0.1314095975,0.0713164804,0.0214438518,0.0627058215,0.043371556,-0.0990091207,-0.1871622267,0.062952372,-0.0071818101,-0.0248922063,-0.0141278415,0.0637954121,0.05712773,-0.0822882926,-0.0605307038,0.0938377021,-0.0231082196,-0.0960214759,-0.0290325075,0.0981978743,0.1474857724,-0.0274457632,0.1160992082,0.0851380238,0.038355443,0.0021989939,0.0653618084,0.0204459676]},"119":{"Abstract":"General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.","Authors":"K. Wongsuphasawat; D. Moritz; A. Anand; J. Mackinlay; B. Howe; J. Heer","DOI":"10.1109\/TVCG.2015.2467191","Keywords":"User interfaces;information visualization;exploratory analysis;visualization recommendation;mixed-initiative systems;User interfaces;information visualization;exploratory analysis;visualization recommendation;mixed-initiative systems","Keywords_Processed":"mixed initiative system;information visualization;visualization recommendation;user interface;exploratory analysis","Title":"Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations","Labels":null,"Keyword_Vector":[0.3743544936,-0.0460021557,-0.0522307644,-0.0912990798,-0.0043245805,-0.1592834617,0.0080821217,0.2417955524,0.0675300709,-0.0221178189,0.1655314932,-0.0629740264,0.3438180434,0.2334645898,0.0803650088,-0.2072046688,0.0852279107,0.1629180279,-0.0969143506,-0.1324386457,0.0190354585,-0.0954688452,-0.0002371575,0.0535559418],"Abstract_Vector":[0.2298004454,-0.1643382617,0.0977745925,-0.0854813488,0.0957231146,0.1145790853,-0.0330663121,-0.0129120246,-0.0904049745,-0.0203712447,-0.0125776228,-0.1028507264,0.0150505993,-0.0454329883,-0.0617676273,0.0011944463,0.0570628644,-0.0079645855,-0.0063246075,0.0254716532,-0.0057859023,0.0851168536,0.0541442761,-0.057257832,0.0624928991,-0.0179458021,-0.0663011817,0.0270089061,0.0098881341,-0.0538132382,0.0230433418,-0.0253860848,0.0369916929,0.0803347098,0.15422536,-0.0043176314,-0.0188026841,0.1151450926,-0.0732579678,-0.1113848035,0.1600863592,-0.0004770418,-0.0223658584,-0.037813478,0.0573345336,0.0281448316,0.0537494551,0.0645846774,0.0353718091,-0.0050045564,-0.2014010084,0.0024925012,-0.0247532218,-0.0095867109,0.0336368547,-0.0516519304,0.0711195749,-0.0256747473,-0.0340718543,-0.1408769356,0.0406434238,-0.0331726511,-0.1247149507,-0.0023714492,-0.0275517582,0.0401551705,-0.0234256894,0.0335541554,0.0441139336,-0.021160975,0.0102756163,0.040808409,0.0321268379,-0.1089854069,0.0230233421,0.0698708402]},"120":{"Abstract":"The results of anomaly detection are sensitive to the choice of detection algorithms as they are specialized for different properties of data, especially for multidimensional data. Thus, it is vital to select the algorithm appropriately. To systematically select the algorithms, ensemble analysis techniques have been developed to support the assembly and comparison of heterogeneous algorithms. However, challenges remain due to the absence of the ground truth, interpretation, or evaluation of these anomaly detectors. In this paper, we present a visual analytics system named EnsembleLens that evaluates anomaly detection algorithms based on the ensemble analysis process. The system visualizes the ensemble processes and results by a set of novel visual designs and multiple coordinated contextual views to meet the requirements of correlation analysis, assessment and reasoning of anomaly detection algorithms. We also introduce an interactive analysis workflow that dynamically produces contextualized and interpretable data summaries that allow further refinements of exploration results based on user feedback. We demonstrate the effectiveness of EnsembleLens through a quantitative evaluation, three case studies with real-world data and interviews with two domain experts.","Authors":"K. Xu; M. Xia; X. Mu; Y. Wang; N. Cao","DOI":"10.1109\/TVCG.2018.2864825","Keywords":"Algorithm Evaluation;Ensemble Analysis;Anomaly Detection;Visual Analysis;Multidimensional Data","Keywords_Processed":"visual analysis;algorithm evaluation;ensemble analysis;anomaly detection;multidimensional data","Title":"EnsembleLens: Ensemble-based Visual Exploration of Anomaly Detection Algorithms with Multidimensional Data","Labels":null,"Keyword_Vector":[0.3223319786,-0.2404257613,0.335151304,-0.2446694694,0.1344545801,-0.0508080843,0.0026970111,-0.1211448208,0.3326166574,0.0585166957,-0.0967649974,0.0393750589,0.2564494272,-0.0616063704,0.0794110486,0.1867108272,0.0354725883,-0.0509094833,0.0412451821,0.0925519802,0.0544202758,0.065951171,-0.0254449657,0.0693046798],"Abstract_Vector":[0.3044309016,-0.1598285395,-0.0550197792,0.0915071229,-0.080249438,-0.0843127293,-0.093338092,-0.0249661872,-0.0463130109,0.0834377553,0.0276164883,0.1198041774,-0.0574754653,0.0098851874,-0.0871462373,-0.1058914057,0.0350223563,-0.0597780869,0.1856843578,-0.0016775352,-0.0080862717,-0.0040285086,-0.1870828623,0.1120043215,-0.0161411307,0.1489041032,-0.1245693385,0.0486445722,0.0639159035,-0.0441403079,-0.0947238911,0.0046267932,0.1163886155,-0.0624976549,0.1010736537,0.1830905068,-0.0551142996,-0.2072763143,-0.0700531254,-0.01273871,0.0433719856,-0.1937901772,0.0224344123,-0.0444467307,-0.019809842,-0.0044291564,-0.0191519872,0.0020838755,-0.0725148421,-0.0590527844,0.0546422922,0.0915772343,0.0052800962,0.0588796321,-0.076491153,0.0346689498,-0.0736232508,0.1567969543,0.0047081569,0.0828982197,0.0613713472,0.0109332422,-0.0140765565,-0.0253219155,-0.0025167071,0.0066552434,-0.0924218473,-0.1264079134,-0.0152621428,0.0573502849,-0.0089216497,-0.0343739689,0.0699300638,-0.007158821,0.0406072413,0.0379817272]},"121":{"Abstract":"Recent visualization research efforts have incorporated experimental techniques and perceptual models from the vision science community. Perceptual laws such as Weber's law, for example, have been used to model the perception of correlation in scatterplots. While this thread of research has progressively refined the modeling of the perception of correlation in scatterplots, it remains unclear as to why such perception can be modeled using relatively simple functions, e.g., linear and log-linear. In this paper, we investigate a longstanding hypothesis that people use visual features in a chart as a proxy for statistical measures like correlation. For a given scatterplot, we extract 49 candidate visual features and evaluate which best align with existing models and participant judgments. The results support the hypothesis that people attend to a small number of visual features when discriminating correlation in scatterplots. We discuss how this result may account for prior conflicting findings, and how visual features provide a baseline for future model-based approaches in visualization evaluation and design.","Authors":"F. Yang; L. T. Harrison; R. A. Rensink; S. L. Franconeri; R. Chang","DOI":"10.1109\/TVCG.2018.2810918","Keywords":"Information visualization;perception and psychophysics;evaluation\/methodology;Weber's law;power law","Keywords_Processed":"information visualization;power law;perception and psychophysic;evaluation methodology;weber law","Title":"Correlation Judgment and Visualization Features: A Comparative Study","Labels":null,"Keyword_Vector":[0.1602255155,0.0201185416,-0.0560197391,-0.0730244909,-0.0937817911,0.1557733519,0.0138832239,-0.0903207676,-0.0282913839,0.0854480014,0.1234508089,-0.0861250324,0.1210866477,0.0326026488,0.0809328357,-0.0116571456,-0.0060496678,-0.014501657,-0.0399478812,0.0014073469,0.0517844962,0.0335239422,-0.0248756111,0.0463232221],"Abstract_Vector":[0.2430982385,-0.0328107572,-0.0412570938,-0.0273351123,0.0833033384,0.1179095807,-0.0342651486,-0.0222822634,0.2127324633,0.2554179679,-0.138412797,-0.1290088475,0.2671249113,0.026867294,-0.1701962082,-0.0768887952,-0.0572703227,0.0345845506,0.026657501,0.0384454451,-0.0748144262,0.0517337092,0.1537986118,-0.0526986861,-0.0084931208,-0.0370994241,0.0479034533,0.071534163,-0.1225027006,-0.046528457,-0.1004925436,-0.0035447638,-0.0255279793,0.0643172441,-0.0122285756,-0.0418646705,-0.049528839,-0.0718697799,0.0261540316,-0.0926156333,-0.0136234869,-0.2288937071,-0.0655923939,0.1077365509,-0.0107070123,0.0184838417,-0.0029244512,-0.0210415248,0.056748623,0.1047090714,0.0421230584,-0.0051716077,-0.0361123156,-0.0640670909,-0.0419935562,0.0477149409,-0.0465447781,-0.0058666512,0.0388052883,-0.0047666442,-0.0495107562,0.0648940676,-0.0272578566,-0.0016230368,-0.0077096455,-0.0832693154,-0.0275274042,-0.0142632777,0.0027262167,0.0295274711,-0.0120105563,0.0224647567,0.0885416032,0.0517035936,0.064140157,-0.0306835161]},"122":{"Abstract":"We present TopoAngler, a visualization framework that enables an interactive user-guided segmentation of fishes contained in a micro-CT scan. The inherent noise in the CT scan coupled with the often disconnected (and sometimes broken) skeletal structure of fishes makes an automatic segmentation of the volume impractical. To overcome this, our framework combines techniques from computational topology with an interactive visual interface, enabling the human-in-the-Ioop to effectively extract fishes from the volume. In the first step, the join tree of the input is used to create a hierarchical segmentation of the volume. Through the use of linked views, the visual interface then allows users to interactively explore this hierarchy, and gather parts of individual fishes into a coherent sub-volume, thus reconstructing entire fishes. Our framework was primarily developed for its application to CT scans of fishes, generated as part of the ScanAllFish project, through close collaboration with their lead scientist. However, we expect it to also be applicable in other biological applications where a single dataset contains multiple specimen; a common routine that is now widely followed in laboratories to increase throughput of expensive CT scanners.","Authors":"A. Bock; H. Doraiswamy; A. Summers; C. Silva","DOI":"10.1109\/TVCG.2017.2743980","Keywords":"Computational topology;join trees;branch decomposition;hierarchical segmentation;interaction;visualization system","Keywords_Processed":"branch decomposition;join tree;computational topology;visualization system;hierarchical segmentation;interaction","Title":"TopoAngler: Interactive Topology-Based Extraction of Fishes","Labels":null,"Keyword_Vector":[0.1391233827,0.0047826167,-0.0722364956,0.0881252209,0.0316602007,-0.0584602513,-0.0168597502,0.1142730923,-0.0167392526,0.0131575091,-0.0184896358,-0.0038376548,0.0339184767,0.0135440136,0.066682109,-0.0124106955,0.0295390415,0.0562312081,0.0667058906,-0.1220657769,0.0404584532,0.0169423717,-0.0926226477,-0.0385069388],"Abstract_Vector":[0.1771869257,-0.0089621141,-0.0086057142,0.035678574,0.1021749338,-0.0783124106,0.0132913191,0.0422567199,-0.0506182501,-0.1054061651,0.039959068,-0.0460614099,0.0092440571,0.0176534305,0.0412103739,0.0058958747,0.0473680118,-0.0479555257,-0.0920382626,-0.0845851107,0.0107696014,0.0945352466,-0.0091526984,0.0730743013,-0.0349361032,-0.1124331942,-0.029993971,-0.0109337272,-0.1491632844,0.0979605069,-0.0632708463,-0.1100520647,0.0421625844,0.0055228425,-0.0753615669,0.1467519943,0.2803121559,-0.0548952858,0.0995818131,-0.057350249,0.0339331713,-0.0869886939,-0.1098778808,0.0057916243,0.0411775764,0.0221909241,-0.0615938625,0.0227207959,0.0549768617,0.0597747484,0.2039094392,0.0204233414,0.0926189057,-0.0396057434,0.0590733806,-0.0390500432,-0.0203551612,-0.0584608841,-0.0507652047,-0.0296391685,-0.0138662901,0.0250464567,-0.0401888637,0.0602808248,0.058195004,0.0197820188,-0.0116171037,-0.022147086,-0.0647410116,0.0462007583,-0.0499782097,0.1129206243,-0.0620593434,-0.0092601468,-0.0458260932,0.0185565544]},"123":{"Abstract":"We present a new method to visualize from an ensemble of flow fields the statistical properties of streamlines passing through a selected location. We use principal component analysis to transform the set of streamlines into a low-dimensional Euclidean space. In this space the streamlines are clustered into major trends, and each cluster is in turn approximated by a multivariate Gaussian distribution. This yields a probabilistic mixture model for the streamline distribution, from which confidence regions can be derived in which the streamlines are most likely to reside. This is achieved by transforming the Gaussian random distributions from the low-dimensional Euclidean space into a streamline distribution that follows the statistical model, and by visualizing confidence regions in this distribution via iso-contours. We further make use of the principal component representation to introduce a new concept of streamline-median, based on existing median concepts in multidimensional Euclidean spaces. We demonstrate the potential of our method in a number of real-world examples, and we compare our results to alternative clustering approaches for particle trajectories as well as curve boxplots.","Authors":"F. Ferstl; K. B\u00fcrger; R. Westermann","DOI":"10.1109\/TVCG.2015.2467204","Keywords":"Ensemble visualization;uncertainty visualization;flow visualization;streamlines;statistical modeling;Ensemble visualization;uncertainty visualization;flow visualization;streamlines;statistical modeling","Keywords_Processed":"streamline;ensemble visualization;statistical modeling;flow visualization;uncertainty visualization","Title":"Streamline Variability Plots for Characterizing the Uncertainty in Vector Field Ensembles","Labels":null,"Keyword_Vector":[0.4119738998,-0.1735016231,-0.336357078,-0.0627477222,-0.1376630703,-0.1948016551,-0.2088607848,-0.2728878631,0.0214334958,-0.1458714731,-0.0263956143,0.1729017261,-0.0213700481,-0.0464408386,0.0467562818,0.0182124032,0.0480861204,0.0241382549,0.0201348924,0.0201633988,-0.0404553874,-0.0362909445,0.1647616244,0.0171533953],"Abstract_Vector":[0.1923779963,-0.0170279111,-0.1643129146,0.1589449006,-0.0577008027,0.030577692,0.0085975729,-0.0368250704,0.0104744373,-0.0229312904,0.1196646006,-0.0184164037,0.0337614881,0.1521023395,-0.0751411004,-0.0660031293,0.0297654228,0.1288414664,0.0661078814,0.0461636749,0.0701171264,-0.0319455261,0.0932350234,-0.102166918,-0.086499548,0.0765080255,0.0010017176,-0.0831791391,0.0158847543,0.1256507067,0.1672328873,0.0123446257,-0.0935478372,0.2115286552,-0.0676071356,-0.0126362829,-0.0004598574,0.0598388966,0.0512539822,0.0123104403,0.1254595006,0.0089093503,-0.0529141169,-0.0449810631,-0.0930772377,-0.0773262354,0.0279357641,-0.1780150199,-0.032497234,-0.0507895711,-0.0135514381,0.0063888558,0.0802810162,0.0898193893,0.1009137408,0.0725530891,-0.0765596388,0.0553338684,-0.1049899956,0.0617287159,0.0193691731,0.0766750291,0.0960225107,-0.041679589,-0.042785878,0.0025752294,0.0825617558,-0.0781946248,0.1369613666,-0.0694414424,-0.0791100036,0.0673919261,0.0955594645,-0.1038614065,-0.0192082238,0.0714611109]},"124":{"Abstract":"Understanding co-occurrence in urban human mobility (i.e. people from two regions visit an urban place during the same time span) is of great value in a variety of applications, such as urban planning, business intelligence, social behavior analysis, as well as containing contagious diseases. In recent years, the widespread use of mobile phones brings an unprecedented opportunity to capture large-scale and fine-grained data to study co-occurrence in human mobility. However, due to the lack of systematic and efficient methods, it is challenging for analysts to carry out in-depth analyses and extract valuable information. In this paper, we present TelCoVis, an interactive visual analytics system, which helps analysts leverage their domain knowledge to gain insight into the co-occurrence in urban human mobility based on telco data. Our system integrates visualization techniques with new designs and combines them in a novel way to enhance analysts' perception for a comprehensive exploration. In addition, we propose to study the correlations in co-occurrence (i.e. people from multiple regions visit different places during the same time span) by means of biclustering techniques that allow analysts to better explore coordinated relationships among different regions and identify interesting patterns. The case studies based on a real-world dataset and interviews with domain experts have demonstrated the effectiveness of our system in gaining insights into co-occurrence and facilitating various analytical tasks.","Authors":"W. Wu; J. Xu; H. Zeng; Y. Zheng; H. Qu; B. Ni; M. Yuan; L. M. Ni","DOI":"10.1109\/TVCG.2015.2467194","Keywords":"Co-occurrence;human mobility;telco data;bicluster;visual analytics;Co-occurrence;human mobility;telco data;bicluster;visual analytics","Keywords_Processed":"telco datum;visual analytic;co occurrence;human mobility;bicluster","Title":"TelCoVis: Visual Exploration of Co-occurrence in Urban Human Mobility Based on Telco Data","Labels":null,"Keyword_Vector":[0.1668950032,-0.058831118,0.2749511942,0.0772597924,-0.04343171,-0.0101743556,-0.1245643167,0.0532874964,-0.1455752851,-0.0581054189,0.0962263321,-0.049813342,-0.1099166067,-0.1199937633,-0.0849267937,-0.0169984596,-0.1430084346,0.0547617337,0.015846394,0.0090159147,-0.0308276729,0.0533240792,0.1473254898,0.0022790969],"Abstract_Vector":[0.2656674122,-0.1359704426,-0.0047381862,-0.0451197103,0.0018907074,-0.0959605485,0.0715999398,-0.0495465521,-0.0248614039,0.138860082,-0.0249763074,0.075510076,0.0310430313,0.0652006641,0.0287604171,0.0151877021,-0.0583868909,-0.1299638161,0.1643453327,0.0056606509,-0.0598709869,-0.0896759616,0.0589968048,-0.0442373683,-0.0944070332,-0.1281439074,0.207730022,0.0525302599,-0.0347584786,-0.003058296,-0.0827672134,-0.1673081943,0.1625226801,-0.129702105,0.0881936672,-0.0116339986,0.1124498734,0.0609504008,0.0306843328,0.1097847067,0.0168865591,0.1794901618,0.0321257475,-0.0235306054,-0.0750060526,-0.0359419569,0.0503762203,0.0278507801,0.0743832221,-0.0465681225,0.0318837188,-0.0610084803,0.0735976558,-0.1073646304,0.0052012498,0.0722460225,-0.1083619696,0.0159772892,-0.0797908531,0.0724481201,-0.0109691501,0.01020004,0.059681311,-0.0129016065,-0.0552031315,0.0785114819,-0.0258128132,0.0629216564,0.0803938135,-0.0101304006,-0.0051452331,0.0276022323,-0.0077251076,-0.0076554388,0.0384465405,0.0986254538]},"125":{"Abstract":"Scientific visualization developed successful methods for scalar and vector fields. For tensor fields, however, effective, interactive visualizations are still missing despite progress over the last decades. We present a general approach for the generation of separating surfaces in symmetric, second-order, three-dimensional tensor fields. These surfaces are defined as fiber surfaces of the invariant space, i.e. as pre-images of surfaces in the range of a complete set of invariants. This approach leads to a generalization of the fiber surface algorithm by Klacansky et al. [16] to three dimensions in the range. This is due to the fact that the invariant space is three-dimensional for symmetric second-order tensors over a spatial domain. We present an algorithm for surface construction for simplicial grids in the domain and simplicial surfaces in the invariant space. We demonstrate our approach by applying it to stress fields from component design in mechanical engineering.","Authors":"F. Raith; C. Blecha; T. Nagel; F. Parisio; O. Kolditz; F. G\u00fcnther; M. Stommel; G. Scheuermann","DOI":"10.1109\/TVCG.2018.2864846","Keywords":"visualization;tensor field;invariants;fiber surface;interaction","Keywords_Processed":"visualization;invariant;fiber surface;tensor field;interaction","Title":"Tensor Field Visualization using Fiber Surfaces of Invariant Space","Labels":null,"Keyword_Vector":[0.2080994866,-0.0391834108,-0.1240946589,0.1062946872,0.0115835223,-0.0639386507,0.0730416118,0.117386077,-0.1020626878,-0.0197082723,0.0183809809,0.144583454,0.0399445416,0.0552066724,0.0781283376,0.0599860703,-0.2476904662,-0.2477448762,0.1517372671,-0.1858582371,-0.0468656908,-0.0645951059,-0.1720975354,0.1521362026],"Abstract_Vector":[0.2043393863,0.0866575891,-0.1306886792,0.0066002497,0.1124025599,0.2248909401,0.1114784691,-0.1297144177,-0.0866696421,-0.0681935302,0.1391130123,-0.0334290347,-0.1513065812,0.031855679,0.1409718009,-0.1220362935,-0.0293499612,0.0931720457,0.2315385057,0.1264161963,0.1743803988,0.0668382444,-0.1716994761,-0.0677281683,0.0273508411,-0.103247473,0.0415335444,-0.0405943986,-0.083978004,0.0625109979,0.0980020985,0.0615637183,0.0523916552,-0.0193202265,0.019886035,0.062551502,-0.0911904008,0.0505239428,-0.0345414399,-0.0849051296,0.0502955878,-0.0356691682,-0.1305266634,-0.0443059368,-0.061596295,-0.0602634556,-0.0081161668,0.0003578549,0.0354150191,-0.03170391,0.0337945144,-0.055559825,-0.0517290021,-0.0271630725,-0.0315148537,-0.0830027258,0.0453908874,0.0281043359,-0.0011005931,0.0208499391,-0.0897283019,0.0331695013,-0.0784621549,0.0463545081,0.0379005865,-0.0387395562,-0.0478061234,-0.0847631957,0.0603661481,0.0194071845,0.0048450604,0.0073415565,0.0524226857,0.0417602965,0.0575893132,-0.0606167621]},"126":{"Abstract":"An ensemble is a collection of related datasets, called members, built from a series of runs of a simulation or an experiment. Ensembles are large, temporal, multidimensional, and multivariate, making them difficult to analyze. Another important challenge is visualizing ensembles that vary both in space and time. Initial visualization techniques displayed ensembles with a small number of members, or presented an overview of an entire ensemble, but without potentially important details. Recently, researchers have suggested combining these two directions, allowing users to choose subsets of members to visualization. This manual selection process places the burden on the user to identify which members to explore. We first introduce a static ensemble visualization system that automatically helps users locate interesting subsets of members to visualize. We next extend the system to support analysis and visualization of temporal ensembles. We employ 3D shape comparison, cluster tree visualization, and glyph based visualization to represent different levels of detail within an ensemble. This strategy is used to provide two approaches for temporal ensemble analysis: (1) segment based ensemble analysis, to capture important shape transition time-steps, clusters groups of similar members, and identify common shape changes over time across multiple members; and (2) time-step based ensemble analysis, which assumes ensemble members are aligned in time by combining similar shapes at common time-steps. Both approaches enable users to interactively visualize and analyze a temporal ensemble from different perspectives at different levels of detail. We demonstrate our techniques on an ensemble studying matter transition from hadronic gas to quark-gluon plasma during gold-on-gold particle collisions.","Authors":"L. Hao; C. G. Healey; S. A. Bass","DOI":"10.1109\/TVCG.2015.2468093","Keywords":"Ensemble visualization;Ensemble visualization","Keywords_Processed":"ensemble visualization","Title":"Effective Visualization of Temporal Ensembles","Labels":null,"Keyword_Vector":[0.3709081354,-0.1912289806,-0.3022199193,-0.1539040871,-0.0557789203,-0.1284668492,-0.3017151867,-0.2172515711,0.0873397515,-0.1164268058,-0.2211828938,0.0762670943,0.0636254956,-0.0977891868,0.0420971994,0.1032982764,0.0065544805,0.0051470296,-0.0101217962,-0.0826829944,-0.0297871203,0.1100944788,0.0070444136,0.0107295359],"Abstract_Vector":[0.2921042549,-0.126172462,-0.0833119598,0.3661988879,-0.1076147474,-0.059182304,-0.1127835357,-0.1305367139,0.0167871592,-0.0463820412,0.0310649071,-0.0928284654,-0.1399294042,0.0462468098,-0.0719283673,-0.100542283,0.0530881173,-0.250535844,-0.0769023469,0.0792069559,-0.2335636717,-0.0438837253,-0.140972983,-0.0006662271,-0.0601813254,0.1676511284,-0.0516061722,-0.0976223039,-0.0245048656,-0.0241708099,-0.0072552489,0.0808815699,0.0953000065,0.000531784,-0.0345129234,-0.0262837917,-0.008148116,-0.0020382565,0.1130547305,0.0116366441,0.1316542603,-0.0568729006,0.012797411,-0.0416100203,-0.0056875164,0.0153573278,-0.0624583401,0.0596975317,0.0726599088,-0.0060130056,0.0135587053,-0.0446353121,-0.0575607215,0.0195165443,-0.0056699375,0.000008475,-0.0037434899,-0.024397257,-0.0128207409,-0.0165063657,-0.0144952387,-0.0841105903,0.1104670181,0.010368554,-0.0442688512,-0.0054639181,0.0637420911,0.0768025784,-0.0352088685,0.0037354029,-0.0191473319,0.0065515264,-0.0441472646,0.0399798904,0.0101404186,0.0759326673]},"127":{"Abstract":"Virtual colonoscopy (VC) is a non-invasive screening tool for colorectal polyps which employs volume visualization of a colon model reconstructed from a CT scan of the patient's abdomen. We present an immersive analytics system for VC which enhances and improves the traditional desktop VC through the use of VR technologies. Our system, using a head-mounted display (HMD), includes all of the standard VC features, such as the volume rendered endoluminal fly-through, measurement tool, bookmark modes, electronic biopsy, and slice views. The use of VR immersion, stereo, and wider field of view and field of regard has a positive effect on polyp search and analysis tasks in our immersive VC system, a volumetric-based immersive analytics application. Navigation includes enhanced automatic speed and direction controls, based on the user's head orientation, in conjunction with physical navigation for exploration of local proximity. In order to accommodate the resolution and frame rate requirements for HMDs, new rendering techniques have been developed, including mesh-assisted volume raycasting and a novel lighting paradigm. Feedback and further suggestions from expert radiologists show the promise of our system for immersive analysis for VC and encourage new avenues for exploring the use of VR in visualization systems for medical diagnosis.","Authors":"S. Mirhosseini; I. Gutenko; S. Ojal; J. Marino; A. Kaufman","DOI":"10.1109\/TVCG.2019.2898763","Keywords":"Immersive Environments;Immersive Analytics;Interaction Design;Volume Rendering;Biomedical Visualization;Colon Cancer Screening;Medical Diagnosis","Keywords_Processed":"colon cancer screening;volume rendering;medical diagnosis;interaction design;biomedical visualization;immersive analytic;immersive environment","Title":"Immersive Virtual Colonoscopy","Labels":null,"Keyword_Vector":[0.2346845746,0.0612306181,-0.0688249539,0.2532441656,0.0522107206,0.1658659595,-0.0252754395,0.1256195415,0.1359730419,-0.0412988218,-0.060636577,0.1166651753,0.0208705302,-0.0304456861,-0.1566566272,-0.043237459,-0.1357494176,-0.0810438976,-0.046517446,-0.0214113835,-0.1215621767,0.129649565,0.1091483095,-0.0386131547],"Abstract_Vector":[0.2236898969,0.0500185941,0.1989919489,0.069630955,0.0231104716,-0.0275842549,0.0545106501,0.025026692,-0.1490722357,-0.0273288741,0.0385701097,-0.0503277566,0.0879370296,0.0011257953,0.0492823279,-0.0325092568,-0.1406002103,-0.0556052103,-0.0005965203,-0.0925818071,0.0735976875,0.0999299092,-0.0064494123,0.1552058062,-0.0255712604,-0.0586931701,-0.078979478,0.0401975459,-0.074106749,-0.024820331,-0.0781201891,-0.014105437,0.0128573338,0.1061348429,-0.0029339048,-0.0673645583,0.0735137578,0.0157746143,0.0520265844,-0.0407794616,0.0454117891,-0.0313776071,-0.0964687947,0.0430531093,-0.0045163175,0.030084433,-0.0122789322,-0.0021784926,0.0452775064,-0.1380559194,0.01838173,-0.0149878537,0.1537150289,-0.0764129406,-0.0969380944,0.1512532935,-0.0185653235,0.0827010404,-0.0835058937,-0.0192591206,0.0064354292,-0.0834371701,-0.0568107906,0.1023263335,0.0002939067,-0.0434247648,0.0382352876,-0.0569759793,-0.0668320765,-0.0124140148,-0.0022622116,-0.0085621734,-0.0502169307,-0.0164525457,0.0822364028,0.0428568469]},"128":{"Abstract":"We propose a novel massively parallel construction algorithm for Bounding Volume Hierarchies (BVHs) based on locally-ordered agglomerative clustering. Our method builds the BVH iteratively from bottom to top by merging a batch of cluster pairs in each iteration. To efficiently find the neighboring clusters, we keep the clusters ordered along the Morton curve. This ordering allows us to identify approximate nearest neighbors very efficiently and in parallel. We implemented our algorithm in CUDA and evaluated it in the context of GPU ray tracing. For complex scenes, our method achieves up to a twofold reduction of build times while providing up to 17 percent faster trace times compared with the state-of-the-art methods.","Authors":"D. Meister; J. Bittner","DOI":"10.1109\/TVCG.2017.2669983","Keywords":"Ray tracing;object hierarchies;three-dimensional graphics and realism","Keywords_Processed":"three dimensional graphic and realism;ray trace;object hierarchy","Title":"Parallel Locally-Ordered Clustering for Bounding Volume Hierarchy Construction","Labels":null,"Keyword_Vector":[0.0299845598,0.0191630667,0.0117617481,0.0307373569,0.1033774745,-0.0435130975,0.0774301951,-0.0806307982,-0.0947077901,-0.0305940434,0.0277185073,-0.1123389479,0.0561584949,0.0627155439,-0.0079233125,0.2541940299,0.1392440604,0.0344257609,0.2388311511,-0.0260474434,-0.0929326368,-0.1069508354,-0.0086097312,-0.2670500399],"Abstract_Vector":[0.1715398198,0.0357597654,-0.1293514224,0.1843893004,-0.1160220476,-0.0860475183,-0.0202062579,0.1399324237,-0.1225557499,-0.0662268692,0.0165921232,-0.1414214202,0.0428723083,-0.0081489856,0.0421888835,0.0149708463,0.0327052573,0.0938227301,0.1274402113,0.0901712978,0.0686393964,0.0077850339,0.1302186707,-0.0510086736,0.0968960673,-0.0757254826,0.0368851939,0.0316011578,-0.013848753,0.0556268505,-0.0352011171,-0.0182709189,-0.0497224283,0.0410628717,0.0180718141,0.0703443194,-0.0735833535,-0.1614607193,0.0506763805,0.0995470838,0.0127842114,0.0535246499,0.0062909082,0.0857821205,-0.0025475806,0.0860676529,0.0423563298,-0.0521770507,-0.136965286,-0.0203073846,-0.0268089164,-0.0154208246,0.0042748644,0.1477624257,-0.0399162104,-0.0134248207,-0.0590282366,-0.0519748817,0.0061620203,-0.0054209213,-0.029988467,-0.0010393289,-0.0082548774,-0.0347350137,0.006477408,-0.0523911933,0.131662996,0.0678705757,-0.0519289618,-0.0514534738,0.0509045941,-0.0219712568,-0.0988161502,-0.0147656508,-0.129293637,-0.0230691718]},"129":{"Abstract":"In single-phase flow visualization, research focuses on the analysis of vector field properties. In two-phase flow, in contrast, analysis of the phase components is typically of major interest. So far, visualization research of two-phase flow concentrated on proper interface reconstruction and the analysis thereof. In this paper, we present a novel visualization technique that enables the investigation of complex two-phase flow phenomena with respect to the physics of breakup and coalescence of inclusions. On the one hand, we adapt dimensionless quantities for a localized analysis of phase instability and breakup, and provide detailed inspection of breakup dynamics with emphasis on oscillation and its interplay with rotational motion. On the other hand, we present a parametric tightly linked space-time visualization approach for an effective interactive representation of the overall dynamics. We demonstrate the utility of our approach using several two-phase CFD datasets.","Authors":"G. K. Karch; F. Beck; M. Ertl; C. Meister; K. Schulte; B. Weigand; T. Ertl; F. Sadlo","DOI":"10.1109\/TVCG.2017.2692781","Keywords":"Flow visualization;two-phase flow;feature deformation;space-time analysis;feature tracking","Keywords_Processed":"space time analysis;feature tracking;flow visualization;feature deformation;two phase flow","Title":"Visual Analysis of Inclusion Dynamics in Two-Phase Flow","Labels":null,"Keyword_Vector":[0.2074205915,-0.0772310155,0.0072638767,-0.08544364,0.0344256606,-0.247902775,0.0831577813,-0.1314313228,0.1346131073,0.1213610727,0.345996213,0.3412176414,-0.225099781,-0.0947574213,0.0925226403,-0.0805645834,0.0270732383,-0.0258913256,-0.0713039431,-0.107637824,-0.0052941657,0.0016839425,0.0309911584,-0.1304935016],"Abstract_Vector":[0.1918983491,-0.0275044324,-0.0183052309,-0.0048101129,0.0293834615,0.0042661129,0.1546751515,-0.2060424664,-0.0616120208,-0.0500987532,0.0273482063,-0.0260778733,0.0451079554,0.0319705927,-0.0141017048,0.0543949849,0.0627684821,0.101051377,-0.0356444392,0.0626225731,-0.0405141617,-0.0243796095,-0.0595027192,0.0132695329,-0.1214328424,0.0544626558,-0.0309226363,0.0792373823,0.1406267059,0.0862734418,0.0273169302,0.0395906646,-0.0875483861,-0.0425517307,0.0175561784,-0.0164247681,0.0538221005,0.1168747424,-0.0217688414,-0.0588183135,-0.0369242503,-0.0256750484,0.164197764,0.0447069645,-0.0541665375,0.0372600481,-0.071540802,-0.0330207665,0.0590881604,-0.0691327084,0.0901026212,0.1293863145,-0.0663236723,-0.030827094,0.0578743583,-0.0579011015,-0.0817327101,-0.0784122602,0.0161084303,0.0440871165,-0.0072877213,0.1249773886,-0.0423201501,0.0706926375,0.0146557274,-0.0069332944,0.0767934369,0.0445680398,-0.0642655726,0.0558837428,0.0379653911,0.0261270059,0.0155181595,-0.0457122237,-0.1034417459,-0.0343834652]},"130":{"Abstract":"This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR, developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate the flexibility of DXR through several examples spanning a wide range of applications.","Authors":"R. Sicat; J. Li; J. Choi; M. Cordeil; W. Jeong; B. Bach; H. Pfister","DOI":"10.1109\/TVCG.2018.2865152","Keywords":"Augmented Reality;Virtual Reality;Immersive Visualization;Immersive Analytics;Visualization Toolkit","Keywords_Processed":"augmented reality;visualization toolkit;virtual reality;immersive analytic;immersive visualization","Title":"DXR: A Toolkit for Building Immersive Data Visualizations","Labels":null,"Keyword_Vector":[0.3500791105,0.4122884274,-0.0780262794,-0.0326357276,-0.0476577003,-0.0143854019,-0.1188179618,0.0117256197,0.1063766692,0.0345321251,-0.0676174681,-0.0215695852,0.0127323618,-0.0417668591,-0.2514425081,-0.0450513273,-0.1437954768,-0.0289593434,-0.0529388627,0.0406510141,-0.1243890585,0.1206521012,0.1001605163,-0.0338503405],"Abstract_Vector":[0.2197676511,-0.0586478386,0.210660527,-0.0786020347,-0.0089653311,0.1204615715,-0.0261574107,-0.0116221953,-0.1421510742,-0.0031993146,-0.0434974104,-0.087502363,0.0005579359,0.0524655996,0.0311894676,-0.0660040245,0.0889806786,-0.0032048125,-0.083489568,-0.0203218137,0.0374154214,0.0546720158,0.0725862898,0.0920432637,0.0066004558,0.1003859277,0.0291661779,-0.0308174462,-0.0721737295,0.0247273852,-0.053598953,0.0522901523,-0.0083508346,-0.0900514067,0.0267494251,-0.0477405457,-0.0638015521,0.0028567503,0.0676409954,-0.0295893116,0.0838042171,0.0418819027,0.0086171251,0.1441452856,0.0793873631,-0.0215976614,-0.0612054048,0.0478041594,0.0152117147,-0.0538767455,-0.0008944823,0.0369035793,0.0521980082,0.0872479714,-0.0609347523,0.0747463525,-0.0823547785,0.013569461,-0.0986683019,0.0843275786,0.0313109499,-0.0887777425,-0.0376492073,-0.0219088316,-0.0181769965,0.0185853278,-0.0574798293,0.010070276,0.0104674975,-0.0540054958,0.0784597179,0.0480518764,-0.0389490463,-0.1258753141,-0.1211494442,-0.1020036191]},"131":{"Abstract":"Map matching is the process of assigning observed geographic positions of vehicles and their trajectories to the actual road links in a road network. In this paper, we present Visual Interactive Map Matching, a visual analytics approach to fine-tune the data preprocessing and matching process. It is based on ST-matching, a state-of-the-art and easy-to-understand map matching algorithm. Parameters of the preprocessing step and algorithm can be optimized with immediate visual feedback. Visualizations show current matching issues and performance metrics on a map and in diagrams. Manual and computer-supported editing of the road network model leads to a refined alignment of trajectories and roads. We demonstrate our approach with large-scale taxi trajectory data. We show that optimizing the matching on a subsample results in considerably improved matching quality, also when later scaled to the full dataset. An optimized matching ensures data faithfulness and prevents misinterpretation when the matched data might be investigated in follow-up analysis.","Authors":"R. Kr\u00fcger; G. Simeonov; F. Beck; T. Ertl","DOI":"10.1109\/TVCG.2018.2816219","Keywords":"Map matching;data cleaning;data transformation and representation;geographic visualization","Keywords_Processed":"datum transformation and representation;geographic visualization;datum cleaning;map match","Title":"Visual Interactive Map Matching","Labels":null,"Keyword_Vector":[0.1893111767,-0.1110526787,0.0357535109,-0.0977396638,0.1608986649,0.0709337411,-0.0641744681,0.0301119309,-0.2033824598,-0.002244057,0.0851410451,-0.0598365909,-0.1926386164,-0.032046709,-0.0619028912,-0.0432760425,-0.0690187467,0.0804392104,-0.0456398617,-0.0245682287,0.0537125954,0.0630269329,0.2089860625,0.0474242295],"Abstract_Vector":[0.1982337389,-0.0597853857,-0.0353973219,-0.0608872468,-0.0302274559,-0.0381346771,-0.0265289171,0.0734936604,0.0492903571,0.0626366144,-0.02194783,0.0350309729,-0.0589206315,0.0076383915,0.0088538934,0.0501670928,-0.0872084002,0.0512315249,-0.0108799091,-0.0532017477,-0.0289106678,0.1508537272,-0.0028541103,0.0602934369,0.0888314375,0.1514314445,0.0475420201,0.0667541936,0.0991653902,0.0153086378,0.0316362805,-0.0819072976,0.0259703157,-0.0281158423,-0.0397925312,0.0719932956,-0.0758038124,0.0093880183,-0.2068469267,-0.0697829134,0.1623880261,-0.0254923021,0.0645301179,-0.043994327,-0.0490177266,-0.0809137912,0.1328182718,0.0484003805,-0.0248445894,0.0739689457,0.0168645677,-0.0286072763,0.1212398102,0.0772289353,0.0243188069,0.0319965194,-0.0344777491,-0.1106271001,0.0343463799,0.0244786994,-0.0245546871,0.0831541954,0.0315832984,0.1584060729,0.0326732,-0.021764131,0.059336054,0.060086039,-0.0619557189,0.0915063585,0.0297211671,0.0883805645,0.0267779958,-0.1566542896,-0.0294322316,-0.0072348769]},"132":{"Abstract":"The analysis of protein-ligand interactions is a time-intensive task. Researchers have to analyze multiple physico-chemical properties of the protein at once and combine them to derive conclusions about the protein-ligand interplay. Typically, several charts are inspected, and 3D animations can be played side-by-side to obtain a deeper understanding of the data. With the advances in simulation techniques, larger and larger datasets are available, with up to hundreds of thousands of steps. Unfortunately, such large trajectories are very difficult to investigate with traditional approaches. Therefore, the need for special tools that facilitate inspection of these large trajectories becomes substantial. In this paper, we present a novel system for visual exploration of very large trajectories in an interactive and user-friendly way. Several visualization motifs are automatically derived from the data to give the user the information about interactions between protein and ligand. Our system offers specialized widgets to ease and accelerate data inspection and navigation to interesting parts of the simulation. The system is suitable also for simulations where multiple ligands are involved. We have tested the usefulness of our tool on a set of datasets obtained from protein engineers, and we describe the expert feedback.","Authors":"D. Duran; P. Hermosilla; T. Ropinski; B. Kozl\u00edkov\u00e1; \u00c1. Vinacua; P. V\u00e1zquez","DOI":"10.1109\/TVCG.2018.2864851","Keywords":"Molecular visualization;simulation inspection;long trajectories","Keywords_Processed":"molecular visualization;simulation inspection;long trajectory","Title":"Visualization of Large Molecular Trajectories","Labels":null,"Keyword_Vector":[0.1315042463,-0.0114219909,-0.072349034,-0.0140674432,-0.030717989,-0.0178141661,-0.019880222,-0.0302646415,-0.0231501731,-0.0048567186,0.0251590119,-0.0003141225,0.0156458927,0.0421866899,-0.0224487205,-0.1316406042,-0.0015892574,0.0391873714,0.1921034847,0.0915817733,-0.0564257513,0.1920460344,-0.065292783,0.1054904226],"Abstract_Vector":[0.2295498695,-0.1029983632,0.0220324561,-0.0658262156,-0.0179612964,-0.086168757,0.0087798406,0.0163392469,-0.1108938313,-0.0521453103,0.0315704675,0.0251585251,-0.0267269358,0.0687173129,-0.047153364,0.0740194556,-0.0875566837,0.0462956651,-0.0899437636,0.0224947902,-0.0147017436,0.0352705057,-0.0939083733,0.0343340561,0.0571387697,-0.000161751,0.0246768722,0.0652440463,0.0660717033,0.0009565092,-0.0525156171,-0.138227894,-0.0317778179,0.109837842,-0.0696563404,-0.1080013409,0.0451249016,0.0542573277,-0.178727657,-0.1747143939,0.0596136204,0.0054665759,-0.0141820847,-0.0904045136,-0.0436441937,0.0119569397,0.0036392995,0.0567380915,0.0634366678,-0.0079403324,-0.0358406654,0.0042086619,0.0986908794,0.0528416683,-0.0201660885,-0.0191062321,-0.0356288799,-0.0080007779,0.0182014452,0.0440737653,0.1385545329,-0.0537535401,0.0497841047,0.0304040864,-0.0931220973,0.073726491,0.0208229395,0.1252991006,-0.0106264991,0.0032180942,-0.1067436622,-0.0050925301,-0.0282735088,0.047166094,-0.0852162795,0.0166764184]},"133":{"Abstract":"Modularity, modifiability, reusability, and API usability are important software qualities that determine the maintainability of software architectures. Virtual, Augmented, and Mixed Reality (VR, AR, MR) systems, modern computer games, as well as interactive human-robot systems often include various dedicated input-, output-, and processing subsystems. These subsystems collectively maintain a real-time simulation of a coherent application state. The resulting interdependencies between individual state representations, mutual state access, overall synchronization, and flow of control implies a conceptual close coupling whereas software quality asks for a decoupling to develop maintainable solutions. This article presents five semantics-based software techniques that address this contradiction: Semantic grounding, code from semantics, grounded actions, semantic queries, and decoupling by semantics. These techniques are applied to extend the well-established entity-component-system (ECS) pattern to overcome some of this pattern's deficits with respect to the implied state access. A walk-through of central implementation aspects of a multimodal (speech and gesture) VR-interface is used to highlight the techniques' benefits. This use-case is chosen as a prototypical example of complex architectures with multiple interacting subsystems found in many VR, AR and MR architectures. Finally, implementation hints are given, lessons learned regarding maintainability pointed-out, and performance implications discussed.","Authors":"M. Fischbach; D. Wiebusch; M. E. Latoschik","DOI":"10.1109\/TVCG.2017.2657098","Keywords":"Real-time interactive systems;virtual reality systems;software architecture;multimodal processing","Keywords_Processed":"multimodal processing;real time interactive system;virtual reality system;software architecture","Title":"Semantic Entity-Component State Management Techniques to Enhance Software Quality for Multimodal VR-Systems","Labels":null,"Keyword_Vector":[0.1533644757,0.2381170357,0.0159553324,-0.0337992748,0.0676737976,-0.116553706,0.0278133042,-0.0174081898,0.1048201036,0.0456072091,0.0458262111,-0.1104481247,-0.1465899197,0.1429203048,-0.0596909522,-0.0518330658,0.0906061375,0.0970830858,-0.0172980085,-0.0760493043,-0.0222286373,-0.0311041212,-0.0567753923,0.0083262027],"Abstract_Vector":[0.1976404074,0.0388312411,0.1036017146,-0.008201287,-0.032556257,-0.0576852267,0.0461926971,-0.0258803465,-0.0273764516,0.0123224394,0.0565092454,-0.0431029334,0.013305078,0.0905077462,0.0278095454,0.010096918,0.0537499776,0.0259180142,-0.01616186,-0.0190741909,0.0064015136,0.0359492659,0.1280834621,0.0958868996,0.0479192518,-0.0049579101,0.0200343913,0.0421481451,0.0652394371,0.0170711491,-0.0869014774,0.1603224151,-0.0437408483,-0.102584582,0.1063555365,0.0344535751,0.06894653,0.2086954425,0.1032356597,0.0098661109,0.0592101835,-0.1122070072,-0.0420371462,0.0995949698,0.0787158906,-0.0371911783,-0.0426846484,0.076955985,-0.1233737777,-0.1118996442,-0.0174206665,-0.0115616275,0.0880615188,-0.0201802524,-0.0910622047,-0.02115703,-0.0128476976,-0.0316899705,-0.1049855743,0.1034394539,0.0800529631,-0.0907055773,-0.0199635353,-0.094362385,0.052221322,-0.0757699776,-0.0074748407,0.0724918711,0.0995888169,-0.0398581435,-0.0920681828,0.0624613512,0.1687732462,0.0392806056,0.0354486658,-0.063912553]},"134":{"Abstract":"Despite significant advances in the analysis and visualization of unsteady flow, the interpretation of it's behavior still remains a challenge. In this work, we focus on the linear correlation and non-linear dependency of different physical attributes of unsteady flows to aid their study from a new perspective. Specifically, we extend the existing spatial correlation quantification, i.e. the Local Correlation Coefficient (LCC), to the spatio-temporal domain to study the correlation of attribute-pairs from both the Eulerian and Lagrangian views. To study the dependency among attributes, which need not be linear, we extend and compute the mutual information (MI) among attributes over time. To help visualize and interpret the derived correlation and dependency among attributes associated with a particle, we encode the correlation and dependency values on individual pathlines. Finally, to utilize the correlation and MI computation results to identify regions with interesting flow behavior, we propose a segmentation strategy of the flow domain based on the ranking of the strength of the attributes relations. We have applied our correlation and dependency metrics to a number of 2D and 3D unsteady flows with varying spatio-temporal kernel sizes to demonstrate and assess their effectiveness.","Authors":"M. Berenjkoub; R. O. Monico; R. S. Laramee; G. Chen","DOI":"10.1109\/TVCG.2018.2864817","Keywords":"Unsteady flow;correlation study;mutual information","Keywords_Processed":"unsteady flow;correlation study;mutual information","Title":"Visual Analysis of Spatia-temporal Relations of Pairwise Attributes in Unsteady Flow","Labels":null,"Keyword_Vector":[0.1419757678,-0.0418863128,-0.1016897901,-0.1140803463,-0.0907124614,-0.0567813945,0.0621908134,-0.0063661954,-0.0177470197,0.0020250874,0.3750324925,0.2165200597,-0.004676036,-0.1440828196,-0.0123732365,-0.0224648074,0.1409594577,0.0392936966,-0.1487711023,0.1884678481,-0.0954497431,-0.0752071009,0.157613874,-0.1306667234],"Abstract_Vector":[0.1904731321,-0.0788379518,-0.0704819757,0.0123181942,-0.0151276633,0.0234175557,0.2439527031,-0.1537696012,0.0958481276,0.2124329236,0.036198323,0.0471120049,0.224735634,0.0746386081,-0.1569824347,-0.1082985046,0.1048844962,-0.0289177012,0.1113808538,0.0430581193,-0.2580455716,0.0779871511,0.0331201649,0.1119910837,0.0758944966,-0.1715442317,-0.0410763531,0.0739849047,0.0346032631,-0.0519177859,-0.0345684587,0.038002388,-0.1417693078,-0.0645351885,0.07254037,-0.0317798181,-0.0466804019,-0.1167406638,0.0410095185,-0.0936798311,-0.0952668487,0.124845494,-0.0948101539,0.0937364567,0.069578902,0.0417078314,0.0106180164,0.048784011,0.0396594591,-0.0289308816,0.049069178,-0.0856669474,0.0284335427,0.0637980184,-0.0246315249,-0.0466384922,-0.0251607224,0.0212102284,-0.0103592015,-0.0620487818,-0.0689197428,0.0567028514,0.0587460963,0.0244046247,-0.0395083214,0.0267142545,-0.0108348918,0.0050954489,0.0349932062,-0.0419023127,-0.0033371844,-0.0147533745,0.0570100486,0.028165517,-0.0001126571,0.0252949204]},"135":{"Abstract":"Visualization designers regularly use color to encode quantitative or categorical data. However, visualizations \u201cin the wild\u201d often violate perceptual color design principles and may only be available as bitmap images. In this work, we contribute a method to semi-automatically extract color encodings from a bitmap visualization image. Given an image and a legend location, we classify the legend as describing either a discrete or continuous color encoding, identify the colors used, and extract legend text using OCR methods. We then combine this information to recover the specific color mapping. Users can also correct interpretation errors using an annotation interface. We evaluate our techniques using a corpus of images extracted from scientific papers and demonstrate accurate automatic inference of color mappings across a variety of chart types. In addition, we present two applications of our method: automatic recoloring to improve perceptual effectiveness, and interactive overlays to enable improved reading of static visualizations.","Authors":"J. Poco; A. Mayhua; J. Heer","DOI":"10.1109\/TVCG.2017.2744320","Keywords":"Visualization;color;chart understanding;information extraction;redesign;computer vision","Keywords_Processed":"information extraction;computer vision;redesign;color;visualization;chart understanding","Title":"Extracting and Retargeting Color Mappings from Bitmap Images of Visualizations","Labels":null,"Keyword_Vector":[0.1631213479,-0.0369127658,-0.0878671717,-0.0220723076,-0.0665140337,-0.0011607118,0.0669566014,0.0007623859,-0.0779094414,-0.0028610548,0.15156036,-0.0304354081,0.0289286387,0.0613899138,-0.0835385078,0.0441045944,-0.0607100622,-0.0660095386,-0.0163659599,-0.1043082963,-0.0865016559,-0.0121049397,-0.1553289003,0.020610643],"Abstract_Vector":[0.2479536794,0.0246918993,-0.0114284015,-0.078245484,-0.0016286754,0.2921470464,-0.0161932047,0.1560421944,0.097265939,0.0509527351,0.1812291084,-0.1998318128,-0.044007104,-0.0762606921,-0.0245834149,0.419002737,0.0722766123,-0.19399491,0.0015025515,0.0588538818,0.0558036324,0.0476789825,0.0050352281,-0.0149889945,-0.0284241428,0.0579447193,0.004637911,0.0524796362,-0.0596326748,-0.0390463063,0.0311327955,0.0286611443,0.0098779078,-0.0357457799,-0.0030963459,0.0368210071,0.0713633589,-0.0459363817,-0.0023884309,-0.0250116847,-0.0765628195,-0.0031110622,0.0323648195,-0.0466801712,0.0215785062,0.0822270879,-0.0231780619,0.0468358652,0.0669806426,0.0180457604,0.0098438355,0.0668540912,0.0212863396,-0.0627300987,-0.0073308762,-0.0044563313,-0.0669073856,0.028968145,-0.0187031301,0.0629581982,-0.0382699226,-0.0121855777,-0.0024961921,-0.028341021,0.0354125406,0.0495726058,-0.0205659829,-0.0167766047,-0.004020939,-0.0310588967,-0.0181831989,-0.0374363388,0.0214991758,-0.0282818921,-0.0683110317,0.0152870847]},"136":{"Abstract":"Drones allow exploring dangerous or impassable areas safely from a distant point of view. However, flight control from an egocentric view in narrow or constrained environments can be challenging. Arguably, an exocentric view would afford a better overview and, thus, more intuitive flight control of the drone. Unfortunately, such an exocentric view is unavailable when exploring indoor environments. This paper investigates the potential of drone-augmented human vision, i.e., of exploring the environment and controlling the drone indirectly from an exocentric viewpoint. If used with a see-through display, this approach can simulate X-ray vision to provide a natural view into an otherwise occluded environment. The user's view is synthesized from a three-dimensional reconstruction of the indoor environment using image-based rendering. This user interface is designed to reduce the cognitive load of the drone's flight control. The user can concentrate on the exploration of the inaccessible space, while flight control is largely delegated to the drone's autopilot system. We assess our system with a first experiment showing how drone-augmented human vision supports spatial understanding and improves natural interaction with the drone.","Authors":"O. Erat; W. A. Isop; D. Kalkofen; D. Schmalstieg","DOI":"10.1109\/TVCG.2018.2794058","Keywords":"X-ray;mixed reality;hololens;drone;pick-and-place","Keywords_Processed":"hololen;drone;pick and place;ray;mixed reality","Title":"Drone-Augmented Human Vision: Exocentric Control for Drones Exploring Hidden Areas","Labels":null,"Keyword_Vector":[0.0748861282,0.1672169251,0.0130662789,-0.0182874844,0.0395018294,-0.0646519718,-0.0003105208,0.0106640169,0.0160967456,-0.0087752562,-0.0009315101,0.0043953886,0.0584148612,0.094560699,-0.0631744425,-0.0345738923,-0.0127772323,0.0772215171,0.0082185588,-0.0220219446,-0.0315141911,-0.0340703893,-0.0377889407,0.0016522511],"Abstract_Vector":[0.1506199661,0.046897059,0.1379181001,0.0506956583,0.0307385255,-0.0342801488,0.0334999227,0.0590807767,0.0357892744,-0.04684403,0.1311434231,0.0249796275,0.0290997208,-0.0140569763,-0.0533924338,-0.0898084818,-0.1405780788,-0.0735711716,0.0224467555,-0.0073229037,0.0551679922,-0.0096209649,0.0384278402,-0.0220294106,-0.0622167076,-0.086789957,0.0728235484,-0.0280950562,0.1440842977,0.0531930361,-0.0653508835,-0.0515067201,-0.0166730792,-0.0173740861,-0.0719172331,0.1285312372,-0.036732363,-0.0092635261,0.0285056614,0.0164706423,-0.0280768733,0.0201828868,0.0656888318,-0.1036786662,0.0960469545,-0.039318502,-0.0605252776,0.0865002672,0.0093506763,0.0330255802,-0.0044651245,0.0112049273,-0.0258124148,-0.0467166503,0.0340532765,0.1163081565,0.0884625009,0.0017043294,0.0588633521,-0.1121910343,0.0504969455,-0.0354992966,-0.0548529575,0.0085513242,0.0111808977,0.0115267926,-0.0118227742,-0.0259934722,-0.0249722149,-0.0720630796,0.0260630238,-0.1136617325,0.0908755531,-0.123279022,0.0230498353,-0.0945543519]},"137":{"Abstract":"With the rapidly growing VR industry, in recent years, more and more attention has been paid for fire sound synthesis. However, previous methods usually ignore the influences of the different solid combustibles, leading to unrealistic sounding results. This paper proposes SSC (sounding solid combustibles), which is a new recording-driven non-premixed flame sound synthesis framework accounting for different solid combustibles. SSC consists of three components: combustion noise, vortex noise and popping sounds. The popping sounds are the keys to distinguish the differences of solid combustibles. To improve the quality of fire sound, we extract the features of popping sounds from the real fire sound examples based on modified Empirical Mode Decomposition (EMD) method. Unlike previous methods, we take both direct combustion noise and vortex noise into account because the fire model is non-premixed flame. In our method, we also greatly resolve the synchronization problem during blending the three components of SSC. Due to the introduction of the popping sounds, it is easy to distinguish the fire sounds of different solid combustibles by our method, with great potential in practical applications such as games, VR system, etc. Various experiments and comparisons are presented to validate our method.","Authors":"Q. Yin; S. Liu","DOI":"10.1109\/TVCG.2016.2642958","Keywords":"Non-premixed fire sound;solid combustibles;direct combustion noise;vortex noise;popping sounds","Keywords_Processed":"direct combustion noise;vortex noise;solid combustible;popping sound;non premixed fire sound","Title":"Sounding Solid Combustibles: Non-Premixed Flame Sound Synthesis for Different Solid Combustibles","Labels":null,"Keyword_Vector":[0.0245238596,-0.0133965428,-0.0025117372,0.0282469733,0.0263464337,0.0062948028,0.0045641033,0.0037179302,0.0249880901,-0.0012463766,0.0124395243,0.0379269423,-0.0514553959,-0.0189956901,0.014469224,-0.0085752849,-0.0264626996,0.0001722748,0.0046780518,-0.025391965,0.0115923696,-0.0341278224,-0.0487636517,0.0119770538],"Abstract_Vector":[0.1120103573,0.059983017,-0.0261219866,-0.0119063371,0.0312143639,0.0176465112,0.0070554144,-0.0124932367,0.0097929326,-0.0093363419,-0.0479367686,-0.0701252961,0.0564326187,0.032073876,0.0585911782,0.0017292379,-0.0404541738,0.0130647004,-0.0486234508,-0.0196182549,0.0487253617,-0.0499529456,0.0214973419,-0.0084741706,0.0095134586,0.0976858049,0.0466919848,0.0205316875,-0.0199346965,0.0078881031,-0.0747589768,0.0565210992,-0.0073148151,0.0569823865,0.053895723,0.0944217738,-0.0863303903,0.0044627387,-0.0087054708,0.0584243124,0.0233149499,0.0917455628,-0.1459469243,-0.1261481929,0.0702835433,-0.0238298102,-0.0894464642,0.1222909279,0.0545041031,-0.0490729714,0.0672341951,0.0221207376,0.1484341506,-0.1452361715,-0.092875897,-0.1400009616,-0.0031025651,-0.0071797675,0.0899037241,0.1660620159,0.0679466825,0.0146901275,0.1411689506,0.0110346309,0.0704775733,0.0097370056,0.1314809726,-0.1347033269,0.1433665692,0.0396498804,0.0597721482,-0.0307036636,-0.0858076795,-0.0924229516,0.1260908571,0.0088141913]},"138":{"Abstract":"Decorating the surfaces of 3D printed objects with color textures is still not readily available in most consumer-level or even high-end 3D printers. Existing techniques such as hydrographics color transfer suffer from the issues of air pockets in concave regions and discoloration in overly stretched regions. We propose a novel thermoforming-based coloring technique to alleviate these problems as well as to simplify the overall procedure. Thermoforming is a widely used technique in industry for plastic thin shell product manufacturing by pressing heated plastic sheets onto molds using atmospheric pressure. We attach on the transparent plastic sheet a precomputed color pattern decal prior to heating, and adhere it to 3D printed models treated as the molds in thermoforming. The 3D models are thus decorated with the desired color texture, as well as a thin, polished protective cover. The precomputation involves a physical simulation of the thermoforming process to compute the correct color pattern on the plastic sheet, and the vent hole layout on the 3D model for air pocket elimination. We demonstrate the effectiveness and accuracy of our computational model and our prototype thermoforming surface coloring system through physical experiments.","Authors":"Y. Zhang; Y. Tong; K. Zhou","DOI":"10.1109\/TVCG.2016.2598570","Keywords":"3D printing;thermoforming;thermoplastic sheet simulation;texture mapping","Keywords_Processed":"3d printing;thermoform;texture mapping;thermoplastic sheet simulation","Title":"Coloring 3D Printed Surfaces by Thermoforming","Labels":null,"Keyword_Vector":[0.0386127633,0.0559258922,-0.0044070896,0.0627717338,0.0740307728,-0.0752327852,0.0274475735,0.0060426017,-0.0172451901,-0.0256294584,0.0131881712,0.0878680821,-0.0512206774,0.0953044638,-0.0239888558,0.0350840192,0.0087197741,-0.0106863488,0.1144578,0.1456379429,0.0677209486,0.1726866735,-0.0716365903,-0.0074338929],"Abstract_Vector":[0.159101595,0.1225030609,-0.0479915484,-0.0593841177,0.0058396596,0.1306655866,-0.032500171,0.0729962275,0.1528749258,0.0941060378,0.1481112253,-0.0702879196,-0.0019918743,0.0558234425,-0.0251083122,0.2754576082,-0.0385615033,-0.0400387091,0.0067567613,0.1203546337,0.0271921111,0.0292760267,-0.1478571497,0.0858208828,-0.0189503847,0.0186084689,-0.0405504046,-0.0190435394,-0.0181767212,-0.0428088164,-0.0448276527,-0.0035830512,0.0843753088,-0.0440824882,-0.0593165397,-0.0836889898,0.0572925243,0.00254582,0.0604145079,0.0210957798,0.0170986253,0.0979358675,0.0152661263,0.1013002424,-0.0286701176,0.0346157224,-0.0670115996,0.0024908783,-0.1165590068,0.0713715471,-0.0152817054,-0.008002466,0.0188746499,-0.0119034425,0.0215988279,0.0172180733,-0.0857556736,0.0873180496,-0.0029309577,0.062502765,0.0580602071,0.0231390146,-0.1259726514,0.0817422131,0.0302260424,0.050620764,0.0455716917,0.001272327,0.1151868627,-0.0439736567,-0.059644979,0.0335832692,-0.0303965457,0.0161337377,0.0021990123,0.056705786]},"139":{"Abstract":"To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.","Authors":"M. El-Assady; F. Sperrle; O. Deussen; D. Keim; C. Collins","DOI":"10.1109\/TVCG.2018.2864769","Keywords":"User-Steerable Topic Modeling;Speculative Execution;Mixed-Initiative Visual Analytics;Explainable Machine Learning","Keywords_Processed":"explainable machine learning;speculative execution;mixed initiative visual analytic;user steerable topic modeling","Title":"Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution","Labels":null,"Keyword_Vector":[0.1630672773,0.0120741442,0.253347708,0.1960873983,-0.1690544905,-0.0730504701,-0.1349481101,0.069116889,-0.0996366349,-0.1240767711,0.0516429972,0.0470833935,0.1565776303,0.1330592445,-0.079300411,-0.067243471,0.1297742074,0.1207294177,-0.0809389926,-0.1126914236,-0.0197798864,0.0121381301,0.0348645537,0.0406665246],"Abstract_Vector":[0.212479307,0.0472606978,0.0144567625,-0.040550751,0.2210436714,-0.1046756474,-0.1870615771,-0.0260274009,0.1052015835,0.0349833692,0.0603322373,0.0617299533,0.0913209776,0.1316110438,-0.0617835992,0.0208560836,-0.0497441103,0.0291052426,0.0425846651,-0.103663929,0.0010182651,0.0012305312,-0.017017874,-0.0756156284,-0.0143979587,0.0327445364,0.056913168,0.0621526211,-0.1056152456,0.0538917666,-0.0536363154,0.0547599255,0.0478791605,0.0116134698,-0.0568778129,0.0093223708,-0.0806102544,-0.0490100567,0.0372204035,0.0436601413,0.0735691579,-0.0987010042,-0.0612415091,0.0658115313,-0.0591240242,0.045023043,0.1241297345,0.031386644,-0.0183611904,-0.0043993348,0.0333302936,-0.0108781857,0.0336926301,0.0520942908,0.1582818281,0.0217578083,0.0637289315,-0.0391226504,-0.0053492751,-0.029830631,0.0781401772,0.0158895175,-0.0479982529,0.0205253307,0.0133597926,0.0291124634,0.0333961137,0.0345714905,0.0493775345,0.013965925,0.074812441,-0.0158375176,0.006084658,-0.0848685105,0.032651814,-0.0652349624]},"140":{"Abstract":"We present a visualization approach for the analysis of CO2 bubble-induced attenuation in porous rock formations. As a basis for this, we introduce customized techniques to extract CO2 bubbles and their surrounding porous structure from X-ray computed tomography data (XCT) measurements. To understand how the structure of porous media influences the occurrence and the shape of formed bubbles, we automatically classify and relate them in terms of morphology and geometric features, and further directly support searching for promising porous structures. To allow for the meaningful direct visual comparison of bubbles and their structures, we propose a customized registration technique considering the bubble shape as well as its points of contact with the porous media surface. With our quantitative extraction of geometric bubble features, we further support the analysis as well as the creation of a physical model. We demonstrate that our approach was successfully used to answer several research questions in the domain, and discuss its high practical relevance to identify critical seismic characteristics of fluid-saturated rock that govern its capability to store CO2.","Authors":"H. Zhang; S. Frey; H. Steeb; D. Uribe; T. Ertl; W. Wang","DOI":"10.1109\/TVCG.2018.2864506","Keywords":"3D volume rendering;bubble visualization;porous media","Keywords_Processed":"bubble visualization;3d volume rendering;porous medium","Title":"Visualization of Bubble Formation in Porous Media","Labels":null,"Keyword_Vector":[0.1934753442,-0.0242368325,-0.1417281044,0.3790125564,0.2131365795,0.1490901527,-0.0860196409,-0.0616846705,0.1668914029,0.0156718636,0.0258267741,0.0362665449,-0.0006814798,0.044269176,-0.0394957001,-0.0210261104,0.0636685796,-0.0842951559,0.0488074306,0.1072795764,0.091811729,-0.0070978391,0.0841844016,0.0019573646],"Abstract_Vector":[0.1773668718,0.0248691433,-0.1084077579,-0.0234218505,0.0405188693,-0.0195745465,0.0251631416,-0.0377683185,-0.0009823167,0.0156475259,-0.0754624068,-0.0674959255,-0.0310339914,0.0526597828,0.0280983267,-0.0126100659,-0.1349310143,-0.0253296746,-0.1532423126,0.0462185569,-0.0086029002,0.0619978519,-0.0622845411,0.0441428857,-0.1037222943,-0.0644195423,0.0826524511,-0.0251784783,0.0337094318,-0.0594038145,-0.0024682091,0.012131007,-0.047619214,-0.0732882702,-0.00754602,0.051477601,-0.0744079675,0.0016923914,-0.0116828332,-0.0254307198,0.0496155779,0.0227556178,0.0604527136,0.2100006643,-0.0531560075,0.0676691385,-0.0377899608,-0.1392379587,0.0410892767,0.1290116386,0.0117494308,0.0353313234,0.0282384243,-0.1889517205,-0.0214688315,0.1050232111,0.1076577367,0.0764091628,-0.0400942238,0.0479047912,-0.028437041,0.0159795441,-0.0141648963,-0.0987368682,0.0874593747,0.1093613655,-0.0823384816,0.0029427604,0.0755809066,0.0514860321,0.0887475613,-0.1117720405,-0.0509982925,0.0859252121,-0.0631499635,-0.038555024]},"141":{"Abstract":"A popular method of force-directed graph drawing is multidimensional scaling using graph-theoretic distances as input. We present an algorithm to minimize its energy function, known as stress, by using stochastic gradient descent (SGD) to move a single pair of vertices at a time. Our results show that SGD can reach lower stress levels faster and more consistently than majorization, without needing help from a good initialization. We then show how the unique properties of SGD make it easier to produce constrained layouts than previous approaches. We also show how SGD can be directly applied within the sparse stress approximation of Ortmann et al. [1], making the algorithm scalable up to large graphs.","Authors":"J. X. Zheng; S. Pawar; D. F. M. Goodman","DOI":"10.1109\/TVCG.2018.2859997","Keywords":"Graph drawing;multidimensional scaling;constraints;relaxation;stochastic gradient descent","Keywords_Processed":"stochastic gradient descent;multidimensional scaling;constraint;relaxation;graph drawing","Title":"Graph Drawing by Stochastic Gradient Descent","Labels":null,"Keyword_Vector":[0.0573240273,-0.0239846949,-0.0268891488,-0.0184005356,-0.0150389481,-0.0336304867,0.0715231358,0.0248726519,-0.0554030209,0.1444780709,-0.0077355128,-0.0486519594,0.0367670088,-0.022932435,-0.1397321199,0.0550353227,0.0807521426,0.037901145,0.0013124308,0.0525537612,0.0474431728,-0.0347079877,0.0091152639,0.0006532804],"Abstract_Vector":[0.1558430885,0.0513627734,-0.043193641,0.0266485843,0.0176806409,0.0016286009,0.0668511691,0.08493745,0.0717265274,-0.0633113469,-0.0485853398,-0.0072378356,-0.0797080019,-0.0259159195,0.025718697,-0.0421254553,0.0815910309,0.1418145485,0.0179664921,-0.0551386124,0.0282928939,-0.0958396127,-0.0194004071,0.1007677716,0.1552441085,-0.0066390054,0.0102030835,0.0213056542,-0.001374128,0.0080811808,-0.0171198998,0.068484836,0.0364846245,0.0412847072,-0.0192501983,0.0481921062,-0.0096657622,-0.1644332215,-0.014017304,-0.0578619797,0.1552691652,0.0036182359,0.0239859815,0.0339594636,-0.0069221069,-0.1287746984,0.0082638143,0.1436691613,0.1507094837,-0.056001649,0.1067200468,0.0088553731,-0.1096632624,0.0944402098,-0.0798320241,0.0261453582,-0.0513947216,0.1316429523,0.0707312484,0.0473568915,-0.1848935926,-0.0213598239,-0.1082438707,-0.064885963,-0.0424755368,0.1113389559,-0.0775085974,-0.0257055759,0.2027025102,0.0718979452,-0.0247832228,-0.0146305648,-0.0755286103,-0.0766857179,-0.0846752179,-0.0815052311]},"142":{"Abstract":"People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.","Authors":"E. Wall; S. Das; R. Chawla; B. Kalidindi; E. T. Brown; A. Endert","DOI":"10.1109\/TVCG.2017.2745078","Keywords":"Mixed-initiative visual analytics;multi-attribute ranking;user interaction","Keywords_Processed":"mixed initiative visual analytic;user interaction;multi attribute ranking","Title":"Podium: Ranking Data Using Mixed-Initiative Visual Analytics","Labels":null,"Keyword_Vector":[0.2236697313,0.0438843184,0.2170963706,0.1682473559,-0.082557083,-0.1091059506,-0.1152077219,0.3332297829,-0.1031862954,-0.1302555126,0.0029892405,0.0551020131,0.1958565935,0.0728789372,0.1311335595,-0.0514134387,-0.0133904911,0.1619094418,0.0106786899,-0.1179931352,-0.027766253,0.0463632688,0.0563802661,0.1205578854],"Abstract_Vector":[0.3049724169,-0.187177013,0.0354247122,-0.087071763,-0.0982581038,0.0220181427,-0.0577615857,0.0265223537,0.0386782163,0.0847457919,0.1087226967,0.1736297239,0.104898906,0.00741669,-0.0426704132,-0.034919141,0.0218809056,-0.0172204124,-0.1020675309,-0.1308272916,0.0333065535,0.1726316525,-0.0796747848,-0.0588064368,0.1301110867,-0.1620562672,-0.0513396862,0.1104977985,-0.0505270934,0.021164833,-0.172988735,0.1216444167,-0.0393242796,-0.0893786689,0.0104781321,-0.0262069196,-0.1292525642,0.0538540343,0.0069016442,0.0328850507,0.0311764453,0.156774457,0.0112805111,-0.0160732758,0.0446584117,-0.1024988923,-0.1042295103,-0.0447902713,-0.0132763797,-0.0409779739,0.025988076,0.0003760322,-0.0121864687,-0.0097479463,0.0510113303,-0.0059282911,0.1750573264,-0.0210540105,-0.0790304455,-0.0892240401,-0.064342566,0.0704555847,0.0235322644,0.0022320923,-0.0929550087,-0.0108127391,-0.0069128654,0.0040700263,0.0866227584,-0.0811724181,0.0274751126,-0.0385532637,-0.1101156373,0.0077885692,-0.0189006803,-0.0505860833]},"143":{"Abstract":"Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action\/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.","Authors":"J. Wang; L. Gou; H. Shen; H. Yang","DOI":"10.1109\/TVCG.2018.2864504","Keywords":"Deep Q-Network (DQN);reinforcement learning;model interpretation;visual analytics","Keywords_Processed":"deep network dqn;model interpretation;reinforcement learning;visual analytic","Title":"DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks","Labels":null,"Keyword_Vector":[0.1725113668,-0.0415801419,0.3412977878,0.2555859408,-0.279919542,-0.0432011192,-0.1289753466,-0.013579455,-0.1226730219,-0.0044465739,-0.0673158916,0.0338090433,-0.0035610233,0.0015542036,-0.1888178757,-0.0389238996,0.1998421211,-0.0238794405,-0.130182526,0.0496313765,0.018321408,0.0516256866,-0.1282072359,-0.0647602465],"Abstract_Vector":[0.2030783742,-0.0047846771,0.0507975095,-0.071717308,0.1887604677,-0.1577440912,-0.1438528964,-0.068753481,0.1161767726,0.0812036883,0.0180243518,0.0209076555,0.0518074724,0.0988039428,0.0034573798,0.0431827434,-0.0095216255,0.0168970767,0.123915747,0.0358695519,-0.0149682468,0.0293001136,-0.0550508906,-0.048136212,0.0019576042,0.0416250608,-0.0284014978,0.0302253826,0.0923013143,-0.0190381009,0.0585529158,-0.0154346827,0.0535063893,0.1000356293,-0.1398357661,-0.106410946,-0.0801097447,-0.0330715339,-0.0003719362,0.1706617199,0.0240336896,0.0536554864,-0.0617859952,0.1297559105,0.0746033402,-0.1341042786,-0.0680265954,0.0336393655,0.089941742,-0.0214156165,0.0893102265,-0.036467661,0.0552114822,-0.042882778,-0.0692122438,-0.0647802788,0.0216461898,-0.0578769691,0.0395292489,0.0147409464,0.0277560884,-0.123273458,-0.0526752383,-0.064764742,-0.069077236,0.0046117248,0.0024968035,0.1485372326,-0.0690388229,-0.0166895935,-0.1034940293,0.0414712353,0.0296778001,0.0144375106,-0.0752596015,-0.0509894282]},"144":{"Abstract":"We present a volume exploration framework, FeatureLego, that uses a novel voxel clustering approach for efficient selection of semantic features. We partition the input volume into a set of compact super-voxels that represent the finest selection granularity. We then perform an exhaustive clustering of these super-voxels using a graph-based clustering method. Unlike the prevalent brute-force parameter sampling approaches, we propose an efficient algorithm to perform this exhaustive clustering. By computing an exhaustive set of clusters, we aim to capture as many boundaries as possible and ensure that the user has sufficient options for efficiently selecting semantically relevant features. Furthermore, we merge all the computed clusters into a single tree of meta-clusters that can be used for hierarchical exploration. We implement an intuitive user-interface to interactively explore volumes using our clustering approach. Finally, we show the effectiveness of our framework on multiple real-world datasets of different modalities.","Authors":"S. Jadhav; S. Nadeem; A. Kaufman","DOI":"10.1109\/TVCG.2018.2856744","Keywords":"Volume visualization;hierarchical exploration;voxel clustering","Keywords_Processed":"voxel clustering;hierarchical exploration;volume visualization","Title":"FeatureLego: Volume Exploration Using Exhaustive Clustering of Super-Voxels","Labels":null,"Keyword_Vector":[0.192500615,-0.0901813748,-0.0869434808,0.2247209999,0.1670460492,0.1533232675,-0.0820655113,-0.0326133278,0.1184172705,0.1305400768,-0.037256619,-0.0343902603,-0.0628525826,-0.0385906598,0.0110006418,-0.1422207083,0.0847152456,-0.0276855454,0.1026713447,0.0131201985,-0.2546018518,-0.1331255649,-0.0602710631,0.0265302812],"Abstract_Vector":[0.2510500916,-0.0395708545,-0.1690810656,0.2645510242,-0.124181939,-0.204150942,-0.0424059866,0.1352829494,-0.0562192772,-0.0930632895,0.0203129828,-0.2228136031,0.0580361952,-0.0325522835,0.0177187501,0.0498106202,0.0827360825,0.1924182055,-0.0347318143,-0.0212909481,0.1254578263,-0.0184646572,0.0980221208,0.0034463463,0.0396894976,-0.1613622223,-0.0386805707,-0.034212833,-0.1079991347,0.0769149585,-0.0418329417,-0.0951622619,0.0929741836,-0.0633408024,-0.0094408106,0.0090136119,-0.0494881138,0.0181473572,0.0732324313,0.0036543685,-0.0747931113,0.0103197811,0.032010987,0.0337667826,0.0808426829,0.0174795163,0.0546835524,0.0137664499,-0.0333725808,-0.0080070724,0.0030287558,0.0539073929,0.0354768492,-0.0161534489,0.0822145563,-0.0352721373,-0.0799304933,0.0404975194,-0.0496410923,-0.0140533122,-0.0252327112,-0.0097822633,-0.0299517126,0.0026865113,0.034403068,-0.048927257,-0.0709877021,-0.0102340796,-0.0318858541,0.0045354788,0.0023784729,-0.0480633082,0.0076780428,-0.0616874259,0.0399002097,-0.0240259732]},"145":{"Abstract":"Dimensionality reduction (DR) is a common strategy for visual analysis of labeled high-dimensional data. Low-dimensional representations of the data help, for instance, to explore the class separability and the spatial distribution of the data. Widely-used unsupervised DR methods like PCA do not aim to maximize the class separation, while supervised DR methods like LDA often assume certain spatial distributions and do not take perceptual capabilities of humans into account. These issues make them ineffective for complicated class structures. Towards filling this gap, we present a perception-driven linear dimensionality reduction approach that maximizes the perceived class separation in projections. Our approach builds on recent developments in perception-based separation measures that have achieved good results in imitating human perception. We extend these measures to be density-aware and incorporate them into a customized simulated annealing algorithm, which can rapidly generate a near optimal DR projection. We demonstrate the effectiveness of our approach by comparing it to state-of-the-art DR methods on 93 datasets, using both quantitative measure and human judgments. We also provide case studies with class-imbalanced and unlabeled data.","Authors":"Y. Wang; K. Feng; X. Chu; J. Zhang; C. Fu; M. Sedlmair; X. Yu; B. Chen","DOI":"10.1109\/TVCG.2017.2701829","Keywords":"Dimensionality reduction;supervised;visual class separation;high-dimensional data","Keywords_Processed":"high dimensional datum;supervised;visual class separation;dimensionality reduction","Title":"A Perception-Driven Approach to Supervised Dimensionality Reduction for Visualization","Labels":null,"Keyword_Vector":[0.1299629584,-0.0916024583,0.1705293734,-0.0759988485,0.1391780762,0.0306846697,-0.0425295551,0.0064334474,-0.1393792755,-0.0097963547,0.0741408152,-0.1002133936,-0.0125007674,0.027428987,-0.1014762605,0.1604459488,0.0678666731,0.0769797374,0.1063960686,-0.0792356133,-0.1477486463,0.0217072418,0.0978980811,-0.0457838822],"Abstract_Vector":[0.2166782364,-0.0240306971,-0.0675837682,-0.0442399414,-0.0734794242,0.0675296122,-0.0949529199,0.1068165173,0.1051655539,0.0204477342,-0.0186666847,0.0434578227,0.0167615068,0.0524004358,0.0356725331,-0.1176700176,-0.0383890295,0.0829623914,0.0372076355,0.0433201223,0.0080596344,0.0225169971,0.1377072032,-0.1091151806,-0.1054895911,-0.0393565059,0.0942204748,0.1190190297,0.0821119554,-0.0152747581,-0.1189309696,0.0910536692,-0.0157302461,-0.0287201064,-0.0155946871,0.0475504568,0.0241867356,0.0369325792,0.0908104578,-0.0436089411,0.121406224,-0.0579458695,-0.052290896,-0.0469681165,0.089740669,0.0293989486,0.0848169211,0.0036208311,-0.0466348541,0.0835655731,0.0585671592,0.0702330304,-0.0381509498,-0.1290648324,0.0575179104,-0.0334789077,-0.1513663386,-0.0383533789,0.1102745981,0.0022141514,-0.0910252402,0.066871845,-0.0034697149,-0.0134823768,-0.0368152681,-0.024477681,0.0359377951,-0.0881498829,0.0133965641,-0.0697450357,0.0366065844,0.0083467767,-0.025456662,0.0755534015,-0.0611383527,-0.0176618985]},"146":{"Abstract":"We present a novel framework for visualizing routes on mobile devices. Our framework is suitable for helping users explore their environment. First, given a starting point and a maximum route length, the system retrieves nearby points of interest (POIs). Second, we automatically compute an attractive walking path through the environment trying to pass by as many highly ranked POIs as possible. Third, we automatically compute a route visualization that shows the current user position, POI locations via pins, and detail lenses for more information about the POIs. The visualization is an animation of an orthographic map view that follows the current user position. We propose an optimization based on a binary integer program (BIP) that models multiple requirements for an effective placement of detail lenses. We show that our path computation method outperforms recently proposed methods and we evaluate the overall impact of our framework in two user studies.","Authors":"M. Birsak; P. Musialski; P. Wonka; M. Wimmer","DOI":"10.1109\/TVCG.2017.2690294","Keywords":"Tourist guide;OpenStreetMap;exploration;binary integer program","Keywords_Processed":"openstreetmap;tourist guide;exploration;binary integer program","Title":"Dynamic Path Exploration on Mobile Devices","Labels":null,"Keyword_Vector":[0.0200754716,-0.016394563,0.0237131497,0.0060564265,0.0369714206,-0.0070815026,0.0157347438,0.0274706523,-0.0015612224,0.0730490294,-0.0542052686,-0.015561379,-0.0292198994,-0.0011627823,0.0606889072,-0.0750382721,0.0540943692,-0.0532720401,0.0406864563,0.0352535672,-0.1451034043,-0.0963951779,-0.0682914647,0.0101269549],"Abstract_Vector":[0.2263226268,0.0118055675,0.039415374,0.0124206834,0.0560512375,-0.0469725164,0.035198535,0.0180500406,0.0773939618,-0.1131240053,0.1056311841,0.0208619333,0.07144224,0.0160426282,-0.0618861827,0.0160950393,-0.0439447677,-0.039579728,-0.0507434892,-0.1574352664,-0.0259793908,-0.009473119,-0.0139851906,-0.0784255934,0.0752981283,0.034902137,0.086841271,-0.1304349992,-0.0469491459,0.139133108,-0.0525794967,0.1374621513,0.0417505666,-0.1256703622,-0.10310109,-0.1003942808,-0.0710667832,-0.0795329108,-0.0898110515,0.0174765052,-0.08812378,0.0751847209,0.0428903812,-0.1702974667,-0.0072238979,-0.0057361516,0.0894572119,-0.0885998917,-0.0586280775,-0.0337504573,0.114025975,-0.0091055404,-0.0703426896,-0.0275573029,0.0180522888,0.0048412922,0.1199281758,0.0791996183,-0.0097384996,-0.0514985086,0.0087166021,0.006174055,0.0172368786,-0.0194739312,0.1336129941,0.0136962864,-0.0572885083,0.1015130663,-0.0350816709,-0.0024614834,-0.0360258355,-0.0577763443,-0.0197212041,0.0187527434,-0.0511145304,-0.050646278]},"147":{"Abstract":"Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.","Authors":"H. Kim; J. Choo; H. Park; A. Endert","DOI":"10.1109\/TVCG.2015.2467615","Keywords":"Scatterplots;user interaction;model steering;Scatterplots;user interaction;model steering","Keywords_Processed":"model steering;scatterplot;user interaction","Title":"InterAxis: Steering Scatterplot Axes via Observation-Level Interaction","Labels":null,"Keyword_Vector":[0.1419368939,0.0575264673,0.0468490226,0.1200363989,-0.0282763495,-0.027240072,0.0067577376,0.2872435096,-0.1661281148,-0.1374524046,-0.0152655734,0.1575661027,0.0457391147,0.104246934,0.250708499,0.0572157191,0.0850536019,0.0936179157,-0.0595837778,-0.0730605769,-0.0225619056,0.0072965797,-0.1035954712,0.0532456196],"Abstract_Vector":[0.2285877387,-0.1648894551,-0.0071685879,-0.1117069394,-0.1544043585,0.0862298734,-0.0209757202,0.0722345389,-0.0024801081,0.0863343489,0.0145075207,0.0579636618,0.0437147635,-0.0026982054,-0.053138407,-0.0712808945,0.016883049,-0.0054400534,-0.0634012092,-0.1024341096,-0.035372576,0.0673187245,-0.0755600347,-0.0863629967,0.0687000668,-0.0879299359,-0.0011538257,0.0886279101,-0.0513638561,-0.0126304596,-0.0746784118,0.1185688265,-0.0877062942,0.0273974247,-0.093226094,-0.0285308737,-0.112382828,0.0486615959,-0.002030298,0.0020841582,-0.0531389823,0.0121127154,-0.0710883451,-0.0066853743,0.1271880368,-0.0618445567,-0.0669130217,-0.0829523883,0.0624237721,0.0126479637,0.0721987015,-0.0588276368,-0.1711220492,-0.0224106502,-0.0409600898,-0.0534290783,0.0435467488,0.0825247848,-0.1181437221,0.0535092776,0.0252017526,0.0171727677,0.0144933495,0.0449777197,0.0073741236,-0.01766791,0.000601701,-0.0089296647,-0.0299624417,0.0526511055,0.0021009675,0.1237565581,-0.023488102,-0.0247626922,-0.0435677979,0.0074907903]},"148":{"Abstract":"Working with noisy meshes and aiming at providing high-fidelity 3D object models without tampering the metric quality of the acquisitions, we propose a mesh denoising technique that, through a normal-diffusion process guided by a curvature saliency map, is able to preserve and emphasize the natural object features, concurrently allowing the introduction of a bound on the maximum distance from the original model. Moreover, both the position of the mesh vertices and the edge orientations are optimized through a tailored geometric-aliasing correction. Thanks to an efficiently parallelized procedure, we are able to process even large models almost instantly with a parameter configuration that does not depend on the scale of the object. An essential survey on mesh denoising is also presented which is functional to the definition of a common framework where to set up our solutions and the related technical and experimental comparisons. The proposed results prove the effectiveness of our method, especially on the challenging target application profiles. Where competing techniques tend to inappropriately recover sharp edges while deforming the surrounding geometry or, on the contrary, to oversmooth shallow features, our method protects and enhances the natural object features and effectively reduces scanning noise on the smooth parts, while guaranteeing the prescribed metric-fidelity to the input model.","Authors":"M. Centin; A. Signoroni","DOI":"10.1109\/TVCG.2017.2731771","Keywords":"Mesh denoising;high-fidelity 3D modeling;feature preservation;saliency-driven geometry processing;scale-invariance;surface normal diffusion;geometric aliasing","Keywords_Processed":"scale invariance;high fidelity 3d modeling;feature preservation;surface normal diffusion;saliency drive geometry processing;mesh denoising;geometric aliasing","Title":"Mesh Denoising with (Geo)Metric Fidelity","Labels":null,"Keyword_Vector":[0.062128849,0.0115083422,-0.0000981837,0.0472782496,0.0791749977,-0.1147382539,0.0676587323,-0.0486025424,-0.0519206512,-0.0336612748,0.043894115,0.0894992112,-0.0560469856,0.0891819309,-0.0644394802,0.1667527274,0.0148716005,-0.0400622577,0.08801159,-0.0515260499,0.0308187763,-0.0406185414,0.0006919989,0.0849600159],"Abstract_Vector":[0.2369480105,0.2050881924,-0.1219359029,-0.0102322038,0.1223569681,-0.0366207012,-0.1262347426,-0.0102108861,0.1009993674,0.1297352508,-0.1278799161,-0.0623935775,0.1229001831,-0.0719148844,0.0808758031,-0.0337411036,-0.0765885979,0.094273591,-0.0814015271,-0.146679907,0.0631028345,-0.0067661348,-0.1468539982,0.1434511527,0.1009656623,0.0583376605,0.1210771494,-0.0532606018,-0.0939428783,-0.1575161229,0.022652209,-0.0163409131,-0.0509536899,-0.0526046458,0.0031622904,0.0816438049,0.0841030721,0.0319588134,0.0500551944,0.0501703991,-0.0635953201,-0.031004644,0.0258140331,-0.0286179025,-0.0211017829,-0.0240378518,0.0548425914,-0.0656907812,-0.0422407726,0.040932992,-0.050373668,0.0474984533,0.0265853468,0.0689585363,0.0542456124,0.0303924705,0.0204779139,-0.0291002094,0.0153788449,-0.0657501712,0.1536615298,-0.0519559115,0.0707352529,-0.0058773721,-0.0681083758,-0.0279855632,0.0224603564,0.0490834211,-0.1083093289,-0.017925087,-0.0612570522,0.0541259262,0.0133241993,-0.0699885189,-0.0379285245,0.0695294665]},"149":{"Abstract":"Clustering, the process of grouping together similar items into distinct partitions, is a common type of unsupervised machine learning that can be useful for summarizing and aggregating complex multi-dimensional data. However, data can be clustered in many ways, and there exist a large body of algorithms designed to reveal different patterns. While having access to a wide variety of algorithms is helpful, in practice, it is quite difficult for data scientists to choose and parameterize algorithms to get the clustering results relevant for their dataset and analytical tasks. To alleviate this problem, we built Clustervision, a visual analytics tool that helps ensure data scientists find the right clustering among the large amount of techniques and parameters available. Our system clusters data using a variety of clustering techniques and parameters and then ranks clustering results utilizing five quality metrics. In addition, users can guide the system to produce more relevant results by providing task-relevant constraints on the data. Our visual user interface allows users to find high quality clustering results, explore the clusters using several coordinated visualization techniques, and select the cluster result that best suits their task. We demonstrate this novel approach using a case study with a team of researchers in the medical domain and showcase that our system empowers users to choose an effective representation of their complex data.","Authors":"B. C. Kwon; B. Eysenbach; J. Verma; K. Ng; C. De Filippi; W. F. Stewart; A. Perer","DOI":"10.1109\/TVCG.2017.2745085","Keywords":"Unsupervised Clustering;Visual Analytics;Quality Metrics;Interactive Visual Clustering","Keywords_Processed":"quality metrics;unsupervised clustering;interactive visual clustering;visual analytic","Title":"Clustervision: Visual Supervision of Unsupervised Clustering","Labels":null,"Keyword_Vector":[0.1919189379,-0.0858092829,0.3111785503,0.0871361614,-0.1049285021,0.0038003171,-0.0870754232,-0.0891240581,-0.0116099142,0.0164237819,-0.0563953751,-0.0747884707,-0.0668077025,0.0991672166,-0.0909246103,-0.0125824552,-0.1109842758,0.0497851959,0.0899817838,-0.0113038549,-0.1784727859,-0.0100460896,0.0973192914,-0.0181182279],"Abstract_Vector":[0.3784114379,-0.2112558961,-0.0561551031,0.1587700682,-0.2149059345,-0.1597878095,-0.2102016107,0.1219026293,-0.0756670116,0.0230544355,0.1328354294,-0.1330776392,-0.0109663964,-0.1034225664,0.0134647799,0.0504511514,-0.0045283802,0.2535775858,0.078041946,-0.0063991376,0.0617462696,-0.0356259113,0.0148315369,0.0281083151,0.0472289743,-0.0642632307,0.0064210553,0.0305615251,-0.0466569562,0.0271750336,-0.0479378789,-0.1316371029,-0.0384943106,-0.0473197853,0.0749352672,-0.0455733614,-0.0935624194,0.0585412373,-0.0383661529,0.1073900255,-0.0868932242,0.0268356047,0.0011233545,0.0745007535,0.0909585813,0.0422903116,-0.009987754,0.0643093376,-0.0307589525,0.0247360792,-0.0283364361,-0.0553416346,0.028383398,0.0140959454,-0.0026461231,-0.051772664,0.0590602134,0.0028834692,0.0041666291,-0.026357464,0.0192678027,0.0069781715,0.0108895903,-0.0761669412,-0.0180412193,0.0424385302,-0.0243007018,0.0032092152,-0.0093257728,0.0129511066,-0.0348976826,0.0097930617,0.0618027173,-0.0054039397,0.0242614265,-0.0214374941]},"150":{"Abstract":"The stated goal for visual data exploration is to operate at a rate that matches the pace of human data analysts, but the ever increasing amount of data has led to a fundamental problem: datasets are often too large to process within interactive time frames. Progressive analytics and visualizations have been proposed as potential solutions to this issue. By processing data incrementally in small chunks, progressive systems provide approximate query answers at interactive speeds that are then refined over time with increasing precision. We study how progressive visualizations affect users in exploratory settings in an experiment where we capture user behavior and knowledge discovery through interaction logs and think-aloud protocols. Our experiment includes three visualization conditions and different simulated dataset sizes. The visualization conditions are: (1) blocking, where results are displayed only after the entire dataset has been processed; (2) instantaneous, a hypothetical condition where results are shown almost immediately; and (3) progressive, where approximate results are displayed quickly and then refined over time. We analyze the data collected in our experiment and observe that users perform equally well with either instantaneous or progressive visualizations in key metrics, such as insight discovery rates and dataset coverage, while blocking visualizations have detrimental effects.","Authors":"E. Zgraggen; A. Galakatos; A. Crotty; J. Fekete; T. Kraska","DOI":"10.1109\/TVCG.2016.2607714","Keywords":"Exploratory analysis;interactive visualization;progressive visualization;scalability;insight-based evaluation","Keywords_Processed":"progressive visualization;interactive visualization;insight base evaluation;exploratory analysis;scalability","Title":"How Progressive Visualizations Affect Exploratory Analysis","Labels":null,"Keyword_Vector":[0.3359148866,-0.1417018389,-0.0459242857,-0.0982256953,0.0177954619,0.0096708367,0.1533508758,-0.0801357038,0.0745371241,0.0606667086,-0.0920250543,-0.0515524836,0.1140356315,0.1822461227,0.0650625488,-0.1717726057,-0.0367403468,-0.0178487159,0.012244174,-0.0090791743,0.1696696163,-0.0007677581,0.065192885,-0.0999935257],"Abstract_Vector":[0.2825099808,-0.1117284079,0.1142469093,-0.0242445822,-0.046497719,0.0881511606,-0.0656505488,0.0295175731,-0.0597783271,-0.0448435787,-0.0458727303,0.054993586,-0.0011300833,-0.05482191,0.0928784149,-0.0173233617,-0.0878082824,0.0346958925,-0.0001041386,-0.0683968422,-0.0755361036,-0.0245231198,0.0040925182,0.0077913795,-0.0648082962,0.0689203351,-0.0586686941,0.0490866382,-0.0500060944,0.0705980091,0.0627513073,-0.0936290655,0.0154456326,-0.0506410511,0.0298052679,-0.0201170046,-0.0149806818,0.016026852,-0.017769645,-0.0318031021,-0.0614506407,0.0028109931,0.039037483,0.0287367118,0.090564291,-0.0570616086,0.1127829617,-0.018185037,0.0684795999,0.137064202,0.038828493,-0.1671807147,0.0583275177,0.133548056,0.0519936319,-0.0276765438,-0.0556461951,-0.0687165636,0.1092078407,0.0241979975,-0.0275096775,-0.0697334369,0.0762983457,0.1073286276,-0.0162678789,-0.0157434039,0.0312318104,0.006066276,0.0254982414,-0.0781435628,0.0778720912,0.031293571,0.0207720484,0.0360225892,0.0941797129,0.1149850015]},"151":{"Abstract":"We present a direct manipulation technique that allows material scientists to interactively highlight relevant parameterized simulation instances located in dimensionally reduced spaces, enabling a user-defined understanding of a continuous parameter space. Our goals are two-fold: first, to build a user-directed intuition of dimensionally reduced data, and second, to provide a mechanism for creatively exploring parameter relationships in parameterized simulation sets, called ensembles. We start by visualizing ensemble data instances in dimensionally reduced scatter plots. To understand these abstract views, we employ user-defined virtual data instances that, through direct manipulation, search an ensemble for similar instances. Users can create multiple of these direct manipulation queries to visually annotate the spaces with sets of highlighted ensemble data instances. User-defined goals are therefore translated into custom illustrations that are projected onto the dimensionally reduced spaces. Combined forward and inverse searches of the parameter space follow naturally allowing for continuous parameter space prediction and visual query comparison in the context of an ensemble. The potential for this visualization technique is confirmed via expert user feedback for a shock physics application and synthetic model analysis.","Authors":"D. Orban; D. F. Keefe; A. Biswas; J. Ahrens; D. Rogers","DOI":"10.1109\/TVCG.2018.2865051","Keywords":"Visual Parameter Space Analysis;Ensemble Visualization;Semantic Interaction;Direct Manipulation;Shock Physics","Keywords_Processed":"direct manipulation;ensemble visualization;visual parameter space analysis;semantic interaction;shock physics","Title":"Drag and Track: A Direct Manipulation Interface for Contextualizing Data Instances within a Continuous Parameter Space","Labels":null,"Keyword_Vector":[0.2869420974,-0.1190057225,0.0601726737,0.0012512087,0.0952088961,-0.0696108376,-0.1404800613,0.0166631763,0.0786991773,-0.0484962664,-0.1305685486,0.1721312229,0.0730820186,-0.0041813889,0.1013245257,0.0374206575,-0.063373291,-0.0271464776,0.0628420766,-0.1335162093,-0.0137017984,0.0698353394,0.0515776306,0.0924129067],"Abstract_Vector":[0.2451055427,-0.0639174233,-0.0049804652,0.1906413731,-0.07611133,-0.0637557813,-0.1078911353,-0.0332675125,-0.0050196853,-0.0951329348,0.1062208611,0.045010136,-0.0325241871,0.092881557,-0.023676424,-0.0673319128,-0.005350424,-0.1168176645,-0.0531572046,0.0015203131,-0.0679805165,0.0518587406,-0.1936751086,-0.0260527007,-0.0893970781,0.1084755156,-0.0080244442,-0.1121884564,-0.0152933357,-0.0264282253,-0.0067902378,0.1446882577,-0.0905192448,0.081003483,0.0230958436,0.0212355273,0.0284705761,-0.0321069967,0.0478524766,0.0609277631,0.0008566143,-0.1362373306,0.0184514272,-0.0652963869,0.001442486,0.0140275002,-0.0017424564,0.1600237469,0.0626647711,0.0901149697,0.0207289279,-0.1082872783,-0.0053478944,-0.0897208771,0.067102696,0.0089058127,0.0718425654,0.0001151172,0.0471192756,-0.0104389469,0.0713681011,0.069588702,0.122580654,-0.0353998845,0.0668266353,0.0357507613,-0.0417507584,0.0500548445,-0.0276546388,-0.0020672145,0.0629542168,0.117386541,0.0286384057,-0.0022726843,-0.1966250767,-0.0072199006]},"152":{"Abstract":"Although visualization design models exist in the literature in the form of higher-level methodological frameworks, these models do not present a clear methodological prescription for the domain characterization step. This work presents a framework and end-to-end model for requirements engineering in problem-driven visualization application design. The framework and model are based on the activity-centered design paradigm, which is an enhancement of human-centered design. The proposed activity-centered approach focuses on user tasks and activities, and allows an explicit link between the requirements engineering process with the abstraction stage - and its evaluation - of existing, higher-level visualization design models. In a departure from existing visualization design models, the resulting model: assigns value to a visualization based on user activities; ranks user tasks before the user data; partitions requirements in activity-related capabilities and nonfunctional characteristics and constraints; and explicitly incorporates the user workflows into the requirements process. A further merit of this model is its explicit integration of functional specifications, a concept this work adapts from the software engineering literature, into the visualization design nested model. A quantitative evaluation using two sets of interdisciplinary projects supports the merits of the activity-centered model. The result is a practical roadmap to the domain characterization step of visualization design for problem-driven data visualization. Following this domain characterization model can help remove a number of pitfalls that have been identified multiple times in the visualization design literature.","Authors":"G. E. Marai","DOI":"10.1109\/TVCG.2017.2744459","Keywords":"Design studies;Tasks and requirements analysis;Visualization models;Domain characterization;Activity-centered design;Functional specifications","Keywords_Processed":"design study;domain characterization;task and requirement analysis;visualization model;functional specification;activity center design","Title":"Activity-Centered Domain Characterization for Problem-Driven Scientific Visualization","Labels":null,"Keyword_Vector":[0.2324652046,-0.0685539121,0.0136960284,-0.0728219857,-0.0883942687,0.2070100012,0.3453045174,0.1135775951,0.1258855521,-0.2819473917,-0.0746616743,0.1218883126,-0.0651935643,-0.001126179,-0.0372737142,-0.0161494194,0.1074297569,-0.0630768623,-0.1189438677,0.0402073315,-0.0401049221,0.0257851299,-0.0347450663,-0.0663421835],"Abstract_Vector":[0.2997451843,-0.0739348493,0.0680992733,-0.1170421693,0.2933298887,0.0093433905,-0.1683897623,-0.0697901088,-0.002678569,-0.0047065821,0.0325893749,-0.0370868494,0.088688847,0.042459994,-0.0991088658,0.0261202567,0.0430835464,0.0328226005,0.0329561608,-0.0027575705,-0.0170089139,-0.0690085609,-0.0394547187,-0.0765491523,0.0637011703,0.0868755638,0.0153676886,-0.0327482477,-0.0538218899,0.0604445724,0.0290509312,-0.0303886748,0.0057235445,-0.0067689266,-0.0470399012,-0.0966565932,-0.0772175662,-0.0411758271,0.0859496994,0.0183290008,-0.0234640043,-0.1146841269,-0.0197149021,0.0572094741,-0.0088126528,0.0755503239,-0.0010169312,-0.0101170007,-0.1090059075,-0.0076988503,0.0489400977,-0.0338547379,-0.0246005577,0.0035355144,0.1056878647,-0.009901466,0.149557962,-0.0210105569,0.0387981581,-0.0129925509,0.0115856014,0.0009893845,-0.0188611718,0.0704989829,-0.0218631617,-0.0596965053,-0.0064744528,-0.0082967465,-0.0249742542,0.1113671337,-0.0859187976,-0.1072490295,-0.0200450032,-0.0440154114,-0.0142626902,0.0236562741]},"153":{"Abstract":"In Computer-Aided Design (CAD), Non-Uniform Rational B-Splines (NURBS) are a common model representation for export, simulation and visualization. In this paper, we present a direct rendering method for trimmed NURBS models based on their parametric description. Our approach builds on a novel trimming method and a three-pass pipeline which both allow for a sub-pixel precise visualization. The rendering pipeline bypasses tessellation limitations of current hardware using a feedback mechanism. In contrast to existing work, our trimming method scales well with a large number of trim curves and estimates the trimmed surface's footprint in screen-space which allows for an anti-aliasing with minimal performance overhead. Fragments with trimmed edges are routed into a designated off-screen buffer for subsequent blending with background faces. The evaluation of the presented algorithms shows that our rendering system can handle CAD models with ten thousands of trimmed NURBS surfaces. The suggested two-level data structure used for trimming outperforms state-of-the-art methods while being more precise and memory efficient. Our curve coverage estimation used for anti-aliasing provides an efficient trade-off between quality and performance compared to multisampling or screen-space anti-aliasing approaches.","Authors":"A. Schollmeyer; B. Froehlich","DOI":"10.1109\/TVCG.2018.2814987","Keywords":"Trimming;NURBS;anti-aliasing;adaptive tessellation","Keywords_Processed":"nurb;adaptive tessellation;anti alias;trim","Title":"Efficient and Anti-Aliased Trimming for Rendering Large NURBS Models","Labels":null,"Keyword_Vector":[0.0026590984,0.0023598495,0.0035319051,0.0054647848,0.0035283549,-0.0094160638,0.0179168753,-0.0123878911,-0.0313730104,-0.0154814063,-0.0054980372,0.0052207053,-0.0099078771,0.0466001357,0.0050329778,-0.0029811602,0.010217517,0.0117667143,0.0234400904,0.0552234787,-0.030018088,0.0390962917,-0.0930375644,-0.0029444621],"Abstract_Vector":[0.1838760124,0.0824681863,-0.0675024772,-0.0145009273,0.0711224073,0.0562261475,-0.0161627176,0.0722116282,-0.0544363136,-0.0623289579,-0.0285557421,-0.0048643333,-0.0487979931,0.0885315535,0.0225700845,0.0045994919,-0.0600094698,0.0422750834,0.0807549133,-0.0021934663,-0.0389564472,0.0165212639,0.0772525599,-0.0317254413,-0.0060962375,0.0166772717,-0.0392173606,-0.0406500193,0.0532876652,0.0554719347,-0.0075246887,0.0528516236,0.0280805638,0.0387670966,0.0209395168,-0.0496717672,-0.0748838078,-0.0115633748,0.0325880036,0.0917477731,-0.0209727073,-0.0108337765,-0.0609105967,0.0364574267,-0.0038044145,-0.0402166284,0.0886828102,-0.0671738959,-0.0407719113,-0.0325194386,-0.1702438028,0.0800511171,0.0487033152,-0.0307644698,-0.1076214271,0.142180156,0.0613894576,-0.0701310771,0.1186892187,-0.1346294507,0.0298074562,0.1576435966,0.1316898672,0.0371084204,0.1578446583,0.0165491166,-0.0075852101,0.129663667,0.0640674389,-0.0143168639,-0.120124039,0.1554355328,0.012017807,0.0643896658,-0.1573624934,-0.0209161821]},"154":{"Abstract":"For neurodegenerative conditions like Parkinson's disease, early and accurate diagnosis is still a difficult task. Evaluations can be time consuming, patients must often travel to metropolitan areas or different cities to see experts, and misdiagnosis can result in improper treatment. To date, only a handful of assistive or remote methods exist to help physicians evaluate patients with suspected neurological disease in a convenient and consistent way. In this paper, we present a low-cost VR interface designed to support evaluation and diagnosis of neurodegenerative disease and test its use in a clinical setting. Using a commercially available VR display with an infrared camera integrated into the lens, we have constructed a 3D virtual environment designed to emulate common tasks used to evaluate patients, such as fixating on a point, conducting smooth pursuit of an object, or executing saccades. These virtual tasks are designed to elicit eye movements commonly associated with neurodegenerative disease, such as abnormal saccades, square wave jerks, and ocular tremor. Next, we conducted experiments with 9 patients with a diagnosis of Parkinson's disease and 7 healthy controls to test the system's potential to emulate tasks for clinical diagnosis. We then applied eye tracking algorithms and image enhancement to the eye recordings taken during the experiment and conducted a short follow-up study with two physicians for evaluation. Results showed that our VR interface was able to elicit five common types of movements usable for evaluation, physicians were able to confirm three out of four abnormalities, and visualizations were rated as potentially useful for diagnosis.","Authors":"J. Orlosky; Y. Itoh; M. Ranchet; K. Kiyokawa; J. Morgan; H. Devos","DOI":"10.1109\/TVCG.2017.2657018","Keywords":"Virtual reality;eye tracking;diagnosis;visualization","Keywords_Processed":"visualization;diagnosis;eye tracking;virtual reality","Title":"Emulation of Physician Tasks in Eye-Tracked Virtual Reality for Remote Diagnosis of Neurodegenerative Disease","Labels":null,"Keyword_Vector":[0.2554175482,0.3349468326,-0.098728852,-0.105262298,0.0559560361,-0.0766329351,-0.0730942801,-0.0585485347,0.1222320514,0.0263734326,0.0023543166,0.0221947814,-0.0892189583,-0.0004405055,-0.1755835644,-0.0174729935,0.0206041351,-0.0278630996,-0.0466450125,-0.1217970822,0.0624725197,0.0564475471,-0.1075968964,-0.0238463185],"Abstract_Vector":[0.1867236307,0.0387634868,0.2018412547,0.0485858632,0.0080920934,0.0204311613,-0.0424948856,0.0024180576,0.0004371774,0.0620257087,0.0004274369,-0.0472170388,0.0448623188,-0.0062853797,0.08509688,0.0313755158,-0.0572034704,0.0085774856,0.1008365614,-0.0116046632,0.0373829966,-0.0642018268,0.0307272307,0.0175876767,0.0309693207,-0.0176825921,-0.0988879949,-0.0018229783,0.0354548741,0.000778975,-0.1104771508,-0.105380318,-0.0300457879,-0.0775127602,0.0138648375,-0.0266227731,0.036529092,-0.0339072118,-0.0201844894,0.0167318375,0.078061283,-0.0118015789,-0.0744244341,-0.0889326024,-0.0215060625,0.0576628524,-0.0064761834,-0.1242478479,0.0331160546,-0.1459735703,-0.0783393574,-0.0895636373,-0.0319544084,0.0249080419,-0.1712520048,0.0692037415,0.0283573141,-0.0061563403,-0.0468524917,0.0991848896,-0.0768190202,-0.0755654847,0.1115009148,-0.0435453605,-0.0015819345,-0.0612927906,-0.0355878279,-0.1341088067,0.0284307782,0.0008870776,-0.0787915819,-0.0872687939,-0.0448732807,0.0293400631,-0.0888434875,-0.0754049985]},"155":{"Abstract":"Many foundational visualization techniques including isosurfacing, direct volume rendering and texture mapping rely on piecewise multilinear interpolation over the cells of a mesh. However, there has not been much focus within the visualization community on techniques that efficiently generate and encode globally continuous functions defined by the union of multilinear cells. Wavelets provide a rich context for analyzing and processing complicated datasets. In this paper, we exploit adaptive regular refinement as a means of representing and evaluating functions described by a subset of their nonzero wavelet coefficients. We analyze the dependencies involved in the wavelet transform and describe how to generate and represent the coarsest adaptive mesh with nodal function values such that the inverse wavelet transform is exactly reproduced via simple interpolation (subdivision) over the mesh elements. This allows for an adaptive, sparse representation of the function with on-demand evaluation at any point in the domain. We focus on the popular wavelets formed by tensor products of linear B-splines, resulting in an adaptive, nonconforming but crack-free quadtree (2D) or octree (3D) mesh that allows reproducing globally continuous functions via multilinear interpolation over its cells.","Authors":"K. Weiss; P. Lindstrom","DOI":"10.1109\/TVCG.2015.2467412","Keywords":"Multilinear interpolation;adaptive wavelets;multiresolution models;octrees;continuous reconstruction;Multilinear interpolation;adaptive wavelets;multiresolution models;octrees;continuous reconstruction","Keywords_Processed":"adaptive wavelet;multilinear interpolation;multiresolution model;continuous reconstruction;octree","Title":"Adaptive Multilinear Tensor Product Wavelets","Labels":null,"Keyword_Vector":[0.0265550791,0.0023259978,0.024988102,0.0530591932,-0.0158511914,-0.0731107524,0.0433124399,-0.0259654704,-0.0843199755,-0.0670133288,-0.0415869298,0.0000383932,-0.0591665727,0.0623413138,0.0678668847,-0.0274674211,0.0853595182,-0.0102465909,-0.0692079363,0.0943919662,-0.0171369879,0.0330352083,-0.0630801562,0.0203251136],"Abstract_Vector":[0.1449595109,0.0296192596,-0.0737480344,-0.0042339204,0.0295388805,0.0659171694,0.0249444935,-0.0038911683,-0.0192755214,-0.0729967227,-0.0416785875,-0.0666987442,-0.0097245821,0.0011783975,-0.0062016884,-0.040238786,-0.0059254138,-0.0586548214,0.0198537907,-0.0296075497,-0.0355810327,0.146237846,-0.0638047906,0.0702765395,0.1136096458,-0.0569683144,0.10325291,-0.0283332099,-0.0074290633,-0.1229930556,0.0160238021,0.0065963792,-0.1320649379,0.0365269724,-0.0053260746,-0.0802890411,0.0589389336,-0.0627943045,0.1151719632,-0.0799968874,-0.0360483503,0.0913768501,0.0222655998,-0.0812351074,-0.0209303004,0.022646347,-0.1137191988,0.071496657,-0.026435234,-0.0297158998,0.0148814892,-0.0148884032,-0.0168303927,0.1733489438,0.1011611384,0.1439506803,-0.0182902127,0.1191652689,0.021367717,-0.0341896135,0.0555965457,0.0737924122,0.0204050753,-0.0768729614,-0.1374164329,-0.0695915877,-0.1382754251,0.0085598769,-0.0257209758,0.010072723,0.0902247201,0.1009836784,0.1635815372,0.0483808912,-0.1026912852,-0.0280259557]},"156":{"Abstract":"Scatterplot matrices (SPLOMs) are widely used for exploring multidimensional data. Scatterplot diagnostics (scagnostics) approaches measure characteristics of scatterplots to automatically find potentially interesting plots, thereby making SPLOMs more scalable with the dimension count. While statistical measures such as regression lines can capture orientation, and graph-theoretic scagnostics measures can capture shape, there is no scatterplot characterization measure that uses both descriptors. Based on well-known results in shape analysis, we propose a scagnostics approach that captures both scatterplot shape and orientation using skeletons (or medial axes). Our representation can handle complex spatial distributions, helps discovery of principal trends in a multiscale way, scales visually well with the number of samples, is robust to noise, and is automatic and fast to compute. We define skeleton-based similarity metrics for the visual exploration and analysis of SPLOMs. We perform a user study to measure the human perception of scatterplot similarity and compare the outcome to our results as well as to graph-based scagnostics and other visual quality metrics. Our skeleton-based metrics outperform previously defined measures both in terms of closeness to perceptually-based similarity and computation time efficiency.","Authors":"J. Matute; A. C. Telea; L. Linsen","DOI":"10.1109\/TVCG.2017.2744339","Keywords":"Multidimensional Data (primary keyword);High-Dimensional Data","Keywords_Processed":"high dimensional data;multidimensional data primary keyword","Title":"Skeleton-Based Scagnostics","Labels":null,"Keyword_Vector":[0.0975622428,-0.079280636,0.0582221517,-0.0451708112,0.1694242822,-0.0310165256,0.0074039524,0.0164120526,0.0870773349,0.0307012114,0.1446555337,-0.180273887,0.0791092188,0.0354778006,-0.0435342184,0.4321035594,0.1973773696,0.0985693811,0.1331499422,-0.0115638388,-0.1638372955,0.0724768307,0.0237143825,-0.0602550993],"Abstract_Vector":[0.2137648444,-0.0295195363,-0.1235645555,-0.0179323994,-0.1080860773,0.0569656664,0.0028609753,0.028298409,0.1458899421,0.0629660323,-0.045232552,-0.0877889783,-0.0031983816,-0.0069647464,-0.0979157765,-0.1422532046,-0.1393616596,-0.0346273566,-0.1025062908,-0.0345757024,0.035543509,-0.0358901756,0.2719081432,-0.054850635,0.0599403427,-0.0011990293,-0.007923669,0.2149860437,-0.0059080148,0.0335245759,-0.0153626429,0.0605956914,0.0640422076,0.062997565,-0.0240956195,-0.1402041832,0.026756496,0.0243788352,-0.0072605304,-0.0320172547,0.1189837951,-0.1905844199,-0.0018151865,0.0845002614,-0.0143866822,-0.1206323183,0.0387520803,-0.0159259129,0.0617675942,0.0812956477,0.0651826935,-0.0490227644,-0.1327959886,-0.0932359588,0.0163394118,-0.0345139483,0.0273320653,0.1021221973,0.0235792429,-0.0052251152,0.0621225303,0.060462397,0.0540253733,0.0156448004,-0.0933265076,-0.0466275173,-0.0373749001,-0.099585287,-0.0605537932,0.0941692154,-0.0669165429,0.091095368,-0.0485558652,0.0117807735,0.0028170974,0.0109911341]},"157":{"Abstract":"Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and \u201cwhat if\u201d-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.","Authors":"H. Strobelt; S. Gehrmann; M. Behrisch; A. Perer; H. Pfister; A. M. Rush","DOI":"10.1109\/TVCG.2018.2865044","Keywords":"Explainable AI;Visual Debugging;Visual Analytics;Machine Learning;Deep Learning;NLP","Keywords_Processed":"visual debugging;visual analytic;deep learning;nlp;machine learning;explainable ai","Title":"Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models","Labels":null,"Keyword_Vector":[0.1630066956,-0.0450537872,0.4080738057,0.2441040252,-0.2427090231,0.0074130016,-0.2101364381,-0.0565013371,-0.0793173713,-0.0371854279,0.0635645096,0.0237540955,0.0530718662,0.0541425964,-0.169672548,-0.0389821113,0.1171057436,-0.0587401401,-0.1087711626,-0.0711461805,-0.0412640777,0.0686768506,-0.0971521582,-0.0955542504],"Abstract_Vector":[0.2105766904,0.001537428,-0.0393931564,-0.0991748771,0.2233709687,-0.1875502162,-0.2153073555,-0.0718878267,0.0093908583,0.1080519322,-0.0578059037,0.0622956987,0.0425720246,0.0218517982,0.0599587843,0.1572728927,-0.055091804,-0.050746853,0.057592785,0.0673429339,0.1091283326,-0.0176041269,0.0690779117,0.0678852938,-0.0140391573,-0.050927371,-0.0412729321,-0.0936378335,0.0249223943,-0.0481645419,0.0605899029,0.1185330213,0.0264580915,0.216107744,-0.0444521948,-0.1516204277,-0.0693566381,0.1547188269,-0.0086235228,-0.0730830089,-0.003510664,-0.0051906714,0.054508033,0.0287965602,0.0368604495,0.0450811985,-0.0652409053,0.0254489516,-0.0398237357,0.0309408658,0.0352081126,0.1038314452,0.0307398681,0.2011363513,-0.0364233101,-0.1289559046,-0.0372169614,0.067304372,-0.0423271654,-0.0311234525,-0.1272813497,0.0547053323,0.1434619512,-0.0760582333,0.0578023165,0.0224074166,-0.0767965102,0.006177945,0.021326628,0.0672145002,-0.0800579605,-0.0304750788,-0.0323514797,0.0128769088,0.0734015792,0.0345830219]},"158":{"Abstract":"We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.","Authors":"B. C. Kwon; M. Choi; J. T. Kim; E. Choi; Y. B. Kim; S. Kwon; J. Sun; J. Choo","DOI":"10.1109\/TVCG.2018.2865027","Keywords":"Interactive Artificial Intelligence;XAI (Explainable Artificial Intelligence);Interpretable Deep Learning;Healthcare","Keywords_Processed":"interpretable deep learning;interactive artificial intelligence;xai explainable artificial intelligence;healthcare","Title":"RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records","Labels":null,"Keyword_Vector":[0.0466337501,-0.0166036853,0.0883517646,0.0859051099,-0.081102131,-0.0125370601,-0.0389843158,-0.0524663663,-0.0590979622,-0.0005909107,0.0070681283,-0.0056793288,-0.0331321759,0.1645089016,-0.0714648823,-0.0486654193,0.0969060535,-0.045784608,-0.0998756122,-0.0379296027,-0.0165645622,0.0344016051,-0.053877781,-0.1229885202],"Abstract_Vector":[0.2360930632,-0.090625838,0.0617943497,0.0025546198,0.1392817075,-0.1068686032,-0.130856749,-0.0485838397,0.0010791553,0.0374461882,0.0113333068,0.0350731827,-0.0041717049,-0.0073780668,0.0136621145,0.0917952775,-0.0336966014,-0.0494050113,0.0655170272,0.0140467075,0.018440973,0.0978186414,0.040016016,-0.0266554335,0.0268480394,-0.0259308925,-0.1191594241,-0.073772151,-0.1018134741,-0.0340585686,-0.1019295929,0.0136810362,-0.0628209145,0.0690904661,0.0282301297,0.01312444,-0.0154259612,-0.0441860351,-0.0589515976,0.0962797724,0.0319599155,-0.0153898775,-0.0577586273,-0.0529007349,0.0001646311,-0.0067620258,0.136960165,-0.013661978,-0.0445519846,0.0634482451,-0.0615307516,-0.1528311829,-0.0450344964,-0.0163913967,-0.0615888257,-0.0258973679,-0.1063721726,0.072418609,-0.0287182728,-0.0085240709,-0.0013889238,-0.0426212039,-0.0918440395,-0.0270842632,0.0094265746,0.0112317397,-0.0875063734,-0.0851728694,-0.03092294,-0.0216069741,0.0469460149,0.1059111329,-0.1223253238,0.003678882,-0.073797022,-0.0066617945]},"159":{"Abstract":"Appropriate choice of colors significantly aids viewers in understanding the structures in multiclass scatterplots and becomes more important with a growing number of data points and groups. An appropriate color mapping is also an important parameter for the creation of an aesthetically pleasing scatterplot. Currently, users of visualization software routinely rely on color mappings that have been pre-defined by the software. A default color mapping, however, cannot ensure an optimal perceptual separability between groups, and sometimes may even lead to a misinterpretation of the data. In this paper, we present an effective approach for color assignment based on a set of given colors that is designed to optimize the perception of scatterplots. Our approach takes into account the spatial relationships, density, degree of overlap between point clusters, and also the background color. For this purpose, we use a genetic algorithm that is able to efficiently find good color assignments. We implemented an interactive color assignment system with three extensions of the basic method that incorporates top K suggestions, user-defined color subsets, and classes of interest for the optimization. To demonstrate the effectiveness of our assignment technique, we conducted a numerical study and a controlled user study to compare our approach with default color assignments; our findings were verified by two expert studies. The results show that our approach is able to support users in distinguishing cluster numbers faster and more precisely than default assignment methods.","Authors":"Y. Wang; X. Chen; T. Ge; C. Bao; M. Sedlmair; C. Fu; O. Deussen; B. Chen","DOI":"10.1109\/TVCG.2018.2864912","Keywords":"Color perception;visual design;scatterplots","Keywords_Processed":"visual design;scatterplot;color perception","Title":"Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots","Labels":null,"Keyword_Vector":[0.2212823199,0.0617623473,0.2259036634,-0.0402049844,-0.16239709,0.3414845695,0.1763235385,-0.0803548222,-0.1079878481,-0.1441315561,0.0891501883,0.0718949544,-0.0443447833,0.0390569947,0.0103619449,0.1586115785,-0.0395846417,0.0353283396,0.0753541813,-0.0985360451,-0.101940253,0.0335928776,-0.1134624233,0.0295260211],"Abstract_Vector":[0.2346330346,-0.0242760814,-0.0627387485,-0.0379996444,-0.1567045638,0.1998094551,-0.0107512827,0.1979650984,0.2004683176,0.0923027748,0.2336035495,-0.1837721121,0.0175242835,-0.0764476485,-0.0647050226,0.355179224,-0.0632802564,-0.1397760184,0.0011960278,0.0759994947,0.1341452405,0.0988814106,-0.0087296839,0.005147472,0.0273984058,0.073093469,-0.0100652273,0.0452379984,-0.0096362169,-0.05111324,-0.0228867242,0.078931479,0.0202537274,-0.0753651631,0.0032546247,-0.0493307171,0.0410709724,-0.0086244635,0.1125771871,0.0727665125,-0.0282420268,-0.0933057352,0.0226806461,0.0434176163,0.0593135739,0.0042287206,-0.0309631488,0.0267597686,0.0572753638,-0.0322006881,0.0034280221,0.044760736,-0.0224621395,-0.078905615,0.0423380583,0.0090461754,-0.0352046758,0.0282084246,0.0971705199,0.030443218,-0.0197288995,0.0466037188,-0.0496291506,0.0612006572,0.0101039916,-0.0467935387,-0.030311244,-0.0210730629,0.0517471134,-0.018954733,-0.0502858402,0.0008199794,-0.0114267061,0.0024615348,0.0047202591,0.010954528]},"160":{"Abstract":"RadViz and star coordinates are two of the most popular projection-based multivariate visualization techniques that arrange variables in radial layouts. Formally, the main difference between them consists of a nonlinear normalization step inherent in RadViz. In this paper we show that, although RadViz can be useful when analyzing sparse data, in general this design choice limits its applicability and introduces several drawbacks for exploratory data analysis. In particular, we observe that the normalization step introduces nonlinear distortions, can encumber outlier detection, prevents associating the plots with useful linear mappings, and impedes estimating original data attributes accurately. In addition, users have greater flexibility when choosing different layouts and views of the data in star coordinates. Therefore, we suggest that analysts and researchers should carefully consider whether RadViz's normalization step is beneficial regarding the data sets' characteristics and analysis tasks.","Authors":"M. Rubio-S\u00e1nchez; L. Raya; F. D\u00edaz; A. Sanchez","DOI":"10.1109\/TVCG.2015.2467324","Keywords":"RadViz;Star coordinates;Exploratory data analysis;Cluster analysis;Classication;Outlier detection;RadViz;Star coordinates;Exploratory data analysis;Cluster analysis;Classification;Outlier detection","Keywords_Processed":"star coordinate;outlier detection;classification;cluster analysis;exploratory data analysis;classication;radviz","Title":"A comparative study between RadViz and Star Coordinates","Labels":null,"Keyword_Vector":[0.195290115,-0.1648776696,0.2112063652,-0.2229947081,0.245872823,-0.0733973959,0.0570340071,0.0408862955,0.2083064379,0.0570606185,-0.0614438054,0.0379281091,0.2077343925,0.0342764745,0.0261201249,0.0463408275,0.0710988376,-0.0901568842,-0.0440741731,0.130876365,0.0730368553,-0.0084955277,-0.0780640569,-0.0742955917],"Abstract_Vector":[0.2088246419,-0.1344188131,-0.0146046165,-0.111696033,-0.1447181546,0.0514048827,-0.017017067,0.0722313171,0.0019562257,0.0212008078,-0.0047466218,0.0605134575,0.0171719123,-0.0427267333,-0.0766801386,-0.0672883941,-0.0047800581,0.0033977159,0.0702342562,-0.0256099643,0.0028971042,-0.0773535533,-0.1303792328,-0.0452524266,0.0817806896,0.0500694524,0.0277581622,-0.0937423481,0.0501238392,-0.017668013,-0.0319144823,-0.0240947744,-0.0560774155,0.1397414342,0.1044226878,0.0631115723,0.1413196096,-0.0074364643,0.026035071,-0.0975484078,0.0695365834,0.1296043848,0.0771054423,0.062805634,-0.0931767532,-0.0092371246,0.0289018981,0.0486963352,0.1086800009,0.0006428516,-0.0433148815,-0.0003091416,-0.0051300336,0.0812972353,-0.1063787552,-0.0614673305,0.1423470479,-0.0103197734,0.0220131021,0.0315203783,0.1111110982,-0.1100573267,-0.007627027,0.0293361478,0.0773119162,0.1110261236,-0.0662835025,-0.0319907613,0.0116073137,-0.0266035952,0.0347607622,0.0247364576,-0.0542534556,0.0056137785,0.042525674,0.0062641562]},"161":{"Abstract":"The inverse diffusion curve problem focuses on automatic creation of diffusion curve images that resemble user provided color fields. This problem is challenging since the 1D curves have a nonlinear and global impact on resulting color fields via a partial differential equation (PDE). We introduce a new approach complementary to previous methods by optimizing curve geometry. In particular, we propose a novel iterative algorithm based on the theory of shape derivatives. The resulting diffusion curves are clean and well-shaped, and the final image closely approximates the input. Our method provides a user-controlled parameter to regularize curve complexity, and generalizes to handle input color fields represented in a variety of formats.","Authors":"S. Zhao; F. Durand; C. Zheng","DOI":"10.1109\/TVCG.2017.2721400","Keywords":"Vector graphics;diffusion curves;inverse problem;shape optimization;Fr\u00e9chet derivative","Keywords_Processed":"diffusion curve;fr chet derivative;inverse problem;vector graphic;shape optimization","Title":"Inverse Diffusion Curves Using Shape Optimization","Labels":null,"Keyword_Vector":[0.0399789839,-0.0002431704,-0.0237888817,0.0492471886,0.0046939572,-0.0267661127,0.0950282946,-0.0606652089,-0.0686125793,-0.0070484382,0.0399981942,-0.04316703,-0.0558527076,0.0504889284,-0.0734280837,0.1239254733,0.0583494884,-0.1230893105,0.0138067997,0.0194164755,0.0101939275,-0.2227701519,0.0626814343,0.1336672535],"Abstract_Vector":[0.2051366285,0.1503380298,-0.1863773651,-0.0025303327,-0.0286914918,0.1602324825,0.0294923538,0.0259812467,0.0677502128,-0.0090303081,0.2576974901,-0.1301031751,-0.1072287469,-0.0147589953,0.0779439841,0.1310120739,-0.0486508216,-0.1571358674,-0.0321619799,-0.0023163137,0.136721191,-0.0578327502,0.0516944585,-0.0126840681,0.0493339613,0.1164676525,-0.0839513303,0.1112326595,-0.019882622,0.0645577866,-0.0285724523,0.0242578699,-0.1280468358,0.0730078996,0.0790329356,0.0258975355,0.0122724567,-0.120481411,0.0445730908,0.0726959825,-0.0829986763,0.0689624518,0.0700413446,0.0537050192,-0.0437819102,-0.1023426465,0.1555112977,-0.1099261312,0.0300984759,-0.0851015779,-0.0176737793,-0.0793049395,0.0665259929,0.109301455,0.0110131556,0.0225272345,0.0542445725,-0.0787660726,0.00122594,-0.0857721387,-0.0102870723,0.0064307662,-0.0025497241,-0.0354660268,-0.0091536034,0.0271247276,0.0337154017,-0.0325472869,0.037485953,-0.0429685551,-0.0131644102,0.0172906152,0.0136605586,-0.0003611057,-0.0329721793,0.0153910189]},"162":{"Abstract":"There currently exist two dominant strategies to reduce data sizes in analysis and visualization: reducing the precision of the data, e.g., through quantization, or reducing its resolution, e.g., by subsampling. Both have advantages and disadvantages and both face fundamental limits at which the reduced information ceases to be useful. The paper explores the additional gains that could be achieved by combining both strategies. In particular, we present a common framework that allows us to study the trade-off in reducing precision and\/or resolution in a principled manner. We represent data reduction schemes as progressive streams of bits and study how various bit orderings such as by resolution, by precision, etc., impact the resulting approximation error across a variety of data sets as well as analysis tasks. Furthermore, we compute streams that are optimized for different tasks to serve as lower bounds on the achievable error. Scientific data management systems can use the results presented in this paper as guidance on how to store and stream data to make efficient use of the limited storage and bandwidth in practice.","Authors":"D. Hoang; P. Klacansky; H. Bhatia; P. Bremer; P. Lindstrom; V. Pascucci","DOI":"10.1109\/TVCG.2018.2864853","Keywords":"data compression;bit ordering;multi-resolution;data analysis","Keywords_Processed":"datum analysis;multi resolution;bit ordering;datum compression","Title":"A Study of the Trade-off Between Reducing Precision and Reducing Resolution for Data Analysis and Visualization","Labels":null,"Keyword_Vector":[0.1658339759,-0.1161565793,0.1612341995,-0.1887599438,0.2846829874,0.0667926518,0.0061130876,0.0849897578,-0.2007222336,-0.0416283314,-0.0332692489,0.0458424157,-0.0930332856,0.012276893,-0.0032274493,-0.1668839934,0.0202066885,0.0353102002,-0.0613009912,-0.0145365337,0.0443600766,0.0536879978,0.1129811478,0.0509555927],"Abstract_Vector":[0.2510219827,-0.0910671761,0.004352796,-0.047442855,-0.0929361666,0.0743927258,-0.0256644696,0.0606473411,-0.091742803,-0.0194024913,-0.0552755708,0.0943142534,0.0129198664,-0.0247944029,0.0597131143,0.0171528341,-0.0784167738,0.085278469,0.0070218062,-0.0367174025,-0.062385372,-0.036873654,-0.0010784439,0.0208467161,0.0327259411,0.1019256692,-0.1025415186,-0.0395500686,-0.0020431532,-0.0180072002,0.1735839956,-0.0074689222,-0.0743783363,-0.0393501472,-0.0374424623,-0.0068940411,0.0999732071,-0.0156486444,0.0368819285,-0.0122997484,-0.0359739409,0.0139186277,-0.0385779665,-0.0336498349,0.0676168844,0.0754706985,0.1177404778,0.0320342891,-0.0985091073,0.0433650637,0.1407280588,0.0053305829,-0.0077105404,-0.0788479912,0.0927308955,-0.033726297,0.0124233189,-0.1701156689,-0.0539009505,0.0595699714,0.0242794872,0.0686821544,0.1486273403,-0.1061945182,0.0282516958,0.0955140517,-0.0582094045,-0.0584295109,0.0121358493,-0.1814740155,0.0464954525,-0.0994844251,-0.098661876,0.1067128828,-0.0180844329,0.0808174131]},"163":{"Abstract":"The heterogeneity and complexity of multivariate characteristics poses a unique challenge to visual exploration of multivariate scientific data sets, as it requires investigating the usually hidden associations between different variables and specific scalar values to understand the data's multi-faceted properties. In this paper, we present a novel association analysis method that guides visual exploration of scalar-level associations in the multivariate context. We model the directional interactions between scalars of different variables as information flows based on association rules. We introduce the concepts of informativeness and uniqueness to describe how information flows between scalars of different variables and how they are associated with each other in the multivariate domain. Based on scalar-level associations represented by a probabilistic association graph, we propose the Multi-Scalar Informativeness-Uniqueness (MSIU) algorithm to evaluate the informativeness and uniqueness of scalars. We present an exploration framework with multiple interactive views to explore the scalars of interest with confident associations in the multivariate spatial domain, and provide guidelines for visual exploration using our framework. We demonstrate the effectiveness and usefulness of our approach through case studies using three representative multivariate scientific data sets.","Authors":"X. Liu; H. Shen","DOI":"10.1109\/TVCG.2015.2467431","Keywords":"Multivariate data;association analysis;visual exploration;multiple views","Keywords_Processed":"multivariate datum;association analysis;multiple view;visual exploration","Title":"Association Analysis for Visual Exploration of Multivariate Scientific Data Sets","Labels":null,"Keyword_Vector":[0.2433492019,-0.1431511104,0.3034534823,-0.1000306815,0.2184985085,0.0389771681,0.001780646,0.0966968256,-0.100714947,0.0798123393,-0.1034793895,-0.0286999749,-0.0717960884,-0.0830901008,0.01258923,-0.1245255708,-0.0027572178,-0.0355481712,0.0098778879,0.1437451428,-0.1425291408,-0.1909486984,-0.0535153733,0.0700677091],"Abstract_Vector":[0.2122414706,-0.1075883795,-0.0853245885,-0.0640026785,-0.0033894434,-0.0054442984,0.1685268357,0.0441945917,0.0621135094,0.0413729404,-0.0098269591,0.0373888025,0.0982464547,0.0875366955,-0.0866351319,-0.0959031561,0.1247225448,-0.0701305401,0.0327378366,0.0794135367,0.1857935275,-0.0777171841,0.0245333693,0.0826454641,0.0052895588,0.0401190141,-0.0920208677,-0.0947590723,-0.0326448205,0.0634008961,0.21009691,-0.0491065881,0.031793283,0.0078610443,-0.0948555784,0.0519160594,-0.073547051,0.1104905328,-0.0823253516,0.0203691583,0.070083302,0.0022453178,0.0030180432,-0.113374995,-0.0637057923,0.1049352277,0.0300443875,0.1219459091,-0.0461517618,-0.0153879665,-0.0925290214,0.0867067591,-0.0140927986,0.021011311,0.0311013426,-0.0014243784,0.083998,-0.0566759641,0.0597400788,-0.1167335198,-0.0180302895,-0.1852339226,-0.1103443632,-0.0043769418,-0.0522745843,0.0346097498,-0.0249011385,-0.0329605042,-0.0547663735,0.0297305647,0.1293165841,-0.0389626812,-0.1202459013,0.0216768868,0.0070646113,-0.051823275]},"164":{"Abstract":"Parallel coordinates plots (PCPs) are a well-studied technique for exploring multi-attribute datasets. In many situations, users find them a flexible method to analyze and interact with data. Unfortunately, using PCPs becomes challenging as the number of data items grows large or multiple trends within the data mix in the visualization. The resulting overdraw can obscure important features. A number of modifications to PCPs have been proposed, including using color, opacity, smooth curves, frequency, density, and animation to mitigate this problem. However, these modified PCPs tend to have their own limitations in the kinds of relationships they emphasize. We propose a new data scalable design for representing and exploring data relationships in PCPs. The approach exploits the point\/line duality property of PCPs and a local linear assumption of data to extract and represent relationship summarizations. This approach simultaneously shows relationships in the data and the consistency of those relationships. Our approach supports various visualization tasks, including mixed linear and nonlinear pattern identification, noise detection, and outlier detection, all in large data. We demonstrate these tasks on multiple synthetic and real-world datasets.","Authors":"H. Nguyen; P. Rosen","DOI":"10.1109\/TVCG.2017.2661309","Keywords":"Correlation;parallel coordinates plot;large data visualization","Keywords_Processed":"parallel coordinate plot;correlation;large datum visualization","Title":"DSPCP: A Data Scalable Approach for Identifying Relationships in Parallel Coordinates","Labels":null,"Keyword_Vector":[0.2467766399,-0.1459993441,-0.0924789863,-0.1918837837,0.2076579039,0.0589603878,-0.0876257574,0.0895913699,-0.3178439859,0.0738385357,0.0583960044,0.0213953377,-0.0274332168,-0.1671740575,-0.1457532359,0.0854710918,0.0391867165,-0.0231959068,-0.0957351346,0.2763083825,0.0049420363,0.0316189267,-0.1182125605,-0.1026014439],"Abstract_Vector":[0.2704860994,-0.1165226744,-0.0805412852,-0.1062020842,-0.1245084192,0.0570959332,0.0552302654,0.0746948031,0.0196219175,0.0688172828,-0.0069670423,0.0340246393,0.0076992148,-0.073751227,0.0537224317,0.0170163506,0.0061400222,-0.0423364005,-0.0209458698,-0.0218384536,0.0135906245,-0.0497491866,-0.0273585855,-0.0165160733,0.0193222891,0.0220362417,0.0267078222,-0.0856748595,0.0217987139,-0.0244112244,-0.1448696519,-0.0409849577,-0.1057770182,0.0946625512,0.0235051274,-0.0171497581,-0.0035245989,0.0000843698,0.1162752803,-0.040025732,-0.1407030637,-0.0312949413,0.0870342635,0.0395167594,0.0410253938,-0.1020726114,0.0045721367,-0.0211996804,0.0145411145,-0.0744760119,-0.0232251753,-0.041046652,0.0842516315,0.0416625558,-0.0036313788,0.067429069,0.069634844,0.0339576657,0.0680746699,0.0991070145,0.106661292,0.0976402071,0.1043197547,-0.0082681303,-0.0744793092,0.0376478894,0.0300595301,0.0301890238,0.0322219821,-0.0460795662,0.0887233217,-0.1561290562,-0.1009509275,0.036257926,0.0537898375,0.0818477275]},"165":{"Abstract":"Many techniques facilitate real-time collision detection against complex models. These typically work by pre-computing information about the spatial distribution of geometry into a form that can be quickly queried. When models deform though, expensive pre-computations are impractical. We present radial fields: a variant of distance fields parameterised in cylindrical space, rather than Cartesian space. This 2D parameterisation significantly reduces the memory and computation requirements of the field, while introducing minimal overhead in collision detection tests. The interior of the mesh is defined implicitly for the entire domain. Importantly, it maps well to the hardware rasteriser of the GPU. Radial fields are much more application-specific than traditional distance fields. For these applications - such as collision detection with articulated characters-however, the benefits are substantial.","Authors":"S. Friston; A. Steed","DOI":"10.1109\/TVCG.2018.2859924","Keywords":"Distance fields;collision detection;deformable meshes;articulated characters","Keywords_Processed":"collision detection;deformable mesh;articulated character;distance field","Title":"Real-Time Collision Detection for Deformable Characters with Radial Fields","Labels":null,"Keyword_Vector":[0.0431118672,-0.0050822502,0.047520372,-0.0053396466,0.0284722863,-0.061100653,0.0913468865,-0.0626821927,0.025072918,0.0099267428,0.0108139486,0.0148939041,0.0689109839,-0.0982265176,-0.0023418614,0.0886724286,-0.0917986412,-0.1511183462,0.0228408049,-0.0435101891,-0.0261629746,0.0046294049,-0.0765619362,0.174177871],"Abstract_Vector":[0.1863676239,0.0840517902,-0.0695638366,0.0222274662,0.0301743282,0.0117724497,0.0742182532,-0.0846215753,0.0076190514,-0.0959260743,0.0068222842,0.0794824393,0.0508784562,0.056032406,0.0622325487,-0.0124382668,-0.0830132828,-0.0493534351,0.1774862161,-0.0204315328,-0.0546888392,0.042453893,-0.1387748018,0.0598808937,0.1092903716,0.0966101295,0.0675912101,0.0366064433,-0.0590183187,0.0998742011,0.0656176918,0.0373276421,-0.0166200394,0.1596523036,0.0343580562,0.0462974683,0.093567678,0.110205729,0.0642712257,0.1193940213,-0.0131568176,-0.0773829758,0.0923351775,0.0088402939,0.0754221902,-0.1657196306,0.0097845129,-0.076670209,-0.0195367236,0.0419629211,-0.0660383658,0.062528647,-0.0382682817,-0.0351508972,-0.0664286994,0.0315710685,0.186673778,0.0942176017,-0.1021518008,0.0697675552,0.0567707188,0.006839727,0.0273345222,0.0461951138,0.0728667812,-0.018742084,-0.1008915372,-0.067031718,-0.1348628897,0.035184168,-0.0658501757,0.0437694645,0.0831482348,-0.0455779674,-0.0735783264,-0.080528898]},"166":{"Abstract":"We show how mouse interaction log classification can help visualization toolsmiths understand how their tools are used \u201cin the wild\u201d through an evaluation of MAGI - a cancer genomics visualization tool. Our primary contribution is an evaluation of twelve visual analysis task classifiers, which compares predictions to task inferences made by pairs of genomics and visualization experts. Our evaluation uses common classifiers that are accessible to most visualization evaluators: k-nearest neighbors, linear support vector machines, and random forests. By comparing classifier predictions to visual analysis task inferences made by experts, we show that simple automated task classification can have up to 73 percent accuracy and can separate meaningful logs from \u201cjunk\u201d logs with up to 91 percent accuracy. Our second contribution is an exploration of common MAGI interaction trends using classification predictions, which expands current knowledge about ecological cancer genomics visualization tasks. Our third contribution is a discussion of how automated task classification can inform iterative tool design. These contributions suggest that mouse interaction log analysis is a viable method for (1) evaluating task requirements of client-side-focused tools, (2) allowing researchers to study experts on larger scales than is typically possible with in-lab observation, and (3) highlighting potential tool evaluation bias.","Authors":"C. C. Gramazio; J. Huang; D. H. Laidlaw","DOI":"10.1109\/TVCG.2017.2734659","Keywords":"Classification;task analysis;visual analysis;biology visualization;visualization;cancer genomics","Keywords_Processed":"visual analysis;biology visualization;task analysis;visualization;cancer genomic;classification","Title":"An Analysis of Automated Visual Analysis Classification: Interactive Visualization Task Inference of Cancer Genomics Domain Experts","Labels":null,"Keyword_Vector":[0.3451478895,-0.1794279342,0.1543941727,-0.1370268918,0.0377360761,-0.0019408011,0.0627176914,0.0210505101,0.1949358728,-0.0083513521,-0.1069812558,0.0122746933,0.1225151275,0.0485281861,0.0112385769,-0.1321091429,-0.0249620777,-0.1347819128,-0.0132775743,0.0161824194,0.0713814856,-0.0055604989,0.0292220779,-0.0524456672],"Abstract_Vector":[0.2112400709,-0.1442533395,0.1071493998,0.0401686718,0.1373609999,0.0265505857,-0.0701318931,-0.0135287283,-0.044562312,-0.0099085686,-0.0698385933,-0.0914028669,0.0294076537,-0.0554110492,-0.0308470669,0.0201272522,-0.0287441665,0.0573353489,0.0554945775,0.0881745454,0.0197428233,0.0444260428,0.0586617502,-0.0213811145,0.029046709,-0.0343591883,-0.1241869287,0.0160894828,0.0696547713,-0.0658614474,-0.0751877257,-0.0383921232,-0.0105366449,0.1386316861,0.0307670111,-0.0914618411,0.0798229638,-0.0951197506,-0.038960466,-0.0440153995,-0.0476649056,0.1117481559,0.0320394713,-0.1806967234,-0.1750177797,0.0145116782,0.031721917,-0.0935813548,0.0746059739,0.059579747,-0.0911932938,-0.0449825476,-0.0853637249,-0.080708815,-0.0528395206,0.0117428529,-0.0853864609,-0.0238644388,0.0213303171,0.0891932921,-0.1095823466,-0.0380263413,0.0834916545,-0.1075083877,0.16067362,-0.0461368708,0.0721472985,-0.0318166418,0.0240154997,0.0276581259,-0.0349430743,-0.0915293103,-0.0257248444,0.0769277047,-0.1043202927,-0.0886388207]},"167":{"Abstract":"Comparing multiple variables to select those that effectively characterize complex entities is important in a wide variety of domains - geodemographics for example. Identifying variables that correlate is a common practice to remove redundancy, but correlation varies across space, with scale and over time, and the frequently used global statistics hide potentially important differentiating local variation. For more comprehensive and robust insights into multivariate relations, these local correlations need to be assessed through various means of defining locality. We explore the geography of this issue, and use novel interactive visualization to identify interdependencies in multivariate data sets to support geographically informed multivariate analysis. We offer terminology for considering scale and locality, visual techniques for establishing the effects of scale on correlation and a theoretical framework through which variation in geographic correlation with scale and locality are addressed explicitly. Prototype software demonstrates how these contributions act together. These techniques enable multiple variables and their geographic characteristics to be considered concurrently as we extend visual parameter space analysis (vPSA) to the spatial domain. We find variable correlations to be sensitive to scale and geography to varying degrees in the context of energy-based geodemographics. This sensitivity depends upon the calculation of locality as well as the geographical and statistical structure of the variable.","Authors":"S. Goodwin; J. Dykes; A. Slingsby; C. Turkay","DOI":"10.1109\/TVCG.2015.2467199","Keywords":"Scale;Geography;Multivariate;Sensitivity Analysis;Variable Selection;Local Statistics;Geodemographics;Energy;Scale;Geography;Multivariate;Sensitivity Analysis;Variable Selection;Local Statistics;Geodemographics;Energy","Keywords_Processed":"scale;geography;variable selection;sensitivity analysis;geodemographic;local statistics;multivariate;energy","Title":"Visualizing Multiple Variables Across Scale and Geography","Labels":null,"Keyword_Vector":[0.0762310925,-0.0446654983,0.0725978686,-0.0639970831,0.1187729916,-0.0084521186,0.0376777255,0.0539398302,-0.0029932592,0.0449608728,-0.0410598021,0.0381995721,0.0345676295,-0.0083980997,-0.0407606124,-0.0103685289,0.0205241656,-0.0005400408,-0.0048413008,0.0711037484,0.0476440711,-0.0514370412,-0.0027816713,0.049684415],"Abstract_Vector":[0.2307708752,-0.1158092229,-0.1073397439,-0.0082909897,-0.0312192519,0.0273404345,0.1655636498,-0.0015582883,0.0899870632,0.1656291387,-0.127587862,-0.0537368993,0.1602290604,0.1406315879,-0.2142444294,-0.1042358593,0.024557159,-0.1477393737,0.0686214357,0.0638069298,-0.0528767409,-0.0111475662,0.0696931567,0.1451218865,-0.0130447682,-0.0043985841,-0.0095270481,-0.0449783797,-0.0461884643,-0.0032575705,0.1169147575,-0.0396070009,-0.0064262714,0.0204031273,0.0444231457,0.0915482816,0.0763997932,-0.0122792952,-0.1210713168,0.0850965198,0.0207840445,0.020871964,-0.1322044055,0.0861103039,-0.0169110315,0.0844512615,0.0225192783,0.0619312122,0.0097320866,-0.0083869475,-0.0250441306,-0.023023416,0.0747860919,0.0535277591,-0.0124236526,-0.0603131843,0.0586882559,-0.0997954986,0.0607456386,-0.0176305563,-0.0264228991,-0.0273311061,-0.0278848313,-0.06023473,0.1377480867,-0.0022913016,0.0071640567,0.0242413882,0.0160014892,-0.0177921354,-0.0021768968,-0.0886037407,0.1155051525,0.0685575984,0.017870932,-0.0629414264]},"168":{"Abstract":"In this paper, we present story curves, a visualization technique for exploring and communicating nonlinear narratives in movies. A nonlinear narrative is a storytelling device that portrays events of a story out of chronological order, e.g., in reverse order or going back and forth between past and future events. Many acclaimed movies employ unique narrative patterns which in turn have inspired other movies and contributed to the broader analysis of narrative patterns in movies. However, understanding and communicating nonlinear narratives is a difficult task due to complex temporal disruptions in the order of events as well as no explicit records specifying the actual temporal order of the underlying story. Story curves visualize the nonlinear narrative of a movie by showing the order in which events are told in the movie and comparing them to their actual chronological order, resulting in possibly meandering visual patterns in the curve. We also present Story Explorer, an interactive tool that visualizes a story curve together with complementary information such as characters and settings. Story Explorer further provides a script curation interface that allows users to specify the chronological order of events in movies. We used Story Explorer to analyze 10 popular nonlinear movies and describe the spectrum of narrative patterns that we discovered, including some novel patterns not previously described in the literature. Feedback from experts highlights potential use cases in screenplay writing and analysis, education and film production. A controlled user study shows that users with no expertise are able to understand visual patterns of nonlinear narratives using story curves.","Authors":"N. W. Kim; B. Bach; H. Im; S. Schriber; M. Gross; H. Pfister","DOI":"10.1109\/TVCG.2017.2744118","Keywords":"Nonlinear narrative;storytelling;visualization","Keywords_Processed":"visualization;storytell;nonlinear narrative","Title":"Visualizing Nonlinear Narratives with Story Curves","Labels":null,"Keyword_Vector":[0.1339422948,-0.0451694606,-0.119760073,0.0028180433,-0.0517297714,-0.0141597047,-0.0057125267,-0.0286181296,-0.0041263498,0.0358496033,-0.0180724047,-0.0519871123,0.0342960052,0.0122457923,-0.0104854157,0.0178293173,-0.0466788957,-0.0018466117,-0.0232717495,-0.0194150448,0.0051267939,0.0910903529,-0.0018198798,-0.0629535579],"Abstract_Vector":[0.1459098888,-0.0567974236,-0.0076770818,-0.0491062719,-0.024403828,-0.0510315802,0.0228515694,-0.0598744905,-0.0387449482,0.0313535463,0.0895137845,0.0340474673,-0.0916189843,-0.005513752,0.1317219133,0.1177421056,0.0332778495,-0.0756737741,0.0527557902,-0.0009797791,0.0420419798,-0.080311862,0.1655742259,-0.0045974432,0.02346932,0.0024985983,0.0532190379,-0.1344590613,-0.0913956497,0.0360089718,-0.1163469793,0.0915318389,-0.1645969947,0.1243620654,0.100939157,0.024105977,0.0872796377,-0.1454801697,0.0524119655,0.0175389359,0.1091220403,0.0742131513,-0.0017655199,0.0514662728,-0.1250449787,-0.1757852242,0.0962747555,-0.1034141401,-0.0390707923,0.0401867612,-0.0476310486,-0.0476548565,-0.0303623569,0.0891922632,-0.0291135205,0.0942406771,0.0324530997,-0.0265353471,0.0224987308,-0.1466633533,0.0536086504,0.0428363333,-0.1113027023,-0.0089985425,-0.0643914718,-0.0087097544,0.1082310587,-0.0628305545,-0.0916848633,0.0189370489,0.0883100533,0.0673506419,0.0208384657,0.069001182,0.0278514082,-0.1385155627]},"169":{"Abstract":"Identification of early signs of rotating stall is essential for the study of turbine engine stability. With recent advancements of high performance computing, high-resolution unsteady flow fields allow in depth exploration of rotating stall and its possible causes. Performing stall analysis, however, involves significant effort to process large amounts of simulation data, especially when investigating abnormalities across many time steps. In order to assist scientists during the exploration process, we present a visual analytics framework to identify suspected spatiotemporal regions through a comparative visualization so that scientists are able to focus on relevant data in more detail. To achieve this, we propose efficient stall analysis algorithms derived from domain knowledge and convey the analysis results through juxtaposed interactive plots. Using our integrated visualization system, scientists can visually investigate the detected regions for potential stall initiation and further explore these regions to enhance the understanding of this phenomenon. Positive feedback from scientists demonstrate the efficacy of our system in analyzing rotating stall.","Authors":"C. Chen; S. Dutta; X. Liu; G. Heinlein; H. Shen; J. Chen","DOI":"10.1109\/TVCG.2015.2467952","Keywords":"Turbine flow visualization;vortex extraction;anomaly detection;juxtaposition;brushing and linking;time series;Turbine flow visualization;vortex extraction;anomaly detection;juxtaposition;brushing and linking;time series","Keywords_Processed":"juxtaposition;turbine flow visualization;time series;anomaly detection;vortex extraction;brush and link","Title":"Visualization and Analysis of Rotating Stall for Transonic Jet Engine Simulation","Labels":null,"Keyword_Vector":[0.1833566307,-0.0814713857,0.0609981659,-0.0516146148,-0.059722182,-0.1780837432,0.083832907,-0.0699976809,0.1968042007,0.2009172736,0.1726373563,0.1588846622,-0.1355555362,-0.1851268937,0.1337330996,0.0780357435,0.00638416,-0.0474749787,0.0384951219,-0.0061086709,0.029485906,0.0895005647,-0.0638353325,0.1073444438],"Abstract_Vector":[0.226274119,-0.0841101186,-0.0279768753,0.0479879829,-0.0195952674,-0.0688863036,0.0128620905,-0.0551286177,-0.1169613734,0.0201055831,-0.00068129,0.014748237,0.0407998471,0.0303363269,0.028492569,0.0177111019,-0.0372672684,0.0020525349,0.0850485247,-0.0215172227,-0.0325387575,0.0158358648,-0.0496018725,0.017270634,-0.0224595257,0.0482990986,-0.0666102573,0.0786336163,0.0294126708,0.0080209738,-0.0642171691,-0.1227843942,-0.0460177872,-0.0461235699,-0.032257278,-0.0245874006,0.0332171894,0.0815514148,-0.0926419792,-0.0193380026,-0.0751009615,-0.051165018,0.0417769606,-0.0425544649,-0.0136034908,-0.0239921064,0.1369511825,-0.0131089606,-0.0725967019,0.0615037728,0.0638210569,-0.0860732779,0.0447816289,-0.0674663788,-0.0444838801,-0.0783749453,0.0590586745,0.1109597939,-0.0843902315,-0.0488374956,-0.0551877832,-0.0119299581,-0.0846850466,0.0561996081,-0.1310565433,0.108325263,0.0228763273,0.0494867499,0.026208823,-0.0999818117,0.0687153061,0.0288396724,-0.1423169525,-0.0093033966,0.0358231245,-0.065441036]},"170":{"Abstract":"Traditional fisheye views for exploring large graphs introduce substantial distortions that often lead to a decreased readability of paths and other interesting structures. To overcome these problems, we propose a framework for structure-aware fisheye views. Using edge orientations as constraints for graph layout optimization allows us not only to reduce spatial and temporal distortions during fisheye zooms, but also to improve the readability of the graph structure. Furthermore, the framework enables us to optimize fisheye lenses towards specific tasks and design a family of new lenses: polyfocal, cluster, and path lenses. A GPU implementation lets us process large graphs with up to 15,000 nodes at interactive rates. A comprehensive evaluation, a user study, and two case studies demonstrate that our structure-aware fisheye views improve layout readability and user performance.","Authors":"Y. Wang; Y. Wang; H. Zhang; Y. Sun; C. Fu; M. Sedlmair; B. Chen; O. Deussen","DOI":"10.1109\/TVCG.2018.2864911","Keywords":"Graph Visualization;Focus-Context Technique;Structure-aware Zoom;Graph Layout Technique","Keywords_Processed":"focus context technique;graph visualization;structure aware zoom;graph layout technique","Title":"Structure-aware Fisheye Views for Efficient Large Graph Exploration","Labels":null,"Keyword_Vector":[0.1639753362,-0.0182960549,-0.0940921195,0.0558333608,-0.1335298235,-0.054956581,0.1378035517,0.1000229409,-0.0704646691,0.3340501362,-0.2091827007,-0.0290427804,-0.0905705358,-0.077615138,-0.0033961001,-0.0385975042,0.0575746321,0.1310911802,0.0795055586,-0.0730560921,-0.0587565099,-0.0281358942,0.0411867418,-0.1245575359],"Abstract_Vector":[0.2123634175,-0.0248551057,0.0264469825,0.0907568349,0.1142651575,-0.163803082,0.1750691375,0.1760020196,0.1869331004,-0.0973481334,0.032298405,0.0063166593,-0.0777079919,-0.123367896,-0.0290179868,-0.0311786453,-0.064762961,0.0606967807,-0.0137929657,-0.1895444039,0.0260646269,-0.0825956126,-0.0745899584,-0.0151683771,0.1552055214,-0.0473405505,0.0755218287,-0.187743352,0.1355836585,0.08347982,0.0396505068,0.137819389,0.0109286675,-0.0083283071,0.0346589672,-0.1111450545,0.1096639062,-0.0730712076,-0.191711337,-0.0302783398,0.0553254005,-0.0068159179,0.0846343576,0.1867949089,0.0307938224,0.2292330214,0.0277982906,-0.0671482345,-0.0172691877,-0.049719196,0.0878529715,-0.0935627548,0.0178202732,-0.1394852868,-0.0180909908,-0.0374280515,-0.0343190074,0.005476232,-0.0362016747,-0.0245765825,-0.0301713146,-0.0164686279,0.0101310699,0.0484259135,-0.0010140921,0.0113272651,0.0130616465,-0.1380697021,-0.0206346343,-0.0979946607,-0.0236340471,0.0515800256,0.03867052,0.0247486659,-0.0044446474,-0.0217002427]},"171":{"Abstract":"Most mountain ranges are formed by the compression and folding of colliding tectonic plates. Subduction of one plate causes large-scale asymmetry while their layered composition (or stratigraphy) explains the multi-scale folded strata observed on real terrains. We introduce a novel interactive modeling technique to generate visually plausible, large scale terrains that capture these phenomena. Our method draws on both geological knowledge for consistency and on sculpting systems for user interaction. The user is provided hands-on control on the shape and motion of tectonic plates, represented using a new geologically-inspired model for the Earth crust. The model captures their volume preserving and complex folding behaviors under collision, causing mountains to grow. It generates a volumetric uplift map representing the growth rate of subsurface layers. Erosion and uplift movement are jointly simulated to generate the terrain. The stratigraphy allows us to render folded strata on eroded cliffs. We validated the usability of our sculpting interface through a user study, and compare the visual consistency of the earth crust model with geological simulation results and real terrains.","Authors":"G. Cordonnier; M. Cani; B. Benes; J. Braun; E. Galin","DOI":"10.1109\/TVCG.2017.2689022","Keywords":"Terrains;mountains;interactive design;geology","Keywords_Processed":"terrain;geology;interactive design;mountain","Title":"Sculpting Mountains: Interactive Terrain Modeling Based on Subsurface Geology","Labels":null,"Keyword_Vector":[0.0910301304,-0.0271423718,-0.0039267343,-0.0052098574,-0.0825000747,0.0930578478,0.2693454015,-0.018631624,0.0384817517,-0.1210866257,-0.0489527056,0.0185644484,-0.1357645239,0.2138819491,-0.0750111988,0.0138159585,-0.0306326501,-0.0311817495,-0.004374839,-0.0016957945,-0.0704289007,0.0085868575,0.0669877147,-0.0879313249],"Abstract_Vector":[0.1780341656,0.0732684943,-0.0172636407,-0.0711295926,0.0070072946,-0.0685212461,-0.0295264969,0.0019248534,0.011170802,-0.0536282302,0.0099151607,-0.0072151534,-0.0184183027,0.0949271244,-0.0558600925,0.0455769666,-0.0500617241,-0.023518224,-0.0084186454,-0.096436467,-0.0090948966,-0.0246222934,-0.0085711068,-0.024076296,-0.075163466,-0.0637114288,-0.0027412147,0.0141415394,-0.0361836994,0.0551592129,0.0384903288,-0.0937567368,0.0756926586,0.061810238,0.0568412364,-0.0822314104,-0.0426007397,0.0441087749,0.0218254294,-0.0139932815,-0.028126482,-0.0564185738,0.0165975398,0.0559059413,0.0273662194,-0.0875007051,-0.0176143025,0.0127988886,0.0892302138,0.1193697325,0.0133442609,-0.162412751,0.0017158041,0.0653145571,0.0415982994,0.077328012,0.0243055398,-0.0298921958,-0.0646215518,0.0193313781,0.1106265922,0.0824368928,-0.0721934289,-0.1169052694,-0.0079839415,-0.0868366095,0.1891242442,-0.0562070733,-0.1321578407,0.0925430014,0.2508294809,-0.1378708511,0.1019256239,0.0747603886,0.0575883598,0.035027861]},"172":{"Abstract":"Applied visualization researchers often work closely with domain collaborators to explore new and useful applications of visualization. The early stages of collaborations are typically time consuming for all stakeholders as researchers piece together an understanding of domain challenges from disparate discussions and meetings. A number of recent projects, however, report on the use of creative visualization-opportunities (CVO) workshops to accelerate the early stages of applied work, eliciting a wealth of requirements in a few days of focused work. Yet, there is no established guidance for how to use such workshops effectively. In this paper, we present the results of a 2-year collaboration in which we analyzed the use of 17 workshops in 10 visualization contexts. Its primary contribution is a framework for CVO workshops that: 1) identifies a process model for using workshops; 2) describes a structure of what happens within effective workshops; 3) recommends 25 actionable guidelines for future workshops; and 4) presents an example workshop and workshop methods. The creation of this framework exemplifies the use of critical reflection to learn about visualization in practice from diverse studies and experience.","Authors":"E. Kerzner; S. Goodwin; J. Dykes; S. Jones; M. Meyer","DOI":"10.1109\/TVCG.2018.2865241","Keywords":"User-centered visualization design;design studies;creativity workshops;critically reflective practice","Keywords_Processed":"creativity workshop;design study;user center visualization design;critically reflective practice","Title":"A Framework for Creative Visualization-Opportunities Workshops","Labels":null,"Keyword_Vector":[0.1932226605,0.0079410091,-0.0721223591,-0.0324759793,-0.1743450874,0.2102710202,0.3073298539,0.1569784438,0.0729050368,-0.2598209137,-0.0115021273,0.1226018333,-0.0063934577,-0.0083278558,-0.0223505282,0.0525651206,0.1012198779,0.0822950595,-0.0765942754,0.0101074729,-0.0883414273,0.0225789614,-0.0098128976,-0.0172093376],"Abstract_Vector":[0.1501566276,-0.0671774725,0.0557610463,-0.0390706129,0.1278720744,0.0205033,-0.0298212261,-0.0311813725,-0.0511314891,-0.017701017,-0.0294272308,-0.0631618145,-0.0038973017,-0.0008291614,-0.027318762,0.0360251249,0.0300235624,-0.0040842273,-0.0019594542,0.066506088,-0.0258364765,0.0469655739,0.0225132568,0.0135568485,-0.0051081183,0.0108184608,0.0069012126,-0.0199046131,0.0112767345,0.0510964908,0.0295849833,-0.0123118556,-0.0037245728,0.0342969151,0.0055036317,0.0092654888,0.0444473349,-0.0849102977,0.0367397547,0.0618590924,-0.1004843243,0.0094338067,-0.0589199604,0.0714963523,0.005073965,0.0161376635,0.0286199779,-0.1196946597,-0.0522611516,-0.008195444,0.1024124414,0.068871016,-0.1184278295,0.0294348241,0.0230463232,-0.0217445534,0.1938822951,0.0627526875,-0.0137073847,0.0834789143,-0.163187366,-0.159832924,0.17584675,-0.012000685,-0.0473613523,0.0035222373,-0.0155771949,-0.0180988324,-0.1008525734,-0.0569056487,-0.0614215349,-0.0504065065,-0.0376481589,0.0590847857,0.0724768797,0.0134133224]},"173":{"Abstract":"In this paper, we propose a novel machine learning-based voxel classification method for highly-accurate volume rendering. Unlike conventional voxel classification methods that incorporate intensity-based features, the proposed method employs dictionary based features learned directly from the input data using hierarchical multi-scale 3D convolutional sparse coding, a novel extension of the state-of-the-art learning-based sparse feature representation method. The proposed approach automatically generates high-dimensional feature vectors in up to 75 dimensions, which are then fed into an intelligent system built on a random forest classifier for accurately classifying voxels from only a handful of selection scribbles made directly on the input data by the user. We apply the probabilistic transfer function to further customize and refine the rendered result. The proposed method is more intuitive to use and more robust to noise in comparison with conventional intensity-based classification methods. We evaluate the proposed method using several synthetic and real-world volume datasets, and demonstrate the methods usability through a user study.","Authors":"T. M. Quan; J. Choi; H. Jeong; W. Jeong","DOI":"10.1109\/TVCG.2017.2744078","Keywords":"Volume Rendering;Machine Learning;Hierarchically Convolutional Sparse Coding","Keywords_Processed":"machine learning;volume rendering;hierarchically convolutional sparse coding","Title":"An Intelligent System Approach for Probabilistic Volume Rendering Using Hierarchical 3D Convolutional Sparse Coding","Labels":null,"Keyword_Vector":[0.0925116157,-0.0282384564,0.0530097198,0.377478106,0.1142359925,0.1999729417,-0.114919631,-0.1034280593,0.0695914115,0.0267018378,0.0928545782,0.0767141666,0.0400367464,0.0501665022,-0.063890332,-0.0947273858,0.1797056235,-0.055466711,-0.1409107077,-0.0326078106,-0.0005445586,-0.0012188079,-0.1113103969,-0.0725002086],"Abstract_Vector":[0.2734441518,0.0833354989,-0.1672582254,0.0011335539,-0.0066613696,-0.0341445595,-0.03532649,0.1306616506,-0.034375544,-0.0705666762,-0.1057302781,-0.1090943385,0.168240801,-0.0492442983,0.0854734184,-0.0382290203,0.0347995083,0.0494511875,-0.082002032,-0.0461303884,0.1301238713,0.0809306265,0.1286338148,-0.0181803026,-0.0179797399,-0.0915400606,-0.0523571331,-0.0865574191,0.0971408806,-0.0540941289,-0.1800045905,0.0385223438,0.2077225139,0.0685992207,-0.0450554644,-0.0551968402,-0.0498036032,-0.0818094392,0.0929869281,-0.0881447728,-0.0834010828,0.0467542993,-0.042301623,-0.1435937451,-0.0723994215,-0.0351248226,0.0382761069,-0.0708760987,0.0780800695,-0.0479752687,-0.0147043722,0.0604650903,-0.0193247145,-0.0403352419,0.0210822517,-0.01108185,-0.1024577671,-0.0530501049,-0.04890935,0.1006401234,-0.0000136012,-0.022194052,0.0014926941,-0.015789056,0.1167243399,-0.0618658953,-0.0193786454,-0.0152238601,-0.0417597094,-0.0637134976,0.0200447187,-0.0300210314,0.0538148869,-0.0599451748,-0.0202762783,0.0093802154]},"174":{"Abstract":"We propose to generate novel animations from a set of elementary examples of video-based surface motion capture, under user-specified constraints. 4D surface capture animation is motivated by the increasing demand from media production for highly realistic 3D content. To this aim, data driven strategies that consider video-based information can produce animation with real shapes, kinematics and appearances. Our animations rely on the combination and the interpolation of textured 3D mesh data, which requires examining two aspects: (1) Shape geometry and (2) appearance. First, we propose an animation synthesis structure for the shape geometry, the Essential graph, that outperforms standard Motion graphs in optimality with respect to quantitative criteria, and we extend optimized interpolated transition algorithms to mesh data. Second, we propose a compact view-independent representation for the shape appearance. This representation encodes subject appearance changes due to viewpoint and illumination, and due to inaccuracies in geometric modelling independently. Besides providing compact representations, such decompositions allow for additional applications such as interpolation for animation.","Authors":"A. Boukhayma; E. Boyer","DOI":"10.1109\/TVCG.2018.2831233","Keywords":"Character animation;3D video;multiview reconstruction;video-based animation;4D modeling;4D performance capture","Keywords_Processed":"character animation;3d video;4d modeling;video base animation;4d performance capture;multiview reconstruction","Title":"Surface Motion Capture Animation Synthesis","Labels":null,"Keyword_Vector":[0.0657252731,0.0650386177,-0.0023572001,0.162615103,0.1285380852,-0.1740088905,0.2041421266,-0.1838000681,-0.2043202455,-0.1995873755,-0.0494836002,-0.1161163755,0.1400690036,-0.1641378833,0.0867776174,-0.002518885,0.1001997904,-0.1757443173,0.0058988017,-0.0093312569,0.0836350194,0.0298055578,0.1293091765,-0.004251277],"Abstract_Vector":[0.2602267129,0.1966108039,-0.1484376058,-0.1381356987,-0.1046649625,-0.0172331493,-0.0192360571,-0.1142125679,0.0029040641,-0.1561532479,0.0212407878,0.0487779678,-0.2255819084,-0.1721337558,-0.1449800513,-0.1651766556,-0.033294899,-0.0182025917,-0.1067646435,-0.0495897678,-0.082212734,-0.0020698467,0.0937757227,0.1755891292,0.0465634683,0.0265297763,0.1084262,-0.0675567269,-0.053693534,-0.1167556251,-0.0743334122,0.0017714305,0.0035552024,-0.0930826794,-0.1098946214,-0.1308909719,-0.0417492344,-0.0094399512,-0.0262791333,0.0634077853,-0.0347193653,0.0935747723,-0.0089951057,-0.0740615342,-0.0281613977,0.0056388261,-0.0276203138,0.0339798697,-0.0194279392,-0.0735120483,-0.0367785361,-0.0648818585,-0.0048305633,-0.0041712658,0.0575076812,-0.0874370341,0.0004353141,0.0088648777,-0.0355808584,0.0162556967,-0.0466848166,-0.0125426286,-0.0705485115,-0.0211059614,-0.0038931159,-0.0110513207,-0.0046257515,-0.0948694257,0.0376487045,0.0675810594,0.0007497893,0.0272075556,0.0504644049,0.0736142959,0.0412907802,0.0429250874]},"175":{"Abstract":"Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clusters. In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy clusters analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. First, we define the analytical tasks and their representative questions specific to fuzzy clusters analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of the volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters.","Authors":"Y. Zhao; F. Luo; M. Chen; Y. Wang; J. Xia; F. Zhou; Y. Wang; Y. Chen; W. Chen","DOI":"10.1109\/TVCG.2018.2865020","Keywords":"Evaluation;multi-dimensional visualization;fuzzy clustering;parallel coordinate plot;scatterplot matrix;principal component analysis;radviz","Keywords_Processed":"principal component analysis;scatterplot matrix;evaluation;parallel coordinate plot;fuzzy clustering;multi dimensional visualization;radviz","Title":"Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy Clusters","Labels":null,"Keyword_Vector":[0.2273855178,-0.1165787333,0.0389270576,-0.1345070112,0.1264462826,0.0903785569,0.0511841663,0.0514986632,-0.1242224814,0.0515647646,-0.0473475357,0.0311244093,0.1234797514,-0.0663301842,0.0260861448,0.0873297849,0.0977597159,0.0291623856,0.0546787624,0.1344880448,-0.0624711973,0.0345320616,-0.1106409648,-0.1003927645],"Abstract_Vector":[0.2415640381,-0.1582943218,-0.0002902818,0.1676187283,-0.1427566958,-0.0461667245,-0.1216019087,0.0553272476,-0.006387405,0.0419189209,0.0134561509,-0.2127227747,0.0063888786,-0.0427734352,-0.0412183715,0.0212151718,0.0217402819,0.2210856173,0.1108360089,0.1415792794,0.0450605597,-0.0959513634,0.0356017887,-0.002898872,0.0392897216,-0.0389448563,0.0543407821,0.0594865445,-0.0125543531,0.0642587679,0.0113479606,-0.0644335853,-0.0371864684,0.0284553069,-0.0417466266,-0.0002488785,0.0066508284,0.0755716975,0.0212529593,0.075922432,-0.0736954091,0.096755628,0.0662088951,0.0719506064,0.0094913337,-0.0014471366,-0.097427245,-0.0223697917,0.0115581715,-0.054808162,-0.0456894381,0.0123962945,-0.0189791951,0.010134529,0.0387024452,0.0395979472,0.0478491682,0.0537767249,-0.0007674797,0.004052776,0.1086883081,0.0034348352,-0.0321548947,0.0398639857,0.0462324764,-0.0047770715,-0.0117950051,0.0223095704,-0.121471702,0.0580322132,0.0266569453,0.0581292282,0.0466738748,0.068246591,-0.0038903492,-0.0346197561]},"176":{"Abstract":"Information visualization has traditionally limited itself to 2D representations, primarily due to the prevalence of 2D displays and report formats. However, there has been a recent surge in popularity of consumer grade 3D displays and immersive head-mounted displays (HMDs). The ubiquity of such displays enables the possibility of immersive, stereoscopic visualization environments. While techniques that utilize such immersive environments have been explored extensively for spatial and scientific visualizations, contrastingly very little has been explored for information visualization. In this paper, we present our considerations of layout, rendering, and interaction methods for visualizing graphs in an immersive environment. We conducted a user study to evaluate our techniques compared to traditional 2D graph visualization. The results show that participants answered significantly faster with a fewer number of interactions using our techniques, especially for more difficult tasks. While the overall correctness rates are not significantly different, we found that participants gave significantly more correct answers using our techniques for larger graphs.","Authors":"O. Kwon; C. Muelder; K. Lee; K. Ma","DOI":"10.1109\/TVCG.2016.2520921","Keywords":"Graph visualization;virtual reality;immersive environments;head-mounted display;Graph visualization;virtual reality;immersive environments;head-mounted display","Keywords_Processed":"graph visualization;immersive environment;head mount display;virtual reality","Title":"A Study of Layout, Rendering, and Interaction Methods for Immersive Graph Visualization","Labels":null,"Keyword_Vector":[0.2949031293,0.4227974803,-0.1115658144,-0.0128127033,-0.0000256209,-0.0162973088,-0.051999588,0.1129624257,0.0310962564,0.1634856704,-0.2069664653,0.0837722434,0.0131684858,-0.0875610438,-0.1537351313,0.0010882208,-0.2288974203,-0.0560568288,-0.0417613917,0.0530258349,-0.0842474343,0.0495436435,0.1813267291,-0.1766785914],"Abstract_Vector":[0.2728791696,-0.0076581812,0.2708193618,0.0869291711,0.0316944169,0.1189607647,0.1757091817,0.1101100338,0.0331766399,-0.0941141247,-0.0007985064,-0.0213423319,-0.0183402714,-0.1280954321,-0.0123589175,0.0047222136,-0.0877279479,0.0429126835,-0.0193392956,0.012964801,-0.1151132033,-0.1342255593,-0.0287725373,0.2430872778,-0.050369827,-0.0812948045,0.0356621672,-0.007017139,-0.0635959834,0.0422369684,0.0335230399,-0.0049220699,0.0533079777,0.0864511936,-0.0156136609,-0.121709029,-0.1050381142,-0.05879234,0.0464678708,-0.0135214731,-0.0047427379,0.0358718476,-0.0168654653,0.0629956608,-0.0108610507,-0.0820248655,-0.0378715577,0.0080749196,0.0457989073,0.0679572694,0.0514669502,0.1520155938,-0.0272441758,0.0112981524,0.0414927336,0.0571428048,-0.0193054875,0.106088017,0.0394479611,-0.0357958931,0.0509370774,-0.1465781475,-0.0069974403,0.0204476472,0.0628048554,-0.0357782298,0.0274996037,-0.0528314194,-0.1010176952,-0.0911786497,0.0370484036,0.0220238858,-0.02409269,-0.0096903119,0.0300625374,-0.024426556]},"177":{"Abstract":"This paper presents a new approach for the visualization and analysis of the spatial variability of features of interest represented by critical points in ensemble data. Our framework, called Persistence Atlas, enables the visualization of the dominant spatial patterns of critical points, along with statistics regarding their occurrence in the ensemble. The persistence atlas represents in the geometrical domain each dominant pattern in the form of a confidence map for the appearance of critical points. As a by-product, our method also provides 2-dimensional layouts of the entire ensemble, highlighting the main trends at a global level. Our approach is based on the new notion of Persistence Map, a measure of the geometrical density in critical points which leverages the robustness to noise of topological persistence to better emphasize salient features. We show how to leverage spectral embedding to represent the ensemble members as points in a low-dimensional Euclidean space, where distances between points measure the dissimilarities between critical point layouts and where statistical tasks, such as clustering, can be easily carried out. Further, we show how the notion of mandatory critical point can be leveraged to evaluate for each cluster confidence regions for the appearance of critical points. Most of the steps of this framework can be trivially parallelized and we show how to efficiently implement them. Extensive experiments demonstrate the relevance of our approach. The accuracy of the confidence regions provided by the persistence atlas is quantitatively evaluated and compared to a baseline strategy using an off-the-shelf clustering approach. We illustrate the importance of the persistence atlas in a variety of real-life datasets, where clear trends in feature layouts are identified and analyzed. We provide a lightweight VTK-based C++ implementation of our approach that can be used for reproduction purposes.","Authors":"G. Favelier; N. Faraj; B. Summa; J. Tierny","DOI":"10.1109\/TVCG.2018.2864432","Keywords":"Topological data analysis;scalar data;ensemble data","Keywords_Processed":"scalar datum;ensemble datum;topological datum analysis","Title":"Persistence Atlas for Critical Point Variability in Ensembles","Labels":null,"Keyword_Vector":[0.2527821439,-0.2075445295,0.1284041557,-0.3021112405,0.3544813865,0.0784913712,-0.1250142684,-0.012366884,-0.2188508455,-0.0850763634,-0.087187019,0.0668214602,-0.1703368289,-0.0389436693,-0.0399913555,-0.0887475115,-0.0184227726,0.0778872731,-0.0781380008,-0.105756333,0.0869898794,0.0237024068,0.1492113976,0.0534792948],"Abstract_Vector":[0.2286354832,-0.0787807745,-0.1193352984,0.2066445559,-0.080123521,0.0235591313,-0.0300296011,-0.0492717161,0.0467797613,0.0131347471,0.0026019308,-0.0399004897,0.0412847786,0.002894841,-0.0083337676,-0.0323339829,-0.0096181775,-0.0400430535,-0.0759201538,0.0266471807,0.0163785875,0.0412629052,-0.0272039679,-0.0451724646,0.0165406872,0.0579864778,0.0806275861,-0.14920066,0.0187765111,-0.0437701953,-0.0251087704,0.0243581899,0.0619557795,-0.0514733787,-0.0343920294,-0.0007611607,-0.0444475519,0.0737704786,0.1402709013,-0.0953902383,0.1147731835,0.0624679281,0.1016976354,-0.0132033341,-0.0765525263,-0.091482311,-0.0222273824,-0.0831505963,0.0125126912,-0.0289577633,0.1600431616,0.0372099151,-0.0584876618,-0.0554981142,-0.0098348669,0.067604169,0.107991271,0.1133726798,0.0009728709,-0.10400012,-0.1010665807,-0.0134097755,0.1745536363,-0.0596731154,-0.0326782026,-0.0844767299,0.0984369637,0.105110064,0.1242994479,0.0619460094,0.0216923957,-0.0234904455,-0.0857805609,0.0377257947,0.0191572662,0.0375208294]},"178":{"Abstract":"Event sequences analysis plays an important role in many application domains such as customer behavior analysis, electronic health record analysis and vehicle fault diagnosis. Real-world event sequence data is often noisy and complex with high event cardinality, making it a challenging task to construct concise yet comprehensive overviews for such data. In this paper, we propose a novel visualization technique based on the minimum description length (MDL) principle to construct a coarse-level overview of event sequence data while balancing the information loss in it. The method addresses a fundamental trade-off in visualization design: reducing visual clutter vs. increasing the information content in a visualization. The method enables simultaneous sequence clustering and pattern extraction and is highly tolerant to noises such as missing or additional events in the data. Based on this approach we propose a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. We demonstrate the usability and effectiveness of our approach through case studies with two real-world datasets. One dataset showcases a new application domain for event sequence visualization, i.e., fault development path analysis in vehicles for predictive maintenance. We also discuss the strengths and limitations of the proposed method based on user feedback.","Authors":"Y. Chen; P. Xu; L. Ren","DOI":"10.1109\/TVCG.2017.2745083","Keywords":"Time Series Data;Data Transformation and Representation;Visual Knowledge Representation;Visual Analytics","Keywords_Processed":"time series data;visual knowledge representation;data transformation and representation;visual analytic","Title":"Sequence Synopsis: Optimize Visual Summary of Temporal Event Data","Labels":null,"Keyword_Vector":[0.2041407329,-0.0812480084,0.3451449909,0.0934525916,-0.0496001566,-0.0653304525,-0.0370419874,0.0172244036,0.1769428185,0.0421997371,0.1291833025,-0.1499025372,-0.1448776511,-0.1039494355,0.0149947142,0.232685945,0.0263927323,0.1088243961,0.0592939882,-0.034781014,-0.1109378731,0.1136351815,0.1425901257,0.1001266233],"Abstract_Vector":[0.3352587577,-0.184939202,-0.0547739533,-0.1033136141,0.0275784485,-0.1318630961,-0.0470497465,-0.1244709718,-0.1099492311,0.1113368308,-0.1239260187,0.0709571828,-0.051081282,-0.0549166065,0.2168797344,0.138739237,0.0048189275,-0.1414731952,0.0041537655,0.0294755979,0.1062885155,-0.140695798,0.1776288647,0.090205035,-0.0102611005,-0.0377324915,0.0280126103,-0.1691692603,0.0072717356,-0.006259713,0.0508962588,0.1996738817,0.0552810463,0.0106474143,-0.0366368726,-0.0872896018,-0.110836813,0.0296558189,-0.0018935267,-0.0462261991,0.0127110136,0.0868642064,0.0599960275,-0.086114769,0.1205595773,0.0259776607,-0.0319081367,-0.0150136986,-0.0539570446,0.0912518657,0.0310954684,0.0130134234,0.0132854914,0.0685178241,-0.0265997364,0.0575824467,-0.0583080325,0.0142733283,0.0115454018,-0.0315102287,-0.0151691589,0.1142894972,-0.0221100577,0.1237987727,0.0699375333,0.0182053022,-0.0068078615,-0.0794276579,-0.0470209472,0.0185054885,-0.0683013752,0.0164547857,-0.0025522325,-0.0055655884,0.0281345947,-0.0156314846]},"179":{"Abstract":"The proliferation of high resolution and affordable virtual reality (VR) headsets is quickly making room-scale VR experiences available in our homes. Most VR experiences strive to achieve complete immersion by creating a disconnect from the real world. However, due to the lack of a standardized notification management system and minimal context awareness in VR, an immersed user may face certain situations such as missing an important phone call (digital scenario), tripping over wandering pets (physical scenario), or losing track of time (temporal scenario). In this paper, we present the results of 1) a survey across 61 VR users to understand common interruptions and scenarios that would benefit from some form of notifications; 2) a design exercise with VR professionals to explore possible notification methods; and 3) an empirical study on the noticeability and perception of 5 different VR interruption scenarios across 6 modality combinations (e.g., audio, visual, haptic, audio + haptic, visual + haptic, and audio + visual) implemented in Unity and presented using the HTC Vive headset. Finally, we combine key learnings from each of these steps along with participant feedback to present a set of observations and recommendations for notification design in VR.","Authors":"S. Ghosh; L. Winston; N. Panchal; P. Kimura-Thollander; J. Hotnog; D. Cheong; G. Reyes; G. D. Abowd","DOI":"10.1109\/TVCG.2018.2793698","Keywords":"Virtual Reality;Notifications;Interruptibility;Multi-Modal;Feedback;Context Awareness","Keywords_Processed":"feedback;multi modal;context awareness;virtual reality;notification;interruptibility","Title":"NotifiVR: Exploring Interruptions and Notifications in Virtual Reality","Labels":null,"Keyword_Vector":[0.1180669141,0.2904834392,0.0023450785,-0.0552276481,0.0371527511,-0.0466922268,0.0009666146,0.0086279917,0.0099032587,0.0983971122,-0.0939636819,-0.0108617085,-0.0701848822,0.0141095146,-0.0221939555,-0.04819653,0.1307383886,-0.0062506654,0.0214617304,0.0020713288,-0.0767456309,0.0732136842,0.0404531435,0.1163007435],"Abstract_Vector":[0.1755264133,0.0473729823,0.2366409505,0.0129198073,-0.0864267126,-0.0362678025,0.0091341585,0.0066181344,-0.0594299492,0.1013611137,-0.0176694977,-0.0637223838,0.0244055417,0.1745639779,0.1040937231,-0.0294023853,-0.0172465749,-0.0116325784,-0.0398560701,-0.056476857,0.0430072993,0.0052368001,0.0457521548,-0.0343198353,0.0417961554,0.0396521484,-0.0836165961,-0.0284367342,-0.090139939,-0.1459741644,-0.036618664,0.0707247233,-0.0382239971,-0.038337613,0.0670939715,-0.0516212715,0.0120910678,0.1869332627,0.0085346134,0.0463896353,0.054392953,-0.0334326039,-0.2048624939,0.0025703882,-0.0160946922,0.0048568628,0.0228704073,0.1272716012,-0.0705007495,-0.1612449016,0.1024802879,-0.0243983194,0.1336149422,0.019489251,0.0223959567,0.0731071273,0.0379603949,0.0405614373,-0.0421670143,0.11201626,-0.1537746644,-0.0347530635,-0.0012950999,-0.0710724687,0.101887374,0.0090815842,0.0544367571,-0.0755788471,-0.1022583776,-0.0238291398,-0.0749761541,-0.040884502,0.0318217388,0.0167544576,-0.1255370164,-0.0792045822]},"180":{"Abstract":"The relative location of human body parts often materializes the semantics of on-going actions, intentions and even emotions expressed, or performed, by a human being. However, traditional methods of performance animation fail to correctly and automatically map the semantics of performer postures involving self-body contacts onto characters with different sizes and proportions. Our method proposes an egocentric normalization of the body-part relative distances to preserve the consistency of self contacts for a large variety of human-like target characters. Egocentric coordinates are character independent and encode the whole posture space, i.e., it ensures the continuity of the motion with and without self-contacts. We can transfer classes of complex postures involving multiple interacting limb segments by preserving their spatial order without depending on temporal coherence. The mapping process exploits a low-cost constraint relaxation technique relying on analytic inverse kinematics; thus, we can achieve online performance animation. We demonstrate our approach on a variety of characters and compare it with the state of the art in online retargeting with a user study. Overall, our method performs better than the state of the art, especially when the proportions of the animated character deviate from those of the performer.","Authors":"E. Molla; H. G. Debarba; R. Boulic","DOI":"10.1109\/TVCG.2017.2708083","Keywords":"Motion retargeting;online performance animation;self-body contact;inverse kinematics;spatial relationship","Keywords_Processed":"self body contact;inverse kinematic;motion retargeting;spatial relationship;online performance animation","Title":"Egocentric Mapping of Body Surface Constraints","Labels":null,"Keyword_Vector":[0.0532833756,0.076892713,0.0093774568,0.0625739962,0.0981917531,-0.1103768855,0.1036490957,-0.0842092095,-0.0229681608,-0.188969888,0.0212497367,-0.2462962862,0.036640675,-0.2563575325,0.0751805719,-0.1219603859,0.0594088695,-0.1333788887,-0.0384125552,-0.1096568731,-0.0321399313,0.0085554708,0.040533763,-0.0573100003],"Abstract_Vector":[0.1957661286,0.1438380259,0.0002290492,-0.1592264082,-0.1474605092,-0.1209309431,-0.0602287004,-0.0693430421,0.1685763743,-0.1858166946,-0.0949579244,-0.0943715359,-0.0895962074,-0.00715312,-0.1080219048,0.0873580535,0.0895191763,-0.0443433808,0.1946670754,-0.0790008915,-0.0277427373,0.1520352856,0.0517985007,0.0684659185,0.0263627772,-0.1181846361,0.1043589193,-0.0767375912,0.0788856017,0.0833966837,-0.0710342389,-0.1172903695,-0.1442070845,-0.0016975842,0.1824061609,0.0877987285,-0.0599444033,0.1436866608,0.0441367019,-0.0384680919,0.1097052921,-0.0654421768,0.0565393895,-0.1128022005,0.0435011875,-0.0806286109,0.0676846997,-0.0984091695,-0.0261060337,-0.0003954743,0.1296145541,0.1022695244,0.006834665,-0.0768892985,-0.0077435189,-0.0513875272,-0.0383248265,-0.0259705666,0.1053344386,-0.033081657,0.0394713311,-0.100121175,-0.01935341,-0.0195881328,0.0539820874,-0.0243566285,-0.0119710926,0.0642288996,-0.0131156039,-0.103197193,-0.0240403883,-0.0152140152,-0.0324767942,0.0551113732,-0.0187157629,0.0579268605]},"181":{"Abstract":"Cloth is made of yarns that are stitched together forming semi-regular patterns. Due to the complexity of stitches and patterns, the macroscopic behavior of cloth is dictated by the contact interactions between yarns, not by the mechanical properties of yarns alone. The computation of cloth mechanics at the yarn level appears as a computationally complex and costly process at first sight, due to the need to resolve many fine-scale contact interactions. We propose instead an efficient representation of cloth at the yarn level that treats yarn-yarn contacts as persistent, but with the possibility to slide, thereby avoiding expensive contact handling altogether. We introduce a compact representation of yarn geometry and kinematics, capturing the essential deformation modes of yarn crossings, loops, stitches, and stacks, with a minimum cost. Based on this representation, we design force models that reproduce the characteristic macroscopic behavior of yarn-based fabrics. Our approach is suited for both woven and knitted fabrics. We demonstrate the efficiency of our method on simulations with millions of degrees of freedom (hundreds of thousands of yarn loops), almost one order of magnitude faster than previous techniques. We also compare the different macroscopic behavior under woven and knitted patterns with the same yarn density.","Authors":"G. Cirio; J. Lopez-Moreno; M. A. Otaduy","DOI":"10.1109\/TVCG.2016.2592908","Keywords":"Yarns;knitted cloth;woven cloth;physically based simulation","Keywords_Processed":"knit cloth;physically base simulation;yarn;weave cloth","Title":"Yarn-Level Cloth Simulation with Sliding Persistent Contacts","Labels":null,"Keyword_Vector":[0.0288335012,0.0199734591,-0.0126283362,0.0485817575,0.0256755198,-0.0751422049,0.0987707581,-0.0709736491,-0.0656496356,-0.0488049068,-0.0001515113,0.0683010851,0.0516633918,-0.0285978512,-0.044075359,-0.1184010753,0.0214451122,0.0690172991,0.2429592494,0.0349319847,0.1040844349,0.081490029,-0.0782985963,0.0600734257],"Abstract_Vector":[0.1012473963,0.048698342,-0.0421473275,-0.0729136514,-0.0401532404,-0.0567302292,0.0065306817,-0.0754469184,0.0158385271,-0.0859805091,-0.0795602719,-0.0084115437,-0.0221568177,0.0426942697,-0.0097417871,0.0445115248,-0.0290971564,0.0052628031,0.029505144,0.0181325389,0.0301674232,-0.0117818343,0.0056205055,0.0106061754,-0.0513725965,-0.0370118696,0.0434858621,0.0763364654,-0.0245907585,-0.0781269291,0.0404683495,-0.0154454529,0.037079344,0.0739421065,0.0621797924,0.0129680085,-0.0315403993,-0.0874011968,-0.0209540901,0.0602540824,0.0526395211,0.0239484143,0.0665910388,0.0989753816,-0.0221497047,-0.1078328438,-0.0776399254,0.0362164672,-0.0520827942,-0.0261049643,0.0095482379,-0.0855931422,-0.0853465881,0.0117670277,0.1208908992,0.0248044734,-0.058356918,-0.0667224938,0.0874790846,-0.1267817401,-0.0221651867,-0.147896638,0.029981094,0.0371138207,0.2237525822,-0.0566799513,-0.1325338779,-0.0080005699,0.0896010418,-0.0386709434,-0.0176466006,0.0601773751,-0.029793138,-0.1589877819,0.0688704033,0.1762624688]},"182":{"Abstract":"We present Charticulator, an interactive authoring tool that enables the creation of bespoke and reusable chart layouts. Charticulator is our response to most existing chart construction interfaces that require authors to choose from predefined chart layouts, thereby precluding the construction of novel charts. In contrast, Charticulator transforms a chart specification into mathematical layout constraints and automatically computes a set of layout attributes using a constraint-solving algorithm to realize the chart. It allows for the articulation of compound marks or glyphs as well as links between these glyphs, all without requiring any coding or knowledge of constraint satisfaction. Furthermore, thanks to the constraint-based layout approach, Charticulator can export chart designs into reusable templates that can be imported into other visualization tools. In addition to describing Charticulator's conceptual framework and design, we present three forms of evaluation: a gallery to illustrate its expressiveness, a user study to verify its usability, and a click-count comparison between Charticulator and three existing tools. Finally, we discuss the limitations and potentials of Charticulator as well as directions for future research. Charticulator is available with its source code at https:\/\/charticulator.com.","Authors":"D. Ren; B. Lee; M. Brehmer","DOI":"10.1109\/TVCG.2018.2865158","Keywords":"Interactive visualization authoring;Chart layout design;Glyph design;Constraint-based design;Reusable chart layout","Keywords_Processed":"constraint base design;interactive visualization author;reusable chart layout;glyph design;chart layout design","Title":"Charticulator: Interactive Construction of Bespoke Chart Layouts","Labels":null,"Keyword_Vector":[0.1867280945,-0.0445469639,-0.0845714707,0.015901848,-0.1454535228,0.1653699636,0.368701796,0.0222077525,0.0232935513,-0.1691882226,-0.0703446386,0.061255873,-0.0866955229,0.0591339083,-0.177471511,0.0566323673,-0.007293137,-0.020622856,0.0336209566,-0.0895823464,-0.0381408793,0.0057458184,-0.010673161,-0.0786068621],"Abstract_Vector":[0.1424487438,-0.0482826306,0.0337683196,-0.0466784762,0.0613889171,0.0625650026,0.0050263279,0.0183525311,-0.0376837551,-0.1026642672,0.0610977228,-0.0700253952,0.0293617632,-0.022967592,-0.0581906183,0.0026508738,0.0705333628,0.0065094192,-0.0210518669,-0.0123832341,-0.0243521105,-0.0630251941,-0.0345480844,-0.1139860845,0.1289022049,-0.0409635653,0.0323231343,-0.0823862738,-0.0066059883,-0.1128322885,-0.0220673464,0.0253304078,0.0529773233,0.0318110182,0.1740542193,0.0351646822,-0.0646625153,0.0732458507,-0.1070133308,-0.1058115142,0.0686451927,-0.0268591761,-0.0393340631,0.0680342042,0.0103671622,0.0731324484,0.152685197,0.0130808827,0.002450091,0.0436398796,-0.0617803068,0.0751895438,-0.1003578026,-0.0150866736,0.0002422633,0.0326233707,0.0595831742,0.1442914664,-0.1048914228,0.0428369917,0.042488342,0.0090771087,-0.1594699611,-0.0993810564,0.0573475507,0.0131659169,0.1836034469,0.0823014597,0.1155062658,0.0003579942,-0.1001687067,0.0735939952,-0.1034520971,-0.0975357478,-0.0540245741,0.1553161389]},"183":{"Abstract":"We address the problem of visualizing multivariate correlations in parallel coordinates. We focus on multivariate correlation in the form of linear relationships between multiple variables. Traditional parallel coordinates are well prepared to show negative correlations between two attributes by distinct visual patterns. However, it is difficult to recognize positive correlations in parallel coordinates. Furthermore, there is no support to highlight multivariate correlations in parallel coordinates. In this paper, we exploit the indexed point representation of p -flats (planes in multidimensional data) to visualize local multivariate correlations in parallel coordinates. Our method yields clear visual signatures for negative and positive correlations alike, and it supports large datasets. All information is shown in a unified parallel coordinates framework, which leads to easy and familiar user interactions for analysts who have experience with traditional parallel coordinates. The usefulness of our method is demonstrated through examples of typical multidimensional datasets.","Authors":"L. Zhou; D. Weiskopf","DOI":"10.1109\/TVCG.2017.2698041","Keywords":"Multidimensional data visualization;multivariate correlations;parallel coordinates","Keywords_Processed":"multivariate correlation;parallel coordinate;multidimensional datum visualization","Title":"Indexed-Points Parallel Coordinates Visualization of Multivariate Correlations","Labels":null,"Keyword_Vector":[0.2863376358,-0.1794472915,-0.096591801,-0.2194895007,0.2530551847,0.0777811912,-0.0787587436,0.1254810511,-0.3289155755,0.1360479631,0.1042834936,-0.0212715293,0.0108583219,-0.181749781,-0.225433932,0.1031716504,0.0884254878,0.0088594005,-0.084755995,0.274848254,0.0605852686,0.0380973745,-0.1163416955,-0.0575480496],"Abstract_Vector":[0.1820126914,-0.1243303481,-0.0855999147,-0.0661286958,-0.110755112,0.0193233216,0.20767877,0.0452917819,0.0872929889,0.1882029013,-0.0829374819,0.0156700967,0.1797535681,0.1530315296,-0.2749340583,-0.1365819528,0.0462266149,-0.0793179235,0.1736300323,0.0021914255,0.0046895983,-0.0505120501,0.0144194946,0.0989131066,-0.0402792726,-0.0623048679,0.0068987597,-0.0602512692,-0.1073116496,0.0326621373,-0.0457411581,-0.0418403046,-0.1552359199,0.0944960163,-0.0132306885,0.0883809499,-0.0630103949,-0.1231561009,0.0350976814,-0.0632717872,-0.1068382754,0.0900400214,-0.0007130693,0.1094656226,-0.0419353041,0.045436608,0.0612852299,0.0483179709,0.0308468272,-0.0582927401,0.0196246976,-0.0696658765,0.0068378617,0.1228579548,-0.1189291219,-0.0174009141,-0.0458965511,-0.032967552,0.0113831697,0.0533139657,-0.0120529909,-0.0315770439,0.0542091189,0.0327065399,0.074990861,-0.0678362993,0.0112180202,0.1221045008,-0.0247739398,-0.0484343357,0.0279102789,-0.0195191451,-0.0732030194,0.0096020594,-0.0060418988,0.0285826195]},"184":{"Abstract":"Data are often viewed as a single set of values, but those values frequently must be compared with another set. The existing evaluations of designs that facilitate these comparisons tend to be based on intuitive reasoning, rather than quantifiable measures. We build on this work with a series of crowdsourced experiments that use low-level perceptual comparison tasks that arise frequently in comparisons within data visualizations (e.g., which value changes the most between the two sets of data?). Participants completed these tasks across a variety of layouts: overlaid, two arrangements of juxtaposed small multiples, mirror-symmetric small multiples, and animated transitions. A staircase procedure sought the difficulty level (e.g., value change delta) that led to equivalent accuracy for each layout. Confirming prior intuition, we observe high levels of performance for overlaid versus standard small multiples. However, we also find performance improvements for both mirror symmetric small multiples and animated transitions. While some results are incongruent with common wisdom in data visualization, they align with previous work in perceptual psychology, and thus have potentially strong implications for visual comparison designs.","Authors":"B. Ondov; N. Jardine; N. Elmqvist; S. Franconeri","DOI":"10.1109\/TVCG.2018.2864884","Keywords":"Graphical perception;visual perception;visual comparison;crowdsourced evaluation","Keywords_Processed":"visual comparison;graphical perception;visual perception;crowdsourc evaluation","Title":"Face to Face: Evaluating Visual Comparison","Labels":null,"Keyword_Vector":[0.2473210439,0.0999797128,0.3305662284,-0.0548988251,-0.2044927547,0.3761271982,-0.0602577512,-0.3181500919,-0.1231952985,0.1219846825,0.0900558664,-0.0514381534,0.1201142745,0.0189882011,0.269329588,0.0252348534,-0.082871734,-0.0132602897,0.0871318065,-0.0208738211,0.0652646018,-0.031193139,-0.0420007971,0.0343836873],"Abstract_Vector":[0.2501035897,-0.0788833793,0.0741290139,-0.0598212396,0.0122050664,0.1223022218,-0.0423330254,0.0019124651,0.0716181028,-0.0625309134,-0.0576990201,-0.0622399872,-0.0047958442,-0.0465144925,0.0251473763,-0.1055416266,0.034916808,-0.0289256407,0.0312857881,0.1077223217,-0.1389164222,-0.2040762616,-0.0628741429,-0.2283480459,0.1938450767,-0.0182884046,-0.0777274696,-0.0902602689,0.0247418146,-0.1302989738,-0.0119997407,-0.1039738626,0.0438275718,-0.1094938494,0.007152973,0.0608479868,-0.0377927881,0.0591699703,0.0739218425,0.0754625111,0.0786558651,-0.0529748497,0.0464912302,-0.0337596765,-0.0338471245,0.0625014412,-0.1716243994,-0.063984112,-0.0077917025,0.1396204942,0.0755113304,-0.0149573021,0.1611478325,0.027044687,-0.1057779092,0.0105022958,-0.0407682702,-0.0951544736,0.0077569915,-0.0518397571,-0.0359746909,0.0074387407,-0.0216592711,-0.0094113068,0.0093965488,-0.012808036,-0.0471394133,-0.0647640432,0.0602823106,-0.0270233814,0.0168022984,0.0184088174,0.0226818892,-0.1081976967,-0.1084949928,0.0120297301]},"185":{"Abstract":"Analyzing social networks reveals the relationships between individuals and groups in the data. However, such analysis can also lead to privacy exposure (whether intentionally or inadvertently): leaking the real-world identity of ostensibly anonymous individuals. Most sanitization strategies modify the graph's structure based on hypothesized tactics that an adversary would employ. While combining multiple anonymization schemes provides a more comprehensive privacy protection, deciding the appropriate set of techniques-along with evaluating how applying the strategies will affect the utility of the anonymized results-remains a significant challenge. To address this problem, we introduce GraphProtector, a visual interface that guides a user through a privacy preservation pipeline. GraphProtector enables multiple privacy protection schemes which can be simultaneously combined together as a hybrid approach. To demonstrate the effectiveness of GraphPro tector, we report several case studies and feedback collected from interviews with expert users in various scenarios.","Authors":"X. Wang; W. Chen; J. Chou; C. Bryan; H. Guan; W. Chen; R. Pan; K. Ma","DOI":"10.1109\/TVCG.2018.2865021","Keywords":"Graph privacy;k-anonymity;structural features;privacy preservation","Keywords_Processed":"structural feature;privacy preservation;anonymity;graph privacy","Title":"GraphProtector: A Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms","Labels":null,"Keyword_Vector":[0.029410102,-0.0006341722,-0.0033967505,0.0106660599,-0.0188207768,-0.0502427783,0.0561530002,0.0010605808,-0.0137595744,0.1073674727,-0.0193036806,0.0819642265,-0.0613709197,-0.0555037672,-0.0407069381,0.0001857892,0.0095431133,0.0620214318,0.0029084846,-0.1120106721,0.0389388756,-0.0584582064,-0.0257687111,-0.0399358671],"Abstract_Vector":[0.1901170301,-0.0727332745,0.0029662889,-0.0149811212,0.0277549251,-0.1385162292,0.0948835244,0.0593753174,0.05358178,0.0227835883,0.0278048899,0.0198546204,-0.1155817883,0.0077124255,0.0215858958,0.0292952802,0.0065696826,0.0201335157,-0.0436188294,-0.0034566539,0.0267295835,-0.0691879501,-0.0379871157,-0.034709177,-0.0316336889,-0.0241041218,-0.063783462,-0.0318669947,0.0331446556,-0.0630504928,0.0472556895,0.0118714164,-0.0270523063,-0.1005235238,-0.0118943496,0.048710036,0.0383936877,-0.0645877606,0.0448135832,0.0246715667,-0.0075946268,-0.0550337439,-0.0973322657,0.0442028867,-0.0199440615,-0.0104421113,-0.0702987718,0.0513754247,0.0727101325,-0.1111930372,0.0193965772,0.0426111593,0.0605952819,-0.0163346848,0.1040033426,0.0214579992,-0.070123304,-0.0684547026,0.0827078097,0.1006115019,0.0511956259,0.0198202744,0.1401004757,-0.0437956556,-0.0339041126,0.0422284225,-0.0232605875,-0.0497191525,-0.0010421453,-0.0122664589,0.1693088531,0.0948883873,-0.0524273171,0.0379762679,0.0137980364,0.126710556]},"186":{"Abstract":"Unified simulation of versatile elastoplastic materials and different dimensions offers many advantages in animation production, contact handling, and hardware acceleration. The unstructured particle representation is particularly suitable for this task, thanks to its simplicity. However, previous meshless techniques either need too much computational cost for addressing stability issues, or lack physical meanings and fail to generate interesting deformation behaviors, such as the Poisson effect. In this paper, we study the development of an elastoplastic model under the state-based peridynamics framework, which uses integrals rather than partial derivatives in its formulation. To model elasticity, we propose a unique constitutive model and an efficient iterative simulator solved in a projective dynamics way. To handle plastic behaviors, we incorporate our simulator with the Drucker-Prager yield criterion and a reference position update scheme, both of which are implemented under peridynamics. Finally, we show how to strengthen the simulator by position-based constraints and spatially varying stiffness models, to achieve incompressibility, particle redistribution, cohesion, and friction effects in viscoelastic and granular flows. Our experiments demonstrate that our unified, meshless simulator is flexible, efficient, robust, and friendly with parallel computing.","Authors":"X. He; H. Wang; E. Wu","DOI":"10.1109\/TVCG.2017.2755646","Keywords":"Peridynamics;projective dynamics;position-based dynamics;elasticity;plasticity;viscoelasticity;granular flows","Keywords_Processed":"position base dynamic;viscoelasticity;elasticity;granular flow;plasticity;peridynamic;projective dynamic","Title":"Projective Peridynamics for Modeling Versatile Elastoplastic Materials","Labels":null,"Keyword_Vector":[0.0525269787,0.0019894042,0.003407875,0.0236965456,-0.0368488766,-0.0807701379,0.0883909987,-0.0772768368,-0.0031100426,0.062489906,0.1095675976,0.1833425033,0.0030119233,-0.0912623065,-0.0090389524,-0.0497886189,-0.0064445589,0.0262165034,0.100417445,0.0369567042,0.0926483332,-0.0847747209,0.1184669103,-0.089852315],"Abstract_Vector":[0.174602638,0.1046795223,-0.0414524135,-0.0572729461,0.0444055006,-0.0347722027,0.0159595691,-0.0373478638,0.0362264775,-0.0760278288,-0.0510670008,0.0172302576,0.0625386437,0.1908617344,-0.0733274809,0.0868122005,-0.0085639962,0.0909699007,0.0143991234,0.0122988009,-0.0692739201,-0.0566105043,0.0300948329,0.0267977021,0.0125824442,0.0375273395,0.0274695461,0.0714292161,-0.0566438714,-0.0145724487,-0.0915539907,-0.0105373005,-0.0486970192,-0.0054235235,-0.0506739222,-0.0356272554,-0.0074879349,-0.0406564444,-0.0044781981,-0.0398061071,0.0187037578,0.0520684421,0.116664443,0.0208765021,0.0196468454,-0.0081888438,0.0050653588,0.0661403222,-0.1395936435,-0.0426956312,-0.0799241481,-0.1447660584,0.0060267668,0.0110586148,0.0243200176,-0.0814359795,-0.0196332031,0.1313331082,-0.0287957773,0.0221578439,-0.0497870762,0.0318770177,0.0072772508,0.1035006569,0.1571196788,-0.0844504987,0.0280998815,0.095058288,-0.0395184642,-0.1001529634,-0.089590091,-0.0187875395,-0.0462351318,0.1048731282,0.0195055793,0.2175879389]},"187":{"Abstract":"Conventional dot plots use a constant dot size and are typically applied to show the frequency distribution of small data sets. Unfortunately, they are not designed for a high dynamic range of frequencies. We address this problem by introducing nonlinear dot plots. Adopting the idea of nonlinear scaling from logarithmic bar charts, our plots allow for dots of varying size so that columns with a large number of samples are reduced in height. For the construction of these diagrams, we introduce an efficient two-way sweep algorithm that leads to a dense and symmetrical layout. We compensate aliasing artifacts at high dot densities by a specifically designed low-pass filtering method. Examples of nonlinear dot plots are compared to conventional dot plots as well as linear and logarithmic histograms. Finally, we include feedback from an expert review.","Authors":"N. Rodrigues; D. Weiskopf","DOI":"10.1109\/TVCG.2017.2744018","Keywords":"Nonlinear dot plot;statistical graphics;sweep algorithm;layout","Keywords_Processed":"layout;statistical graphic;sweep algorithm;nonlinear dot plot","Title":"Nonlinear Dot Plots","Labels":null,"Keyword_Vector":[0.0464533932,-0.0201299508,-0.0156173602,-0.0259469707,0.0089877712,-0.0163956478,0.0367856375,-0.0712727633,-0.0446968995,-0.0066711527,-0.0504839925,-0.008024924,0.0315174097,0.0000310075,-0.0293137135,0.122027644,0.0568798346,0.0171731692,0.0818218082,0.0207096382,-0.035171343,-0.0455838078,0.0061782326,-0.1216700677],"Abstract_Vector":[0.147051931,-0.0159897745,-0.0263235554,-0.0073988243,-0.0683228592,0.110864713,0.0079732999,0.0723980402,-0.0272664766,-0.0594211324,-0.0620438857,0.0271856983,0.002306614,0.0251466345,0.0440103764,-0.0065143548,0.0409043646,0.0152568526,0.0523755166,-0.0369217162,-0.098416935,-0.1097177475,-0.0049910589,-0.1177861452,0.045273961,0.0550890432,0.0072363978,-0.1217034869,0.0571037162,-0.1398187074,-0.1344285871,-0.0074768463,0.0152475764,0.2440302354,0.1785382334,0.0253491087,0.0436611584,-0.0234535752,-0.04480454,-0.0793679541,-0.1046413041,-0.0261969146,0.0232494447,0.1223775849,-0.0886074013,-0.106206249,0.1093122042,0.0638344373,-0.1268780365,-0.0412909546,-0.0411221256,0.0549511418,0.0649421916,-0.0902563604,0.042542564,0.0274040377,0.0708204643,-0.1676828973,0.0310634065,0.016518523,-0.0203403995,0.0979125499,-0.0695492644,0.0735632501,-0.1180303831,0.1330821777,-0.0655501492,-0.0821068733,-0.0003981262,0.0855529487,-0.0319562253,-0.0355274462,-0.0372185807,0.0519898004,0.0362302664,-0.0341801229]},"188":{"Abstract":"We present a method for the fast computation of the intersection between a ray and the geometry of a scene. The scene geometry is simplified with a 2D array of voxelizations computed from different directions, sampling the space of all possible directions. The 2D array of voxelizations is compressed using a vector quantization approach. The ray-scene intersection is approximated using the voxelization whose rows are most closely aligned with the ray. The voxelization row that contains the ray is looked up, the row is truncated to the extent of the ray using bit operations, and a truncated row with non-zero bits indicates that the ray intersects the scene. We support dynamic scenes with rigidly moving objects by building a separate 2D array of voxelizations for each type of object, and by using the same 2D array of voxelizations for all instances of an object type. We support complex dynamic scenes and scenes with deforming geometry by computing and rotating a single voxelization on the fly. We demonstrate the benefits of our method in the context of interactive rendering of scenes with thousands of moving lights, where we compare our method to ray tracing, to conventional shadow mapping, and to imperfect shadow maps.","Authors":"L. Wang; X. Liang; C. Meng; V. Popescu","DOI":"10.1109\/TVCG.2018.2828422","Keywords":"Real time rendering;many lights;visibility determination;photorealism","Keywords_Processed":"many light;visibility determination;photorealism;real time render","Title":"Fast Ray-Scene Intersection for Interactive Shadow Rendering with Thousands of Dynamic Lights","Labels":null,"Keyword_Vector":[0.0330552286,0.0442808991,0.0259125231,-0.0063327346,0.0127717956,-0.0575321988,0.0356003049,-0.0197876809,0.0396282258,0.0535107031,0.0269658215,-0.0476346439,-0.1444004268,-0.0547783347,0.0655102647,0.0375939398,0.0457535687,0.0737665129,-0.0338647294,-0.0497284702,0.0267445396,0.0077856134,-0.0689767274,0.0794336739],"Abstract_Vector":[0.1429997344,0.2005812052,-0.0171970466,0.1118481084,-0.0204458781,0.0006308943,0.0953280633,0.12018275,-0.2081592787,0.0213393402,-0.0521571741,0.0937929354,0.0794095813,-0.0487508553,-0.0435506152,0.0640916742,0.0340828986,-0.0437089882,-0.1117971855,0.1207913604,-0.0062916221,0.1191591137,0.0242522447,-0.1433769359,0.1025785215,-0.1048597059,0.1125998714,0.0614829333,-0.0002202556,-0.0540133585,0.1383383189,-0.0036322974,-0.0385108272,0.0883768977,0.0582162064,0.0551941731,-0.0536790881,-0.1350406623,0.030217571,0.1834715441,0.1167924839,-0.0326723682,0.0937011677,-0.0542417171,0.0073800485,-0.0172944963,0.0026130869,0.0128590982,-0.109274808,0.0729816735,0.0707308931,0.0566444183,-0.0186086402,0.0246670126,-0.1177378791,-0.0928641902,0.0263581983,0.0210639027,-0.0400861217,0.0429012769,-0.015848125,-0.0381599905,0.0450236523,0.0871583405,-0.0041141742,0.0265992001,0.0070096375,0.0322021696,-0.1302777678,-0.112512402,0.0379624549,-0.0518815592,-0.042580702,0.0142943994,0.1130324557,-0.0200846617]},"189":{"Abstract":"Action sequences, where atomic user actions are represented in a labelled, timestamped form, are becoming a fundamental data asset in the inspection and monitoring of user behaviour in digital systems. Although the analysis of such sequences is highly critical to the investigation of activities in cyber security applications, existing solutions fail to provide a comprehensive understanding due to the complex semantic and temporal characteristics of these data. This paper presents a visual analytics approach that aims to facilitate a user-involved, multi-faceted decision making process during the identification and the investigation of \u201cunusual\u201d action sequences. We first report the results of the task analysis and domain characterisation process. Then we describe the components of our multi-level analysis approach that comprises of constraint-based sequential pattern mining and semantic distance based clustering, and multi-scalar visualisations of users and their sequences. Finally, we demonstrate the applicability of our approach through a case study that involves tasks requiring effective decision-making by a group of domain experts. Although our solution here is tightly informed by a user-centred, domain-focused design process, we present findings and techniques that are transferable to other applications where the analysis of such sequences is of interest.","Authors":"P. H. Nguyen; C. Turkay; G. Andrienko; N. Andrienko; O. Thonnard; J. Zouaoui","DOI":"10.1109\/TVCG.2018.2859969","Keywords":"Action sequence;event sequence;sequential pattern mining;visual analytics;cyber security;user behaviour","Keywords_Processed":"user behaviour;event sequence;visual analytic;action sequence;cyber security;sequential pattern mining","Title":"Understanding User Behaviour through Action Sequences: From the Usual to the Unusual","Labels":null,"Keyword_Vector":[0.0975862522,-0.0037371792,0.1571333383,0.0671227364,-0.106791799,-0.0094515344,-0.0061970369,0.0726589686,0.016665264,-0.0578238956,0.0201434456,-0.0118548819,0.0178979066,-0.028706806,0.0426965474,0.0189020887,0.0033062553,0.1163474832,-0.0298890565,-0.0015290738,-0.0037340398,0.0352775503,0.0687550051,0.0599052991],"Abstract_Vector":[0.3020603699,-0.1460538108,0.0015266961,-0.0730676816,0.0834630523,-0.2175865555,-0.170601424,-0.0798836494,-0.0276845563,0.0270146463,0.0836088478,0.0758318224,0.0320250269,-0.0109101245,0.0488481439,0.0831311866,-0.0060462046,-0.0449788268,0.0335611372,0.0356136196,0.1354517581,-0.0041029331,0.0814115131,0.0860816786,0.0460737196,-0.0709128735,-0.0549938992,0.0098434586,0.1513013909,-0.0674464652,0.1204491161,0.1852294681,0.000919547,0.0214512015,0.0127048382,-0.0571584317,-0.067416469,0.2269715833,-0.0034195748,-0.0720522424,-0.0472594753,-0.0091874159,0.1025347968,0.0134270262,0.0002410692,-0.0595524403,-0.0807935959,-0.009670563,-0.0690523689,0.0009935075,0.1039218404,0.092685894,0.0088106454,0.1603477051,0.0417714877,-0.0835963914,0.1390040689,-0.0040649204,0.0923539551,-0.0078275222,-0.0674189546,-0.0416119173,-0.0221512304,-0.0514898145,0.0709755497,0.0162735714,-0.1101253874,0.0300505435,0.0560935167,0.0327509454,0.0736031342,-0.0003450342,-0.0776772125,0.0690915848,0.1503637037,0.0399236701]},"190":{"Abstract":"Building Information Modeling (BIM) provides an integrated 3D environment to manage large-scale engineering projects. The Architecture, Engineering and Construction (AEC) industry explores 4D visualizations over these datasets for virtual construction planning. However, existing solutions lack adequate visual mechanisms to inspect the underlying schedule and make inconsistencies readily apparent. The goal of this paper is to apply best practices of information visualization to improve 4D analysis of construction plans. We first present a review of previous work that identifies common use cases and limitations. We then consulted with AEC professionals to specify the main design requirements for such applications. These guided the development of CasCADe, a novel 4D visualization system where task sequencing and spatio-temporal simultaneity are immediately apparent. This unique framework enables the combination of diverse analytical features to create an information-rich analysis environment. We also describe how engineering collaborators used CasCADe to review the real-world construction plans of an Oil & Gas process plant. The system made evident schedule uncertainties, identified work-space conflicts and helped analyze other constructability issues. The results and contributions of this paper suggest new avenues for future research in information visualization for the AEC industry.","Authors":"P. Ivson; D. Nascimento; W. Celes; S. D. Barbosa","DOI":"10.1109\/TVCG.2017.2745105","Keywords":"Visualization in physical sciences and engineering;design studies;integrating spatial and non-spatial data visualization;task and requirements analysis","Keywords_Processed":"visualization in physical science and engineering;design study;task and requirement analysis;integrate spatial and non spatial datum visualization","Title":"CasCADe: A Novel 4D Visualization System for Virtual Construction Planning","Labels":null,"Keyword_Vector":[0.3610811869,-0.0902967888,-0.0233106293,-0.1140050234,0.0099031567,0.1441140308,0.1572654695,0.1815315621,0.1558607597,-0.3209793138,0.0357359545,-0.17783998,-0.1912193148,-0.0928157962,0.0421065144,-0.105958713,-0.0793125703,0.0043701352,-0.0106450047,0.1031947844,0.1547334349,-0.0220321439,-0.0238868554,-0.0927081414],"Abstract_Vector":[0.2338150001,-0.0828932214,0.0978238991,0.0202311668,0.1125983521,0.0387538028,-0.0029800016,-0.0896399667,-0.0631513564,-0.0358555103,-0.0337308817,-0.0395992946,-0.0120952908,0.0137426937,-0.009201986,0.0131627499,0.0189791613,0.0257472842,0.0047539039,0.0361787728,0.0082613715,0.0513321285,0.0728440993,0.0861114538,-0.0384435516,0.0058389951,0.1150113706,-0.031598994,-0.037355705,0.0126581426,-0.021517039,-0.004897787,0.0808680753,-0.0509823674,0.0600248618,-0.0542277122,0.0799140259,-0.1085254822,0.0371127669,-0.0288027015,-0.0416164912,0.040694785,-0.0592903974,-0.0081822332,0.0319073967,0.0773348379,0.0664857623,0.0422098159,-0.1088923968,0.0454949138,-0.0237988005,0.1014313273,-0.1148540407,0.0609426068,0.0418633208,-0.0264790027,0.1619325923,-0.1154277586,0.0213829634,0.0863429351,-0.0761617879,-0.0456701524,-0.0019999472,0.0125891109,-0.0062581423,0.1730646361,0.2627382089,-0.0464471702,0.0765980647,0.1227434674,0.0515868645,0.0719399092,0.054331675,-0.0970689205,-0.1507690883,0.0497927213]},"191":{"Abstract":"Type 1 diabetes is a chronic, incurable autoimmune disease affecting millions of Americans in which the body stops producing insulin and blood glucose levels rise. The goal of intensive diabetes management is to lower average blood glucose through frequent adjustments to insulin protocol, diet, and behavior. Manual logs and medical device data are collected by patients, but these multiple sources are presented in disparate visualization designs to the clinician-making temporal inference difficult. We conducted a design study over 18 months with clinicians performing intensive diabetes management. We present a data abstraction and novel hierarchical task abstraction for this domain. We also contribute IDMVis: a visualization tool for temporal event sequences with multidimensional, interrelated data. IDMVis includes a novel technique for folding and aligning records by dual sentinel events and scaling the intermediate timeline. We validate our design decisions based on our domain abstractions, best practices, and through a qualitative evaluation with six clinicians. The results of this study indicate that IDMVis accurately reflects the workflow of clinicians. Using IDMVis, clinicians are able to identify issues of data quality such as missing or conflicting data, reconstruct patient records when data is missing, differentiate between days with different patterns, and promote educational interventions after identifying discrepancies.","Authors":"Y. Zhang; K. Chanana; C. Dunne","DOI":"10.1109\/TVCG.2018.2865076","Keywords":"Design study;task analysis;event sequence visualization;time series data;qualitative evaluation;health applications","Keywords_Processed":"qualitative evaluation;design study;health application;time series datum;task analysis;event sequence visualization","Title":"IDMVis: Temporal Event Sequence Visualization for Type 1 Diabetes Treatment Decision Support","Labels":null,"Keyword_Vector":[0.3104462628,-0.1070622352,0.0794448844,-0.1734278221,-0.0290309907,0.2299283139,0.2954825743,0.076848147,0.1338257136,-0.111003013,-0.0103112843,0.0404064791,-0.1644614965,-0.0996187595,0.1170452203,-0.0254339703,0.1083249519,0.042294217,-0.117351258,-0.0155000089,0.046429212,0.1492465915,0.0913461421,0.0542745451],"Abstract_Vector":[0.1849654687,-0.1219368947,0.0264531159,-0.1109775866,-0.0122060356,-0.0111527509,-0.0812965816,-0.0487838093,-0.0831481469,0.0503078224,-0.0106660698,0.0214447498,-0.0103192698,-0.0336000959,0.082455931,0.0397167751,0.0201185975,-0.0355448646,0.0168523251,-0.0136618321,0.0345713912,-0.0673160461,0.0652002854,0.0375307738,0.0935350155,-0.0831717349,-0.1168966254,-0.0547067845,-0.024987971,-0.0160983973,0.0054288634,-0.0178344472,-0.0298929507,0.0115882851,0.0895478536,-0.0640109471,0.005200288,-0.1136529098,0.0079100354,0.0334888927,0.0776329094,-0.0890950493,-0.0713008358,-0.0706244109,0.0630449655,0.0740441075,-0.126057423,-0.0775543477,-0.0375722871,0.002635089,-0.0274096558,-0.1223719745,-0.151524624,-0.0227081679,-0.0480344431,0.076601036,-0.0293357787,-0.0937974175,0.0617499336,-0.0120105037,-0.0411990259,0.0265963073,0.0807267934,0.0033760008,0.1373951929,0.0634875084,0.0573350016,-0.0689466208,-0.0385797218,0.0403604653,0.0122385891,-0.0606087425,0.050648854,-0.0287421628,0.0306119353,0.0039712803]},"192":{"Abstract":"Superfluidity is a special state of matter exhibiting macroscopic quantum phenomena and acting like a fluid with zero viscosity. In such a state, superfluid vortices exist as phase singularities of the model equation with unique distributions. This paper presents novel techniques to aid the visual understanding of superfluid vortices based on the state-of-the-art non-linear Klein-Gordon equation, which evolves a complex scalar field, giving rise to special vortex lattice\/ring structures with dynamic vortex formation, reconnection, and Kelvin waves, etc. By formulating a numerical model with theoretical physicists in superfluid research, we obtain high-quality superfluid flow data sets without noise-like waves, suitable for vortex visualization. By further exploring superfluid vortex properties, we develop a new vortex identification and visualization method: a novel mechanism with velocity circulation to overcome phase singularity and an orthogonal-plane strategy to avoid ambiguity. Hence, our visualizations can help reveal various superfluid vortex structures and enable domain experts for related visual analysis, such as the steady vortex lattice\/ring structures, dynamic vortex string interactions with reconnections and energy radiations, where the famous Kelvin waves and decaying vortex tangle were clearly observed. These visualizations have assisted physicists to verify the superfluid model, and further explore its dynamic behavior more intuitively.","Authors":"Y. Guo; X. Liu; C. Xiong; X. Xu; C. Fu","DOI":"10.1109\/TVCG.2017.2719684","Keywords":"Superfluid dynamics;vortex structure;visual analysis","Keywords_Processed":"superfluid dynamic;visual analysis;vortex structure","Title":"Towards High-Quality Visualization of Superfluid Vortices","Labels":null,"Keyword_Vector":[0.1591741104,-0.0870822289,0.2789150236,-0.0562012235,0.0053827692,-0.0558530819,0.0694332399,-0.0225286026,0.1208860343,0.0903379723,-0.0448143894,0.1138847159,0.0524239658,-0.0185938056,0.0041161046,-0.0827564442,-0.0450796318,-0.0496999328,0.0924824066,-0.0004333827,0.0279386935,-0.1409598133,0.0055479435,-0.0548428326],"Abstract_Vector":[0.1439875006,0.0102247479,-0.0767641835,-0.0180596898,0.1051206444,0.0216236565,0.1090460603,-0.1042795786,-0.006704777,-0.0410081,-0.052566792,-0.0788581872,0.1193756302,0.0176204248,0.1056806309,0.0430364867,-0.0094269476,0.0635452797,-0.0962028129,0.0029591302,-0.0165934992,-0.0402069652,-0.0523906999,0.0067497989,-0.1443553146,0.1666334541,-0.0007453752,0.0794499472,0.1127886458,0.2142195486,-0.2003625837,0.0726758965,-0.2180948174,0.0892750889,0.1530669516,-0.0412166753,-0.0602779828,0.0036657032,-0.1643944937,0.2135220968,0.0541351041,0.0905119473,-0.1032075138,-0.0337083632,0.0171182408,0.0510153078,-0.1304942468,0.1757923872,-0.0031008994,0.0631138499,0.1456126668,0.1096381601,0.0281870982,-0.0089787784,-0.0373242511,0.0014150963,-0.0582691132,-0.0697234876,0.0059858226,-0.063884494,0.0344090922,-0.0555965498,-0.0318195502,-0.0023860134,0.0132153142,-0.0976688498,-0.012764969,-0.0443476637,0.0356437735,0.0202810017,0.0599672422,-0.0292812226,-0.0869121301,-0.0204203181,0.069479619,0.0799291972]},"193":{"Abstract":"Results of planetary mapping are often shared openly for use in scientific research and mission planning. In its raw format, however, the data is not accessible to non-experts due to the difficulty in grasping the context and the intricate acquisition process. We present work on tailoring and integration of multiple data processing and visualization methods to interactively contextualize geospatial surface data of celestial bodies for use in science communication. As our approach handles dynamic data sources, streamed from online repositories, we are significantly shortening the time between discovery and dissemination of data and results. We describe the image acquisition pipeline, the pre-processing steps to derive a 2.5D terrain, and a chunked level-of-detail, out-of-core rendering approach to enable interactive exploration of global maps and high-resolution digital terrain models. The results are demonstrated for three different celestial bodies. The first case addresses high-resolution map data on the surface of Mars. A second case is showing dynamic processes, such as concurrent weather conditions on Earth that require temporal datasets. As a final example we use data from the New Horizons spacecraft which acquired images during a single flyby of Pluto. We visualize the acquisition process as well as the resulting surface data. Our work has been implemented in the OpenSpace software [8], which enables interactive presentations in a range of environments such as immersive dome theaters, interactive touch tables, and virtual reality headsets.","Authors":"K. Bladin; E. Axelsson; E. Broberg; C. Emmart; P. Ljung; A. Bock; A. Ynnerman","DOI":"10.1109\/TVCG.2017.2743958","Keywords":"Astronomical visualization;globe rendering;public dissemination;science communication;space mission visualization","Keywords_Processed":"globe rendering;public dissemination;science communication;space mission visualization;astronomical visualization","Title":"Globe Browsing: Contextualized Spatio-Temporal Planetary Surface Visualization","Labels":null,"Keyword_Vector":[0.2012689854,-0.0644951821,-0.1713781355,0.1192030445,0.0507143474,0.0486400993,-0.0306018184,-0.0338856155,0.0813642655,0.0503801428,0.0299480624,-0.03666308,0.0197849313,0.0417925145,-0.0138579995,-0.0683383029,-0.0662357962,-0.0006719677,-0.0702228884,-0.0220844836,-0.0052338861,0.1367641386,0.0001131582,-0.0035895574],"Abstract_Vector":[0.3227550022,0.0183832226,-0.0226026819,-0.110301616,-0.0590805854,0.113995235,-0.0517871183,0.0571256497,-0.1003195671,0.0394963729,0.001463315,0.0722804982,-0.1051482686,0.0656982032,0.070231281,0.0303451933,-0.018468644,0.0395278861,0.0280152032,-0.0790060016,-0.0112340586,0.0546772512,-0.0923996979,0.0474608746,-0.0358710751,0.0708658944,-0.0446659602,-0.0559650188,0.0541662713,0.0729202667,0.1627533939,-0.1281355885,0.0301981865,-0.0695421122,0.1141616626,-0.0598112715,-0.0914223064,0.1007098615,0.0249640943,-0.0099152782,-0.1189212769,-0.0955615593,-0.0930748683,0.0120678379,0.0006073462,-0.0697753658,-0.0613391538,0.0160796077,0.0132180784,0.0816334673,0.1494522251,0.0019279745,-0.0645001999,0.0677836907,-0.055698939,-0.1169620072,-0.0621529013,0.0094984594,-0.0755297208,-0.0766079229,-0.0263349919,-0.0419392556,0.0316087578,0.0333842733,0.0331069394,-0.0698202913,0.1028954239,-0.0664886043,0.0094518965,-0.0449596708,0.0566391724,-0.0292054466,0.0411949593,0.0144690422,-0.014922565,-0.0870041113]},"194":{"Abstract":"Constructing distributed representations for words through neural language models and using the resulting vector spaces for analysis has become a crucial component of natural language processing (NLP). However, despite their widespread application, little is known about the structure and properties of these spaces. To gain insights into the relationship between words, the NLP community has begun to adapt high-dimensional visualization techniques. In particular, researchers commonly use t-distributed stochastic neighbor embeddings (t-SNE) and principal component analysis (PCA) to create two-dimensional embeddings for assessing the overall structure and exploring linear relationships (e.g., word analogies), respectively. Unfortunately, these techniques often produce mediocre or even misleading results and cannot address domain-specific visualization challenges that are crucial for understanding semantic relationships in word embeddings. Here, we introduce new embedding techniques for visualizing semantic and syntactic analogies, and the corresponding tests to determine whether the resulting views capture salient structures. Additionally, we introduce two novel views for a comprehensive study of analogy relationships. Finally, we augment t-SNE embeddings to convey uncertainty information in order to allow a reliable interpretation. Combined, the different views address a number of domain-specific tasks difficult to solve with existing tools.","Authors":"S. Liu; P. Bremer; J. J. Thiagarajan; V. Srikumar; B. Wang; Y. Livnat; V. Pascucci","DOI":"10.1109\/TVCG.2017.2745141","Keywords":"Natural Language Processing;Word Embedding;High-Dimensional Data","Keywords_Processed":"word embedding;high dimensional data;natural language processing","Title":"Visual Exploration of Semantic Relationships in Neural Word Embeddings","Labels":null,"Keyword_Vector":[0.0612180166,-0.03716485,0.0376798101,-0.0111803639,0.1325601789,-0.0364616691,0.0125115114,0.0156528965,0.0102683044,-0.0095574038,0.0732832487,-0.1057807586,0.0325922865,0.0773231,-0.0309263168,0.3367640488,0.1374531116,0.1194310004,0.1718641779,-0.1431761142,-0.2100309434,0.044701079,0.0345512402,-0.0837891503],"Abstract_Vector":[0.2336359554,-0.0498712615,0.0249712104,0.0124895028,0.0948591043,-0.0198147437,0.0527006458,-0.0120085675,0.0318273337,-0.0070426175,0.0482974806,0.0082868046,-0.0405878627,-0.0582821761,-0.0065839015,-0.0335872086,0.0573754031,-0.0101053095,0.0721383677,0.0402745639,0.0421815621,0.0131761046,-0.0044055815,0.0524596892,-0.0068852395,-0.0318796877,0.130174918,-0.0244378934,0.1990894859,-0.0220680191,-0.0014477406,0.043225074,-0.175394635,0.112564828,-0.0636530953,0.1357253609,0.0784056862,0.0977237729,0.0362961028,0.0528663681,-0.1284335387,-0.0032361625,0.0328272705,0.0838772205,0.0010241333,0.0323743291,-0.0482626365,0.0871616887,0.0088998021,0.0073172499,-0.0887041576,-0.0592434281,-0.0562205382,-0.0465420109,0.1205446308,0.0647117305,-0.0899899062,0.0197760125,-0.1294279649,0.0902698692,-0.0298543934,0.0123321659,0.0725394984,-0.1439512828,0.0099301547,0.0606045077,0.0471818832,0.0282559087,-0.0229854909,-0.049969848,-0.0665829211,0.0581464754,-0.0429763981,0.0341907191,0.1913947296,-0.0444827075]},"195":{"Abstract":"Spatial time series is a common type of data dealt with in many domains, such as economic statistics and environmental science. There have been many studies focusing on finding and analyzing various kinds of events in time series; the term `event' refers to significant changes or occurrences of particular patterns formed by consecutive attribute values. We focus on a further step in event analysis: discover temporal relationship patterns between event locations, i.e., repeated cases when there is a specific temporal relationship (same time, before, or after) between events occurring at two locations. This can provide important clues for understanding the formation and spreading mechanisms of events and interdependencies among spatial locations. We propose a visual exploration framework COPE (Co-Occurrence Pattern Exploration), which allows users to extract events of interest from data and detect various co-occurrence patterns among them. Case studies and expert reviews were conducted to verify the effectiveness and scalability of COPE using two real-world datasets.","Authors":"J. Li; S. Chen; K. Zhang; G. Andrienko; N. Andrienko","DOI":"10.1109\/TVCG.2018.2851227","Keywords":"Co-occurrence patterns;spatiotemporal visualization;spatial time series;visual analytics","Keywords_Processed":"co occurrence pattern;spatial time series;visual analytic;spatiotemporal visualization","Title":"COPE: Interactive Exploration of Co-Occurrence Patterns in Spatial Time Series","Labels":null,"Keyword_Vector":[0.2562995984,-0.035047162,0.1858497827,0.117631551,-0.1613486936,-0.090790361,-0.0831713602,0.0874905577,0.1103161919,-0.0184787286,0.0763086037,-0.1879342624,-0.2323649607,-0.1948751562,0.0513439058,0.0193232438,-0.0935506002,0.0509772013,0.0680292607,-0.0305592799,0.0348392921,0.0869180255,0.0377231786,0.0739003469],"Abstract_Vector":[0.2287265117,-0.162670611,-0.0591335123,-0.0853656927,-0.0506957945,-0.140794423,0.0881423128,-0.1523628019,-0.0421061952,0.168721788,-0.0141073082,0.1348080738,-0.0362596984,-0.0110325606,0.2529265238,0.1496737915,-0.0027019186,-0.2210342967,0.045671559,0.0606935122,-0.0702085442,-0.1493962949,0.1504705514,-0.0259441469,0.0512347059,-0.0798846708,0.0998610754,-0.1736061158,-0.1042223525,0.0144197183,-0.0724510543,0.0187437079,-0.027639163,-0.1074876709,0.0417669462,0.0245501717,0.0072493295,-0.0387571972,0.0122411088,0.0331377515,0.0343404162,0.1141257016,0.0024951835,-0.0070967891,-0.0202238804,-0.0017345445,-0.0323271135,-0.0523949124,-0.012820466,0.0063437724,-0.0269893663,-0.0311619145,-0.1148346283,-0.0759025067,0.0367380437,0.0855650361,-0.1185844908,0.0930824638,0.0436980442,-0.0556809065,0.0782153788,0.1075403113,-0.0080938229,0.0964171309,-0.0368675778,0.0463151331,0.0757993128,0.0644422555,-0.0068654386,0.0038388946,-0.0731648838,0.002272148,0.0548528154,-0.0166959152,0.0241688199,-0.0983662607]},"196":{"Abstract":"Physical visualizations, or data physicalizations, encode data in attributes of physical shapes. Despite a considerable body of work on visual variables, \u201cphysical variables\u201d remain poorly understood. One of them is physical size. A difficulty for solid elements is that \u201csize\u201d is ambiguous - it can refer to either length\/diameter, surface, or volume. Thus, it is unclear for designers of physicalizations how to effectively encode quantities in physical size. To investigate, we ran an experiment where participants estimated ratios between quantities represented by solid bars and spheres. Our results suggest that solid bars are compared based on their length, consistent with previous findings for 2D and 3D bars on flat media. But for spheres, participants' estimates are rather proportional to their surface. Depending on the estimation method used, judgments are rather consistent across participants, thus the use of perceptually-optimized size scales seems possible. We conclude by discussing implications for the design of data physicalizations and the need for more empirical studies on physical variables.","Authors":"Y. Jansen; K. Hornb\u00e6k","DOI":"10.1109\/TVCG.2015.2467951","Keywords":"Data physicalization;Physical visualization;Psychophysics;Experiment;Data physicalization;physical visualization;psychophysics;experiment;physical variable","Keywords_Processed":"experiment;psychophysic;physical visualization;physical variable;datum physicalization","Title":"A Psychophysical Investigation of Size as a Physical Variable","Labels":null,"Keyword_Vector":[0.1581155443,-0.0756464829,-0.0483399947,-0.082986942,0.068441476,0.0599408352,-0.0233971792,0.0602422303,-0.0725405821,-0.0405874414,0.0692667071,-0.117153295,-0.078379016,0.0042092207,-0.0352049842,-0.054402003,-0.0373796885,0.0727472095,-0.0218188328,0.0117184874,0.0472503359,0.0644026301,0.032413472,0.0811809198],"Abstract_Vector":[0.188849004,0.0255806599,0.0650102775,-0.0661571653,-0.073888456,0.1790985194,-0.0148669808,-0.0499449768,0.0998136083,0.0380632156,-0.0325792241,-0.0636349898,-0.0272739065,0.1373409563,0.0443182854,-0.0407472353,0.0445455147,0.0023386963,-0.1060335284,0.0564881345,-0.1213227858,0.0618987249,-0.0090822321,0.0669303379,0.0399556954,-0.1724786332,-0.0461113678,-0.0776757613,-0.0073321564,0.0047926303,0.1133994847,-0.0283963208,0.1152601364,0.1369371836,0.0434179278,0.0941390246,-0.0127880053,-0.0770345095,-0.1392109605,-0.0517046695,-0.0453106992,0.1483552411,-0.0722158041,-0.0220123922,0.0987943166,-0.0954765879,-0.037864293,-0.0099133275,-0.2031585198,0.0193251425,-0.0754789876,-0.033729784,0.2383283634,-0.1577683109,-0.0045177499,-0.1599228448,0.0166245945,0.0521768488,0.0201546097,-0.0637124214,0.0105430274,0.0189588608,0.0386003465,-0.054957581,-0.0530037587,0.0028986083,-0.0515836208,-0.0205806796,0.0292377728,0.045678453,0.0188922608,0.0481870084,0.0275318472,0.0365805774,0.0147960525,0.0147059133]},"197":{"Abstract":"Data with multiple probabilistic labels are common in many situations. For example, a movie may be associated with multiple genres with different levels of confidence. Despite their ubiquity, the problem of visualizing probabilistic labels has not been adequately addressed. Existing approaches often either discard the probabilistic information, or map the data to a low-dimensional subspace where their associations with original labels are obscured. In this paper, we propose a novel visual technique, UnTangle Map, for visualizing probabilistic multi-labels. In our proposed visualization, data items are placed inside a web of connected triangles, with labels assigned to the triangle vertices such that nearby labels are more relevant to each other. The positions of the data items are determined based on the probabilistic associations between items and labels. UnTangle Map provides both (a) an automatic label placement algorithm, and (b) adaptive interactions that allow users to control the label positioning for different information needs. Our work makes a unique contribution by providing an effective way to investigate the relationship between data items and their probabilistic labels, as well as the relationships among labels. Our user study suggests that the visualization effectively helps users discover emergent patterns and compare the nuances of probabilistic information in the data labels.","Authors":"N. Cao; Y. Lin; D. Gotz","DOI":"10.1109\/TVCG.2015.2424878","Keywords":"Visualization;Multidimensional Visualization;Probability Vector;Visualization;multidimensional visualization;probability vector","Keywords_Processed":"visualization;probability vector;multidimensional visualization","Title":"UnTangle Map: Visual Analysis of Probabilistic Multi-Label Data","Labels":null,"Keyword_Vector":[0.3236239966,-0.1294956565,-0.2618181101,-0.0708382072,-0.0928170227,-0.0184820662,-0.0266129564,-0.0164415837,-0.0286620642,0.0710534503,0.0871667377,-0.1620075922,0.1107454292,0.030053581,-0.1062736985,0.073922225,0.0280125995,-0.1028720312,-0.0590264332,0.0846944487,-0.0103111247,-0.0438369071,-0.0391740494,0.1148285009],"Abstract_Vector":[0.2034034222,-0.0831934206,-0.0351559388,-0.1132471603,-0.0548383969,-0.0302219746,-0.1091722145,0.2353826863,0.1626362589,-0.1680538645,0.1870105636,0.1937295609,0.1934885327,0.0222416023,0.2899390749,-0.1857528235,0.2133519603,-0.0004475136,-0.0164709689,0.2012607715,-0.0302516743,0.0452590994,0.0581888318,0.1250903759,-0.1258707131,0.0489610605,0.0807832974,0.0249960331,-0.0330813404,-0.0776910168,0.1092337104,-0.0361607734,-0.1049589882,-0.036993846,-0.001543322,-0.0459570966,-0.0710998469,-0.0101261378,-0.0458810414,-0.0749984407,-0.0158841671,0.0057402917,0.0590271945,-0.0408460817,-0.0565856101,0.0035515627,0.0194487583,0.0140485724,0.0181414422,-0.0873230379,-0.0306967582,-0.0363546274,-0.0371829956,-0.0252735065,-0.022713082,0.0809892448,0.0596063478,0.0034657311,0.0492024302,0.0203081272,-0.0169180206,-0.0126044763,-0.0296890735,-0.022440462,0.0174563233,0.0076745347,0.0823406799,0.0240305475,-0.0184598514,0.0485668334,0.045686431,-0.0297435674,0.0125486904,0.0066141478,0.0427362609,0.0154585144]},"198":{"Abstract":"We define the concept of Dynamic Passive Haptic Feedback (DPHF) for virtual reality by introducing the weight-shifting physical DPHF proxy object Shifty. This concept combines actuators known from active haptics and physical proxies known from passive haptics to construct proxies that automatically adapt their passive haptic feedback. We describe the concept behind our ungrounded weight-shifting DPHF proxy Shifty and the implementation of our prototype. We then investigate how Shifty can, by automatically changing its internal weight distribution, enhance the user's perception of virtual objects interacted with in two experiments. In a first experiment, we show that Shifty can enhance the perception of virtual objects changing in shape, especially in length and thickness. Here, Shifty was shown to increase the user's fun and perceived realism significantly, compared to an equivalent passive haptic proxy. In a second experiment, Shifty is used to pick up virtual objects of different virtual weights. The results show that Shifty enhances the perception of weight and thus the perceived realism by adapting its kinesthetic feedback to the picked-up virtual object. In the same experiment, we additionally show that specific combinations of haptic, visual and auditory feedback during the pick-up interaction help to compensate for visual-haptic mismatch perceived during the shifting process.","Authors":"A. Zenner; A. Kr\u00fcger","DOI":"10.1109\/TVCG.2017.2656978","Keywords":"Dynamic passive haptic feedback;input devices;virtual reality;haptics;perception","Keywords_Processed":"dynamic passive haptic feedback;haptic;virtual reality;perception;input device","Title":"Shifty: A Weight-Shifting Dynamic Passive Haptic Proxy to Enhance Object Perception in Virtual Reality","Labels":null,"Keyword_Vector":[0.150459772,0.4246533923,0.0506921314,-0.0887650046,0.037876513,0.0944981943,-0.0392681528,-0.0672545307,-0.0146033214,0.0937893389,-0.0206168796,0.1180179214,0.023346059,0.0397369664,0.0554396,0.0671266452,0.0318734567,-0.0963152162,0.0857739782,0.0441411123,0.0217902425,-0.048274747,0.0535228482,0.033213722],"Abstract_Vector":[0.1405780424,0.1314845672,0.2282692462,0.0345616514,-0.1369571193,-0.0057084249,-0.0265807851,-0.0632813011,0.1006474565,0.1528773322,-0.0028315818,-0.067007809,-0.0486463311,0.1952220359,0.0678147796,-0.1131689744,0.0293100142,0.0510836592,-0.0958227061,-0.0879117089,0.0765374273,-0.0017878661,0.0180588886,-0.0596144933,-0.0059000769,0.0164519547,0.007247111,-0.0306945472,-0.0523232262,-0.0756405471,0.0074029117,0.007046869,-0.0828681701,-0.0161896916,0.0047005884,-0.0127282402,0.0415349756,-0.0437916857,-0.0983281486,0.0416106332,-0.0447601918,-0.1082778656,0.0570055369,-0.1076044566,-0.1289796155,-0.0106311603,0.0305540559,0.0550404978,0.0059834871,0.1158900547,-0.0384995946,0.0854191131,-0.0413268387,0.030024519,0.1269952559,0.0352710994,-0.0396556664,0.0363067119,-0.0510529645,-0.021663858,-0.1787245446,-0.0042622539,-0.0503883061,0.0453223522,-0.0354749012,0.032362862,-0.0882904652,0.0112863468,0.0325528265,-0.0175765069,-0.0983009929,0.0553252704,-0.0565597388,0.0126897998,0.0790906348,-0.0657221058]},"199":{"Abstract":"Surface remeshing is a key component in many geometry processing applications. The typical goal consists in finding a mesh that is (1) geometrically faithful to the original geometry, (2) as coarse as possible to obtain a low-complexity representation and (3) free of bad elements that would hamper the desired application (e.g.,\u00a0the minimum interior angle is above an application-dependent threshold). Our algorithm is designed to address all three optimization goals simultaneously by targeting prescribed bounds on approximation error  $\\delta$ , minimal interior angle $\\theta$  and maximum mesh complexity $N$  (number of vertices). The approximation error bound $\\delta$  is a hard constraint, while the other two criteria are modeled as optimization goals to guarantee feasibility. Our optimization framework applies carefully prioritized local operators in order to greedily search for the coarsest mesh with minimal interior angle above $\\theta$  and approximation error bounded by $\\delta$ . Fast runtime is enabled by a local approximation error estimation, while implicit feature preservation is obtained by specifically designed vertex relocation operators. Experiments show that for reasonable angle bounds ( $\\theta \\leq 35^\\circ$ ) our approach delivers high-quality meshes with implicitly preserved features (no tagging required) and better balances between geometric fidelity, mesh complexity and element quality than the state-of-the-art.","Authors":"K. Hu; D. Yan; D. Bommes; P. Alliez; B. Benes","DOI":"10.1109\/TVCG.2016.2632720","Keywords":"Surface remeshing;error-bounded;feature preserving;minimal angle improvement;saliency function","Keywords_Processed":"surface remesh;feature preserving;saliency function;error bound;minimal angle improvement","Title":"Error-Bounded and Feature Preserving Surface Remeshing with Minimal Angle Improvement","Labels":null,"Keyword_Vector":[0.020726211,-0.0059431673,0.0048437749,0.0067487823,0.0210001307,-0.0640633443,0.0812709313,-0.0432798601,0.0043867897,-0.0067336489,0.0572725725,0.0968581353,-0.0338661201,0.0150120676,0.0242931201,0.0034223454,-0.052415131,-0.0246386575,0.0328917147,-0.0697325744,0.0062902694,-0.0187882742,-0.0755763251,-0.0005783333],"Abstract_Vector":[0.1569765093,0.1195778641,-0.0953124043,-0.0163272924,0.0790925514,0.0341779397,-0.0936565473,-0.0398191143,0.0169865478,-0.0045498003,-0.099944822,-0.0112274395,0.041895557,-0.0710144035,0.0197825739,-0.0691757371,-0.1508042393,0.1341137029,0.0251457497,-0.1231468516,-0.0182141935,0.0001331002,-0.0397023292,0.0932005658,0.1420480866,0.0786942356,0.0437382315,-0.0435855823,-0.0991010127,-0.1567468215,0.2154382316,0.104134855,-0.1405806445,-0.0799155123,0.058833383,0.1471031543,0.1351274617,-0.0256967145,0.1460796124,0.0262178639,-0.021430249,0.1061687026,-0.0040761845,-0.0639703945,0.1379645978,-0.0278699987,0.0567272542,0.0794843901,0.0114496135,-0.0923798385,-0.0064619708,0.045155415,-0.0153916126,-0.0101755788,0.0439700433,0.0783678853,-0.0675155178,-0.0372190148,-0.017551708,0.0009603705,0.1190292531,0.0335079415,-0.0420112466,-0.0073020303,-0.0528455884,-0.0208961528,-0.0657570257,0.0234694016,-0.0649422305,0.0849360965,0.0028182326,-0.0824460095,-0.0603275135,-0.0564916541,-0.0794915239,-0.0294456474]},"200":{"Abstract":"A wide variety of color schemes have been devised for mapping scalar data to color. We address the challenge of color-mapping multivariate data. While a number of methods can map low-dimensional data to color, for example, using bilinear or barycentric interpolation for two or three variables, these methods do not scale to higher data dimensions. Likewise, schemes that take a more artistic approach through color mixing and the like also face limits when it comes to the number of variables they can encode. Our approach does not have these limitations. It is data driven in that it determines a proper and consistent color map from first embedding the data samples into a circular interactive multivariate color mapping display (ICD) and then fusing this display with a convex (CIE HCL) color space. The variables (data attributes) are arranged in terms of their similarity and mapped to the ICD's boundary to control the embedding. Using this layout, the color of a multivariate data sample is then obtained via modified generalized barycentric coordinate interpolation of the map. The system we devised has facilities for contrast and feature enhancement, supports both regular and irregular grids, can deal with multi-field as well as multispectral data, and can produce heat maps, choropleth maps, and diagrams such as scatterplots.","Authors":"S. Cheng; W. Xu; K. Mueller","DOI":"10.1109\/TVCG.2018.2808489","Keywords":"Multivariate data;color mapping;color space;high dimensional data;pseudo coloring","Keywords_Processed":"multivariate datum;pseudo color;color mapping;high dimensional datum;color space","Title":"ColorMapND: A Data-Driven Approach and Tool for Mapping Multivariate Data to Color","Labels":null,"Keyword_Vector":[0.1284403074,-0.073432773,0.0723024195,-0.0932754062,0.220938676,0.1235620114,0.0260669361,0.0148765458,-0.2514020865,-0.0379903895,0.1008198825,-0.0024492444,-0.1477893663,0.0095660376,-0.10380141,0.1610483068,0.0153606944,0.1706751069,0.0887197699,-0.0981306252,-0.109876746,0.0871616143,-0.0588309298,0.0049426737],"Abstract_Vector":[0.2308790196,-0.0222888387,-0.1294986111,-0.1412795472,-0.1884439275,0.2895007097,0.090932658,0.2311124295,0.1727792174,0.1037922421,0.1669824581,-0.0433665874,0.0884039424,-0.0311143267,-0.0864931883,0.2802111778,-0.0481577981,-0.1827253913,0.0249294165,0.0104828376,0.1373091492,0.1249494044,-0.0604187289,0.0794082671,0.0633923225,0.1466559034,-0.0360931704,-0.0626661415,0.052363447,-0.0070366844,0.0957004091,-0.0266798396,0.0487236435,-0.0253897141,-0.0233325596,0.0132759807,-0.011754755,0.0363024381,0.042419456,0.0448035615,0.0023511558,0.0173073363,0.0485565488,-0.0447957511,-0.0038445769,0.0169965774,-0.0023673336,0.0732112227,0.0079838735,-0.0271544983,-0.028968357,0.0182382122,0.0266914723,-0.0282159612,0.0430642808,-0.0083921208,-0.0093775199,-0.0307281288,-0.0227116819,0.0451548375,-0.0548019869,-0.0526692683,0.0120022277,0.0042227803,0.0257135888,-0.0231384593,0.0463315388,-0.0015190093,-0.0131828303,0.0585827836,0.0390715687,0.0820017115,-0.0138379885,-0.0349905973,0.028290529,-0.0227757923]},"201":{"Abstract":"Recent advances in data acquisition produce volume data of very high resolution and large size, such as terabyte-sized microscopy volumes. These data often contain many fine and intricate structures, which pose huge challenges for volume rendering, and make it particularly important to efficiently skip empty space. This paper addresses two major challenges: (1) The complexity of large volumes containing fine structures often leads to highly fragmented space subdivisions that make empty regions hard to skip efficiently. (2) The classification of space into empty and non-empty regions changes frequently, because the user or the evaluation of an interactive query activate a different set of objects, which makes it unfeasible to pre-compute a well-adapted space subdivision. We describe the novel SparseLeap method for efficient empty space skipping in very large volumes, even around fine structures. The main performance characteristic of SparseLeap is that it moves the major cost of empty space skipping out of the ray-casting stage. We achieve this via a hybrid strategy that balances the computational load between determining empty ray segments in a rasterization (object-order) stage, and sampling non-empty volume data in the ray-casting (image-order) stage. Before ray-casting, we exploit the fast hardware rasterization of GPUs to create a ray segment list for each pixel, which identifies non-empty regions along the ray. The ray-casting stage then leaps over empty space without hierarchy traversal. Ray segment lists are created by rasterizing a set of fine-grained, view-independent bounding boxes. Frame coherence is exploited by re-using the same bounding boxes unless the set of active objects changes. We show that SparseLeap scales better to large, sparse data than standard octree empty space skipping.","Authors":"M. Hadwiger; A. K. Al-Awami; J. Beyer; M. Agus; H. Pfister","DOI":"10.1109\/TVCG.2017.2744238","Keywords":"Empty Space Skipping;Volume Rendering;Segmented Volume Data;Hybrid Image\/Object-Order Approaches","Keywords_Processed":"hybrid image object order approach;segmented volume data;volume rendering;empty space skipping","Title":"SparseLeap: Efficient Empty Space Skipping for Large-Scale Volume Rendering","Labels":null,"Keyword_Vector":[0.1012413278,-0.0383215556,-0.0439206846,0.3841448843,0.3372731156,0.158100969,0.0092995019,-0.1160895525,0.2242684301,0.0737236896,0.086353565,0.0080363732,0.0109108278,0.0199008743,-0.001644616,0.0796003038,0.0281711759,0.1574584423,-0.0747663609,0.0312891911,-0.0722948289,-0.0110748474,-0.0038430847,-0.0061116649],"Abstract_Vector":[0.1895603572,0.0671776602,-0.0638316588,0.0458036957,-0.0124950964,-0.0304004433,0.0251952077,0.1822445919,-0.1385165757,-0.0736842908,-0.081995513,0.0200415546,0.0188581879,0.043891848,0.0935531015,-0.052254597,0.0968100008,-0.0709151134,-0.0019636806,-0.0238806434,-0.0292190978,0.2271879375,-0.0042008324,-0.0739344924,-0.0435435972,-0.1692755995,0.0045907369,-0.041431609,0.0663246301,-0.0253069419,0.0669907449,-0.0647477595,-0.0511783949,0.052710736,-0.0361197287,0.0149226916,0.1528748528,-0.0832442197,0.0494522042,0.2152691729,0.1160870132,0.0025808867,0.0556816455,0.0234327891,-0.0297142581,0.0638525227,-0.0058707156,-0.0036360937,-0.0363903753,0.0411122683,0.0557899458,-0.0198590977,0.0985549819,0.0568186055,0.0335947749,-0.0849453018,0.0171790412,0.0668603847,-0.0113969732,0.0312496733,-0.0455634514,0.0216549644,0.08243213,0.088242592,0.0253348003,-0.0515812821,-0.0181621033,0.0504572934,0.0229879279,0.0677418125,-0.0261836203,-0.0410884202,0.0572681576,-0.1683048699,0.0363344056,-0.0803271494]},"202":{"Abstract":"We introduce a shape synthesis approach especially for functional hybrid creation that can be potentially used by a human operator under a certain pose. Shape synthesis by reusing parts in existing models has been an active research topic in recent years. However, how to combine models across different categories to design multi-function objects remains challenging, since there is no natural correspondence between models across different categories. We tackle this problem by introducing a human pose to describe object affordance which establishes a bridge between cross-class objects for composite design. Specifically, our approach first identifies groups of candidate shapes which provide affordances desired by an input human pose, and then recombines them as well-connected composite models. Users may control the design process by manipulating the input pose, or optionally specifying one or more desired categories. We also extend our approach to be used by a single operator with multiple poses or by multiple human operators. We show that our approach enables easy creation of nontrivial, interesting synthesized models.","Authors":"Q. Fu; X. Chen; X. Su; H. Fu","DOI":"10.1109\/TVCG.2017.2739159","Keywords":"3D modeling;shape synthesis;pose-inspired;functional hybrid","Keywords_Processed":"shape synthesis;functional hybrid;pose inspire;3d modeling","Title":"Pose-Inspired Shape Synthesis and Functional Hybrid","Labels":null,"Keyword_Vector":[0.0536950624,0.0336280948,0.0006875061,0.1188242882,0.0450289132,-0.0882071507,0.0591001772,-0.0493641011,-0.0308320057,-0.0592918837,-0.0704345483,0.0493774917,-0.1017896784,0.1410575407,-0.0316032232,0.1610476747,0.1519434748,-0.1233619477,-0.0984268493,0.1149901497,0.2027389557,-0.0395101605,0.1517076595,0.1312173556],"Abstract_Vector":[0.2347022936,0.1508178868,-0.0323418066,-0.0750611153,0.1698156814,-0.0351494964,-0.1815442338,-0.0501524396,0.1507629756,0.1056275782,0.0774133455,-0.0679259506,-0.0750253974,0.0863043391,-0.0475628862,-0.112643088,0.0885856033,-0.0693044851,-0.1488514888,-0.0013798503,-0.0396523737,-0.0995483957,-0.0635748541,0.0007071667,0.0243866721,-0.069838209,0.0066351456,0.089644824,0.0440299581,0.0544507303,-0.0125278511,-0.1443743807,-0.0082971568,0.0373701634,-0.039047822,-0.0172920605,0.0351282766,0.0116249894,0.1734247647,0.0960259515,0.0847043137,-0.0149811939,-0.0140639086,0.0497138704,0.0586759419,0.0231981894,0.2139735574,0.0211324254,-0.0729253557,-0.0409977173,0.000697122,0.119588114,0.0721478688,-0.0370153421,0.0502682669,0.00884869,-0.0412982337,0.0227356072,0.0355289248,-0.0023160996,0.0604932578,-0.0281369136,-0.0049888221,0.0474211021,-0.0217929423,0.0663121876,0.0118210476,-0.0207144709,0.0368226101,0.0001308113,0.1177540827,-0.0479455377,-0.0471052177,0.0553103746,0.0509871336,-0.0830014025]},"203":{"Abstract":"Realistic Rendering of thin transparent layers bounded by rough surfaces involves substantial expense of computation time to account for multiple internal reflections. Resorting to Monte Carlo rendering for such material is usually impractical since recursive importance sampling is inevitable. To reduce the burden of sampling for simulating subsurface scattering and hence improve rendering performance, we adapt the microfacet model to the material with a single thin layer by introducing the extended normal distribution function (ENDF), a new representation of this model, to express visually perceived roughness due to multiple bounces of reflections and refractions. With such a representation, both surface reflection and subsurface scattering can be treated in the same microfacet framework, and the sampling process can be reduced to only once for each bounce of scattering. We derive analytical expressions of the ENDF for several cases using joint spherical warping. We also show how to choose proper shadowing-masking and Fresnel terms to make the proposed bidirectional scattering distribution function (BSDF) model energy-conserving. Experiments demonstrate that our model can be easily incorporated into a Monte Carlo path tracer with little extra computational and storage overhead, enabling some real-time applications.","Authors":"J. Guo; J. Qian; Y. Guo; J. Pan","DOI":"10.1109\/TVCG.2016.2617872","Keywords":"Microfacet;layered material;normal distribution function;BSDF","Keywords_Processed":"microfacet;normal distribution function;bsdf;layer material","Title":"Rendering Thin Transparent Layers with Extended Normal Distribution Functions","Labels":null,"Keyword_Vector":[0.0106529836,-0.0036922617,-0.0114890779,0.0021276091,-0.0000053528,-0.0089729551,0.0058622154,-0.0216866267,-0.0192312719,-0.0310660519,-0.000961718,0.026367921,-0.0246791185,0.0334319887,0.0142143906,0.0230440136,0.0016687058,0.0513781285,0.0262509108,0.0173724101,-0.012758987,-0.0709113692,-0.0312121649,0.0282765572],"Abstract_Vector":[0.1657001374,0.1162940381,-0.0733286624,0.0204670604,0.0926067724,0.0184931452,-0.03852695,0.0022994582,0.0165342398,-0.0448719186,-0.050424971,0.1001895471,0.0097747465,0.1593690713,-0.0200516593,0.03187814,-0.0869666205,0.0468555269,0.0341126417,0.0261428538,-0.0550880027,0.0161600352,-0.0363459496,-0.0453627073,0.0760031158,0.0125030807,-0.019383034,-0.0950619067,-0.0541773416,0.0874084373,0.0623140479,-0.0303153574,-0.0112085973,0.055818304,-0.1201603061,-0.0976020165,0.0021728983,0.0467041267,-0.0008972543,-0.002634622,-0.0281854331,-0.0509854881,0.0050072454,0.0675993194,0.0359807644,-0.1046010254,-0.0301976671,0.0760011027,0.0061483328,0.0629254422,0.0061662225,0.0205320479,-0.087702553,-0.1112851333,0.0602141303,0.0117361853,-0.139173738,0.0349560935,0.0257832925,0.0434655966,0.0137746543,0.0385977754,-0.1206140857,-0.0523511626,-0.0836166574,0.0805523524,0.0110508526,-0.0969851503,0.0119889938,-0.1278145238,0.0047668614,0.0414172043,0.0592522285,0.0588623359,-0.0416695385,0.0164520792]},"204":{"Abstract":"Shape provides significant discriminating power in time series matching of visual or geometric data as required in many important applications in graphics and vision. The well established dynamic time warping (DTW) algorithm and its variants do this matching by determining a non-linear time mapping to minimise euclidean distances between corresponding time-warped points. However the shape of curves is not considered. In this paper, we present a new shape-aware algorithm which uses time and shape correspondence (TSC) at increasing levels of detail to define a similarity measure with an L0 norm to aggregate the results, making it robust to noise and missing data. The L0 norm is implicitly regularised using a shape-based error. Through extensive experiments we empirically show that our algorithm outperforms existing state of the art algorithms, works more effectively with high dimensional data, and handles noise and missing data better. We demonstrate its versatile applicability and comparative performance using a large in-house created gait data base, an action data base from Microsoft, exercise action data from a local company, a large public time series data base from University of California, Riverside and hand movement in quaternion stream data format.","Authors":"K. Mendhurwar; Q. Gu; S. Mudur; T. Popa","DOI":"10.1109\/TVCG.2017.2691322","Keywords":"Action recognition;gait authentication;shape awareness;time and shape correspondence;time series matching","Keywords_Processed":"time series matching;time and shape correspondence;action recognition;shape awareness;gait authentication","Title":"The Discriminative Power of Shape an Empirical Study in Time Series Matching","Labels":null,"Keyword_Vector":[0.057202043,0.0119853584,0.0658061294,0.0381112153,-0.0214368051,-0.0980304042,0.0763373931,-0.012818313,0.0462111169,0.1108081245,-0.0033080036,-0.0389168922,-0.2931664613,-0.0296289792,0.0876199621,0.1121089143,0.1768686828,-0.0501842065,-0.1248600317,0.0243225713,0.057321767,0.0003055314,0.0609008772,0.3213547894],"Abstract_Vector":[0.2962674216,-0.012881614,-0.149629164,-0.1106639184,-0.1538614876,0.0440112943,-0.088961955,-0.0053961716,-0.000985716,0.035330799,-0.0289809643,-0.0062531037,-0.1147047275,-0.0028211719,0.148146885,-0.1682070874,-0.1812992502,-0.1123217823,-0.138439252,-0.0527415055,-0.0957234536,0.0010521816,0.1466721704,0.0381297996,0.1349098186,0.1483823399,-0.0886751333,0.128543051,0.1099662724,0.0813289788,0.0352337271,-0.0553243027,0.0405320865,0.0015956411,-0.0345682121,0.0665160785,-0.0815877368,-0.0087383741,-0.0080161432,0.0256467518,0.0395391804,0.0045720155,0.0491531575,0.1052688051,-0.0076492693,0.0112051073,-0.0129315096,-0.0722582759,-0.0102619422,-0.0842709299,0.0258438599,-0.0279260258,-0.0016710101,0.019515615,-0.0278935154,-0.0707980133,-0.0375661179,0.0676340446,-0.0069514565,0.0083144028,-0.0380243342,0.0164241179,0.0208021695,-0.0126816199,0.0255554837,0.0141943032,0.0478847803,0.0576462257,0.0176865631,0.0618035112,-0.0289338438,-0.0395939622,-0.1035971494,-0.0959606963,-0.0271652188,-0.0525577147]},"205":{"Abstract":"Rendering in virtual reality (VR) requires substantial computational power to generate 90 frames per second at high resolution with good-quality antialiasing. The video data sent to a VR headset requires high bandwidth, achievable only on dedicated links. In this paper we explain how rendering requirements and transmission bandwidth can be reduced using a conceptually simple technique that integrates well with existing rendering pipelines. Every even-numbered frame is rendered at a lower resolution, and every odd-numbered frame is kept at high resolution but is modified in order to compensate for the previous loss of high spatial frequencies. When the frames are seen at a high frame rate, they are fused and perceived as high-resolution and high-frame-rate animation. The technique relies on the limited ability of the visual system to perceive high spatio-temporal frequencies. Despite its conceptual simplicity, correct execution of the technique requires a number of non-trivial steps: display photometric temporal response must be modeled, flicker and motion artifacts must be avoided, and the generated signal must not exceed the dynamic range of the display. Our experiments, performed on a high-frame-rate LCD monitor and OLED-based VR headsets, explore the parameter space of the proposed technique and demonstrate that its perceived quality is indistinguishable from full-resolution rendering. The technique is an attractive alternative to reprojection and resolution reduction of all frames.","Authors":"G. Denes; K. Maruszczyk; G. Ash; R. K. Mantiuk","DOI":"10.1109\/TVCG.2019.2898741","Keywords":"Temporal multiplexing;rendering;graphics;perception;virtual reality","Keywords_Processed":"temporal multiplexing;graphic;virtual reality;render;perception","Title":"Temporal Resolution Multiplexing: Exploiting the limitations of spatio-temporal vision for more efficient VR rendering","Labels":null,"Keyword_Vector":[0.1692824094,0.4401768184,0.0490515812,-0.1021583155,0.0331313581,0.0763344387,-0.0105800861,-0.1922959016,-0.0429134587,0.0322900928,0.0288915279,-0.0672550299,-0.0088471387,0.016573986,-0.0278829121,0.0786215487,0.0293291257,0.0513381259,0.1284144728,-0.065730852,0.0653221434,-0.1486688724,-0.0301866402,0.0000922828],"Abstract_Vector":[0.2228049054,0.1601992231,0.1068637626,0.0267011176,-0.1071958835,0.066387848,0.0146383296,0.076702079,-0.1469566251,-0.0048294637,-0.126856787,0.0559224883,-0.0394382935,0.025927234,0.0734723304,0.0254291316,-0.0139656554,0.0430138134,0.1792201419,-0.1257767062,-0.1139544274,0.0103473188,-0.0099699229,0.0484776253,0.0016106199,0.0689159773,-0.193582769,-0.000231982,0.0239466377,-0.0434878104,-0.0827632161,0.0806687969,-0.0799602393,-0.0301204791,-0.0834043772,-0.070985894,-0.0357191465,0.1830247972,0.0605684021,-0.0122924992,-0.0695723275,-0.0100370698,-0.182909588,0.1261634513,-0.0982651475,0.170292253,0.0005239705,0.0390551627,0.0463645973,0.0541889545,0.0571630335,0.0307445542,0.0747338237,-0.0979651377,0.1610712381,0.0189654203,0.1133745897,0.0013994241,-0.0176916571,-0.0270563783,0.0170025918,0.0233208828,-0.0399598945,-0.026912481,-0.0165991341,-0.0165308068,0.1394151694,-0.0012323841,-0.0058109371,0.0335528801,-0.010546226,0.0624749367,-0.012467336,0.0345347423,0.0956465968,0.008849168]},"206":{"Abstract":"Preattentive visual features such as hue or flickering can effectively draw attention to an object of interest - for instance, an important feature in a scientific visualization. These features appear to pop out and can be recognized by our visual system, independently from the number of distractors. Most cues do not take advantage of the fact that most humans have two eyes. In cases where binocular vision is applied, it is almost exclusively used to convey depth by exposing stereo pairs. We present Deadeye, a novel preattentive visualization technique based on presenting different stimuli to each eye. The target object is rendered for one eye only and is instantly detected by our visual system. In contrast to existing cues, Deadeye does not modify any visual properties of the target and, thus, is particularly suited for visualization applications. Our evaluation confirms that Deadeye is indeed perceived preattentively. We also explore a conjunction search based on our technique and show that, in contrast to 3D depth, the task cannot be processed in parallel.","Authors":"A. Krekhov; J. Kr\u00fcger","DOI":"10.1109\/TVCG.2018.2864498","Keywords":"Popout;preattentive vision;comparative visualization;dichoptic presentation","Keywords_Processed":"dichoptic presentation;popout;comparative visualization;preattentive vision","Title":"Deadeye: A Novel Preattentive Visualization Technique Based on Dichoptic Presentation","Labels":null,"Keyword_Vector":[0.1087696478,-0.0236473982,-0.0989782863,-0.0055661817,-0.0542830921,0.0068708999,-0.0070167144,-0.0158854804,-0.0143124456,-0.0136392307,-0.0051455208,-0.0148077268,0.0109804082,-0.0192111693,-0.0274444129,0.0326264902,-0.0917002913,-0.0790393877,-0.0050364981,0.006890156,-0.0606628786,-0.0240087986,-0.0755647902,-0.0179270102],"Abstract_Vector":[0.2343442519,0.0499350063,0.1047819005,0.0569484718,0.0838470909,0.08633631,0.0117896639,-0.0454268767,-0.0083331181,0.1409087864,-0.0804133341,-0.0947930282,0.136209234,-0.1177075517,0.0681441314,-0.0420944574,-0.0356840407,0.0703697335,0.0296050225,-0.0518235982,0.0302765118,0.0228920335,-0.0527056274,0.0808918522,-0.1486875259,-0.0532715596,-0.0302340834,-0.0429779648,-0.0118907721,-0.0440634165,-0.1047035749,-0.0209218669,0.0512785701,-0.0250610046,-0.1501495364,0.0948044661,0.0153754004,0.0438178041,-0.065892737,0.0129520331,0.0731129671,-0.1264470006,0.140431237,-0.0382562344,-0.0439211138,-0.0112203509,0.0174631683,-0.0523558833,0.0137275578,-0.1411298099,-0.0604598118,-0.0985955327,-0.0849856064,0.0218007006,-0.0565867097,0.0235137595,-0.0028044964,-0.1283981892,0.038025118,0.0681100837,0.0999300287,-0.0353665257,0.0016476985,-0.0095880794,0.0130042406,0.0381320038,0.0712316829,0.0544159073,0.0523982649,0.0486504055,0.1264151374,0.1644282395,-0.1043350277,0.1522046971,0.0369231815,-0.002124749]},"207":{"Abstract":"In this work, we propose an original scheme for generic content-based retrieval of marker-based motion capture data. It works on motion capture data of arbitrary subject types and arbitrary marker attachment and labelling conventions. Specifically, we propose a novel motion signature to statistically describe both the high-level and the low-level morphological and kinematic characteristics of a motion capture sequence, and conduct the content-based retrieval by computing and ordering the motion signature distance between the query and every item in the database. The distance between two motion signatures is computed by a weighted sum of differences in separate features contained in them. For maximum retrieval performance, we propose a method to pre-learn an optimal set of weights for each type of motion in the database through biased discriminant analysis, and adaptively choose a good set of weights for any given query at the run time. Excellence of the proposed scheme is experimentally demonstrated on various data sets and performance metrics.","Authors":"N. Lv; Z. Jiang; Y. Huang; X. Meng; G. Meenakshisundaram; J. Peng","DOI":"10.1109\/TVCG.2017.2702620","Keywords":"Motion capture;content-based retrieval;minimal motion spanning tree;motion signature;biased discriminant analysis","Keywords_Processed":"motion signature;content base retrieval;minimal motion span tree;bias discriminant analysis;motion capture","Title":"Generic Content-Based Retrieval of Marker-Based Motion Capture Data","Labels":null,"Keyword_Vector":[0.0823029989,0.017892201,0.051763138,0.0231920242,0.1154792772,-0.1309800806,0.1884487036,-0.094779617,-0.0334853459,-0.0546400585,-0.1304984752,-0.0912092729,0.1398628869,-0.2322608621,0.0074483841,-0.1796472058,0.0884599141,-0.0582786263,-0.0856336313,-0.2145703906,0.0206837875,-0.0185690229,0.0611657421,-0.0507764616],"Abstract_Vector":[0.2061134165,0.1125526052,-0.0764938921,-0.2000376548,-0.2004957495,-0.1058221899,-0.0487990658,-0.1747105864,0.0580569272,-0.166060411,-0.1106333409,-0.0103146936,0.0774217032,-0.2193809942,-0.0158880893,-0.0027726855,0.0763856603,-0.000272742,0.0383922207,0.0148646484,0.0382486753,0.0082621968,0.0031295777,-0.0918041219,-0.0694567699,-0.0230260889,-0.1173332523,0.0331841442,-0.0245800875,-0.0326569818,0.1172219978,0.1215883587,0.0685700813,-0.1090756952,-0.0692725688,-0.0235455276,0.0137036322,-0.0251487919,-0.0283785067,0.0110658834,-0.0142301378,0.0175489839,-0.0334928226,0.0861468823,-0.1461576224,-0.0192610934,-0.0172070231,0.0641200089,-0.1000445543,0.0780247172,-0.0314898549,-0.0343625174,0.0720250862,-0.0294415493,-0.0656476675,0.0719813275,0.0918445843,0.0085284301,-0.129161397,-0.0085385585,-0.0371098368,-0.0382232985,0.0851684779,0.0797525673,-0.0827450616,0.0051262967,-0.0543792014,-0.0365236811,0.0208650086,-0.0311496773,-0.0401704876,0.0168977233,0.0072944664,0.0188730978,-0.0860360627,-0.0530113165]},"208":{"Abstract":"Clustering of trajectories of moving objects by similarity is an important technique in movement analysis. Existing distance functions assess the similarity between trajectories based on properties of the trajectory points or segments. The properties may include the spatial positions, times, and thematic attributes. There may be a need to focus the analysis on certain parts of trajectories, i.e., points and segments that have particular properties. According to the analysis focus, the analyst may need to cluster trajectories by similarity of their relevant parts only. Throughout the analysis process, the focus may change, and different parts of trajectories may become relevant. We propose an analytical workflow in which interactive filtering tools are used to attach relevance flags to elements of trajectories, clustering is done using a distance function that ignores irrelevant elements, and the resulting clusters are summarized for further analysis. We demonstrate how this workflow can be useful for different analysis tasks in three case studies with real data from the domain of air traffic. We propose a suite of generic techniques and visualization guidelines to support movement data analysis by means of relevance-aware trajectory clustering.","Authors":"G. Andrienko; N. Andrienko; G. Fuchs; J. M. C. Garcia","DOI":"10.1109\/TVCG.2017.2744322","Keywords":"Visual analytics;movement data analysis;trajectory clustering;air traffic","Keywords_Processed":"trajectory clustering;movement datum analysis;air traffic;visual analytic","Title":"Clustering Trajectories by Relevant Parts for Air Traffic Analysis","Labels":null,"Keyword_Vector":[0.2344115499,-0.1344428967,0.3493816805,-0.0618518309,0.0636832393,0.0766976815,-0.108842644,-0.0099857587,-0.0618477339,-0.0433752944,-0.0401407185,-0.034817441,-0.0618116491,-0.0168707787,-0.08276777,-0.1146911993,-0.1105929992,0.0555592474,0.104905229,-0.0128839831,-0.0335484079,0.0199496064,0.1232823599,0.050895318],"Abstract_Vector":[0.2407317946,-0.1379774455,-0.0501301934,0.0910126078,-0.1458063201,-0.1034160514,-0.1042564745,-0.0171355271,-0.0374830066,0.0946645944,0.0394893441,-0.1068439294,-0.0008219569,-0.0216686365,-0.0087932949,0.047486496,-0.0443572756,0.0928093708,-0.0353768258,0.0497959745,-0.076447847,0.0742399195,-0.0633041124,0.0597316032,0.1652330765,-0.0635042625,0.1297767156,0.1845456402,0.0928989754,0.1172965199,0.0965624268,-0.1021671947,-0.073500913,0.0053442087,-0.1616541509,-0.0133375505,0.0551668202,0.0594529831,-0.1736417783,-0.117971421,0.1207588023,0.0875597207,0.0158792588,-0.0539494032,-0.0811618229,-0.0557340524,-0.0103544552,0.0128540156,-0.0466566309,-0.0125212361,-0.0029761158,0.1517163329,0.0406763026,0.0813868408,-0.0331943171,0.1398642767,0.0658876661,-0.0010559548,0.0032400127,0.0741193544,0.0609264635,0.1442473889,-0.0314096501,0.0554652766,-0.0607887041,-0.0634922032,0.0029261911,-0.0643774768,0.0596541301,-0.0239005918,-0.0871008972,-0.0291766767,-0.0087888052,0.0056943796,0.0246780569,0.0416479737]},"209":{"Abstract":"State-of-the-art lighting design is based on physically accurate lighting simulations of scenes such as offices. The simulation results support lighting designers in the creation of lighting configurations, which must meet contradicting customer objectives regarding quality and price while conforming to industry standards. However, current tools for lighting design impede rapid feedback cycles. On the one side, they decouple analysis and simulation specification. On the other side, they lack capabilities for a detailed comparison of multiple configurations. The primary contribution of this paper is a design study of LiteVis, a system for efficient decision support in lighting design. LiteVis tightly integrates global illumination-based lighting simulation, a spatial representation of the scene, and non-spatial visualizations of parameters and result indicators. This enables an efficient iterative cycle of simulation parametrization and analysis. Specifically, a novel visualization supports decision making by ranking simulated lighting configurations with regard to a weight-based prioritization of objectives that considers both spatial and non-spatial characteristics. In the spatial domain, novel concepts support a detailed comparison of illumination scenarios. We demonstrate LiteVis using a real-world use case and report qualitative feedback of lighting designers. This feedback indicates that LiteVis successfully supports lighting designers to achieve key tasks more efficiently and with greater certainty.","Authors":"J. Sorger; T. Ortner; C. Luksch; M. Schw\u00e4rzler; E. Gr\u00f6ller; H. Piringer","DOI":"10.1109\/TVCG.2015.2468011","Keywords":"Integrating Spatial and Non-Spatial Data Visualization;Visualization in Physical Sciences and Engineering;Coordinated and Multiple Views;Visual Knowledge Discovery;Integrating Spatial and Non-Spatial Data Visualization;Visualization in Physical Sciences and Engineering;Coordinated and Multiple Views;Visual Knowledge Discovery","Keywords_Processed":"integrate spatial and non spatial data visualization;visual knowledge discovery;visualization in physical sciences and engineering;coordinated and multiple view","Title":"LiteVis: Integrated Visualization for Simulation-Based Decision Support in Lighting Design","Labels":null,"Keyword_Vector":[0.2782606048,-0.0548694439,0.0407862612,0.0400819532,-0.0299371359,-0.0287117895,-0.0400912309,0.1276740992,0.1765329089,-0.1728637728,0.0888302894,-0.3238547484,-0.1439867557,-0.1253099132,0.0939503959,0.0532283086,-0.1176792322,0.0216588491,0.0816260822,0.1648985819,0.0527561079,-0.0503548495,-0.0932190954,-0.0065735729],"Abstract_Vector":[0.2107075677,0.0113114665,0.0245476731,0.0068378811,0.0445302188,-0.0246035871,-0.0101430855,0.0026308752,-0.154108034,-0.0104191897,-0.0567391117,0.0619822858,0.0319183668,0.0819397494,-0.1444314273,0.0350187853,-0.0338135334,0.0027895519,-0.0865875977,0.146415519,0.0269804386,0.0033948957,0.0197731884,-0.0148565838,0.1092845508,-0.0310458694,-0.0850500005,0.120798792,-0.0093606382,-0.0703191586,-0.0439498101,0.0646039665,0.0307885735,-0.0572815929,0.1767106746,0.031987318,0.0197631816,-0.0270357439,-0.0874983141,0.034703021,0.0103761942,0.0087778988,0.0700052464,-0.1219771712,-0.1165419504,0.0731564823,-0.052293563,-0.0363489839,0.0632015681,-0.0615767128,-0.0422248682,0.0359273898,0.0899704598,-0.0607245616,0.0934919938,0.0137754501,0.0343124731,-0.0333489687,0.0681916085,-0.0598391559,0.0839095277,-0.0078826706,-0.0400829689,0.0106903406,0.0258008486,-0.0439050314,0.0884967371,-0.0307825948,-0.0074701426,-0.1311516466,-0.0705985875,0.1067863023,-0.012367398,0.0035012853,-0.0289056868,0.0088036674]},"210":{"Abstract":"We further describe and analyze the idea of hashed alpha testing from Wyman and McGuire\u00a0[1] , which builds on stochastic alpha testing and simplifies stochastic transparency. Typically, alpha testing provides a simple mechanism to mask out complex silhouettes using simple proxy geometry with applied alpha textures. While widely used, alpha testing has a long-standing problem: geometry can disappear entirely as alpha mapped polygons recede with distance. As foveated rendering for virtual reality spreads, this problem worsens as peripheral minification and prefiltering introduce this problem on nearby objects. We first introduce the notion of stochastic alpha testing, which replaces a fixed alpha threshold of $\\alpha _\\tau =0.5$ with a randomly chosen $\\alpha _\\tau \\in [0..1)$. This entirely avoids the problem of disappearing alpha-tested geometry, but introduces temporal noise. Hashed alpha testing uses a hash function to choose $\\alpha _\\tau$ procedurally. With a good hash function and inputs, hashed alpha testing maintains distant geometry without introducing more temporal flicker than traditional alpha testing. We also describe how hashed alpha interacts with temporal antialiasing and applies to alpha-to-coverage and screen-door transparency. Because hashed alpha testing addresses alpha test aliasing by introducing stable sampling, it has implications in other domains where increased sample stability is desirable. We show how our hashed sampling might apply to other stochastic effects.","Authors":"C. Wyman; M. McGuire","DOI":"10.1109\/TVCG.2017.2739149","Keywords":"Anisotropy;alpha map;alpha test;hash;hashed alpha test;stable shading;stochastic sampling","Keywords_Processed":"stable shading;stochastic sampling;alpha map;hash;hash alpha test;anisotropy;alpha test","Title":"Improved Alpha Testing Using Hashed Sampling","Labels":null,"Keyword_Vector":[0.0035778218,-0.0025121561,0.0035345408,0.0010110741,0.0028477486,-0.0031828663,-0.0001127832,-0.0007523935,-0.0127510586,0.0047013617,0.0159611472,-0.0007648738,-0.015354417,0.0000348818,-0.0071239613,-0.0013338777,-0.009538015,0.0007613657,0.0000012201,0.0149071683,0.0069536815,-0.01300426,0.0270148888,-0.0063370724],"Abstract_Vector":[0.0575553707,0.0433028369,0.0030334709,0.0099100571,0.0012815872,0.0116609352,0.0052003224,0.0036433314,0.0047224208,-0.0009037363,-0.0095044946,0.0115287456,-0.0214956445,0.0256866472,0.0036247928,0.0160260609,0.0163765218,0.0030716996,0.0040853109,-0.014959781,-0.0174859281,-0.015089443,0.0070397619,0.0086674233,0.0822798172,0.0346426518,0.0121738722,-0.0303003928,0.037070034,0.009167942,-0.0498441374,-0.0186923408,-0.0428945823,0.0296048789,-0.0398323189,0.0199751334,0.0572783363,0.0131919079,0.020298777,0.0088803506,-0.0092935743,0.0092653081,-0.0090303906,0.0495393175,0.0574140082,-0.0345169027,0.0614968435,0.0004832047,0.0780376307,-0.0556025955,0.0126806456,0.0266159594,-0.0825465808,-0.0174930335,-0.0140598047,0.0478867775,0.0138111319,-0.0086664868,0.055616246,-0.0633707806,-0.0380961337,0.041627299,0.1193436206,-0.1193142104,-0.0661468064,0.1662100036,0.017767971,0.0135780464,-0.0592842757,0.0335537519,0.0034452687,0.1643473546,0.099589146,-0.0967063084,0.269615108,-0.2143766717]},"211":{"Abstract":"In the field of connectomics, neuroscientists acquire electron microscopy volumes at nanometer resolution in order to reconstruct a detailed wiring diagram of the neurons in the brain. The resulting image volumes, which often are hundreds of terabytes in size, need to be segmented to identify cell boundaries, synapses, and important cell organelles. However, the segmentation process of a single volume is very complex, time-intensive, and usually performed using a diverse set of tools and many users. To tackle the associated challenges, this paper presents NeuroBlocks, which is a novel visualization system for tracking the state, progress, and evolution of very large volumetric segmentation data in neuroscience. NeuroBlocks is a multi-user web-based application that seamlessly integrates the diverse set of tools that neuroscientists currently use for manual and semi-automatic segmentation, proofreading, visualization, and analysis. NeuroBlocks is the first system that integrates this heterogeneous tool set, providing crucial support for the management, provenance, accountability, and auditing of large-scale segmentations. We describe the design of NeuroBlocks, starting with an analysis of the domain-specific tasks, their inherent challenges, and our subsequent task abstraction and visual representation. We demonstrate the utility of our design based on two case studies that focus on different user roles and their respective requirements for performing and tracking the progress of segmentation and proofreading in a large real-world connectomics project.","Authors":"A. K. Ai-Awami; J. Beyer; D. Haehn; N. Kasthuri; J. W. Lichtman; H. Pfister; M. Hadwiger","DOI":"10.1109\/TVCG.2015.2467441","Keywords":"Neuroscience;Segmentation;Proofreading;Data and Provenance Tracking;Neuroscience;Segmentation;Proofreading;Data and Provenance Tracking","Keywords_Processed":"neuroscience;segmentation;proofread;data and provenance tracking","Title":"NeuroBlocks \u2013 Visual Tracking of Segmentation and Proofreading for Large Connectomics Projects","Labels":null,"Keyword_Vector":[0.0747636726,-0.0189762902,0.0161980398,0.0045729251,0.0895106784,-0.0898326904,-0.0077512593,0.0193897959,0.1261187407,0.0023329733,0.0745524753,-0.0322022517,-0.0680726233,0.051195178,0.0149436458,0.2343319653,0.0676439707,0.0355217573,0.0008262251,-0.1635308549,0.0684741198,0.128071047,-0.1038832237,-0.1280744184],"Abstract_Vector":[0.2625884732,-0.0799758016,0.0159398703,-0.0244997401,0.0639548671,-0.0722547622,0.0155243765,0.0164624255,-0.1446438856,-0.071562519,0.0520565506,-0.0632046268,-0.0018877496,-0.0062095208,0.0331315328,-0.0187165025,0.0791286058,-0.0653673483,0.0144965112,-0.0255192659,0.03132316,0.0930418122,0.0155574248,0.0249345407,0.0348336143,-0.1107900462,-0.1240696619,0.0575688152,-0.0069602619,-0.0121362913,0.0051981374,-0.1031308603,0.0426673054,0.0817710891,0.0094801572,-0.0255100402,0.3150218329,-0.0649535985,0.0103919025,-0.100647126,-0.0101054804,-0.0855029292,-0.0954932626,-0.0456106303,0.0530250973,0.0579280988,-0.0656999993,0.0043247146,-0.0358965872,0.0313541063,0.0916541197,-0.096193505,0.0311614971,0.041776359,0.0935476723,-0.0135789522,0.0566498947,-0.1016543899,0.0199837529,-0.065698279,-0.0043010523,0.0751901349,-0.0554163322,0.0595582627,0.0420107173,-0.0469515795,-0.0131077794,0.110271587,0.0890198011,0.0222430401,-0.0388080861,-0.0029962532,-0.0894626902,-0.0514484395,0.0308922957,-0.0081317717]},"212":{"Abstract":"Many researchers across diverse disciplines aim to analyze the behavior of cohorts whose behaviors are recorded in large event databases. However, extracting cohorts from databases is a difficult yet important step, often overlooked in many analytical solutions. This is especially true when researchers wish to restrict their cohorts to exhibit a particular temporal pattern of interest. In order to fill this gap, we designed COQUITO, a visual interface that assists users defining cohorts with temporal constraints. COQUITO was designed to be comprehensible to domain experts with no preknowledge of database queries and also to encourage exploration. We then demonstrate the utility of COQUITO via two case studies, involving medical and social media researchers.","Authors":"J. Krause; A. Perer; H. Stavropoulos","DOI":"10.1109\/TVCG.2015.2467622","Keywords":"Visual temporal queries;cohort definition;electronic medical records;information visualization;Visual temporal queries;cohort definition;electronic medical records;information visualization","Keywords_Processed":"information visualization;cohort definition;visual temporal query;electronic medical record","Title":"Supporting Iterative Cohort Construction with Visual Temporal Queries","Labels":null,"Keyword_Vector":[0.1699762252,-0.0434294494,0.0201804524,0.00760894,-0.0966808598,-0.01421506,-0.0402946568,0.0009518691,0.0146215782,0.015684181,0.1005387907,-0.1047437767,0.0713362452,0.01268925,-0.0858599009,-0.0164543503,-0.0661120981,0.0025595237,0.0173477078,-0.022813769,-0.0050476315,-0.0801397893,-0.0006366251,0.0772763596],"Abstract_Vector":[0.1492821028,-0.0860684679,0.010256547,-0.0518972556,0.0074435074,-0.1433436135,-0.0103907983,-0.0929523513,-0.0414594618,0.0206061308,0.0208438766,0.0155293206,-0.0522496684,-0.01269582,0.087278271,0.1412832441,0.0128783016,-0.1003615403,0.0199795164,0.010552872,-0.0283890482,-0.0433844916,0.0824225134,-0.0270806691,0.0576074279,-0.0682530318,-0.0650482851,-0.1119357068,-0.0781425066,-0.0415203034,0.0017590336,-0.006220976,-0.1661704414,-0.0097999483,0.0090995355,0.05548336,0.0567374974,-0.0116289336,-0.0007311135,0.0499655433,0.033377384,0.0112629645,-0.0408261652,0.0585356783,-0.0108690215,0.0085029361,0.0436970427,0.0835030658,0.0120613308,0.1325802343,-0.0369683905,-0.1064684932,-0.0951543186,-0.0753486034,0.0080229691,0.0131286285,0.0131258716,0.0944095949,-0.0023154331,0.1265050807,0.0514309667,-0.1279915263,-0.0024366842,0.0859881279,-0.0635692667,-0.1279890824,-0.1020956338,-0.020214458,0.0199922298,-0.0182467596,0.0048533434,0.1353914574,-0.0115354356,0.0842182661,-0.0391077611,0.043992]},"213":{"Abstract":"Shader lamp systems augment the real environment by projecting new textures on known target geometries. In dynamic scenes, object tracking maintains the illusion if the physical and virtual objects are well aligned. However, traditional trackers based on texture or contour information are often distracted by the projected content and tend to fail. In this paper, we present a model-based tracking strategy, which directly takes advantage from the projected content for pose estimation in a projector-camera system. An iterative pose estimation algorithm captures and exploits visible distortions caused by object movements. In a closed-loop, the corrected pose allows the update of the projection for the subsequent frame. Synthetic frames simulating the projection on the model are rendered and an optical flow-based method minimizes the difference between edges of the rendered and the camera image. Since the thresholds automatically adapt to the synthetic image, a complicated radiometric calibration can be avoided. The pixel-wise linear optimization is designed to be easily implemented on the GPU. Our approach can be combined with a regular contour-based tracker and is transferable to other problems, like the estimation of the extrinsic pose between projector and camera. We evaluate our procedure with real and synthetic images and obtain very precise registration results.","Authors":"N. Gard; A. Hilsmann; P. Eisert","DOI":"10.1109\/TVCG.2019.2932223","Keywords":"Projector-camera systems;projector-camera calibration;shader lamp systems;object tracking;object registration;spatial augmented reality;projection mapping","Keywords_Processed":"projector camera system;projector camera calibration;object tracking;projection mapping;shader lamp system;object registration;spatial augment reality","Title":"Projection Distortion-based Object Tracking in Shader Lamp Scenarios","Labels":null,"Keyword_Vector":[0.1053643681,0.155020215,0.0126586382,0.0106109482,0.1016730441,-0.1376059107,-0.0145373076,0.0466979791,0.0531656882,-0.0847400955,0.0588210996,-0.1465218732,-0.108580496,-0.0106693382,0.0030699202,-0.0455752952,0.0219882329,0.0785441572,-0.04181736,0.0196050746,0.0778082495,-0.0775148404,-0.2033556043,-0.1575132322],"Abstract_Vector":[0.2413978897,0.2903672678,-0.0101746477,-0.0304596138,0.0179592433,0.001321209,-0.0481463478,0.0275058609,-0.0062210703,0.1324449683,-0.0343046307,0.0696017838,-0.0570179338,-0.0080783777,-0.0303433394,-0.0324804913,0.0997189882,0.008526905,0.0162435666,-0.0315577823,0.0079896975,-0.0573089422,-0.088351798,-0.0950785022,-0.105629755,-0.0698929992,-0.0109751372,0.0167408499,0.1337389052,0.1571146971,-0.0443010535,0.0638476501,0.0487227212,0.0112521396,-0.0297369234,-0.0615431828,0.0895312502,-0.0441203815,0.0892011964,-0.0669388363,-0.001482623,0.0260462735,-0.0074233909,-0.0252839659,0.0459905085,0.2111234567,0.1038797855,0.0774504319,-0.0214762312,0.045740923,-0.1818342121,0.0831202997,0.0440284679,0.0500987792,-0.0485474519,0.0506994863,0.030605039,0.0334262401,0.1596882619,-0.0119227552,0.0445311028,-0.0377186586,0.0977961045,0.0042980943,-0.0954133488,-0.0552884923,0.0129741947,0.0263777256,-0.0041041433,-0.0122990798,0.0419921294,-0.0228959172,-0.0398924417,-0.003722605,-0.0125354189,-0.0364082895]},"214":{"Abstract":"Fish Tank Virtual Reality (FTVR) displays create a compelling 3D spatial effect by rendering to the perspective of the viewer with head-tracking. Combining FTVR with a spherical display enhances the 3D experience with unique properties of the spherical screen such as the enclosing shape, consistent curved surface, and borderless views from all angles around the display. The ability to generate a strong 3D effect on a spherical display with head-tracked rendering is promising for increasing user's performance in 3D tasks. An unanswered question is whether these natural affordances of spherical FTVR displays can improve spatial perception in comparison to traditional flat FTVR displays. To investigate this question, we conducted an experiment to see whether users can perceive the depth and size of virtual objects better on a spherical FTVR display compared to a flat FTVR display on two tasks. Using the spherical display, we found significantly that users had 1cm depth accuracy compared to 6.5cm accuracy using the flat display on a depth-ranking task. Likewise, their performance on a size-matching task was also significantly better with the size error of 2.3mm on the spherical display compared to 3.1mm on the flat display. Furthermore, the perception of size-constancy is stronger on the spherical display than the flat display. This study indicates that the natural affordances provided by the spherical form factor improve depth and size perception in 3D compared to a flat display. We believe that spherical FTVR displays have potential as a 3D virtual environment to provide better task performance for various 3D applications such as 3D designs, scientific visualizations, and virtual surgery.","Authors":"Q. Zhou; G. Hagemann; D. Fafard; I. Stavness; S. Fels","DOI":"10.1109\/TVCG.2019.2898742","Keywords":"Fish tank virtual reality;spherical display;depth cues;3D perception;size constancy;3D visualization","Keywords_Processed":"3d perception;fish tank virtual reality;3d visualization;depth cue;size constancy;spherical display","Title":"An Evaluation of Depth and Size Perception on a Spherical Fish Tank Virtual Reality Display","Labels":null,"Keyword_Vector":[0.2105838647,0.3283605071,-0.0387505305,0.0147306021,0.0446863216,0.012707005,-0.0426312596,-0.0449473467,-0.0199775483,0.0314173681,-0.0304583759,0.0173053599,-0.0092216983,0.0898853737,0.060319685,0.1491367707,0.0214288915,-0.1611472783,0.0189760442,0.0942257776,0.1543899394,0.0653020684,0.0684435579,-0.0677093207],"Abstract_Vector":[0.1793339366,0.1362206997,0.2498793424,0.125633155,-0.0239587888,0.0939180924,0.0247734593,0.0485556715,0.0744970205,-0.0118149356,0.0215626477,0.0422914164,0.0570087082,-0.1086499559,0.044064048,-0.0379698617,-0.1772288021,-0.0603594065,0.077362443,0.0436874865,-0.0713500365,-0.081783401,0.0140971576,0.1412426508,-0.0341467627,-0.0263095197,-0.1239675627,-0.0468412885,0.0024078814,-0.0197661852,-0.0069550533,-0.1020730935,-0.0126905848,0.0234169915,-0.0876797844,-0.0272482569,-0.0318048679,-0.0574220975,-0.0397902643,-0.0373746848,0.1049012424,0.1325045953,-0.0332024033,0.0130908733,0.0798790881,-0.122428003,-0.051207021,-0.1086739748,-0.0957507459,0.1526834937,0.0050829472,0.0295047501,0.0089113816,-0.0239732209,0.1776469511,-0.1305694654,-0.0957001634,-0.0394646658,0.0528830408,-0.0251727575,0.031921953,0.0227175309,-0.0132770296,-0.0764280478,-0.090876206,-0.0576010072,-0.005871464,0.0430468035,-0.0103417579,-0.0780626224,0.0473969302,0.0750519802,-0.0274797557,-0.0067318352,-0.0234210321,-0.0612640828]},"215":{"Abstract":"A recently developed light projection technique can add dynamic impressions to static real objects without changing their original visual attributes such as surface colors and textures. It produces illusory motion impressions in the projection target by projecting gray-scale motion-inducer patterns that selectively drive the motion detectors in the human visual system. Since a compelling illusory motion can be produced by an inducer pattern weaker than necessary to perfectly reproduce the shift of the original pattern on an object's surface, the technique works well under bright environmental light conditions. However, determining the best deformation sizes is often difficult: When users try to add a large deformation, the deviation in the projected patterns from the original surface pattern on the target object becomes apparent. Therefore, to obtain satisfactory results, they have to spend much time and effort to manually adjust the shift sizes. Here, to overcome this limitation, we propose an optimization framework that adaptively retargets the displacement vectors based on a perceptual model. The perceptual model predicts the subjective inconsistency between a projected pattern and an original one by simulating responses in the human visual system. The displacement vectors are adaptively optimized so that the projection effect is maximized within the tolerable range predicted by the model. We extensively evaluated the perceptual model and optimization method through a psychophysical experiment as well as user studies.","Authors":"T. Fukiage; T. Kawabe; S. Nishida","DOI":"10.1109\/TVCG.2019.2898738","Keywords":"Spatial augmented reality;human visual system;perceptual model","Keywords_Processed":"human visual system;spatial augment reality;perceptual model","Title":"Perceptually Based Adaptive Motion Retargeting to Animate Real Objects by Light Projection","Labels":null,"Keyword_Vector":[0.2251203264,0.2256625406,0.2072322942,0.0780759375,-0.0377058533,-0.146232677,-0.0827506075,0.0784157599,0.0540017247,-0.1798999499,0.0942987482,-0.1787798175,-0.1081117245,-0.0528328822,0.0104551275,-0.1113753444,-0.0492140794,0.0748897551,-0.015092009,0.0907268872,0.025903159,-0.1396026258,-0.1827422605,-0.1024913975],"Abstract_Vector":[0.249166242,0.1705680003,-0.0283745482,-0.1439774139,-0.0404950781,0.0105345417,-0.0783952659,-0.0882597718,0.1215726299,0.1215597094,0.0512380819,0.107834816,-0.0143514654,-0.0441323716,-0.0024017703,0.0458834225,-0.0292419158,0.1128009649,0.0425927186,0.0971172718,-0.022671132,0.0392342175,-0.1167705819,-0.1484610497,-0.13266489,-0.1079745474,0.0027211289,-0.047535993,-0.1201976749,0.0296660268,-0.0793944337,0.0961529302,0.06754373,-0.0661597138,0.007366685,-0.0703355368,0.2087890244,-0.0644382476,-0.007686601,0.0099553527,0.0798453054,-0.0297799442,0.0424241363,0.063002338,-0.0057686234,-0.0168137446,-0.0414234669,0.0880738895,-0.1257397911,0.032594697,-0.0815389356,-0.0079395653,-0.0179616054,0.0868823202,-0.0097020348,-0.0187621582,0.0001186872,-0.0364408601,0.0366463087,-0.0243086803,-0.0036470606,-0.0625758495,-0.0193141435,-0.0410161289,-0.0069909223,-0.0337913899,0.0983393626,0.0783778766,-0.0398617999,0.060572109,-0.0330074315,-0.0302692921,0.0117658633,-0.0068474613,0.0301402642,-0.0115879549]},"216":{"Abstract":"Economic globalization is increasing connectedness among regions of the world, creating complex interdependencies within various supply chains. Recent studies have indicated that changes and disruptions within such networks can serve as indicators for increased risks of violence and armed conflicts. This is especially true of countries that may not be able to compete for scarce commodities during supply shocks. Thus, network-induced vulnerability to supply disruption is typically exported from wealthier populations to disadvantaged populations. As such, researchers and stakeholders concerned with supply chains, political science, environmental studies, etc. need tools to explore the complex dynamics within global trade networks and how the structure of these networks relates to regional instability. However, the multivariate, spatiotemporal nature of the network structure creates a bottleneck in the extraction and analysis of correlations and anomalies for exploratory data analysis and hypothesis generation. Working closely with experts in political science and sustainability, we have developed a highly coordinated, multi-view framework that utilizes anomaly detection, network analytics, and spatiotemporal visualization methods for exploring the relationship between global trade networks and regional instability. Requirements for analysis and initial research questions to be investigated are elicited from domain experts, and a variety of visual encoding techniques for rapid assessment of analysis and correlations between trade goods, network patterns, and time series signatures are explored. We demonstrate the application of our framework through case studies focusing on armed conflicts in Africa, regional instability measures, and their relationship to international global trade.","Authors":"H. Wang; Y. Lu; S. T. Shutters; M. Steptoe; F. Wang; S. Landis; R. Maciejewski","DOI":"10.1109\/TVCG.2018.2864844","Keywords":"Global trade network;anomaly detection;visual analytics","Keywords_Processed":"global trade network;anomaly detection;visual analytic","Title":"A Visual Analytics Framework for Spatiotemporal Trade Network Analysis","Labels":null,"Keyword_Vector":[0.1716460124,-0.0829995853,0.3453012416,0.0936904656,-0.1668789519,-0.0872262579,-0.0091134324,-0.0299643182,0.1418901722,0.1427676616,-0.0857855201,0.0300975938,0.1257933729,-0.1509695934,-0.115829702,0.1345888483,-0.0441542131,-0.0243517676,0.0903724337,0.1358142449,0.0983699467,0.075933647,-0.0239878201,0.1382288804],"Abstract_Vector":[0.2083375305,-0.156744945,-0.0118913001,-0.0457256214,0.0803184412,-0.1782571002,0.2088060033,0.0233062496,0.0916401615,0.1250499262,-0.103929731,0.0026349597,-0.1248552435,-0.0596875039,-0.0294567396,-0.0431928572,-0.0580368914,-0.0333321288,0.0828644488,0.1264487691,-0.040432627,0.146772462,-0.018419981,-0.0755314644,-0.1276019683,0.1465623011,-0.1007353373,0.0101790365,0.0003599106,-0.0484422793,-0.0215415767,-0.0749577598,-0.1365637705,-0.1008461047,0.011353694,0.1107282797,0.0203514745,-0.131618313,-0.0203998848,-0.0168152012,-0.0846158824,-0.0020564855,-0.0442642866,-0.0078773882,0.0696339335,-0.0585645611,0.0669389761,-0.0528586524,-0.0841169432,-0.029955752,0.0325110254,0.1283707736,-0.0527447087,0.0379589678,-0.05760472,-0.0071353056,0.0440149684,0.0686984653,-0.0328184703,0.0222662125,0.0378540281,0.0656641746,-0.0817557826,-0.0755429172,0.0194526724,0.0209296412,-0.0205687772,-0.0066505778,-0.0086112127,-0.0066511038,-0.1157626373,0.0013114242,0.0848205681,-0.0312755388,0.072331177,0.017038021]},"217":{"Abstract":"This paper presents a method to reconstruct a functional mechanical assembly from raw scans. Given multiple input scans of a mechanical assembly, our method first extracts the functional mechanical parts using a motion-guided, patch-based hierarchical registration and labeling algorithm. The extracted functional parts are then parameterized from the segments and their internal mechanical relations are encoded by a graph. We use a joint optimization to solve for the best geometry, placement, and orientation of each part, to obtain a final workable mechanical assembly. We demonstrated our algorithm on various types of mechanical assemblies with diverse settings and validated our output using physical fabrication.","Authors":"M. Lin; T. Shao; Y. Zheng; N. J. Mitra; K. Zhou","DOI":"10.1109\/TVCG.2017.2662238","Keywords":"3D scanning;mechanical assembly;functionality;mechanical constraints;motion","Keywords_Processed":"motion;3d scanning;functionality;mechanical constraint;mechanical assembly","Title":"Recovering Functional Mechanical Assemblies from Raw Scans","Labels":null,"Keyword_Vector":[0.0295452605,0.0463758127,0.0015789397,0.0570462369,0.0581442802,-0.0609112139,0.0580903821,-0.0193435215,-0.0119321073,-0.0410660183,-0.045180739,-0.0288130334,0.0142154283,-0.0388721898,0.0099299648,0.0249525993,0.0762641285,-0.1055299451,-0.0550401424,-0.0269624188,0.0501620763,0.0549217421,0.0495323401,-0.0467833149],"Abstract_Vector":[0.1207272384,0.084827444,-0.0559788986,-0.0421553537,0.0134153109,-0.0237556032,-0.0335346544,0.0130374554,0.0380859664,-0.0280737125,0.0219470471,-0.0365224732,-0.0223359907,-0.0361988124,0.0199632809,0.0001538127,0.1278421665,0.023803864,-0.0202761869,-0.0086805672,0.0269670024,-0.0030046685,-0.0250231282,0.0723597632,0.0582590678,-0.0515550542,0.0041545459,0.0064368746,-0.0853392708,0.070330837,-0.0050855048,-0.0225534197,0.1108803591,0.03040389,-0.0353425772,0.2042668171,0.0633082397,-0.1113570407,-0.0775445826,-0.049507249,0.0101882262,-0.043530217,-0.1269083704,-0.0254995829,0.0115978732,-0.0463824035,-0.022709779,0.0701778529,0.0008177801,-0.0423524469,0.1760423712,0.0309909056,0.0206679259,-0.0795616329,-0.0456083103,0.1437146508,-0.0315701119,0.115728896,-0.0187565559,-0.066868724,0.0912792246,-0.0025144208,0.1411270416,0.0802781502,0.0170317818,0.1034579172,-0.0032718938,-0.1390110739,-0.0111749837,0.0360974584,-0.1472956613,0.0953995149,-0.0629229834,0.0702427213,-0.0049073474,0.1172199389]},"218":{"Abstract":"Star Coordinates are a popular projection technique in order to analyze and to disclose characteristic patterns of multidimensional data. Unfortunately, the shape, appearance, and distribution of such patterns are strongly affected by the given scaling of the data and can mislead the projection-based data analysis. In an extreme case, patterns might be more related to the choice of scaling than to the data themselves. Thus, we present the LloydRelaxer: a tool to minimize scaling-based effects in Star Coordinates. Our algorithm enforces a scaling configuration for which the data explains the observed patterns better than any scaling of them could do. It does so by an iterative minimizing and optimization process based on Voronoi diagrams and on the Lloyd relaxation within the projection space. We evaluate and test our approach by real benchmark multidimensional data of the UCI data repository.","Authors":"D. J. Lehmann; H. Theisel","DOI":"10.1109\/TVCG.2017.2705189","Keywords":"Information visualization;multidimensional data;scaling;star coordinates;projections","Keywords_Processed":"star coordinate;information visualization;scale;projection;multidimensional datum","Title":"The LloydRelaxer: An Approach to Minimize Scaling Effects for Multivariate Projections","Labels":null,"Keyword_Vector":[0.2740988258,-0.1294045037,-0.0764657384,-0.1946238643,0.190176777,0.0109578997,-0.0437664108,0.1326278333,-0.1886053314,0.1047448237,0.2006660492,-0.1391186789,0.0966165897,-0.0182539761,-0.2136210348,0.0352613734,0.0288807228,0.0047234643,-0.1343216371,0.1873373156,0.0690694221,0.0361240407,-0.0661009159,0.0482640727],"Abstract_Vector":[0.2208312047,-0.0946585945,-0.1094448066,-0.16099809,-0.2009344308,0.0530577721,-0.0776734439,0.0320568834,-0.0596834829,0.0838125867,0.0009172759,0.1793710807,-0.0806846256,-0.0069454334,0.0292299118,-0.0720683157,-0.075809595,0.0879146718,0.0099344368,-0.04406816,0.0554065818,-0.0739134204,-0.0613479088,-0.0733124107,-0.0601735088,0.0192754581,0.0360477063,0.0005261088,0.0744485055,0.0031218437,-0.07754139,0.1016068744,0.0554010162,0.1063516631,-0.0138304451,-0.0051127389,0.1850580508,-0.1057133574,0.0182348706,-0.0098924628,0.1721089548,-0.0103885854,0.0127482862,0.055548399,0.0348783924,0.0362230269,-0.0878297285,-0.0222045476,0.0022035462,-0.0340142638,-0.0359505722,0.0520525545,-0.0461793517,0.1015661483,-0.0592260756,-0.0675367487,-0.062611498,0.0894643621,0.0399853987,-0.1139266439,0.0137927225,-0.1482288119,-0.0816183678,-0.1312855495,0.060002399,-0.0214175009,0.0397923283,0.0530199754,0.0156294585,0.0798377563,-0.0038858206,0.0191633526,0.0712249497,-0.0012540675,0.006664539,0.0718042215]},"219":{"Abstract":"This paper examines how humans adapt to novel physical situations with unknown gravitational acceleration in immersive virtual environments. We designed four virtual reality experiments with different tasks for participants to complete: strike a ball to hit a target, trigger a ball to hit a target, predict the landing location of a projectile, and estimate the flight duration of a projectile. The first two experiments compared human behavior in the virtual environment with real-world performance reported in the literature. The last two experiments aimed to test the human ability to adapt to novel gravity fields by measuring their performance in trajectory prediction and time estimation tasks. The experiment results show that: 1) based on brief observation of a projectile's initial trajectory, humans are accurate at predicting the landing location even under novel gravity fields, and 2) humans' time estimation in a familiar earth environment fluctuates around the ground truth flight duration, although the time estimation in unknown gravity fields indicates a bias toward earth's gravity.","Authors":"T. Ye; S. Qi; J. Kubricht; Y. Zhu; H. Lu; S. Zhu","DOI":"10.1109\/TVCG.2017.2657235","Keywords":"Virtual reality;intuitive physics;mental simulation","Keywords_Processed":"intuitive physics;mental simulation;virtual reality","Title":"The Martian: Examining Human Physical Judgments across Virtual Gravity Fields","Labels":null,"Keyword_Vector":[0.1159943855,0.3404448715,0.0014636602,-0.0773526708,0.0768461916,-0.0637795292,-0.0191458597,-0.0514153042,0.0638296505,-0.0200157737,-0.0112001812,0.0755962268,-0.0160031209,0.0463422742,-0.1549877569,-0.0914873414,0.0524033053,0.0530661906,0.1430564984,0.0535134296,-0.059111642,0.1452001454,-0.048394914,0.1504865223],"Abstract_Vector":[0.1687638816,0.0995874084,0.1771510324,0.050802677,-0.0430736125,0.0072576545,-0.0004614945,-0.1145969424,0.121859744,0.021625645,0.0186241285,-0.016769668,-0.0029840807,0.1159901751,0.1172368401,-0.0535456517,-0.0990280714,0.0382618594,0.1003408204,0.0863595148,-0.0122542227,-0.0031596412,0.066998036,-0.0965692833,0.0703250658,-0.0193507267,0.0601789656,0.05781197,0.0657087123,0.1301446718,-0.013662773,-0.0990577291,0.089934106,-0.028197876,-0.0284665478,-0.0324854579,0.0976405177,0.0238938116,-0.0282001543,-0.0838281414,-0.0684510717,0.0357375332,0.0847739467,-0.10774656,0.1077591592,-0.007319773,-0.0413242573,0.0256579748,-0.0091300897,-0.0153148821,-0.0501263571,0.0268460925,-0.001717211,0.0384467451,0.0226164824,0.0593795326,-0.0552293013,0.0657774218,0.051273037,-0.0367486618,0.1487933507,-0.1177543363,0.0516644342,0.1805874507,0.0355936555,0.0454426896,-0.0473047132,-0.0595237359,-0.0056047257,0.0126356794,0.0297324829,-0.1146399446,-0.0292734949,-0.0443943857,-0.0078074676,-0.034579977]},"220":{"Abstract":"Visual exploration of flow fields is important for studying dynamic systems. We introduce semantic flow graph (SFG), a novel graph representation and interaction framework that enables users to explore the relationships among key objects (i.e., field lines, features, and spatiotemporal regions) of both steady and unsteady flow fields. The objects and their relationships are organized as a heterogeneous graph. We assign each object a set of attributes, based on which a semantic abstraction of the heterogeneous graph is generated. This semantic abstraction is SFG. We design a suite of operations to explore the underlying flow fields based on this graph representation and abstraction mechanism. Users can flexibly reconfigure SFG to examine the relationships among groups of objects at different abstraction levels. Three linked views are developed to display SFG, its node split criteria and history, and the objects in the spatial volume. For simplicity, we introduce SFG construction and exploration for steady flow fields with critical points being the only features. Then we demonstrate that SFG can be naturally extended to deal with unsteady flow fields and multiple types of features. We experiment with multiple data sets and conduct an expert evaluation to demonstrate the effectiveness of our approach.","Authors":"J. Tao; C. Wang; N. V. Chawla; L. Shi; S. H. Kim","DOI":"10.1109\/TVCG.2017.2773071","Keywords":"Flow visualization;heterogeneous graph;semantic abstraction;critical points;vortex cores;FTLE;field lines","Keywords_Processed":"heterogeneous graph;field line;vortex core;ftle;flow visualization;critical point;semantic abstraction","Title":"Semantic Flow Graph: A Framework for Discovering Object Relationships in Flow Fields","Labels":null,"Keyword_Vector":[0.1542024579,-0.0511615326,-0.0760603925,0.0215979753,-0.071643808,-0.1277473919,0.0758701352,-0.0272688848,-0.012354952,0.1249206849,0.1483642966,0.1594618721,-0.0622580129,-0.0990658151,-0.0102916034,0.0029228678,-0.0911775908,-0.0415044831,0.0387547002,-0.0468795683,-0.0130568936,-0.1582578368,0.0820949339,0.016270103],"Abstract_Vector":[0.2469780271,0.0277803923,-0.0441168334,0.0752298675,0.0462407806,-0.0342541318,0.3290252696,-0.1483893819,0.0522836228,0.0855884925,0.1196528843,0.0445383408,0.0976581211,-0.092735087,0.0835283148,-0.0664171661,0.2078960046,0.0858367217,-0.0726969828,-0.09110012,0.0560544719,-0.0873948546,-0.0499016593,0.1015687204,0.0801023533,-0.1101207756,-0.0786089016,0.0532001731,0.0654871961,-0.0361414166,0.0022678924,-0.0190777338,-0.0250586479,-0.0548940147,0.0173425813,-0.0853918745,0.0572629276,0.0262031803,0.066754431,0.0550560313,-0.003981828,-0.1565150333,0.099537023,-0.0219797516,0.075134359,-0.0881732176,0.0448774212,-0.0152662386,0.0080735718,-0.0167906433,-0.0710942077,-0.0154483797,-0.0227678847,-0.1171542044,0.0588011499,-0.0229195321,-0.0026132376,0.0553127256,0.0518751425,-0.0596859318,-0.0417469823,0.0230087419,0.0045865476,0.0161448537,0.0358062965,0.0227045046,0.0176667698,0.0017425824,-0.0346139935,-0.0031611993,0.023066617,-0.0539071159,0.0061914867,-0.0567075531,0.0079140592,0.0840012866]},"221":{"Abstract":"Alternative splicing is a process by which the same DNA sequence is used to assemble different proteins, called protein isoforms. Alternative splicing works by selectively omitting some of the coding regions (exons) typically associated with a gene. Detection of alternative splicing is difficult and uses a combination of advanced data acquisition methods and statistical inference. Knowledge about the abundance of isoforms is important for understanding both normal processes and diseases and to eventually improve treatment through targeted therapies. The data, however, is complex and current visualizations for isoforms are neither perceptually efficient nor scalable. To remedy this, we developed Vials, a novel visual analysis tool that enables analysts to explore the various datasets that scientists use to make judgments about isoforms: the abundance of reads associated with the coding regions of the gene, evidence for junctions, i.e., edges connecting the coding regions, and predictions of isoform frequencies. Vials is scalable as it allows for the simultaneous analysis of many samples in multiple groups. Our tool thus enables experts to (a) identify patterns of isoform abundance in groups of samples and (b) evaluate the quality of the data. We demonstrate the value of our tool in case studies using publicly available datasets.","Authors":"H. Strobelt; B. Alsallakh; J. Botros; B. Peterson; M. Borowsky; H. Pfister; A. Lex","DOI":"10.1109\/TVCG.2015.2467911","Keywords":"Biology visualization;protein isoforms;mRNA-seq;directed acyclic graphs;multivariate networks;Biology visualization;protein isoforms;mRNA-seq;directed acyclic graphs;multivariate networks","Keywords_Processed":"biology visualization;direct acyclic graph;mrna seq;protein isoform;multivariate network","Title":"Vials: Visualizing Alternative Splicing of Genes","Labels":null,"Keyword_Vector":[0.1560293137,-0.0569210685,-0.0656247568,0.0815559751,-0.0427449355,0.0171752869,0.0438539658,0.0547601114,-0.060438037,0.2007395209,-0.134579129,0.0348878991,0.0243475236,-0.1308444075,-0.1132251425,-0.0118736975,0.0735318503,0.0746188026,0.0656390647,-0.0076394248,0.1358379231,-0.0606231689,-0.0063750158,0.0048338392],"Abstract_Vector":[0.1894611714,-0.1151246518,-0.0200680676,0.002497193,0.0143660003,-0.0313693814,-0.0744163819,-0.0077528293,-0.0894782669,0.0292906721,-0.0362084991,0.030337912,0.0049267523,-0.0079512395,0.0019480493,0.0654765008,-0.0020214652,0.0006605375,0.0301577447,0.0192567862,0.0311012521,0.0384607732,-0.0035724377,0.0205010978,-0.0138171282,-0.0046695292,-0.0365766724,-0.0035018327,0.0240557431,-0.0479941492,-0.0322360534,-0.0587517996,-0.0198296977,0.0497966164,0.0170980085,-0.1167979041,0.1077772655,0.0009938911,-0.1189189097,-0.0223925564,0.0337704577,0.0375794958,-0.0068979915,-0.0834862345,-0.03880129,-0.0776617051,0.0131041056,-0.06349871,0.0932575757,-0.036808467,-0.0466047776,0.070198665,0.076920493,-0.0000807622,-0.0395013673,-0.023571494,-0.164055117,0.1195488272,-0.0865106892,-0.0007646711,-0.0159573734,0.006954902,0.0843485167,-0.09651057,-0.0135804512,0.3037829751,0.0275903009,0.0612074895,-0.0587526052,0.0425402198,0.0199817196,-0.1194970226,-0.0400180362,-0.0359071436,-0.1018610177,0.0005507326]},"222":{"Abstract":"We investigate whether the notion of active reading for text might be usefully applied to visualizations. Through a qualitative study we explored whether people apply observable active reading techniques when reading paper-based node-link visualizations. Participants used a range of physical actions while reading, and from these we synthesized an initial set of active reading techniques for visualizations. To learn more about the potential impact such techniques may have on visualization reading, we implemented support for one type of physical action from our observations (making freeform marks) in an interactive node-link visualization. Results from our quantitative study of this implementation show that interactive support for active reading techniques can improve the accuracy of performing low-level visualization tasks. Together, our studies suggest that the active reading space is ripe for research exploration within visualization and can lead to new interactions that make for a more flexible and effective visualization reading experience.","Authors":"J. Walny; S. Huron; C. Perin; T. Wun; R. Pusch; S. Carpendale","DOI":"10.1109\/TVCG.2017.2745958","Keywords":"active reading of visualizations;active reading;information visualization;spectrum of physical engagement","Keywords_Processed":"active reading;spectrum of physical engagement;information visualization;active reading of visualization","Title":"Active Reading of Visualizations","Labels":null,"Keyword_Vector":[0.1587133104,-0.0535811939,-0.1337780368,-0.0309812539,-0.0625787178,-0.0102542722,-0.0132265979,0.0288391595,0.0169363905,0.015696429,0.0886574931,-0.117415859,0.0515255249,0.0255710958,-0.0527311668,-0.0397703469,-0.0291835563,-0.0095151318,-0.0348035235,-0.0001471558,-0.0005782977,0.0466850889,-0.0100682853,0.0611158626],"Abstract_Vector":[0.223275831,-0.1079915847,0.1773296505,-0.0428194913,0.0863085279,0.1335888408,0.0207809045,-0.0167333605,0.0234552611,-0.0211168844,-0.0664299564,-0.1295841493,-0.0853787166,-0.0095592896,0.0025642247,0.0347739631,0.0902102033,0.1099973017,-0.0773237565,0.0831746455,-0.0766995416,0.0333665818,-0.0250806823,0.0001338178,-0.0814269873,-0.027665458,0.0294202112,0.0252949421,0.0218325858,0.0094490179,0.0438141453,0.0632956553,0.0435316896,0.0746020483,-0.0865390428,0.0504096326,-0.0286059719,0.0015107093,0.0253893907,0.0469778286,0.0066784751,0.0392861964,0.1067581335,0.0218860886,-0.115019023,0.0309585847,0.037732121,-0.0834730943,0.1294537843,0.0044396435,0.0641850026,-0.0664806806,0.0871438291,0.0556433511,0.0512730968,0.0469369125,0.0137952353,0.0707870409,0.0008071489,-0.0073470655,-0.0492162348,0.0525013022,-0.0207974104,0.0484938138,0.0649809456,0.1141447339,-0.0542100463,0.0038359039,-0.0440015716,-0.090592974,-0.0237874093,-0.0355073846,0.0605152067,-0.0581083562,0.1033867937,-0.0502872159]},"223":{"Abstract":"Motion in depth is commonly misperceived in Virtual Reality (VR), making it difficult to intercept moving objects, for example, in games. We investigate whether motion cues could be modified to improve these interactions in VR. We developed a time-to-contact estimation task, in which observers (n = 18) had to indicate by button press when a looming virtual object would collide with their head. We show that users consistently underestimate speed. We construct a user-specific model of motion-in-depth perception, and use this model to propose a novel method to modify monocular depth cues tailored to the specific user, correcting individual response errors in speed estimation. A user study was conducted in a simulated baseball environment and observers were asked to hit a looming baseball back in the direction of the pitcher. The study was conducted with and without intervention and demonstrates the effectiveness of the method in reducing interception errors following cue modifications. The intervention was particularly effective at fast ball speeds where performance is most limited by the user's sensorimotor constraints. The proposed approach is easy to implement and could improve the user experience of interacting with dynamic virtual environments.","Authors":"R. A. Rolin; J. Fooken; M. Spering; D. K. Pai","DOI":"10.1109\/TVCG.2018.2859987","Keywords":"Virtual reality;motion perception;time to contact;augmented reality;games","Keywords_Processed":"game;time to contact;augment reality;virtual reality;motion perception","Title":"Perception of Looming Motion in Virtual Reality Egocentric Interception Tasks","Labels":null,"Keyword_Vector":[0.2177267159,0.5598675266,0.0767865158,-0.1204046676,0.0891090503,-0.0281825319,0.0241338833,-0.1661899202,0.0627439281,0.0165295397,0.0214825712,-0.1130439492,-0.0665894985,-0.1532583919,-0.0093066084,-0.0948403224,0.1090296396,-0.000303548,-0.0807701783,-0.1621160378,-0.0628151338,-0.0001137024,-0.0443466957,0.0670996342],"Abstract_Vector":[0.2462382105,0.2029450551,0.242091838,-0.0331607349,-0.0974239359,-0.1104874922,-0.0242948226,-0.0623133229,0.0897768874,-0.0047066621,0.0085754079,-0.066500906,0.1199729896,-0.0102864521,0.0535499672,0.0476050881,-0.010319818,0.0180022126,0.0230959834,-0.0929147729,0.0210505705,0.0001305756,-0.0366635729,-0.149637489,-0.0865269082,-0.019559714,-0.0529356574,0.0083409338,-0.0488658851,-0.0020691546,0.0911454062,0.0809878816,0.0224616853,-0.040772057,-0.077042068,-0.004318804,-0.0315298905,-0.0627889391,-0.1042971707,0.0111931081,-0.0189157124,-0.0044667561,-0.0392375606,-0.0083225833,0.1210973101,-0.052880454,0.0524904818,0.0360005866,0.0121491856,-0.1321665253,0.0309987426,-0.0108735591,-0.1010401011,0.0630729176,-0.0162864345,0.0997901877,-0.0735941747,-0.0508711364,-0.0155108352,0.0587982516,-0.0037759066,0.0488954597,0.0523574576,-0.0139905136,-0.0278405447,0.0753778355,0.085661785,0.105785447,-0.0438202178,-0.1062784476,0.0518181082,-0.0110488073,-0.10172641,0.0526779383,0.0253450804,0.0031358523]},"224":{"Abstract":"Bipartite graphs model the key relations in many large scale real-world data: customers purchasing items, legislators voting for bills, people's affiliation with different social groups, faults occurring in vehicles, etc. However, it is challenging to visualize large scale bipartite graphs with tens of thousands or even more nodes or edges. In this paper, we propose a novel visual summarization technique for bipartite graphs based on the minimum description length (MDL) principle. The method simultaneously groups the two different set of nodes and constructs aggregated bipartite relations with balanced granularity and precision. It addresses the key trade-off that often occurs for visualizing large scale and noisy data: acquiring a clear and uncluttered overview while maximizing the information content in it. We formulate the visual summarization task as a co-clustering problem and propose an efficient algorithm based on locality sensitive hashing (LSH) that can easily scale to large graphs under reasonable interactive time constraints that previous related methods cannot satisfy. The method leads to the opportunity of introducing a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. In the framework, we also introduce a compact visual design inspired by adjacency list representation of graphs as the building block for a small multiples display to compare the bipartite relations for different subsets of data. We showcase the applicability and effectiveness of our approach by applying it on synthetic data with ground truth and performing case studies on real-world datasets from two application domains including roll-call vote record analysis and vehicle fault pattern analysis. Interviews with experts in the political science community and the automotive industry further highlight the benefits of our approach.","Authors":"G. Y. Chan; P. Xu; Z. Dai; L. Ren","DOI":"10.1109\/TVCG.2018.2864826","Keywords":"Bipartite Graph;Visual Summarization;Minimum Description Length;Information Theory","Keywords_Processed":"bipartite graph;information theory;visual summarization;minimum description length","Title":"ViBr: Visualizing Bipartite Relations at Scale with the Minimum Description Length Principle","Labels":null,"Keyword_Vector":[0.1103242737,-0.0158012393,0.0762130338,0.0131440905,-0.1001690093,-0.0364934208,0.0244293064,0.0359765077,-0.0008236404,0.1212670492,0.0348904503,-0.0448362116,0.0947256479,-0.0278482495,-0.1260579804,-0.0263745261,-0.0297520905,0.0757367629,0.0005458454,-0.0597904915,-0.0267641956,-0.0813229082,0.0237747006,0.0353756296],"Abstract_Vector":[0.3162212956,-0.1202410475,-0.0799035743,-0.0184452127,-0.0018995052,-0.1229869305,0.1295406019,0.067129826,0.0673218759,0.0433150602,-0.1343051625,0.0596153556,-0.1160576969,-0.0472534976,0.0746134231,0.0089741069,0.0634664588,0.0359913066,0.0293647894,-0.0258131414,0.0333890549,-0.2100904396,0.0296243607,0.1261886783,0.045097196,-0.031844066,0.0278340449,0.0011318014,-0.0390413493,0.0089915813,0.0667142564,0.0098328741,0.168355893,-0.0160698316,0.0488865942,-0.1023928071,0.0036477191,-0.0463851128,-0.0060884023,0.068757178,0.0038687257,-0.0429736327,0.0302357664,-0.0652591826,0.1460714482,-0.043904075,-0.0028165585,0.0594199947,0.0796562914,0.0142151684,-0.0026222705,-0.0076099153,0.0985520441,0.0001379176,-0.0115138364,0.0227822782,-0.0469488923,-0.0011456795,-0.0118776675,0.0409679649,-0.0290294681,0.0426195879,-0.0395567643,0.1192146909,0.012713586,-0.0144832225,0.0387459044,-0.0401746369,0.0272671039,0.0085826812,-0.0091494193,-0.0229477804,0.0268250539,0.0328730431,-0.0230659546,0.0544774784]},"225":{"Abstract":"We present a method for realtime reconstruction of an animating human body,which produces a sequence of deforming meshes representing a given performance captured by a single commodity depth camera. We achieve realtime single-view mesh completion by enhancing the parameterized SCAPE model.Our method, which we call Realtime SCAPE, performs full-body reconstruction without the use of markers.In Realtime SCAPE, estimations of body shape parameters and pose parameters, needed for reconstruction, are decoupled. Intrinsic body shape is first precomputed for a given subject, by determining shape parameters with the aid of a body shape database. Subsequently, per-frame pose parameter estimation is performed by means of linear blending skinning (LBS); the problem is decomposed into separately finding skinning weights and transformations. The skinning weights are also determined offline from the body shape database,reducing online reconstruction to simply finding the transformations in LBS. Doing so is formulated as a linear variational problem;carefully designed constraints are used to impose temporal coherence and alleviate artifacts. Experiments demonstrate that our method can produce full-body mesh sequences with high fidelity.","Authors":"Y. Chen; Z. Cheng; C. Lai; R. R. Martin; G. Dang","DOI":"10.1109\/TVCG.2015.2478779","Keywords":"Realtime reconstruction;human animation;depth camera;SCAPE","Keywords_Processed":"scape;realtime reconstruction;depth camera;human animation","Title":"Realtime Reconstruction of an Animating Human Body from a Single Depth Camera","Labels":null,"Keyword_Vector":[0.042666979,0.0299720245,0.0307977267,0.1218292869,0.054831539,-0.1315222871,0.0924119189,-0.1037972527,-0.1636619801,-0.1451349359,0.0428537246,-0.1397820814,0.0381848751,-0.1057367143,0.1278123936,-0.0054132247,0.0267184757,-0.1083912563,-0.0103274365,0.0991190341,-0.0227536156,0.0608942816,0.0373451213,-0.0716544074],"Abstract_Vector":[0.1645438907,0.1652952381,-0.0840290799,-0.0790349288,-0.0531841117,-0.0403618505,-0.1563744459,-0.0845247809,0.0910875271,-0.0059094587,-0.0479334107,-0.1042091292,-0.1211233428,-0.0637784329,-0.0232513419,-0.0931087582,-0.1070456045,-0.1036266572,-0.0154663347,-0.0749546239,-0.0170240522,-0.0316060271,0.0473959816,0.0724089899,0.0188411972,-0.1270938298,-0.1241233969,-0.0189360442,0.0493986466,0.0249526987,0.058492959,-0.0996968238,-0.1398983632,0.0989125396,0.2045807286,-0.0022652109,-0.0945570132,0.1313670294,0.0887146871,0.0436151781,-0.0533552653,0.0145523287,0.084506217,-0.0795303379,0.0759791719,0.2157353483,-0.0142688434,0.1131821061,-0.0393080017,-0.0377306569,0.0658112493,0.0236019577,-0.0786007868,-0.0477208517,-0.0318061496,0.0327628386,0.0133303582,0.0441644919,0.0058621236,-0.0440910839,-0.0390549731,-0.0709540296,0.0350319915,0.0419809612,-0.1566213542,0.0807832099,0.0278622144,0.0458083276,0.0349200139,-0.0511534409,-0.0411049944,0.1148569176,0.0642655168,-0.0083353816,-0.0478255702,-0.0338862707]},"226":{"Abstract":"We present a novel method to optimize the attenuation of light for the single scattering model in direct volume rendering. A common problem of single scattering is the high dynamic range between lit and shadowed regions due to the exponential attenuation of light along a ray. Moreover, light is often attenuated too strong between a sample point and the camera, hampering the visibility of important features. Our algorithm employs an importance function to selectively illuminate important structures and make them visible from the camera. With the importance function, more light can be transmitted to the features of interest, while contextual structures cast shadows which provide visual cues for perception of depth. At the same time, more scattered light is transmitted from the sample point to the camera to improve the primary visibility of important features. We formulate a minimization problem that automatically determines the extinction along a view or shadow ray to obtain a good balance between sufficient transmittance and attenuation. In contrast to previous approaches, we do not require a computationally expensive solution of a global optimization, but instead provide a closed-form solution for each sampled extinction value along a view or shadow ray and thus achieve interactive performance.","Authors":"M. Ament; T. Zirr; C. Dachsbacher","DOI":"10.1109\/TVCG.2016.2569080","Keywords":"Direct volume rendering;volume illumination;extinction optimization","Keywords_Processed":"extinction optimization;direct volume rendering;volume illumination","Title":"Extinction-Optimized Volume Illumination","Labels":null,"Keyword_Vector":[0.1035859772,-0.0505557977,-0.10672186,0.4479093418,0.3359510606,0.2945152351,-0.0706266374,-0.0769469986,0.1928381745,0.0648194648,0.0652079804,0.081536999,-0.0221340716,-0.0164076902,-0.0468372471,-0.0827431832,0.0641510478,0.0244917648,0.0041598919,0.0141033789,-0.0396068524,-0.1177892117,0.0164501959,0.0430981481],"Abstract_Vector":[0.1929742031,0.1475626087,-0.0842613418,0.0936251632,0.0617491156,0.0064310661,-0.0001810417,0.1010161105,-0.0941741204,0.0336560684,-0.1416108255,0.0583409133,0.113399253,-0.0548772208,-0.0171558332,-0.0326084001,-0.0717714246,-0.0806334784,-0.1304333669,0.1215039162,0.0466055078,0.1867103862,-0.0197054599,-0.1712738025,0.0698699661,-0.1635080704,-0.0824812853,-0.1474782236,0.0175396075,0.020947035,-0.0473593933,-0.01428459,-0.0614804287,-0.0854950984,-0.0348244299,0.0592162925,0.0230171634,-0.0050094986,-0.1229812188,0.1441835873,0.0410294139,-0.0827690756,0.213398282,0.0723659495,0.0361440383,-0.0805214104,-0.0386408312,0.0842433508,0.0717053233,-0.0469777431,0.052790455,0.0508923452,0.0200317752,-0.0402206307,0.0318252126,0.1076101164,-0.009498376,-0.0204920386,0.0398851034,0.0204288783,-0.1370125858,-0.0298441114,-0.0241836174,-0.0003518049,-0.062449887,0.018187444,0.1020858259,0.0172292165,0.0502955719,0.0427935669,0.0453916576,-0.0065487797,0.0865965248,0.024631082,-0.0281601166,0.0089337169]},"227":{"Abstract":"Understanding and accounting for uncertainty is critical to effectively reasoning about visualized data. However, evaluating the impact of an uncertainty visualization is complex due to the difficulties that people have interpreting uncertainty and the challenge of defining correct behavior with uncertainty information. Currently, evaluators of uncertainty visualization must rely on general purpose visualization evaluation frameworks which can be ill-equipped to provide guidance with the unique difficulties of assessing judgments under uncertainty. To help evaluators navigate these complexities, we present a taxonomy for characterizing decisions made in designing an evaluation of an uncertainty visualization. Our taxonomy differentiates six levels of decisions that comprise an uncertainty visualization evaluation: the behavioral targets of the study, expected effects from an uncertainty visualization, evaluation goals, measures, elicitation techniques, and analysis approaches. Applying our taxonomy to 86 user studies of uncertainty visualizations, we find that existing evaluation practice, particularly in visualization research, focuses on Performance and Satisfaction-based measures that assume more predictable and statistically-driven judgment behavior than is suggested by research on human judgment and decision making. We reflect on common themes in evaluation practice concerning the interpretation and semantics of uncertainty, the use of confidence reporting, and a bias toward evaluating performance as accuracy rather than decision quality. We conclude with a concrete set of recommendations for evaluators designed to reduce the mismatch between the conceptualization of uncertainty in visualization versus other fields.","Authors":"J. Hullman; X. Qiao; M. Correll; A. Kale; M. Kay","DOI":"10.1109\/TVCG.2018.2864889","Keywords":"Uncertainty visualization;user study;subjective confidence;probability distribution","Keywords_Processed":"user study;probability distribution;uncertainty visualization;subjective confidence","Title":"In Pursuit of Error: A Survey of Uncertainty Visualization Evaluation","Labels":null,"Keyword_Vector":[0.214034184,-0.0274341616,-0.1614209555,-0.0260378699,-0.1503685797,0.0288367491,-0.0404012445,0.0094001324,-0.0030426783,-0.1866485656,-0.0559586076,0.0750233172,0.0407901985,0.04309826,0.1505390729,0.0310788939,0.126141484,0.224227189,-0.1068129123,0.0941542247,-0.0634122363,-0.1971670985,-0.0214755933,0.0598257804],"Abstract_Vector":[0.2256831746,-0.1622644476,0.118636322,0.2105997962,0.1112056075,0.260454542,-0.1117996009,-0.2012840473,0.1905566565,-0.1485947413,-0.1327836501,0.2034085074,-0.0213198265,-0.014598595,-0.0395958702,0.0800755117,0.0783319853,0.050838248,-0.075181613,-0.047673106,0.1134451757,0.0825438898,0.1249850611,-0.069675226,0.0363954825,-0.0764569131,-0.0061320947,0.1102835096,0.0783208604,0.0004788749,0.0323695485,-0.0583480738,-0.0629691707,-0.0961253786,0.0512651184,0.0129419929,-0.0089548609,-0.0242689992,0.0813293654,0.0487738572,0.0120340045,0.0513795068,-0.0684990267,0.0505830802,-0.0407247398,0.0868595371,-0.0354085616,-0.0095982242,-0.0297496831,-0.0811944963,-0.0772838275,-0.0369208761,-0.0633463621,0.0329374738,-0.0647199139,0.0123636971,0.025013421,-0.0555895953,0.0105254284,0.003699929,0.0900915529,0.0148208461,0.0088224571,-0.0182327288,0.1109381701,0.0095263292,0.0049800605,-0.0302882586,-0.0227900137,0.0661424761,-0.0355338606,-0.0115457869,0.008752655,0.0365091684,0.0457937045,0.0428639864]},"228":{"Abstract":"We present a voxel-based rendering pipeline for large 3D line sets that employs GPU ray-casting to achieve scalable rendering including transparency and global illumination effects. Even for opaque lines we demonstrate superior rendering performance compared to GPU rasterization of lines, and when transparency is used we can interactively render amounts of lines that are infeasible to be rendered via rasterization. We propose a direction-preserving encoding of lines into a regular voxel grid, along with the quantization of directions using face-to-face connectivity in this grid. On the regular grid structure, parallel GPU ray-casting is used to determine visible fragments in correct visibility order. To enable interactive rendering of global illumination effects like low-frequency shadows and ambient occlusions, illumination simulation is performed during ray-casting on a level-of-detail (LoD) line representation that considers the number of lines and their lengths per voxel. In this way we can render effects which are very difficult to render via GPU rasterization. A detailed performance and quality evaluation compares our approach to rasterization-based rendering of lines.","Authors":"M. Kanzler; M. Rautenhaus; R. Westermann","DOI":"10.1109\/TVCG.2018.2834372","Keywords":"Ray-casting;large 3D line sets;transparency;global illumination","Keywords_Processed":"global illumination;ray casting;large 3d line set;transparency","Title":"A Voxel-Based Rendering Pipeline for Large 3D Line Sets","Labels":null,"Keyword_Vector":[0.0535951272,0.0278590762,-0.0003606761,0.0925724573,0.0684379502,-0.0657456703,0.0315647203,-0.0041961892,-0.0096835583,0.0474548007,0.0080569172,0.0415692628,0.0227439508,0.0762353913,0.0006697812,0.1362356259,-0.0049678312,-0.0868622991,-0.0212690034,0.1633049701,0.0812526358,-0.080712902,-0.0319377457,0.0116310765],"Abstract_Vector":[0.1622720203,0.1337894911,-0.0491205716,0.081162507,-0.0030428845,0.052959296,0.1040776272,0.1839960943,-0.1596169794,-0.094332768,-0.1315435316,0.0070044014,0.0173154749,0.0687547503,0.0095885701,0.0352413721,-0.0354135489,-0.0319424423,0.095764544,0.0070911482,-0.0547484653,0.1631966355,0.0902604409,-0.0911806373,0.0525552071,-0.1521232033,-0.0633042806,-0.0575720662,0.0936543127,-0.0505236879,0.0208611061,0.069586349,-0.0442451829,-0.0537356605,-0.0752635356,-0.1432090643,-0.087893995,-0.0907706171,0.0949342432,0.1493761488,0.0150956673,-0.1151810167,0.0731240025,0.0107201298,-0.0967319217,-0.0376176725,0.0324893158,0.0476424041,-0.0063747533,0.0134910625,0.0092059452,0.0470403404,0.0372714137,0.0053896728,-0.031446956,-0.1093979462,-0.0214333562,0.0515442866,0.0241928344,-0.0491987154,0.1967474208,-0.062665557,0.0426016024,-0.006121009,-0.0450096432,0.0567816564,0.0634417562,-0.034779032,0.0212433036,0.0268416517,-0.0072246498,-0.0407414227,-0.0580893658,0.0032087632,0.0593625327,0.002216018]},"229":{"Abstract":"Visual designs can be complex in modern data visualization systems, which poses special challenges for explaining them to the non-experts. However, few if any presentation tools are tailored for this purpose. In this study, we present Narvis, a slideshow authoring tool designed for introducing data visualizations to non-experts. Narvis targets two types of end users: teachers, experts in data visualization who produce tutorials for explaining a data visualization, and students, non-experts who try to understand visualization designs through tutorials. We present an analysis of requirements through close discussions with the two types of end users. The resulting considerations guide the design and implementation of Narvis. Additionally, to help teachers better organize their introduction slideshows, we specify a data visualization as a hierarchical combination of components, which are automatically detected and extracted by Narvis. The teachers craft an introduction slideshow through first organizing these components, and then explaining them sequentially. A series of templates are provided for adding annotations and animations to improve efficiency during the authoring process. We evaluate Narvis through a qualitative analysis of the authoring experience, and a preliminary evaluation of the generated slideshows.","Authors":"Q. Wang; Z. Li; S. Fu; W. Cui; H. Qu","DOI":"10.1109\/TVCG.2018.2865232","Keywords":"Education;Narrative Visualization;Authoring Tools","Keywords_Processed":"author tool;education;narrative visualization","Title":"Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs","Labels":null,"Keyword_Vector":[0.117992015,-0.0398510537,-0.1069661561,0.0048820957,-0.0531897216,-0.0034033829,0.0115887545,-0.018757689,0.0020509708,0.0221407461,-0.0141129125,-0.0412389332,0.0225011508,0.0139078402,-0.0206166258,0.0075952031,-0.0510461217,-0.0078659372,-0.0250198272,-0.0312948731,0.0057435382,0.0891196229,-0.002057782,-0.0545192582],"Abstract_Vector":[0.2186070909,-0.1276589702,0.0922530666,-0.1009624983,0.0919600374,0.0409157007,-0.093433982,-0.0599527029,-0.1222188008,-0.0368530571,0.0551984849,-0.0739464973,-0.0077815404,-0.0691484398,-0.0568144898,0.0136012199,0.0695926266,-0.0002459637,-0.03583851,-0.0316685005,-0.0098516238,-0.0166155196,-0.0254768371,-0.0422766057,0.025983815,0.0980618281,0.0080652305,0.0131119085,0.0460656737,-0.0004963365,0.0260051065,0.0046929396,-0.0624777402,-0.0518667621,0.0561105856,-0.0575830483,0.0498633362,-0.092961147,-0.0756149779,0.0615152201,-0.0094286284,0.0014660412,-0.0689272453,-0.0513519696,-0.0125462932,0.0242644775,-0.036039176,-0.0616233887,-0.0008916268,-0.0361570286,-0.0576574146,0.0009588637,-0.0379399384,-0.1689225017,-0.0135701138,-0.0132228526,-0.0950407193,0.0980350177,0.0674536544,-0.0292288245,0.0408411059,-0.0846537338,-0.0266900871,-0.0075807127,-0.0521304838,-0.0987075506,0.0365128406,0.1369762556,0.0054840232,0.1778331505,0.061192785,-0.048394121,0.0256454767,-0.0130693037,0.1601702519,-0.0047891203]},"230":{"Abstract":"Due to the perceptual characteristics of the head, vibrotactile Head-mounted Displays are built with low actuator density. Therefore, vibrotactile guidance is mostly assessed by pointing towards objects in the azimuthal plane. When it comes to multisensory interaction in 3D environments, it is also important to convey information about objects in the elevation plane. In this paper, we design and assess a haptic guidance technique for 3D environments. First, we explore the modulation of vibration frequency to indicate the position of objects in the elevation plane. Then, we assessed a vibrotactile HMD made to render the position of objects in a 3D space around the subject by varying both stimulus loci and vibration frequency. Results have shown that frequencies modulated with a quadratic growth function allowed a more accurate, precise, and faster target localization in an active head pointing task. The technique presented high usability and a strong learning effect for a haptic search across different scenarios in an immersive VR setup.","Authors":"V. A. de Jesus Oliveira; L. Brayda; L. Nedel; A. Maciel","DOI":"10.1109\/TVCG.2017.2657238","Keywords":"Vibrotactile head-mounted display;haptic interaction;spatial awereness;3D environments","Keywords_Processed":"vibrotactile head mount display;3d environment;haptic interaction;spatial awereness","Title":"Designing a Vibrotactile Head-Mounted Display for Spatial Awareness in 3D Spaces","Labels":null,"Keyword_Vector":[0.1571715581,0.2317744313,-0.0250488101,0.1193334827,0.0751345454,-0.0300287957,-0.0865989317,0.256728119,-0.0218008516,-0.0287126258,-0.1227091712,0.0870267423,-0.043862328,-0.0051824724,0.1549742455,0.1359497415,-0.2046377428,-0.2167948456,0.0248171617,0.1266693583,0.1004229802,0.0080953438,0.1375232607,-0.1957163803],"Abstract_Vector":[0.182550877,0.1376613567,0.2074207802,0.0856404251,-0.0394146246,0.0416132406,0.0421457984,0.0413456166,0.0073329634,0.1120242899,0.0060554576,-0.0294364921,0.0616294211,0.0096079806,0.0638808164,-0.058637989,-0.0204277627,0.0086136726,-0.0857978446,-0.0143209967,0.0029367792,-0.052251143,-0.1112239482,0.0818784023,-0.0056849721,-0.0333882204,0.0165354155,-0.0482087709,0.0068991168,-0.1711156898,-0.0790462296,-0.0593893201,-0.0795922442,0.0836151499,-0.1215934399,-0.1156916245,-0.0485723459,0.0457113796,0.034699476,0.082228502,0.0881624939,0.0436690442,-0.0650372207,-0.0069053881,-0.1520904066,-0.0390718907,0.0616725173,0.0701100827,-0.1061914442,-0.0423287424,0.0294050994,0.1686127829,-0.0892102193,0.0630968666,0.0239498633,0.0794773762,0.0348929971,-0.0032560788,0.015630977,-0.0736922316,0.011605851,0.0437957799,0.0152883344,0.0789328953,0.1358651945,0.028631331,-0.0314270227,0.0220640753,-0.0149731019,0.0092916557,0.0657537818,-0.1125586059,0.0465479016,0.0844020104,-0.0041082131,0.0161510571]},"231":{"Abstract":"System schematics, such as those used for electrical or hydraulic systems, can be large and complex. Fisheye techniques can help navigate such large documents by maintaining the context around a focus region, but the distortion introduced by traditional fisheye techniques can impair the readability of the diagram. We present SchemeLens, a vector-based, topology-aware fisheye technique which aims to maintain the readability of the diagram. Vector-based scaling reduces distortion to components, but distorts layout. We present several strategies to reduce this distortion by using the structure of the topology, including orthogonality and alignment, and a model of user intention to foster smooth and predictable navigation. We evaluate this approach through two user studies: Results show that (1) SchemeLens is 16-27% faster than both round and rectangular flat-top fisheye lenses at finding and identifying a target along one or several paths in a network diagram; (2) augmenting SchemeLens with a model of user intentions aids in learning the network topology.","Authors":"A. Coh\u00e9; B. Liutkus; G. Bailly; J. Eagan; E. Lecolinet","DOI":"10.1109\/TVCG.2015.2467035","Keywords":"Fisheye;vector-scaling;content-aware;network schematics;interactive zoom;navigation;information visualization;Fisheye;vector-scaling;content-aware;network schematics;interactive zoom;navigation;information visualization","Keywords_Processed":"information visualization;fisheye;navigation;network schematic;content aware;vector scaling;interactive zoom","Title":"SchemeLens: A Content-Aware Vector-Based Fisheye Technique for Navigating Large Systems Diagrams","Labels":null,"Keyword_Vector":[0.1865126353,-0.0524460144,-0.1062943569,0.0224317745,-0.1356688801,-0.0858867205,0.1172954436,0.0306717135,-0.0429211293,0.1777350064,0.0215300774,-0.163785982,0.0506583171,0.1263171756,-0.1480807935,-0.023455865,0.0791679072,-0.0629716685,-0.0339733367,-0.0025832945,0.0162007535,-0.1177668131,0.0556173735,0.0524267319],"Abstract_Vector":[0.1938984978,-0.0140226883,0.0404454641,0.0417162185,0.1056625292,-0.1520035381,0.1003079581,0.1517258011,0.1538218613,-0.0328905901,0.0350367193,0.035684904,-0.0699827028,-0.0592486293,-0.0347824279,0.027022315,-0.1423549969,0.1071169414,0.0029364536,-0.1234129552,-0.0013093902,0.0193923133,-0.1061711818,-0.0867583853,0.0616771491,0.0011885344,0.0644826913,-0.1452373989,0.0973751384,0.0593318684,0.004094744,0.1351778917,-0.0339374358,0.0863713772,0.0309891627,-0.1161571433,0.1249900892,-0.0348411931,-0.1309996405,-0.1211155178,0.1137815925,-0.0296116543,0.0656139455,0.2172313223,-0.0270084034,0.2287223781,0.0030922828,-0.0548252641,-0.1094494468,-0.1674663191,0.0555168305,-0.0954890121,0.0027841664,-0.1317183202,-0.0813322718,-0.1043189546,-0.0485100802,-0.0977446006,-0.046729087,-0.0723268135,0.0258418392,-0.0240983335,0.0692983905,0.0608933813,-0.0464188826,-0.0159651191,-0.0234361387,-0.0752008573,0.0060122636,-0.0932193958,-0.0125809894,-0.0105788988,0.0553172501,-0.038688191,0.0229639293,-0.0641056739]},"232":{"Abstract":"Selecting a good aspect ratio is crucial for effective 2D diagrams. There are several aspect ratio selection methods for function plots and line charts, but only few can handle general, discrete diagrams such as 2D scatter plots. However, these methods either lack a perceptual foundation or heavily rely on intermediate isoline representations, which depend on choosing the right isovalues and are time-consuming to compute. This paper introduces a general image-based approach for selecting aspect ratios for a wide variety of 2D diagrams, ranging from scatter plots and density function plots to line charts. Our approach is derived from Federer's co-area formula and a line integral representation that enable us to directly construct image-based versions of existing selection methods using density fields. In contrast to previous methods, our approach bypasses isoline computation, so it is faster to compute, while following the perceptual foundation to select aspect ratios. Furthermore, this approach is complemented by an anisotropic kernel density estimation to construct density fields, allowing us to more faithfully characterize data patterns, such as the subgroups in scatterplots or dense regions in time series. We demonstrate the effectiveness of our approach by quantitatively comparing to previous methods and revisiting a prior user study. Finally, we present extensions for ROI banking, multi-scale banking, and the application to image data.","Authors":"Y. Wang; Z. Wang; C. Fu; H. Schmauder; O. Deussen; D. Weiskopf","DOI":"10.1109\/TVCG.2018.2865266","Keywords":"Aspect ratio;image-based method;Federer's co-area formula;density field;anisotropic kernel density estimation","Keywords_Processed":"aspect ratio;density field;federer co area formula;anisotropic kernel density estimation;image base method","Title":"Image-Based Aspect Ratio Selection","Labels":null,"Keyword_Vector":[0.0491801283,-0.0057468411,-0.0106907868,0.0755112309,0.0240238106,-0.0899562961,0.0991754992,-0.0784801588,-0.0541146847,-0.0008689121,-0.0076103607,0.0599376941,0.0245967396,-0.0538282601,-0.0174563402,0.0587179423,-0.1581594028,0.0910475758,-0.0413167268,-0.0517072085,0.0024194793,-0.0279259285,-0.0494016633,0.027644725],"Abstract_Vector":[0.2358662576,0.0532599246,-0.1404021963,0.0701189622,-0.0411724785,0.102152416,0.0825337695,0.0676121163,0.0099090426,-0.0554954958,-0.0040037011,-0.0535428948,0.0136040166,0.0301254308,0.0567081456,0.0559730413,0.0411463583,-0.0722288374,0.0356651332,-0.0358313226,-0.0854072543,-0.0724956009,0.0683023729,-0.1767145315,-0.0094675565,-0.0728848034,0.0502066333,-0.0073645315,0.1000390357,-0.1091120018,-0.0765545344,0.0464042058,0.1440817753,0.1304008986,0.0878919194,-0.0737396823,-0.0065791533,0.016184431,-0.0653090958,-0.1392941948,-0.113673822,-0.1252397836,-0.0358121565,0.027225242,0.0412496056,-0.2162275265,0.0182317407,0.1107711492,-0.0660527991,-0.0696763148,-0.0736198764,0.1322480369,-0.0911287635,-0.0108648923,-0.0448389887,-0.0393717123,-0.0001380156,-0.1196112253,-0.077743329,0.0305973479,0.0112545365,0.0218767884,-0.0394275911,0.0284017026,-0.0810790264,0.0153845916,-0.1789471267,0.0137571369,-0.0545164349,-0.0268769186,-0.0065191783,-0.0896461116,-0.0360229913,-0.0865592532,-0.0001749694,-0.0282092903]},"233":{"Abstract":"Understanding the movement patterns of collectives, such as flocks of birds or fish swarms, is an interesting open research question. The collectives are driven by mutual objectives or react to individual direction changes and external influence factors and stimuli. The challenge in visualizing collective movement data is to show space and time of hundreds of movements at the same time to enable the detection of spatiotemporal patterns. In this paper, we propose MotionRugs, a novel space efficient technique for visualizing moving groups of entities. Building upon established space-partitioning strategies, our approach reduces the spatial dimensions in each time step to a one-dimensional ordered representation of the individual entities. By design, MotionRugs provides an overlap-free, compact overview of the development of group movements over time and thus, enables analysts to visually identify and explore group-specific temporal patterns. We demonstrate the usefulness of our approach in the field of fish swarm analysis and report on initial feedback of domain experts from the field of collective behavior.","Authors":"J. Buchm\u00fcller; D. J\u00e4ckle; E. Cakmak; U. Brandes; D. A. Keim","DOI":"10.1109\/TVCG.2018.2865049","Keywords":"Spatio-Temporal Visualization;Spatial Abstraction;Spatial Index Structures;Collective Movement","Keywords_Processed":"spatial abstraction;collective movement;spatio temporal visualization;spatial index structures","Title":"MotionRugs: Visualizing Collective Trends in Space and Time","Labels":null,"Keyword_Vector":[0.1653848189,0.0067323033,-0.0323335407,0.0395817885,-0.0275242448,-0.027543542,-0.0730166889,0.0644792462,0.0626940811,-0.1647894903,0.0976865365,-0.2396954042,-0.1954533908,-0.0955084045,0.0693883136,-0.041218087,-0.1726993513,0.019318649,0.14609239,0.0514883903,0.2270967273,-0.1530254663,-0.0512688958,-0.0749550714],"Abstract_Vector":[0.2122337506,-0.0503469511,-0.0533584163,0.0129477641,-0.0660645246,-0.0857000381,0.0794163341,-0.1299008254,-0.033373777,0.0202011925,0.0576838083,0.0618444481,-0.039265426,0.0774592289,0.1431108121,0.0703501591,-0.0492063245,-0.0686788695,0.0278067869,0.0211916342,-0.1469127012,-0.0194833648,-0.0775643284,-0.0154966944,0.0297696104,-0.0680654718,0.0838370562,0.1134258222,0.04054392,0.0823951934,0.055964291,-0.0294590979,0.0004596874,0.0674056428,-0.098542251,0.0794183091,0.0757387049,-0.0888583786,0.1444085202,0.0247820961,0.1065952261,-0.1316472356,-0.0963860082,-0.053059565,-0.03701901,0.0095459608,-0.0398748994,-0.0156065898,0.1072649968,-0.1094229059,-0.0876059065,0.0601965177,0.0021339474,-0.0095533774,0.0538252347,-0.0137372433,0.0638770863,-0.0519052795,-0.0505735967,0.04570355,-0.0698910842,-0.0708542906,-0.1356909554,-0.0713837785,0.115291106,-0.0688422458,-0.0409411161,0.0183400922,-0.0297524766,-0.0060121924,-0.0262159223,0.0540834347,-0.0042617807,0.0567882576,-0.0345684704,-0.0401987382]},"234":{"Abstract":"Although there has been a great deal of interest in analyzing customer opinions and breaking news in microblogs, progress has been hampered by the lack of an effective mechanism to discover and retrieve data of interest from microblogs. To address this problem, we have developed an uncertainty-aware visual analytics approach to retrieve salient posts, users, and hashtags. We extend an existing ranking technique to compute a multifaceted retrieval result: the mutual reinforcement rank of a graph node, the uncertainty of each rank, and the propagation of uncertainty among different graph nodes. To illustrate the three facets, we have also designed a composite visualization with three visual components: a graph visualization, an uncertainty glyph, and a flow map. The graph visualization with glyphs, the flow map, and the uncertainty analysis together enable analysts to effectively find the most uncertain results and interactively refine them. We have applied our approach to several Twitter datasets. Qualitative evaluation and two real-world case studies demonstrate the promise of our approach for retrieving high-quality microblog data.","Authors":"M. Liu; S. Liu; X. Zhu; Q. Liao; F. Wei; S. Pan","DOI":"10.1109\/TVCG.2015.2467554","Keywords":"microblog data;mutual reinforcement model;uncertainty modeling;uncertainty visualization;uncertainty propagation;microblog data;mutual reinforcement model;uncertainty modeling;uncertainty visualization;uncertainty propagation","Keywords_Processed":"uncertainty modeling;uncertainty propagation;uncertainty visualization;microblog datum;mutual reinforcement model","Title":"An Uncertainty-Aware Approach for Exploratory Microblog Retrieval","Labels":null,"Keyword_Vector":[0.2215564029,-0.1079705532,-0.1155605601,-0.0068574039,-0.0322685444,-0.0617696613,-0.1931455586,-0.1647443073,-0.1401904264,-0.2593704688,-0.1474400639,0.114916265,-0.1272412758,0.032926257,0.0222119378,-0.0134332514,0.1196680821,0.1272263222,-0.0892634173,0.0820270489,-0.0359919611,-0.1646092956,0.0198373975,0.1030465786],"Abstract_Vector":[0.2594611603,-0.1488885678,0.0122390666,0.1658092158,0.0938423864,0.1378860889,0.1083229767,-0.1092460169,0.1829686978,-0.0737839542,-0.0557637241,0.2292099288,-0.0849783389,-0.0915357226,-0.0193079082,0.0605525426,0.1557943099,0.1144122978,-0.0900021174,-0.1340964552,0.1433764338,-0.0171523878,0.0675352165,0.0559792693,0.0550019541,-0.076609681,0.0432885948,0.0985471407,0.0953651934,-0.0094265309,0.0570887509,0.0125813171,0.0251083192,-0.1318224193,0.0962456653,-0.048711951,-0.0312165498,0.0340098595,-0.0329831377,0.1022417441,-0.0105255403,-0.0227928805,-0.025290468,0.0477947394,0.0000590489,0.0283490719,0.0194994832,-0.0097741357,0.074247505,-0.0356627658,0.0323400634,-0.022356079,0.0404565596,0.0620286671,-0.0201604327,0.0220826457,0.0028891163,0.0317646485,-0.0111450661,0.0704390621,0.0554483914,0.0374560379,0.0157467035,0.0519073515,0.0414676478,0.004072457,0.0676207096,0.0727169665,-0.0046724501,0.0780323311,0.0059714195,0.0179067422,-0.0148175842,0.0353399979,0.001869344,0.0042972232]},"235":{"Abstract":"We present VisTiles, a conceptual framework that uses a set of mobile devices to distribute and coordinate visualization views for the exploration of multivariate data. In contrast to desktop-based interfaces for information visualization, mobile devices offer the potential to provide a dynamic and user-defined interface supporting co-located collaborative data exploration with different individual workflows. As part of our framework, we contribute concepts that enable users to interact with coordinated & multiple views (CMV) that are distributed across several mobile devices. The major components of the framework are: (i) dynamic and flexible layouts for CMV focusing on the distribution of views and (ii) an interaction concept for smart adaptations and combinations of visualizations utilizing explicit side-by-side arrangements of devices. As a result, users can benefit from the possibility to combine devices and organize them in meaningful spatial layouts. Furthermore, we present a web-based prototype implementation as a specific instance of our concepts. This implementation provides a practical application case enabling users to explore a multivariate data collection. We also illustrate the design process including feedback from a preliminary user study, which informed the design of both the concepts and the final prototype.","Authors":"R. Langner; T. Horak; R. Dachselt","DOI":"10.1109\/TVCG.2017.2744019","Keywords":"Mobile devices;coordinated & multiple views;multi-display environment;cross-device interaction","Keywords_Processed":"mobile device;cross device interaction;coordinate multiple view;multi display environment","Title":"VisTiles: Coordinating and Combining Co-located Mobile Devices for Visual Data Exploration","Labels":null,"Keyword_Vector":[0.1303810675,0.1064882782,0.0249798801,0.0311727925,0.103442502,-0.0142553547,-0.0518842786,0.2632089478,-0.158990037,0.0361938854,-0.1006001901,0.1127280719,0.0787992727,-0.0640433743,0.1709571242,0.0399671809,-0.0584912214,-0.0962843954,-0.0408354665,0.1800654607,-0.1781681178,-0.0551255834,-0.0390103717,-0.008479949],"Abstract_Vector":[0.3009146015,-0.1380026139,0.1167669414,-0.0859627681,0.0447519002,-0.0317803169,0.0645805968,0.0728412885,-0.049631034,-0.1153115201,0.1574272437,-0.0249726524,0.1086056195,0.0842385794,-0.1845660106,-0.106193039,-0.0350473938,-0.0859894888,-0.1101879357,-0.1129269445,0.1032093567,-0.1705355563,-0.0170963513,-0.0630843137,-0.0407967173,-0.008414727,0.016960558,-0.1618621244,-0.034274049,0.1529333537,0.0507270794,-0.0012991339,-0.0866741297,-0.0931532414,0.0236128796,0.1649430295,0.042728276,0.0054952861,0.0060513625,-0.0285776827,-0.015581489,-0.0086102222,0.0450741336,-0.0592013671,-0.1291613675,-0.0159273573,-0.0671765001,0.0610338876,-0.0090401171,0.0709933464,-0.0751370291,0.0815697295,-0.0223978597,0.0347589625,0.0255124485,-0.0989530552,-0.069615599,-0.0034240698,-0.0751442126,-0.0099822403,0.0151361582,0.0672912376,0.0368606515,-0.0286327187,0.0354002869,0.0162649128,-0.0140908482,-0.0243632121,-0.0430194379,0.014579798,-0.1146347534,0.1089794935,-0.0888365412,0.0012049798,0.0395728932,-0.0027332227]},"236":{"Abstract":"We empirically evaluate the extent to which people perceive non-constant time and speed encoded on 2D paths. In our graphical perception study, we evaluate nine encodings from the literature for both straight and curved paths. Visualizing time and speed information is a challenge when the x and y axes already encode other data dimensions, for example when plotting a trip on a map. This is particularly true in disciplines such as time-geography and movement analytics that often require visualizing spatio-temporal trajectories. A common approach is to use 2D+time trajectories, which are 2D paths for which time is an additional dimension. However, there are currently no guidelines regarding how to represent time and speed on such paths. Our study results provide InfoVis designers with clear guidance regarding which encodings to use and which ones to avoid; in particular, we suggest using color value to encode speed and segment length to encode time whenever possible.","Authors":"C. Perin; T. Wun; R. Pusch; S. Carpendale","DOI":"10.1109\/TVCG.2017.2743918","Keywords":"Trajectory visualization;visual encoding;movement data;graphical perception;quantitative evaluation","Keywords_Processed":"quantitative evaluation;trajectory visualization;graphical perception;visual encoding;movement datum","Title":"Assessing the Graphical Perception of Time and Speed on 2D+Time Trajectories","Labels":null,"Keyword_Vector":[0.299449767,-0.028957915,0.1634895546,-0.1118368795,-0.0681328071,0.3291562085,-0.0956751958,-0.2031094223,-0.1847929787,0.0652977304,0.0701367025,-0.0917007497,-0.0084626294,0.0047027366,0.1688745584,-0.0688863625,-0.1028660349,0.0462782698,0.0979664073,-0.0111581515,0.1174090883,0.0552433947,0.0270545391,0.0852629432],"Abstract_Vector":[0.1966745965,-0.031865136,0.0287509427,0.0172058977,-0.1160315132,0.0419500515,0.0495490624,-0.0006276515,0.1015720602,0.0063470668,0.0111534433,-0.0170545903,0.0411482203,0.0334100562,0.1164647866,0.128761675,-0.0599163501,-0.0948867745,-0.0402447489,0.0154285321,-0.2972217082,0.1138925773,0.0276677881,-0.1147145139,0.275392598,0.0063710676,0.057357589,0.0807191103,0.0067711448,0.1473690627,0.1075271069,0.031109661,0.0797966746,0.0822289581,-0.0998010987,0.0190385114,-0.0180569329,-0.031209119,-0.1139150283,-0.0913143382,-0.0795519225,0.0930895899,-0.0657064978,-0.0837731737,0.006878831,0.0682746114,0.0597854519,0.0142918912,-0.0678244548,-0.1282107115,0.0488398907,0.0079767804,-0.02850664,0.1040446542,0.0380660344,0.0933702355,-0.0607486116,-0.0279243161,0.0118640814,0.1250130648,-0.0586397961,-0.1287314082,-0.0502023437,-0.0248993212,-0.0613417186,-0.0658571773,0.0294385947,-0.0611944165,-0.0871827827,0.0241040434,0.0008347321,0.0414962856,0.0198933965,0.091020353,0.0454981241,0.0761881877]},"237":{"Abstract":"Semi-automatic text analysis involves manual inspection of text. Often, different text annotations (like part-of-speech or named entities) are indicated by using distinctive text highlighting techniques. In typesetting there exist well-known formatting conventions, such as bold typeface, italics, or background coloring, that are useful for highlighting certain parts of a given text. Also, many advanced techniques for visualization and highlighting of text exist; yet, standard typesetting is common, and the effects of standard typesetting on the perception of text are not fully understood. As such, we surveyed and tested the effectiveness of common text highlighting techniques, both individually and in combination, to discover how to maximize pop-out effects while minimizing visual interference between techniques. To validate our findings, we conducted a series of crowd-sourced experiments to determine: i) a ranking of nine commonly-used text highlighting techniques; ii) the degree of visual interference between pairs of text highlighting techniques; iii) the effectiveness of techniques for visual conjunctive search. Our results show that increasing font size works best as a single highlighting technique, and that there are significant visual interferences between some pairs of highlighting techniques. We discuss the pros and cons of different combinations as a design guideline to choose text highlighting techniques for text viewers.","Authors":"H. Strobelt; D. Oelke; B. C. Kwon; T. Schreck; H. Pfister","DOI":"10.1109\/TVCG.2015.2467759","Keywords":"Text highlighting techniques;visual document analytics;text annotation;crowdsourced study;Text highlighting techniques;visual document analytics;text annotation;crowdsourced study","Keywords_Processed":"crowdsourc study;text highlighting technique;visual document analytic;text annotation","Title":"Guidelines for Effective Usage of Text Highlighting Techniques","Labels":null,"Keyword_Vector":[0.1232127474,-0.0057280222,0.1759560721,0.0549585061,-0.1513429748,0.0610170906,0.0040451807,0.0240474773,0.0130809716,-0.0111201868,-0.0412432282,0.005465499,0.0266893476,-0.0366047627,-0.0146963951,0.023327585,-0.0291231841,0.102632484,0.0621405205,0.0363912684,-0.1222115336,-0.0531475335,0.0966159889,-0.1514936899],"Abstract_Vector":[0.1486847834,-0.0305967394,0.0718004529,-0.0416594601,-0.0072715644,0.0740572337,-0.0242802756,0.0110475203,0.0728992669,0.0264480728,0.0040768525,-0.0453426351,-0.0634498832,-0.0344145321,0.0221858624,0.1412829218,-0.0003956722,0.0472038837,0.0176470251,0.1403663739,-0.0865434503,-0.0176007134,0.0277684919,0.1641439063,-0.0505547895,-0.1384116143,0.0348054907,0.0542987618,-0.078840783,-0.0905482181,-0.0659569588,0.1165407677,-0.0114412291,0.0787807035,-0.1332761975,0.2045026752,0.0527720039,0.1574384073,-0.1258333361,0.1239683217,-0.0334220489,-0.1365240961,0.0250558802,0.0107734114,-0.0533566386,0.0946826339,0.0128733547,0.0524005282,0.0910489074,-0.0020419102,0.0871235861,-0.011020583,0.0169313182,0.093596785,-0.0627771214,-0.0468295059,-0.03723448,0.0414853495,-0.0902296412,-0.1604381222,0.0607761802,0.0778964529,0.1043193336,-0.0569710911,-0.0584929909,-0.0031992735,0.00285784,0.0013236374,0.0470202245,0.0596853894,-0.0664486219,-0.1334309101,-0.0064490043,-0.0719405659,-0.0690864755,-0.0149701322]},"238":{"Abstract":"Analyzing depressions plays an important role in meteorology, especially in the study of cyclones. In particular, the study of the temporal evolution of cyclones requires a robust depression tracking framework. To cope with this demand we propose a pipeline for the exploration of cyclones and their temporal evolution. This entails a generic framework for their identification and tracking. The fact that depressions and cyclones are not well-defined objects and their shape and size characteristics change over time makes this task especially challenging. Our method combines the robustness of topological approaches and the detailed tracking information from optical flow analysis. At first cyclones are identified within each time step based on well-established topological concepts. Then candidate tracks are computed from an optical flow field. These tracks are clustered within a moving time window to distill dominant coherent cyclone movements, which are then forwarded to a final tracking step. In contrast to previous methods our method requires only a few intuitive parameters. An integration into an exploratory framework helps in the study of cyclone movement by identifying smooth, representative tracks. Multiple case studies demonstrate the effectiveness of the method in tracking cyclones, both in the northern and southern hemisphere.","Authors":"A. A. Valsangkar; J. M. Monteiro; V. Narayanan; I. Hotz; V. Natarajan","DOI":"10.1109\/TVCG.2018.2810068","Keywords":"Cyclone;scalar field;time-varying data;track graph;spatio-temporal clustering;tracking","Keywords_Processed":"spatio temporal clustering;scalar field;track graph;time vary datum;cyclone;tracking","Title":"An Exploratory Framework for Cyclone Identification and Tracking","Labels":null,"Keyword_Vector":[0.1472418658,-0.035959243,0.0588919147,-0.0608065163,0.0932289436,-0.0311964347,0.0163511564,-0.0014518733,-0.1075195058,0.0816528912,0.0040382079,0.0151928827,-0.295611602,-0.1384441919,-0.0757687603,-0.0002692094,-0.0544399581,0.0635246024,-0.0125291095,-0.2275006036,0.0617050185,-0.1169916599,-0.0853941473,0.0485729982],"Abstract_Vector":[0.1848392021,0.0195260647,-0.0846417548,0.0599189385,-0.0747348447,-0.0804472688,0.0674989372,-0.1017502188,-0.0270645962,0.0420790795,0.0022642392,-0.075592151,0.0620323513,-0.0008794976,0.1037407206,0.0567291989,0.0127898377,-0.0359183981,-0.0845820671,0.0385275178,-0.0871820609,-0.0622559578,-0.0218675387,-0.0371820034,0.0034459145,0.0014012879,-0.0285572256,-0.0422418595,0.0510934459,0.0867296244,0.0157115203,-0.082408824,0.0052982892,-0.0845651674,-0.0458297245,0.0008392652,0.0539888837,0.0023753154,0.0310074888,-0.0083365099,0.039811416,-0.011828564,-0.0709773058,-0.0587837079,0.0297083761,0.0894449851,0.1167581041,-0.0342830898,0.0817208387,-0.0104174359,-0.1124680783,0.003285549,-0.1025325256,0.0994363635,0.0316977484,-0.1837823107,0.1342798604,-0.0085475724,0.1161977598,0.0519018117,-0.0264649039,0.031365672,-0.0498695262,-0.049690849,-0.0093470496,-0.0263088616,-0.1036390302,-0.0001884578,0.0417630725,-0.0999790097,-0.1078186049,0.0709460395,-0.1148945392,-0.0024221273,-0.0501102937,-0.0968382985]},"239":{"Abstract":"Does it feel the same when you touch an object in Augmented Reality (AR) or in Virtual Reality (VR)? In this paper we study and compare the haptic perception of stiffness of a virtual object in two situations: (1) a purely virtual environment versus (2) a real and augmented environment. We have designed an experimental setup based on a Microsoft HoloLens and a haptic force-feedback device, enabling to press a virtual piston, and compare its stiffness successively in either Augmented Reality (the virtual piston is surrounded by several real objects all located inside a cardboard box) or in Virtual Reality (the same virtual piston is displayed in a fully virtual scene composed of the same other objects). We have conducted a psychophysical experiment with 12 participants. Our results show a surprising bias in perception between the two conditions. The virtual piston is on average perceived stiffer in the VR condition compared to the AR condition. For instance, when the piston had the same stiffness in AR and VR, participants would select the VR piston as the stiffer one in 60% of cases. This suggests a psychological effect as if objects in AR would feel \u201dsofter\u201d than in pure VR. Taken together, our results open new perspectives on perception in AR versus VR, and pave the way to future studies aiming at characterizing potential perceptual biases.","Authors":"Y. Gaffary; B. Le Gouis; M. Marchal; F. Argelaguet; B. Arnaldi; A. L\u00e9cuyer","DOI":"10.1109\/TVCG.2017.2735078","Keywords":"Augmented Reality;Virtual Reality;Haptic;Perception;Stiffness;Psychophysical Study","Keywords_Processed":"augmented reality;haptic;virtual reality;perception;stiffness;psychophysical study","Title":"AR Feels \u201cSofter\u201d than VR: Haptic Perception of Stiffness in Augmented versus Virtual Reality","Labels":null,"Keyword_Vector":[0.2215781363,0.5748877275,0.0310590078,-0.1595107446,-0.0006252627,0.1561602601,0.0038713799,-0.0908209236,0.0389042454,0.0161848697,0.0053062245,0.0640621333,-0.0190651055,-0.008642463,0.0055472828,0.0544202201,0.0343640463,0.006278957,-0.0354883979,0.082709806,-0.0418041628,-0.0311948228,0.0225468393,-0.0210369046],"Abstract_Vector":[0.1565503437,0.1971826965,0.3721622449,0.0647207365,-0.1594434238,-0.017107352,0.0094446394,-0.0086489468,0.0342740887,0.1770735136,-0.0373862144,-0.0560148124,-0.0054509673,0.2144474645,0.1356146738,-0.0826330314,0.0546644047,0.0406671354,-0.0333411476,0.0033112079,0.0965847612,-0.0055391224,0.0841606087,-0.0218738588,0.0334443288,0.1206558431,0.0216770602,-0.0410790121,-0.03514804,-0.0962656417,0.0014065562,0.0629164862,-0.0358818805,-0.080744094,0.1052718749,-0.0578255863,0.0636762518,0.0439697985,0.0061121515,-0.0117165511,0.0255693124,-0.0680027511,-0.0331350556,0.0035632573,0.0164255566,0.0064764178,-0.0248908525,0.0277443016,0.0217356965,-0.0016471528,-0.0333766103,0.0103359169,0.0244968871,0.0538500128,-0.0041793586,-0.0704807442,-0.0298581262,-0.0015520151,-0.0599902981,0.0736469284,-0.0085280445,0.0205061593,0.045902772,-0.0151954138,0.0419603409,-0.0659040101,0.0063978903,0.0233740252,0.0012851203,-0.0939191646,-0.0257384262,-0.0319669239,0.0498358427,-0.0546841929,-0.007129282,-0.0427865843]},"240":{"Abstract":"The problem of isosurface extraction in uncertain data is an important research problem and may be approached in two ways. One can extract statistics (e.g., mean) from uncertain data points and visualize the extracted field. Alternatively, data uncertainty, characterized by probability distributions, can be propagated through the isosurface extraction process. We analyze the impact of data uncertainty on topology and geometry extraction algorithms. A novel, edge-crossing probability based approach is proposed to predict underlying isosurface topology for uncertain data. We derive a probabilistic version of the midpoint decider that resolves ambiguities that arise in identifying topological configurations. Moreover, the probability density function characterizing positional uncertainty in isosurfaces is derived analytically for a broad class of nonparametric distributions. This analytic characterization can be used for efficient closed-form computation of the expected value and variation in geometry. Our experiments show the computational advantages of our analytic approach over Monte-Carlo sampling for characterizing positional uncertainty. We also show the advantage of modeling underlying error densities in a nonparametric statistical framework as opposed to a parametric statistical framework through our experiments on ensemble datasets and uncertain scalar fields.","Authors":"T. Athawale; E. Sakhaee; A. Entezari","DOI":"10.1109\/TVCG.2015.2467958","Keywords":"Uncertainty quantification;linear interpolation;isosurface extraction;marching cubes;nonparametric statistics;Uncertainty quantification;linear interpolation;isosurface extraction;marching cubes;nonparametric statistics","Keywords_Processed":"nonparametric statistic;uncertainty quantification;isosurface extraction;marching cube;linear interpolation","Title":"Isosurface Visualization of Data with Nonparametric Models for Uncertainty","Labels":null,"Keyword_Vector":[0.0431189498,-0.0288811141,-0.0399192967,-0.0052185287,-0.0197715579,-0.0521041746,-0.0534815748,-0.0869291795,-0.0060767915,-0.0791956059,-0.052531448,0.068370082,-0.076378112,0.0558340099,0.0386332663,0.0110744617,0.0097392821,0.0314425753,-0.0388514138,0.0096852823,-0.0388815073,-0.1065999983,-0.1040187924,0.0266374394],"Abstract_Vector":[0.2126731562,-0.0696741535,-0.086450508,0.1758859252,-0.0097793441,0.1841461552,-0.0722228043,-0.1193301514,0.1633393442,-0.096263668,-0.1288696384,0.2752608568,0.0162273922,0.0668051246,0.0306652573,0.0701686372,0.0066853464,0.0264455154,-0.0899380853,-0.0804749358,0.1550264853,-0.0080613421,0.0400473848,-0.093240649,0.0001024331,0.0701971291,0.0396522704,-0.0179977938,0.0615932259,0.0102124016,0.0297528993,-0.1448796492,-0.080942651,0.0047555918,-0.0639055121,-0.0004948147,0.0309688431,-0.0176953037,0.0198012809,-0.0064599833,-0.0553626338,-0.0627394992,-0.085348268,0.0390695503,0.0298474051,-0.0149608537,-0.004363246,0.0207419695,-0.0106180825,-0.0241574771,0.0167032325,0.0559409139,0.0278359617,-0.0140020959,-0.1016009803,0.0442701218,-0.0092081223,0.0532751453,-0.0132852095,-0.0308489857,-0.0456601229,0.0263864176,-0.081098974,0.0803639843,-0.0194580575,0.0804454138,-0.0086949321,0.0608461235,0.0167646763,-0.0551990438,-0.0066349691,-0.0083624191,0.0238431452,0.0028585955,-0.0107537572,0.0406233163]},"241":{"Abstract":"We present the first approach to integrative structural modeling of the biological mesoscale within an interactive visual environment. These complex models can comprise up to millions of molecules with defined atomic structures, locations, and interactions. Their construction has previously been attempted only within a non-visual and non-interactive environment. Our solution unites the modeling and visualization aspect, enabling interactive construction of atomic resolution mesoscale models of large portions of a cell. We present a novel set of GPU algorithms that build the basis for the rapid construction of complex biological structures. These structures consist of multiple membrane-enclosed compartments including both soluble molecules and fibrous structures. The compartments are defined using volume voxelization of triangulated meshes. For membranes, we present an extension of the Wang Tile concept that populates the bilayer with individual lipids. Soluble molecules are populated within compartments distributed according to a Halton sequence. Fibrous structures, such as RNA or actin filaments, are created by self-avoiding random walks. Resulting overlaps of molecules are resolved by a forced-based system. Our approach opens new possibilities to the world of interactive construction of cellular compartments. We demonstrate its effectiveness by showcasing scenes of different scale and complexity that comprise blood plasma, mycoplasma, and HIV.","Authors":"T. Klein; L. Autin; B. Kozl\u00edkov\u00e1; D. S. Goodsell; A. Olson; M. E. Gr\u00f6ller; I. Viola","DOI":"10.1109\/TVCG.2017.2744258","Keywords":"Interactive modeling;population;biological data;interactive visualization","Keywords_Processed":"interactive visualization;interactive modeling;biological datum;population","Title":"Instant Construction and Visualization of Crowded Biological Environments","Labels":null,"Keyword_Vector":[0.248475949,-0.112618452,-0.0539917931,-0.0293104631,0.0653845336,-0.0358128489,0.0746127122,-0.1155827019,-0.1597959587,-0.0213152056,-0.0692117734,-0.1159721937,-0.2176383629,0.3697105677,-0.0757161253,-0.1064450419,-0.033534322,0.001445796,-0.0112288083,0.0020740146,0.024302707,-0.0000113271,0.2552817902,-0.0479451905],"Abstract_Vector":[0.1778877079,0.0162084134,-0.0261527509,0.0130875042,0.1201896558,-0.0705851808,0.0548063842,0.056417114,-0.0606420872,-0.0636375758,-0.0301903993,-0.0330069706,-0.0040460841,0.0533997418,0.0217317266,-0.0113279967,-0.0646382742,-0.0501346755,-0.0369512897,0.00374603,0.053923286,0.1119733645,-0.011731501,0.0000153944,-0.0489358049,-0.0083318476,0.155449723,-0.0101908814,0.0322048524,-0.0444574737,0.0280821437,0.0277293279,0.0489930941,0.0481869051,0.1519153572,0.0459007507,0.009186171,0.0114009989,0.0211978107,0.0220308691,-0.0351149149,-0.0719238498,-0.05469553,0.0740541578,0.1503875063,0.1022261485,-0.1260822668,-0.0817118639,-0.037914231,0.0326098522,-0.0242506979,0.0166840709,-0.1122788936,0.214796759,0.0643363627,-0.0092172533,-0.0215564681,-0.0486257746,0.0474265057,0.0217146279,-0.1270743777,-0.0239715457,-0.0562401895,0.0608276922,-0.0429498398,0.0837095857,-0.0110390379,-0.1288575304,0.1427301632,0.0118764962,0.0973612414,0.0390644108,-0.0625175002,0.0987348339,-0.0397115146,0.0183723615]},"242":{"Abstract":"While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely \u201cVA-assisted ML\u201d. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.","Authors":"D. Sacha; M. Kraus; D. A. Keim; M. Chen","DOI":"10.1109\/TVCG.2018.2864838","Keywords":"Visual Analytics;Visualization;Machine Learning;Human-Computer Interaction;Ontology;VIS4ML","Keywords_Processed":"visual analytic;visualization;ontology;machine learning;human computer interaction;vis4ml","Title":"VIS4ML: An Ontology for Visual Analytics Assisted Machine Learning","Labels":null,"Keyword_Vector":[0.2701190126,-0.0311576542,0.2315340033,0.2737282894,-0.1886965268,-0.0509631749,-0.1953773524,0.0914472766,-0.1505660561,-0.0699482553,0.0792600515,0.0501133664,0.069140185,0.0043294653,-0.0350853596,0.0118355844,-0.0122683662,-0.1326364041,-0.0198637474,-0.1029141675,-0.0219896116,0.0862533015,-0.0335306021,-0.0335764269],"Abstract_Vector":[0.1220903004,-0.0075042187,0.0212624253,-0.0343834011,0.0867193867,-0.0347258825,-0.0822146643,0.0083216749,-0.0206626921,0.0052736899,0.0096609891,-0.0171973909,0.0491372606,0.0432117235,-0.0383546298,0.0327243273,-0.008506256,0.0409757398,0.049313506,0.0065495045,0.0050517034,0.0079068558,0.0203106345,0.0385845078,0.0414207582,0.0191679223,0.056471438,0.0491661607,0.0261428007,0.0608004757,-0.0040537359,-0.0010194557,-0.0323421097,0.0339842341,-0.0460867881,-0.0478161881,-0.0306253668,-0.0044658455,0.056443509,0.080388262,0.0202545533,-0.0776170515,0.017796805,0.0480594897,-0.066189957,-0.0645158713,0.0039327492,0.0621753907,-0.1089156954,0.1352826023,-0.1266817535,-0.0050617575,0.0162959203,-0.0434600061,0.0670531463,0.0767753514,-0.003562024,0.0219349349,-0.0111070369,0.1059738342,-0.1621646237,0.0694875389,0.0996379722,0.112371752,-0.060217647,0.0197574831,0.0561230412,-0.1280993755,-0.0184222949,0.0475129077,0.0248560491,0.052673847,-0.0684026785,-0.1078470476,0.0079505551,-0.0061478112]},"243":{"Abstract":"We present a new algorithm for calculating the external labeling of ghosted views of moderately complex 3D models. The algorithm uses multiple criteria decision making, based on fuzzy logic, to optimize positions of the labels associated with different parts of the input model. The proposed method can be used with various existing algorithms for creating ghosted views from 3D models. The method operates in real-time, which allows the user to acquire a good understanding of the structure of the input model by studying the model and its labels from different viewpoints. We have conducted a user study to evaluate label layouts produced by our algorithm and those created by humans. The results show that the proposed method can significantly improve user understanding of labeled ghosted views of complicated 3D models, and its label layouts are comparable with label layouts created by humans.","Authors":"L. \u010cmol\u00edk; J. Bittner","DOI":"10.1109\/TVCG.2018.2833479","Keywords":"External labeling;ghosted views;illustrative visualization;empirical evaluation;visualization for the masses","Keywords_Processed":"visualization for the mass;empirical evaluation;external labeling;ghosted view;illustrative visualization","Title":"Real-Time External Labeling of Ghosted Views","Labels":null,"Keyword_Vector":[0.1987712667,-0.057409376,-0.1069015604,-0.0164207833,-0.0726593146,0.0608515799,0.0040101464,-0.025461032,0.0087601055,0.0353320615,-0.0124585384,-0.0795948204,0.0611729421,-0.0354963106,0.080764843,-0.0204638186,-0.0358910484,-0.035326876,-0.0346047706,0.0902869188,0.0192795469,0.0476876745,-0.0303803481,0.0352834653],"Abstract_Vector":[0.2654203677,0.0953036041,-0.0268424841,-0.0629281951,0.139166669,-0.1191659543,-0.1964185387,0.2557842349,0.2202265447,-0.1499870303,0.2062414403,0.1388060275,0.210193504,0.0260291734,0.1736151558,-0.1863824207,0.1150107765,-0.0075446294,0.0345028138,0.1579385543,-0.0530619387,-0.0109912347,0.0236119471,0.0911719628,-0.0652398059,0.0543550437,0.0714163137,0.0156547374,-0.0132752898,-0.0043403034,-0.0361419713,-0.0617178799,-0.0392358496,-0.0456366621,0.0294066122,-0.0202688001,-0.0847953159,-0.0537316894,-0.0606764821,-0.0771841508,0.0447439572,0.0309807179,0.013012469,0.0177807535,-0.0363564819,0.0448519421,-0.0122230914,-0.0187167512,0.0194791158,-0.0485518552,0.0134808779,0.0288538859,-0.0864120652,-0.0027568915,0.0194284598,0.0608649898,-0.0021387375,0.0604519877,-0.0381722324,0.0054872117,0.0170120503,0.00543798,-0.0763369902,-0.0079313089,0.0608163739,0.0575623194,0.0228114856,-0.0560846595,0.0000712301,0.0137363695,-0.0494030507,-0.0102554512,-0.0382002551,0.0195844207,0.0227681225,0.0608212839]},"244":{"Abstract":"We present a framework to design inverse rig-functions-functions that map low level representations of a character's pose such as joint positions or surface geometry to the representation used by animators called the animation rig. Animators design scenes using an animation rig, a framework widely adopted in animation production which allows animators to design character poses and geometry via intuitive parameters and interfaces. Yet most state-of-the-art computer animation techniques control characters through raw, low level representations such as joint angles, joint positions, or vertex coordinates. This difference often stops the adoption of state-of-the-art techniques in animation production. Our framework solves this issue by learning a mapping between the low level representations of the pose and the animation rig. We use nonlinear regression techniques, learning from example animation sequences designed by the animators. When new motions are provided in the skeleton space, the learned mapping is used to estimate the rig controls that reproduce such a motion. We introduce two nonlinear functions for producing such a mapping: Gaussian process regression and feedforward neural networks. The appropriate solution depends on the nature of the rig and the amount of data available for training. We show our framework applied to various examples including articulated biped characters, quadruped characters, facial animation rigs, and deformable characters. With our system, animators have the freedom to apply any motion synthesis algorithm to arbitrary rigging and animation pipelines for immediate editing. This greatly improves the productivity of 3D animation, while retaining the flexibility and creativity of artistic input.","Authors":"D. Holden; J. Saito; T. Komura","DOI":"10.1109\/TVCG.2016.2628036","Keywords":"Animation rig;character animation;regression","Keywords_Processed":"animation rig;character animation;regression","Title":"Learning Inverse Rig Mappings by Nonlinear Regression","Labels":null,"Keyword_Vector":[0.0215911724,0.0382105309,0.0034812119,0.0842297317,0.1167132661,-0.0775270365,0.1401547696,-0.1574846677,-0.1428333288,-0.1445666639,0.0198722736,-0.1477332325,0.1348863626,-0.188272017,0.0963406166,-0.0348612103,0.100157863,-0.1826588912,0.0606529306,-0.0691931578,-0.0421870023,0.0973531631,0.0705720598,-0.0177704604],"Abstract_Vector":[0.1957067013,0.1381482852,-0.0362015063,-0.1672797588,-0.0810701845,-0.0663470116,-0.0326988355,-0.0709983857,0.0497352115,-0.2039509253,-0.0485247199,-0.0101515693,-0.1286210786,-0.0777768416,-0.2080575339,0.0248213682,0.1332709729,0.0066907101,0.0895179618,-0.0039845648,0.0039992612,0.0869388869,0.0573277253,0.1223901011,0.0836941133,0.1137892442,0.106676078,-0.1672827303,-0.0197882114,-0.0530233704,-0.1717947072,-0.0641538901,-0.1084889161,0.029578788,-0.141296779,-0.043446321,0.0100606128,0.0615583134,0.0200839909,0.0549075227,0.0600052466,0.0222330525,-0.0246585839,-0.0730340495,-0.0041393119,-0.1251852889,0.0317844752,-0.0131909339,-0.0602748819,-0.1060035006,-0.0161167158,-0.0024698562,0.0679044868,0.0001743705,-0.0435235458,-0.0417847423,-0.0138998631,-0.012442368,0.1346773248,0.0090721898,-0.0435801022,0.0619457311,-0.0141945519,0.0115038329,0.0739121155,-0.0098985602,-0.0502576839,-0.1149451227,0.0006597911,-0.0746936717,0.0398381668,-0.0100540275,0.0281217793,0.0504346532,0.0642727363,-0.0185687371]},"245":{"Abstract":"In Augmented Reality (AR), search performance for outdoor tasks is an important metric for evaluating the success of a large number of AR applications. Users must be able to find content quickly, labels and indicators must not be invasive but still clearly noticeable, and the user interface should maximize search performance in a variety of conditions. To address these issues, we have set up a series of experiments to test the influence of virtual characteristics such as color, size, and leader lines on the performance of search tasks and noticeability in both real and simulated environments. We evaluate two primary areas, including 1) the effects of peripheral field of view (FOV) limitations and labeling techniques on target acquisition during outdoor mobile search, and 2) the influence of local characteristics such as color, size, and motion on text labels over dynamic backgrounds. The first experiment showed that limited FOV will severely limit search performance, but that appropriate placement of labels and leaders within the periphery can alleviate this problem without interfering with walking or decreasing user comfort. In the second experiment, we found that different types of motion are more noticeable in optical versus video see-through displays, but that blue coloration is most noticeable in both. Results can aid in designing more effective view management techniques, especially for wider field of view displays.","Authors":"E. Kruijff; J. Orlosky; N. Kishishita; C. Trepkowski; K. Kiyokawa","DOI":"10.1109\/TVCG.2018.2854737","Keywords":"Augmented reality;head mounted display;perception;peripheral vision;visualization","Keywords_Processed":"augmented reality;head mount display;peripheral vision;visualization;perception","Title":"The Influence of Label Design on Search Performance and Noticeability in Wide Field of View Augmented Reality Displays","Labels":null,"Keyword_Vector":[0.243892379,0.3432627782,-0.0608370844,-0.0615333254,-0.045582567,0.1084696945,-0.0587352957,-0.0326619688,-0.0646215703,0.1060838871,-0.0571257848,0.0189193396,0.0295663049,-0.047966433,0.066003861,0.0703402054,-0.2373179596,-0.1298033357,-0.0790057665,0.1073077305,-0.0959316051,-0.0222765825,0.0476224172,-0.1966106537],"Abstract_Vector":[0.2348104139,0.1174608738,0.1819424047,-0.0192102295,-0.107926229,0.0191348518,0.0120376517,0.0763664534,0.1854398164,-0.1407899459,0.1572355388,0.0413084113,0.1164270579,-0.0987327249,0.1271096621,-0.0265755805,-0.0733314555,-0.0692658481,0.0873714495,0.1987582994,0.022778987,-0.0390182015,0.0497500125,0.0432555599,-0.0601626656,0.0397324355,-0.0746124439,-0.0066720792,0.0166114866,-0.054333886,-0.0043217367,0.0390353015,-0.0914442119,-0.1186268254,0.0031561989,0.0312961695,0.1211065238,0.0545345529,0.0120309868,0.0125728858,-0.0724321174,-0.0417382864,0.0563957872,0.0498758482,0.0179245417,0.0385740645,-0.0421153196,0.0344904574,0.0459753605,0.0251370603,-0.0261078858,-0.075272233,0.0877738395,0.0009173357,-0.0706542462,-0.0556048701,0.0805794667,-0.0079157943,-0.0425842886,-0.0608748518,0.0905280478,-0.013376552,0.0558215437,0.0060222857,0.0286453074,0.021870951,-0.0742554716,-0.0265589775,0.0520482543,0.0841314454,-0.0421900946,-0.0751410872,0.0144026488,-0.0242266139,-0.0361669658,0.070438161]},"246":{"Abstract":"Diffusion Tensor Imaging (DTI) is a magnetic resonance imaging modality that enables the in-vivo reconstruction and visualization of fibrous structures. To inspect the local and individual diffusion tensors, glyph-based visualizations are commonly used since they are able to effectively convey full aspects of the diffusion tensor. For several applications it is necessary to compare tensor fields, e.g., to study the effects of acquisition parameters, or to investigate the influence of pathologies on white matter structures. This comparison is commonly done by extracting scalar information out of the tensor fields and then comparing these scalar fields, which leads to a loss of information. If the glyph representation is kept, simple juxtaposition or superposition can be used. However, neither facilitates the identification and interpretation of the differences between the tensor fields. Inspired by the checkerboard style visualization and the superquadric tensor glyph, we design a new glyph to locally visualize differences between two diffusion tensors by combining juxtaposition and explicit encoding. Because tensor scale, anisotropy type, and orientation are related to anatomical information relevant for DTI applications, we focus on visualizing tensor differences in these three aspects. As demonstrated in a user study, our new glyph design allows users to efficiently and effectively identify the tensor differences. We also apply our new glyphs to investigate the differences between DTI datasets of the human brain in two different contexts using different b-values, and to compare datasets from a healthy and HIV-infected subject.","Authors":"C. Zhang; T. Schultz; K. Lawonn; E. Eisemann; A. Vilanova","DOI":"10.1109\/TVCG.2015.2467435","Keywords":"Glyph Design;Comparative Visualization;Diffusion Tensor Field;Glyph Design;Comparative Visualization;Diffusion Tensor Field","Keywords_Processed":"diffusion tensor field;comparative visualization;glyph design","Title":"Glyph-Based Comparative Visualization for Diffusion Tensor Fields","Labels":null,"Keyword_Vector":[0.2211884678,-0.0710503503,-0.1549718088,0.0512351256,-0.1077754786,0.1347068772,0.2556650884,0.0587893482,-0.0276672328,-0.1407546777,-0.004392683,0.1080701436,-0.0520309804,-0.0425149512,-0.1848577668,0.1235922721,-0.1789944837,-0.1970082231,0.0763197769,-0.1590407168,-0.0880386464,-0.104328281,-0.1565713657,0.1333605988],"Abstract_Vector":[0.1858952625,-0.0236008689,-0.038719962,0.0469020066,0.0859274703,0.1316558585,0.133050499,-0.1497436797,-0.0132216019,-0.1518867017,0.0785863816,-0.0897728677,-0.103609766,0.0047415202,0.1452127633,-0.1754049361,-0.0223118958,-0.1731179704,0.1807138075,0.0635504667,0.298267369,0.0390434826,-0.1711772434,-0.0453962128,0.1340659591,-0.0512476623,0.1371681111,0.1416803365,-0.2349129282,-0.0416052094,-0.0130550081,-0.016297067,-0.019921033,0.009603866,0.0007680434,-0.0942720175,-0.0724652978,0.0213849796,-0.1135669756,-0.064859773,-0.0329505931,0.0049234566,-0.0063747262,0.0750491802,0.0109524187,0.1236262756,0.0724843439,-0.0145065437,0.0293163101,0.0704665681,0.0026811606,0.0884049717,0.0620149027,0.0221118677,-0.005978577,-0.0848470774,-0.0361814693,-0.1072338217,0.095755054,0.034803138,0.0160883627,-0.0041781206,0.1185021218,-0.0517993429,-0.0476614749,-0.0196970039,0.0228990276,0.0491761901,-0.0722242936,-0.0247269237,-0.0269192961,-0.0148619068,0.0751208197,0.0112249308,0.040294698,-0.0434143839]},"247":{"Abstract":"A narrative collage is an interesting image editing method for summarizing the main theme or storyline behind an image collection. We present a novel method to generate narrative images with plausible semantic scene structures. To achieve this goal, we introduce a layer graph and a scene graph to represent the relative depth order and semantic relationship between image objects, respectively. We first cluster the input image collection to select representative images, and then we extract a group of semantic salient objects from each representative image. Both layer graphs and scene graphs are constructed and combined according to our specific rules for reorganizing the extracted objects in every image. We design an energy model to appropriately locate every object on the final canvas. The experimental results show that our method can produce competitive narrative collage results and that it performs well on a wide range of image collections.","Authors":"F. Fang; M. Yi; H. Feng; S. Hu; C. Xiao","DOI":"10.1109\/TVCG.2017.2759265","Keywords":"Narrative collage;image collections;image segmentation;scene graphs;image synthesis","Keywords_Processed":"scene graph;image segmentation;image collection;image synthesis;narrative collage","Title":"Narrative Collage of Image Collections by Scene Graph Recombination","Labels":null,"Keyword_Vector":[0.0512942266,0.0088955317,-0.0019035093,0.1508543051,0.0758124916,-0.1229076764,0.2019033995,-0.1347400428,-0.0485934699,0.1288821066,-0.0943860531,0.0077067991,0.0606745443,0.0010698113,0.0053254664,0.1227813905,-0.2396979596,0.3329675911,-0.2399892283,0.001051597,0.0288602277,0.0908942858,-0.0199790428,-0.0192274874],"Abstract_Vector":[0.2098539173,0.2314885309,-0.0529042624,0.1369270622,0.0937705482,-0.0159181148,0.0423899605,0.1456701978,-0.0150796518,0.175384039,0.0260693633,-0.025834445,-0.1391692253,-0.106885466,0.0005493104,0.0909721374,0.3299703542,-0.0341827758,-0.0128258667,-0.1574103833,0.0552722055,-0.2147573178,0.0364211122,-0.0214626822,0.0169655504,-0.0261847872,0.1227024153,0.1554597036,0.0259679827,0.037536089,0.0674721242,-0.0307089868,-0.1004702395,-0.0133100975,0.0714351213,0.0317706167,0.0494264624,-0.0275187445,-0.053716414,-0.0545911014,-0.0039679363,-0.0642393901,0.0011000249,-0.0391077753,0.0116241832,-0.1027800988,-0.0731352067,0.0142503679,-0.0214837149,0.0634762709,-0.0622755636,-0.0577048007,-0.0258246516,-0.0028113424,-0.0044948332,-0.0436736085,0.0564972256,-0.0215836532,-0.0468565147,-0.044898248,-0.0680858213,-0.0424097213,0.0124658648,-0.0835439728,0.015113792,0.0163754192,-0.0414470389,-0.0191786898,-0.0676738432,-0.0604134596,0.107853778,-0.0007770679,-0.0104498486,0.1034174546,-0.0336134977,-0.0239776053]},"248":{"Abstract":"Decades of research have repeatedly shown that people perform poorly at estimating and understanding conditional probabilities that are inherent in Bayesian reasoning problems. Yet in the medical domain, both physicians and patients make daily, life-critical judgments based on conditional probability. Although there have been a number of attempts to develop more effective ways to facilitate Bayesian reasoning, reports of these findings tend to be inconsistent and sometimes even contradictory. For instance, the reported accuracies for individuals being able to correctly estimate conditional probability range from 6% to 62%. In this work, we show that problem representation can significantly affect accuracies. By controlling the amount of information presented to the user, we demonstrate how text and visualization designs can increase overall accuracies to as high as 77%. Additionally, we found that for users with high spatial ability, our designs can further improve their accuracies to as high as 100%. By and large, our findings provide explanations for the inconsistent reports on accuracy in Bayesian reasoning tasks and show a significant improvement over existing methods. We believe that these findings can have immediate impact on risk communication in health-related fields.","Authors":"A. Ottley; E. M. Peck; L. T. Harrison; D. Afergan; C. Ziemkiewicz; H. A. Taylor; P. K. J. Han; R. Chang","DOI":"10.1109\/TVCG.2015.2467758","Keywords":"Bayesian Reasoning;Visualization;Spatial Ability;Individual Differences;Bayesian Reasoning;Visualization;Spatial Ability;Individual Differences","Keywords_Processed":"spatial ability;visualization;bayesian reasoning;individual difference","Title":"Improving Bayesian Reasoning: The Effects of Phrasing, Visualization, and Spatial Ability","Labels":null,"Keyword_Vector":[0.1250166336,-0.0096659216,-0.0752070256,0.0237316518,-0.0335647646,-0.0254319711,-0.0404771487,0.0555335381,0.0622633212,-0.090222676,0.0453463585,-0.157265729,-0.0710511789,-0.0543310763,0.057181084,-0.0380027114,-0.0837518568,-0.018199281,0.0579794479,0.0474634612,0.1328285199,-0.0297238318,-0.0590771162,-0.0673413156],"Abstract_Vector":[0.1814535965,-0.0392838582,0.0785829317,0.0278212041,0.04864269,0.0794069448,-0.0387690075,-0.0315608465,0.0737798787,-0.0553405,-0.018199389,0.0339036934,-0.0284793362,-0.0152456311,0.024061571,0.0615346611,-0.0471486844,0.0652857156,0.0411666271,0.0159753044,0.0403504962,-0.0361032493,0.0344446375,0.0088550318,0.030985996,-0.0474617664,-0.0666227995,-0.0400247163,0.0334988641,-0.070534054,-0.0369165671,-0.0480358541,-0.0005082927,0.0267119945,-0.0255985696,0.077537801,0.0180396573,0.0279425137,0.1029762676,0.0499066997,-0.1364661386,0.0218510959,-0.0240135018,-0.046990662,-0.1123466006,0.0109291762,-0.0106911848,-0.0887903584,0.0247418888,0.0198900012,0.1252863568,-0.0768916701,0.0236542274,-0.0550333742,-0.0394470034,-0.0588814141,0.1141713379,0.1008729735,0.1057589888,-0.075000896,-0.0775627881,0.0030219089,-0.0594120304,0.1606898736,-0.0920232362,-0.0593606832,-0.1515031643,0.1068773355,0.020770671,0.0197000065,0.0764596753,0.0554502931,-0.0251957685,-0.0833438301,-0.066020617,-0.0895731387]},"249":{"Abstract":"Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.","Authors":"P. Law; R. C. Basole; Y. Wu","DOI":"10.1109\/TVCG.2018.2864526","Keywords":"Pairwise comparison;novices;data analysis;automatic insight generation","Keywords_Processed":"automatic insight generation;novice;datum analysis;pairwise comparison","Title":"Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification","Labels":null,"Keyword_Vector":[0.1235923841,-0.0939452065,0.1317619124,-0.1440596252,0.1910007522,0.053917648,0.0445197574,0.002575862,-0.0751741608,-0.0048253129,-0.0265960471,0.0183162041,-0.0031578324,0.0660767559,0.0254551157,-0.1383884809,-0.0476507151,-0.0092885881,-0.0237268102,-0.0450684834,0.0898007871,0.005870977,0.0759236967,0.0095063719],"Abstract_Vector":[0.1483809223,-0.0531567262,0.0536187976,-0.0250114252,0.0050907195,0.0213584021,-0.043293954,-0.0309875035,-0.030202807,0.0940634721,-0.0029840249,-0.0256257849,-0.0221023479,0.0021302971,0.0035498758,-0.0170982409,0.1478519243,0.0290714815,-0.0478041276,0.0025532082,-0.051343837,-0.045562887,-0.1010578422,0.0224168472,0.1004777043,0.0001827913,-0.0420164057,0.0709643204,0.020460514,-0.0951177477,0.0372771403,0.0338045962,-0.0241122425,-0.087444813,0.0006067846,-0.0348357338,-0.0260567497,-0.0212069455,0.0354219482,0.0457482294,0.1228051595,0.0311969869,-0.002129368,0.0203727055,0.0833452767,-0.0757566049,-0.0359737584,-0.0116718581,0.0151862333,0.0175385187,-0.1956312133,0.0249173161,-0.0066711297,-0.1124587737,-0.0459519714,0.0361004543,0.0299685497,-0.0492866004,-0.0632287975,-0.0283800826,-0.0205802724,0.0311536513,-0.016855652,0.0027821584,0.0297882502,0.1805896621,-0.0105002618,-0.0757597565,-0.0034412118,-0.193060658,0.1093792949,0.009686127,0.101489356,-0.1214523157,0.0632848974,0.0712020553]},"250":{"Abstract":"Visualizations often appear in multiples, either in a single display (e.g., small multiples, dashboard) or across time or space (e.g., slideshow, set of dashboards). However, existing visualization design guidelines typically focus on single rather than multiple views. Solely following these guidelines can lead to effective yet inconsistent views (e.g., the same field has different axes domains across charts), making interpretation slow and error-prone. Moreover, little is known how consistency balances with other design considerations, making it difficult to incorporate consistency mechanisms in visualization authoring software. We present a wizard-of-oz study in which we observed how Tableau users achieve and sacrifice consistency in an exploration-to-presentation visualization design scenario. We extend (from our prior work) a set of encoding-specific constraints defining consistency across multiple views. Using the constraints as a checklist in our study, we observed cases where participants spontaneously maintained consistent encodings and warned cases where consistency was overlooked. In response to the warnings, participants either revised views for consistency or stated why they thought consistency should be overwritten. We categorize participants' actions and responses as constraint validations and exceptions, depicting the relative importance of consistency and other design considerations under various circumstances (e.g., data cardinality, available encoding resources, chart layout). We discuss automatic consistency checking as a constraint-satisfaction problem and provide design implications for communicating inconsistencies to users.","Authors":"Z. Qu; J. Hullman","DOI":"10.1109\/TVCG.2017.2744198","Keywords":"Visualization Design;Qualitative Study;Evaluation","Keywords_Processed":"evaluation;visualization design;qualitative study","Title":"Keeping Multiple Views Consistent: Constraints, Validations, and Exceptions in Visualization Authoring","Labels":null,"Keyword_Vector":[0.3045713328,-0.0547896642,-0.1114232672,-0.0909208359,-0.2325376567,0.3651050914,0.3492292789,0.0377019636,0.129784191,-0.1991507385,-0.0120055633,0.0743244305,-0.0059449589,-0.0722212858,0.0683235895,0.0431689844,0.0963162526,0.0419317392,-0.0889891181,0.0663257699,0.0092756503,0.1316381282,0.0380475988,-0.0462993086],"Abstract_Vector":[0.2104492388,-0.0364714793,0.1155128624,-0.0968583921,0.0832694992,0.0387216445,-0.0024114126,-0.0135815534,0.0322985154,-0.1345180938,0.0465506087,-0.0878855976,0.0469825824,-0.0640615699,-0.0726484016,-0.0244126365,0.0785028196,-0.1111201522,-0.0058797584,0.0052385057,-0.0811933787,-0.0843574093,-0.0107369923,-0.1545773804,0.1211421607,-0.0837287452,-0.0863685492,-0.1327562371,0.0416842898,0.0004885786,0.0413350075,-0.0539485028,-0.0242075326,0.0311620119,0.106498958,0.0579928933,-0.0096943761,0.1211144151,-0.0538248982,-0.0008949928,0.0681775006,-0.0153551276,-0.0767087808,0.0106340372,0.1675997334,-0.0197883546,0.1168148741,0.0201170112,0.093204416,0.0546782136,0.019936631,-0.017161665,0.0072973194,-0.0001692418,0.1255663013,0.002472122,0.0316169728,0.1930298936,-0.0117549901,0.1466250932,0.0962830371,0.0682096775,-0.116416604,-0.0149756923,0.0430167057,-0.1175189448,0.0502543377,0.0290705269,-0.0666045852,0.0529020854,0.0269285618,-0.1186972141,0.1242032525,0.0258948831,0.0237469265,0.1205948269]},"251":{"Abstract":"This paper presents a framework for externalizing and analyzing expert knowledge about discrepancies in data through the use of visualization. Grounded in an 18-month design study with global health experts, the framework formalizes the notion of data discrepancies as implicit error, both in global health data and more broadly. We use the term implicit error to describe measurement error that is inherent to and pervasive throughout a dataset, but that isn't explicitly accounted for or defined. Instead, implicit error exists in the minds of experts, is mainly qualitative, and is accounted for subjectively during expert interpretation of the data. Externalizing knowledge surrounding implicit error can assist in synchronizing, validating, and enhancing interpretation, and can inform error analysis and mitigation. The framework consists of a description of implicit error components that are important for downstream analysis, along with a process model for externalizing and analyzing implicit error using visualization. As a second contribution, we provide a rich, reflective, and verifiable description of our research process as an exemplar summary toward the ongoing inquiry into ways of increasing the validity and transferability of design study research.","Authors":"N. Mccurdy; J. Gerdes; M. Meyer","DOI":"10.1109\/TVCG.2018.2864913","Keywords":"implicit error;knowledge externalization;design study","Keywords_Processed":"design study;implicit error;knowledge externalization","Title":"A Framework for Externalizing Implicit Error Using Visualization","Labels":null,"Keyword_Vector":[0.0966669645,-0.0002464086,-0.0019350895,-0.0352215323,-0.1300593129,0.1768767329,0.252755271,0.0616001167,0.108115266,-0.2310056554,-0.007108335,0.089685824,-0.0934178205,-0.0653598421,-0.0157065866,0.0864514304,0.0778962206,0.0663879026,-0.0391044271,0.0730958643,-0.1037328807,0.0240492997,-0.0022637561,-0.0130424956],"Abstract_Vector":[0.1778784112,-0.0872053648,0.014174969,-0.0704028749,0.0215095805,0.0317215412,-0.0944765012,-0.037029384,-0.0680249507,-0.0271523388,-0.0806378662,0.0441076502,0.0706156089,-0.0117464795,-0.0348028979,0.0521791778,-0.0912681521,0.0372763736,0.0150476842,-0.0331822247,0.0271703871,-0.0494156411,-0.0603170804,-0.0273819687,0.0277364739,0.1067261572,-0.029143551,0.0393416249,-0.0557325142,-0.1037213448,0.1696971027,0.0180295355,-0.1241480563,-0.0646593196,-0.0043591229,-0.0240221349,0.1289441271,-0.0835284147,-0.0066351546,0.0445725819,-0.0195447811,-0.0075620343,0.0282046577,-0.0926823995,0.1952004984,0.0109870862,0.1411890613,-0.0316215279,0.0235512133,-0.007821535,0.0713457046,0.1261933816,-0.0325212076,-0.0899251567,-0.0268724671,0.0375122586,-0.1536812171,0.0220142253,-0.0430812536,-0.017520615,-0.0601096598,0.1477575907,0.0896935029,-0.0197608185,0.0513214379,-0.0579839618,0.0182420663,-0.0143017881,0.114692139,0.0544297169,0.0651574331,-0.0567673686,-0.0126810165,0.0441106255,0.1072258449,-0.1158370787]},"252":{"Abstract":"Visualizing network data is applicable in domains such as biology, engineering, and social sciences. We report the results of a study comparing the effectiveness of the two primary techniques for showing network data: node-link diagrams and adjacency matrices. Specifically, an evaluation with a large number of online participants revealed statistically significant differences between the two visualizations. Our work adds to existing research in several ways. First, we explore a broad spectrum of network tasks, many of which had not been previously evaluated. Second, our study uses two large datasets, typical of many real-life networks not explored by previous studies. Third, we leverage crowdsourcing to evaluate many tasks with many participants. This paper is an expanded journal version of a Graph Drawing (GD'17) conference paper. We evaluated a second dataset, added a qualitative feedback section, and expanded the procedure, results, discussion, and limitations sections.","Authors":"M. Okoe; R. Jianu; S. Kobourov","DOI":"10.1109\/TVCG.2018.2865940","Keywords":"Evaluation;user study;graphs;networks;node-link;adjacency matrices","Keywords_Processed":"adjacency matrix;evaluation;node link;user study;network;graph","Title":"Node-Link or Adjacency Matrices: Old Question, New Insights","Labels":null,"Keyword_Vector":[0.1594345711,0.010552103,0.0049960013,0.0290111213,-0.1925620982,0.1006766213,0.1999851373,0.1105198983,-0.0246813098,0.1481961742,-0.116196541,0.0941761859,0.1008145509,-0.1588946136,0.0746555663,0.0176518136,0.1992101279,0.2781112868,-0.0149741528,0.0648539848,0.171826359,-0.0763202688,0.0070411307,-0.0482486507],"Abstract_Vector":[0.2429686838,-0.1384095947,0.0849977603,-0.0457125646,0.1060147176,-0.1071903991,0.1857269252,0.0935416683,0.1508344563,0.0456515464,-0.1163078132,-0.0072384523,-0.2413551434,-0.1086390757,0.0626073094,0.0286645304,-0.0641658346,0.1564355462,0.0567649012,0.0959441307,-0.0369968765,0.0260860776,0.0078086535,-0.1184796232,-0.0228111232,0.0844522369,-0.042833225,0.0121983011,-0.0867048746,-0.0541058301,0.0150442176,-0.0827971346,-0.0554786024,-0.0016464331,0.0362342825,-0.0788247792,-0.0335543879,-0.1260873819,0.0622962715,-0.068541592,-0.1138343226,-0.0081054018,-0.0329299057,-0.0967488171,-0.0692450531,-0.0332923053,0.0023361778,0.0487787777,0.0124426952,-0.0072055427,-0.0248106433,0.047616132,0.0454180401,-0.0371657035,0.034549749,-0.0459912154,0.0789294834,0.0393169472,-0.0754893162,-0.0492089626,0.0145224966,-0.033966691,0.1138446246,0.0436826966,0.0642208469,-0.0406937131,0.060964653,0.0712177286,-0.0183527459,0.0712218514,-0.0344749189,0.066205383,0.0160056676,-0.0662283684,-0.0556285584,-0.0239988872]},"253":{"Abstract":"With the rapid increase in raw volume data sizes, such as terabyte-sized microscopy volumes, the corresponding segmentation label volumes have become extremely large as well. We focus on integer label data, whose efficient representation in memory, as well as fast random data access, pose an even greater challenge than the raw image data. Often, it is crucial to be able to rapidly identify which segments are located where, whether for empty space skipping for fast rendering, or for spatial proximity queries. We refer to this process as culling. In order to enable efficient culling of millions of labeled segments, we present a novel hybrid approach that combines deterministic and probabilistic representations of label data in a data-adaptive hierarchical data structure that we call the label list tree. In each node, we adaptively encode label data using either a probabilistic constant-time access representation for fast conservative culling, or a deterministic logarithmic-time access representation for exact queries. We choose the best data structures for representing the labels of each spatial region while building the label list tree. At run time, we further employ a novel query-adaptive culling strategy. While filtering a query down the tree, we prune it successively, and in each node adaptively select the representation that is best suited for evaluating the pruned query, depending on its size. We show an analysis of the efficiency of our approach with several large data sets from connectomics, including a brain scan with more than 13 million labeled segments, and compare our method to conventional culling approaches. Our approach achieves significant reductions in storage size as well as faster query times.","Authors":"J. Beyer; H. Mohammed; M. Agus; A. K. Al-Awami; H. Pfister; M. Hadwiger","DOI":"10.1109\/TVCG.2018.2864847","Keywords":"Hierarchical Culling;Segmented Volume Data;Bloom Filter;Volume Rendering;Spatial Queries","Keywords_Processed":"spatial queries;volume rendering;hierarchical culling;segmented volume data;bloom filter","Title":"Culling for Extreme-Scale Segmentation Volumes: A Hybrid Deterministic and Probabilistic Approach","Labels":null,"Keyword_Vector":[0.1183597806,-0.03666312,-0.0542498865,0.3784703193,0.3024026406,0.2150650395,-0.0741695317,-0.0197548137,0.2896473186,0.0007746703,0.1196225591,-0.0845152286,-0.0436826948,-0.0510644655,0.0192255749,-0.0241322685,0.0744129027,0.0635189304,0.0554470458,0.0414288302,-0.0345117539,-0.1050463453,-0.0345497208,-0.0363849496],"Abstract_Vector":[0.239208147,-0.0372252286,-0.1047720118,-0.0577765551,-0.0791135112,-0.0811584969,-0.0736851322,0.2854252997,0.0812836758,-0.2385890737,0.0607864173,0.1634287877,0.1150545485,0.0443070213,0.3001516819,-0.1542152196,0.2304134726,-0.0445401033,-0.0543425997,0.1411853059,-0.1283294287,0.1270993848,0.0534789109,0.106872891,-0.1221578554,-0.0111441134,-0.015776411,0.0891073728,-0.067257527,-0.0156456224,0.1174936138,-0.0200583816,-0.03007657,0.0435799719,0.0543569666,-0.0502761827,0.1335392898,0.0028764793,0.0011215968,-0.039682071,-0.0161186586,-0.0257626383,0.0078900102,-0.0205082274,-0.0295799611,-0.0066554249,-0.0334916651,-0.0147593566,-0.0397186249,-0.0606646491,-0.0351943782,-0.0653551159,0.0067088558,-0.0525198329,-0.0121830247,0.0077610485,-0.0078435635,-0.0432528828,0.0021451354,0.044440996,-0.0111823509,0.0107232046,0.0191867978,0.0025334363,-0.0788352835,-0.08247643,-0.0188530389,0.0485186073,0.0850788073,-0.0022140796,0.0038624431,-0.0058130255,0.0520027723,-0.0052639106,0.0099736996,-0.0318115959]},"254":{"Abstract":"In this paper we present a novel GPU-friendly real-time voxelization technique for rendering homogeneous media that is defined by particles, e.g., fluids obtained from particle-based simulations such as Smoothed Particle Hydrodynamics (SPH). Our method computes view-adaptive binary voxelizations with on-the-fly compression of a tiled perspective voxel grid, achieving higher resolutions than previous approaches. It allows for interactive generation of realistic images, enabling advanced rendering techniques such as ray casting-based refraction and reflection, light scattering and absorption, and ambient occlusion. In contrast to previous methods, it does not rely on preprocessing such as expensive, and often coarse, scalar field conversion or mesh generation steps. Our method directly takes unsorted particle data as input. It can be further accelerated by identifying fully populated simulation cells during simulation. The extracted surface can be filtered to achieve smooth surface appearance. Finally, we provide a new scheme for accelerated ray casting inside the voxelization.","Authors":"T. Zirr; C. Dachsbacher","DOI":"10.1109\/TVCG.2017.2656897","Keywords":"Surface extraction;interactive particle visualization;ray tracing","Keywords_Processed":"ray tracing;surface extraction;interactive particle visualization","Title":"Memory-Efficient On-the-Fly Voxelization and Rendering of Particle Data","Labels":null,"Keyword_Vector":[0.175342505,-0.0583363097,-0.0826599512,0.0238775833,-0.0093397106,-0.1335930159,0.1297708697,-0.1219035172,-0.0280440849,0.0303134087,0.0269336805,0.0074470647,-0.0730586306,0.2996117948,0.0178745792,-0.0755352016,-0.1165259948,-0.0755293103,0.1414743197,0.0096458612,-0.0050930488,0.0107498815,-0.1278709874,-0.0978520225],"Abstract_Vector":[0.2159016896,0.1878135785,-0.1719719819,0.0309313562,0.0117000403,0.0615189816,0.1046613655,0.1204582526,-0.2118522285,-0.1069510406,-0.1152858065,0.0362666366,-0.0053192073,0.207140849,0.0089221154,0.0889657113,-0.0805321181,0.0196721629,-0.0382904758,0.0978576976,-0.0065424614,0.078920906,-0.0054029508,-0.0458755149,0.0430993995,-0.1816031522,-0.0024861508,-0.0494217726,0.053853428,-0.0921951785,0.03362401,0.0682207288,-0.0954513132,-0.0825808233,-0.0912583958,0.0091348223,-0.0873331971,-0.1099552853,0.0149610849,0.0235694591,0.0623084806,-0.0403149431,0.0912072155,-0.1309142683,0.0104346009,0.0344989683,-0.0354714076,0.023865748,0.0492176981,0.0236077628,-0.0340195026,-0.0468910656,-0.0033020269,-0.0073459359,-0.0090977383,-0.0772372016,0.0772324795,0.0457110471,-0.1562037155,0.0093677121,0.0832040235,-0.166683379,0.0189433155,0.0498038188,-0.0705052471,0.0441990406,0.0069891868,-0.0519228453,-0.0614535379,0.0041595417,-0.0147804275,-0.0281354753,0.030460269,0.0800429499,0.0915125815,0.0614105256]},"255":{"Abstract":"Financial institutions are interested in ensuring security and quality for their customers. Banks, for instance, need to identify and stop harmful transactions in a timely manner. In order to detect fraudulent operations, data mining techniques and customer profile analysis are commonly used. However, these approaches are not supported by Visual Analytics techniques yet. Visual Analytics techniques have potential to considerably enhance the knowledge discovery process and increase the detection and prediction accuracy of financial fraud detection systems. Thus, we propose EVA, a Visual Analytics approach for supporting fraud investigation, fine-tuning fraud detection algorithms, and thus, reducing false positive alarms.","Authors":"R. A. Leite; T. Gschwandtner; S. Miksch; S. Kriglstein; M. Pohl; E. Gstrein; J. Kuntner","DOI":"10.1109\/TVCG.2017.2744758","Keywords":"Visual Knowledge Discovery;Time Series Data;Business and Finance Visualization;Financial Fraud Detection","Keywords_Processed":"time series data;business and finance visualization;financial fraud detection;visual knowledge discovery","Title":"EVA: Visual Analytics to Identify Fraudulent Events","Labels":null,"Keyword_Vector":[0.2017977728,-0.0838020856,0.1628780238,0.0081276526,-0.0446089101,-0.0802989493,0.024176461,-0.0038764909,0.2400757655,0.0945840631,0.0733400521,-0.1263742671,-0.1088494871,-0.1199951302,0.1013839635,0.2108349189,0.0252894033,0.0206986634,0.05863645,-0.006982327,-0.0477498088,0.1326453431,-0.0149073231,0.1840610919],"Abstract_Vector":[0.1594514381,-0.0717905151,0.0005202431,0.0350189864,-0.0267359168,-0.0392889183,-0.0646295196,-0.0123644131,-0.0214621428,0.0543273687,-0.0360298611,0.1042243328,-0.0319516106,0.0030522335,-0.0199091165,0.0299122199,-0.0504495163,0.0265632044,0.1020024803,-0.0302685692,0.0030267991,0.0727830212,-0.083278585,0.1576826774,-0.005304295,-0.0160288901,-0.0538156387,0.1092561615,0.0083703858,-0.033519751,-0.0789003504,0.1032997482,0.022755913,0.0562007037,0.036725795,0.1648189393,0.016975238,0.0525523938,-0.0824180112,0.0511455516,0.0622387944,-0.1402526853,0.1217178542,-0.0639058351,-0.0536506766,-0.0446837378,0.0643510919,-0.0800587741,0.0005410706,0.1084730931,-0.0030896244,-0.0026966169,0.050463168,0.02884969,-0.0408433839,-0.0177747811,0.0421045568,0.0504952563,0.0946956352,0.1441409967,-0.0097570941,-0.0103537517,-0.0243996853,0.0628251663,0.0377081023,0.0601400093,-0.0402561743,-0.0779889162,-0.0945139818,-0.025593664,0.0590606661,-0.0194231531,0.0101013294,0.0033712082,-0.0445682148,0.0392918338]},"256":{"Abstract":"A variety of human movement datasets are represented in an Origin-Destination(OD) form, such as taxi trips, mobile phone locations, etc. As a commonly-used method to visualize OD data, flow map always fails to discover patterns of human mobility, due to massive intersections and occlusions of lines on a 2D geographical map. A large number of techniques have been proposed to reduce visual clutter of flow maps, such as filtering, clustering and edge bundling, but the correlations of OD flows are often neglected, which makes the simplified OD flow map present little semantic information. In this paper, a characterization of OD flows is established based on an analogy between OD flows and natural language processing (NPL) terms. Then, an iterative multi-objective sampling scheme is designed to select OD flows in a vectorized representation space. To enhance the readability of sampled OD flows, a set of meaningful visual encodings are designed to present the interactions of OD flows. We design and implement a visual exploration system that supports visual inspection and quantitative evaluation from a variety of perspectives. Case studies based on real-world datasets and interviews with domain experts have demonstrated the effectiveness of our system in reducing the visual clutter and enhancing correlations of OD flows.","Authors":"Z. Zhou; L. Meng; C. Tang; Y. Zhao; Z. Guo; M. Hu; W. Chen","DOI":"10.1109\/TVCG.2018.2864503","Keywords":"Visual abstraction;human mobility;origin-destination;flow map;representation learning","Keywords_Processed":"representation learning;human mobility;flow map;origin destination;visual abstraction","Title":"Visual Abstraction of Large Scale Geospatial Origin-Destination Movement Data","Labels":null,"Keyword_Vector":[0.1352445,-0.0475945992,0.2083061674,0.1163347391,-0.1082905703,-0.1040409866,-0.0811092432,-0.062803921,-0.0688446649,-0.0128075742,0.286898474,0.0948335297,-0.1192499271,-0.0760795203,-0.0251315158,-0.0093261148,-0.0504512871,-0.0194551192,-0.0147093166,0.0586512141,-0.0472224675,-0.03033741,0.1819751682,-0.1224656485],"Abstract_Vector":[0.1883046966,-0.0268143512,-0.0619844925,0.0065028086,-0.0025055592,-0.013452274,0.205578381,-0.129968023,0.0103564652,0.1019459178,0.1349499319,0.0986742525,0.1402136561,0.0548833308,-0.0617585584,0.0471947937,0.0659330866,0.083079513,0.0766767072,-0.0266118924,-0.0867412331,0.0479257627,0.0441590296,0.0609260038,0.007735829,-0.0524470896,-0.0059406682,-0.0109996266,0.1332531306,-0.0473702157,0.0340288664,-0.0435080582,0.0001280888,-0.0535380685,0.0876464535,-0.0453305158,-0.0862788547,0.0881803998,0.0307482266,-0.000008549,0.0356857866,-0.0788635245,0.0717830422,-0.0476463947,0.0355320366,0.0941445266,0.1240159091,0.0235952237,-0.009715226,0.0241380007,-0.0274846723,-0.0394377799,0.0869680681,-0.0176559817,0.0974141859,0.04519488,-0.1429572502,-0.0233400125,0.0401990417,-0.0078498695,-0.0719675394,-0.0278532631,0.1123439631,-0.0614987832,-0.0126295369,-0.0377714394,-0.0680724024,-0.0742553063,-0.1374960182,0.1961526051,0.0135934524,0.0390721747,-0.085895218,-0.1107974842,-0.0365825861,0.0153824937]},"257":{"Abstract":"Numerical Weather Prediction (NWP) ensembles are commonly used to assess the uncertainty and confidence in weather forecasts. Spaghetti plots are conventional tools for meteorologists to directly examine the uncertainty exhibited by ensembles, where they simultaneously visualize isocontours of all ensemble members. To avoid visual clutter in practical usages, one needs to select a small number of informative isovalues for visual analysis. Moreover, due to the complex topology and variation of ensemble isocontours, it is often a challenging task to interpret the spaghetti plot for even a single isovalue in large ensembles. In this paper, we propose an interactive framework for uncertainty visualization of weather forecast ensembles that significantly improves and expands the utility of spaghetti plots in ensemble analysis. Complementary to state-of-the-art methods, our approach provides a complete framework for visual exploration of ensemble isocontours, including isovalue selection, interactive isocontour variability exploration, and interactive sub-region selection and re-analysis. Our framework is built upon the high-density clustering paradigm, where the mode structure of the density function is represented as a hierarchy of nested subsets of the data. We generalize the high-density clustering for isocontours and propose a bandwidth selection method for estimating the density function of ensemble isocontours. We present novel visualizations based on high-density clustering results, called the mode plot and the simplified spaghetti plot. The proposed mode plot visually encodes the structure provided by the high-density clustering result and summarizes the distribution of ensemble isocontours. It also enables the selection of subsets of interesting isocontours, which are interactively highlighted in a linked spaghetti plot for providing spatial context. To provide an interpretable overview of the positional variability of isocontours, our system allows for selection of informative isovalues from the simplified spaghetti plot. Due to the spatial variability of ensemble isocontours, the system allows for interactive selection and focus on sub-regions for local uncertainty and clustering re-analysis. We examine a number of ensemble datasets to establish the utility of our approach and discuss its advantages over state-of-the-art visual analysis tools for ensemble data.","Authors":"B. Ma; A. Entezari","DOI":"10.1109\/TVCG.2018.2864815","Keywords":"Spaghetti plots;ensemble visualization;uncertainty visualization;high-density clustering;ensemble forecasting","Keywords_Processed":"spaghetti plot;ensemble visualization;ensemble forecasting;high density clustering;uncertainty visualization","Title":"An Interactive Framework for Visualization of Weather Forecast Ensembles","Labels":null,"Keyword_Vector":[0.2893580227,-0.1696343104,-0.2176195378,-0.1130682022,-0.0120174531,-0.1012255431,-0.2897524774,-0.2287284042,0.0149483589,-0.1514314701,-0.2206840225,0.0835863035,0.0142068937,-0.0847746761,0.0177278429,0.1753340252,0.0378267771,0.1049098054,0.0331595044,-0.0498735128,-0.1560036749,0.0225739899,-0.015322345,0.011380885],"Abstract_Vector":[0.2267663073,-0.1348991405,-0.106089538,0.3957320267,-0.0851870265,0.0108904477,-0.1194733131,-0.0707330262,0.0319730239,-0.0707874743,-0.0629474866,0.0046763972,-0.0905232333,0.039081873,-0.071140045,0.0169412198,0.0570791776,-0.1219366776,-0.009165542,-0.0196676874,-0.0713533928,0.0049943016,-0.0885206998,-0.0416496672,-0.1209339808,0.0797557335,0.0170218333,-0.117441538,0.086393064,-0.1228584221,-0.0911603844,0.0502194528,0.0811650003,0.077969122,0.0425923091,-0.0193270294,-0.0307768588,0.0039916808,0.0337833654,-0.1181515113,-0.0308403943,-0.0498879982,-0.0019729185,0.0376843269,-0.014045972,-0.0784544995,0.0034085578,0.1371362288,-0.020816473,-0.042622578,0.0051820931,0.0537523612,0.0145852182,-0.039859949,-0.0051618,0.055407725,-0.0464430402,-0.1150139953,-0.0658200083,-0.047039363,-0.0230188477,0.0354024353,0.0141160017,0.1030497361,-0.0436586121,-0.1119685265,-0.0198518646,-0.0062682237,0.0073853684,0.0662361858,-0.0044333326,-0.0788219471,-0.0497501976,0.0095836161,0.0398648315,0.0297595427]},"258":{"Abstract":"We propose a method for the vortex extraction and tracking of superconducting magnetic flux vortices for both structured and unstructured mesh data. In the Ginzburg-Landau theory, magnetic flux vortices are well-defined features in a complex-valued order parameter field, and their dynamics determine electromagnetic properties in type-II superconductors. Our method represents each vortex line (a 1D curve embedded in 3D space) as a connected graph extracted from the discretized field in both space and time. For a time-varying discrete dataset, our vortex extraction and tracking method is as accurate as the data discretization. We then apply 3D visualization and 2D event diagrams to the extraction and tracking results to help scientists understand vortex dynamics and macroscale superconductor behavior in greater detail than previously possible.","Authors":"H. Guo; C. L. Phillips; T. Peterka; D. Karpeyev; A. Glatz","DOI":"10.1109\/TVCG.2015.2466838","Keywords":"Superconductor;Vortex extraction;Feature tracking;Unstructured grid;Superconductor;Vortex extraction;Feature tracking;Unstructured grid","Keywords_Processed":"unstructured grid;feature tracking;superconductor;vortex extraction","Title":"Extracting, Tracking, and Visualizing Magnetic Flux Vortices in 3D Complex-Valued Superconductor Simulation Data","Labels":null,"Keyword_Vector":[0.0513212344,-0.0111785746,0.0202300467,-0.0394204623,0.03603248,-0.1360920747,0.0359572109,-0.0720749124,0.0668236977,0.0621718823,0.1041169956,0.1599688682,-0.1697501072,-0.0141319409,-0.0001758555,0.0056175697,-0.0266366532,-0.0462020062,-0.0770453494,-0.195796005,0.0753135355,-0.040084209,-0.2503695916,-0.1053850352],"Abstract_Vector":[0.1706649053,0.0456258294,-0.1414456758,0.0119189957,0.005358318,0.0654947014,0.1440053681,-0.1261461531,-0.0066113318,-0.0037503145,-0.0462422251,-0.0917815682,0.1561199541,-0.0812568393,0.2712841142,0.0601926506,-0.0212619345,0.0197004059,-0.1058998492,-0.0240357564,-0.0086588672,-0.0344427532,-0.0450676071,-0.0243836291,-0.0642244672,0.1372229441,0.0134028451,0.00533076,0.0717338426,0.2622412566,-0.1839773765,0.0116087966,-0.1547094394,0.1279718416,0.1376962686,-0.0480696853,-0.0665919546,-0.0545539431,-0.1169074541,0.2291470283,0.0385564326,0.1265218207,-0.1328333953,-0.0824501801,-0.0216560178,-0.0333947274,-0.0795711451,0.1976099346,0.0045526073,0.1082338749,0.0876102861,0.0376725835,-0.0055059129,0.0382841676,-0.0366468286,-0.0435906941,-0.0001542521,0.0089290321,-0.0072264401,-0.0724752457,0.0646393944,-0.0388002355,-0.0211816945,-0.0164963235,0.0009523961,-0.0273821333,-0.0240339064,-0.0427861438,0.0292970735,-0.0055841459,0.0112423817,-0.0173837819,-0.0209915009,-0.0289036862,-0.0018761264,0.0318955828]},"259":{"Abstract":"The increasing availability of spatiotemporal data continuously collected from various sources provides new opportunities for a timely understanding of the data in their spatial and temporal context. Finding abnormal patterns in such data poses significant challenges. Given that there is often no clear boundary between normal and abnormal patterns, existing solutions are limited in their capacity of identifying anomalies in large, dynamic and heterogeneous data, interpreting anomalies in their multifaceted, spatiotemporal context, and allowing users to provide feedback in the analysis loop. In this work, we introduce a unified visual interactive system and framework, Voila, for interactively detecting anomalies in spatiotemporal data collected from a streaming data source. The system is designed to meet two requirements in real-world applications, i.e., online monitoring and interactivity. We propose a novel tensor-based anomaly analysis algorithm with visualization and interaction design that dynamically produces contextualized, interpretable data summaries and allows for interactively ranking anomalous patterns based on user input. Using the \u201csmart city\u201d as an example scenario, we demonstrate the effectiveness of the proposed framework through quantitative evaluation and qualitative case studies.","Authors":"N. Cao; C. Lin; Q. Zhu; Y. Lin; X. Teng; X. Wen","DOI":"10.1109\/TVCG.2017.2744419","Keywords":"Anomaly Detection;Visual Analysis","Keywords_Processed":"visual analysis;anomaly detection","Title":"Voila: Visual Anomaly Detection and Monitoring with Streaming Spatiotemporal Data","Labels":null,"Keyword_Vector":[0.2356092874,-0.1723172089,0.476935379,-0.1277403619,0.045706808,-0.0980804564,0.0584489656,-0.0705764538,0.3119038283,0.0967416523,-0.077509566,0.0789587513,0.1971791356,-0.0701981187,0.043179014,0.0651070889,-0.1028054917,-0.1066264417,0.0709155287,0.0950324118,0.0685180141,0.033441959,-0.0476801929,0.12852202],"Abstract_Vector":[0.3084873741,-0.1633094323,-0.0285808253,-0.1076195651,-0.0732568405,-0.0551764817,-0.0648323684,-0.0442019521,-0.1155281914,0.0463793558,0.0395769364,0.1654201824,-0.0878680249,0.0035241521,0.0061201365,-0.0489382681,0.023494271,-0.0431720718,0.0437784276,-0.0415832414,0.0862832788,-0.014763104,-0.1390317159,0.0372464195,-0.0196766813,0.0297163852,-0.1241255849,0.029739442,-0.0229462901,-0.0185171537,-0.1190920687,-0.0495701403,0.0845898194,-0.1030441778,0.0735865728,0.0659271481,-0.0665598969,-0.1832736216,-0.0392853518,0.022724364,-0.099217249,-0.1390757356,0.0442764965,-0.033993713,0.0190959772,0.041520118,0.0317814805,-0.0810971933,-0.2081101446,-0.1027375904,0.1213639934,0.1116867335,0.0124652034,0.0703955826,-0.0166288407,-0.0442920407,-0.0683811299,0.0847441667,0.0137625661,0.0020210426,0.1055264058,-0.0083715234,-0.0209229355,-0.0891567155,0.0008500574,-0.0646768966,-0.0147054087,-0.0919365957,-0.0000798794,0.0035860385,-0.0179195317,0.0028929048,0.1266494331,0.0140396528,0.047213541,-0.0887458673]}}