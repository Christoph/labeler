{
  "309": {
    "Abstract": "Line graphs have been the visualization of choice for temporal data ever since the days of William Playfair (1759-1823), but realistic temporal analysis tasks often include multiple simultaneous time series. In this work, we explore user performance for comparison, slope, and discrimination tasks for different line graph techniques involving multiple time series. Our results show that techniques that create separate charts for each time series--such as small multiples and horizon graphs--are generally more efficient for comparisons across time series with a large visual span. On the other hand, shared-space techniques--like standard line graphs--are typically more efficient for comparisons over smaller visual spans where the impact of overlap and clutter is reduced.",
    "Authors": "Javed, W.;McDonnel, B.;Elmqvist, N.",
    "Clusters": "ChartsDiagramsPlots;EvaluationGeneral;VisualDesignDesignGuidelines;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2010.162",
    "Keywords": "stacked graphs;design guidelines;braided graphs;horizon graphs;line charts;small multiples;evaluation",
    "Keywords_Processed": "braid graph;design guideline;line chart;small multiple;stack graph;horizon graphs;evaluation",
    "Title": "Graphical Perception of Multiple Time Series"
  },
  "6": {
    "Abstract": "Regression models play a key role in many application domains for analyzing or predicting a quantitative dependent variable based on one or more independent variables. Automated approaches for building regression models are typically limited with respect to incorporating domain knowledge in the process of selecting input variables (also known as feature subset selection). Other limitations include the identification of local structures, transformations, and interactions between variables. The contribution of this paper is a framework for building regression models addressing these limitations. The framework combines a qualitative analysis of relationship structures by visualization and a quantification of relevance for ranking any number of features and pairs of features which may be categorical or continuous. A central aspect is the local approximation of the conditional target distribution by partitioning 1D and 2D feature domains into disjoint regions. This enables a visual investigation of local patterns and largely avoids structural assumptions for the quantitative ranking. We describe how the framework supports different tasks in model building (e.g., validation and comparison), and we present an interactive workflow for feature subset selection. A real-world case study illustrates the step-wise identification of a five-dimensional model for natural gas consumption. We also report feedback from domain experts after two months of deployment in the energy sector, indicating a significant effort reduction for building and improving regression models.",
    "Authors": "Muhlbacher, T.;Piringer, H.",
    "Clusters": "AlgorithmicPatternFeatureDetectionTracking;KnowledgeDiscovery;MachineLearningAndStatistics;SegmentationAndClassification;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2013.125",
    "Keywords": "regression;data partitioning;guided visualization;feature selection;visual knowledge discovery;model building",
    "Keywords_Processed": "guide visualization;feature selection;model building;regression;datum partitioning;visual knowledge discovery",
    "Title": "A Partition-Based Framework for Building and Validating Regression Models"
  },
  "362": {
    "Abstract": "Over the past few years, large human populations around the world have been affected by an increase in significant seismic activities. For both conducting basic scientific research and for setting critical government policies, it is crucial to be able to explore and understand seismic and geographical information obtained through all scientific instruments. In this work, we present a visual analytics system that enables explorative visualization of seismic data together with satellite-based observational data, and introduce a suite of visual analytical tools. Seismic and satellite data are integrated temporally and spatially. Users can select temporal ;and spatial ranges to zoom in on specific seismic events, as well as to inspect changes both during and after the events. Tools for designing high dimensional transfer functions have been developed to enable efficient and intuitive comprehension of the multi-modal data. Spread-sheet style comparisons are used for data drill-down as well as presentation. Comparisons between distinct seismic events are also provided for characterizing event-wise differences. Our system has been designed for scalability in terms of data size, complexity (i.e. number of modalities), and varying form factors of display environments.",
    "Authors": "Xiaoru Yuan;He Xiao;Hanqi Guo;Peihong Guo;Kendall, W.;Huang, J.;Yongxian Zhang",
    "Clusters": "EarthSpaceAndEnvironmentalSciences;LargeScaleDataAndScalability;MultidimensionalMultivariateMultifieldDataAndTechniques",
    "DOI": "10.1109\/TVCG.2010.192",
    "Keywords": "scalable visualization;seismic data;earth science visualization;multivariate visualization",
    "Keywords_Processed": "scalable visualization;earth science visualization;multivariate visualization;seismic datum",
    "Title": "Scalable Multi-variate Analytics of Seismic and Satellite-based Observational Data"
  },
  "220": {
    "Abstract": "Working with three domain specialists we investigate human-centered approaches to geovisualization following an ISO13407 taxonomy covering context of use, requirements and early stages of design. Our case study, undertaken over three years, draws attention to repeating trends: that generic approaches fail to elicit adequate requirements for geovis application design; that the use of real data is key to understanding needs and possibilities; that trust and knowledge must be built and developed with collaborators. These processes take time but modified human-centred approaches can be effective. A scenario developed through contextual inquiry but supplemented with domain data and graphics is useful to geovis designers. Wireframe, paper and digital prototypes enable successful communication between specialist and geovis domains when incorporating real and interesting data, prompting exploratory behaviour and eliciting previously unconsidered requirements. Paper prototypes are particularly successful at eliciting suggestions, especially for novel visualization. Enabling specialists to explore their data freely with a digital prototype is as effective as using a structured task protocol and is easier to administer. Autoethnography has potential for framing the design process. We conclude that a common understanding of context of use, domain data and visualization possibilities are essential to successful geovis design and develop as this progresses. HC approaches can make a significant contribution here. However, modified approaches, applied with flexibility, are most promising. We advise early, collaborative engagement with data - through simple, transient visual artefacts supported by data sketches and existing designs - before moving to successively more sophisticated data wireframes and data prototypes.",
    "Authors": "Lloyd, D.;Dykes, J.",
    "Clusters": "EvaluationGeneral;FieldStudies;GeographyGeospatialVisCartographyTerrainVis;HumanComputerInteractionHumanFactors;InteractionTechniquesGeneral;TasksTaskRequirementsAnalysis;VisualDesignDesignGuidelines",
    "DOI": "10.1109\/TVCG.2011.209",
    "Keywords": "evaluation;sketch;prototypes;requirements;field study;design;geovisualization;context of use",
    "Keywords_Processed": "prototype;sketch;context of use;geovisualization;requirement;design;field study;evaluation",
    "Title": "Human-Centered Approaches in Geovisualization Design: Investigating Multiple Methods Through a Long-Term Case Study"
  },
  "139": {
    "Abstract": "Color mapping and semitransparent layering play an important role in many visualization scenarios, such as information visualization and volume rendering. The combination of color and transparency is still dominated by standard alpha-compositing using the Porter-Duff over operator which can result in false colors with deceiving impact on the visualization. Other more advanced methods have also been proposed, but the problem is still far from being solved. Here we present an alternative to these existing methods specifically devised to avoid false colors and preserve visual depth ordering. Our approach is data driven and follows the recently formulated knowledge-assisted visualization (KAV) paradigm. Preference data, that have been gathered in web-based user surveys, are used to train a support-vector machine model for automatically predicting an optimized hue-preserving blending. We have applied the resulting model to both volume rendering and a specific information visualization technique, illustrative parallel coordinate plots. Comparative renderings show a significant improvement over previous approaches in the sense that false colors are completely removed and important properties such as depth ordering and blending vividness are better preserved. Due to the generality of the defined data-driven blending operator, it can be easily integrated also into other visualization frameworks.",
    "Authors": "Kuhne, L.;Giesen, J.;Zhiyuan Zhang;Sungsoo Ha;Mueller, K.",
    "Clusters": "ColorColorPerception;ParallelCoordinates;VisualizationTechniquesAndToolsGeneral;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2012.186",
    "Keywords": "knowledge-assisted visualization;volume rendering;color blending;hue preservation;parallel coordinates",
    "Keywords_Processed": "volume render;parallel coordinate;knowledge assist visualization;color blending;hue preservation",
    "Title": "A Data-Driven Approach to Hue-Preserving Color-Blending"
  },
  "379": {
    "Abstract": "Data sets in astronomy are growing to enormous sizes. Modern astronomical surveys provide not only image data but also catalogues of millions of objects (stars, galaxies), each object with hundreds of associated parameters. Exploration of this very high-dimensional data space poses a huge challenge. Subspace clustering is one among several approaches which have been proposed for this purpose in recent years. However, many clustering algorithms require the user to set a large number of parameters without any guidelines. Some methods also do not provide a concise summary of the datasets, or, if they do, they lack additional important information such as the number of clusters present or the significance of the clusters. In this paper, we propose a method for ranking subspaces for clustering which overcomes many of the above limitations. First we carry out a transformation from parametric space to discrete image space where the data are represented by a grid-based density field. Then we apply so-called connected morphological operators on this density field of astronomical objects that provides visual support for the analysis of the important subspaces. Clusters in subspaces correspond to high-intensity regions in the density image. The importance of a cluster is measured by a new quality criterion based on the dynamics of local maxima of the density. Connected operators are able to extract such regions with an indication of the number of clusters present. The subspaces are visualized during computation of the quality measure, so that the user can interact with the system to improve the results. In the result stage, we use three visualization toolkits linked within a graphical user interface so that the user can perform an in-depth exploration of the ranked subspaces. Evaluation based on synthetic as well as real astronomical datasets demonstrates the power of the new method. We recover various known astronomical relations directly from the data with little or no a pri- - ori assumptions. Hence, our method holds good prospects for discovering new relations as well.",
    "Authors": "Ferdosi, B.J.;Buddelmeijer, H.;Trager, S.;Wilkinson, M.H.F.;Roerdink, J.B.T.",
    "Clusters": "AnalysisProcessGeneral;AstronomyAstrophysics;DataClusteringAndAggregation;ImageBasedDataImageSignalProcessing;SpaceRelatedSpatialDataAndTechniques",
    "DOI": "10.1109\/VAST.2010.5652450",
    "Keywords": "astronomical data;clustering high-dimensional data;visual exploration;connected morphological operators;subspace finding",
    "Keywords_Processed": "visual exploration;cluster high dimensional datum;connected morphological operator;astronomical datum;subspace finding",
    "Title": "finding and visualizing relevant subspaces for clustering high-dimensional astronomical data using connected morphological operators"
  },
  "38": {
    "Abstract": "Analyzing large textual collections has become increasingly challenging given the size of the data available and the rate that more data is being generated. Topic-based text summarization methods coupled with interactive visualizations have presented promising approaches to address the challenge of analyzing large text corpora. As the text corpora and vocabulary grow larger, more topics need to be generated in order to capture the meaningful latent themes and nuances in the corpora. However, it is difficult for most of current topic-based visualizations to represent large number of topics without being cluttered or illegible. To facilitate the representation and navigation of a large number of topics, we propose a visual analytics system - HierarchicalTopic (HT). HT integrates a computational algorithm, Topic Rose Tree, with an interactive visual interface. The Topic Rose Tree constructs a topic hierarchy based on a list of topics. The interactive visual interface is designed to present the topic content as well as temporal evolution of topics in a hierarchical fashion. User interactions are provided for users to make changes to the topic hierarchy based on their mental model of the topic space. To qualitatively evaluate HT, we present a case study that showcases how HierarchicalTopics aid expert users in making sense of a large number of topics and discovering interesting patterns of topic groups. We have also conducted a user study to quantitatively evaluate the effect of hierarchical topic structure. The study results reveal that the HT leads to faster identification of large number of relevant topics. We have also solicited user feedback during the experiments and incorporated some suggestions into the current version of HierarchicalTopics.",
    "Authors": "Wenwen Dou;Li Yu;Xiaoyu Wang;Zhiqiang Ma;Ribarsky, W.",
    "Clusters": "BiologyAndBioinformatics;TextDocumentTopicAnalysisDataAndTechniques;",
    "DOI": "10.1109\/TVCG.2013.162",
    "Keywords": "visual analytics;rose tree;hierarchical topic representation;topic modeling",
    "Keywords_Processed": "visual analytic;topic modeling;hierarchical topic representation;rise tree",
    "Title": "HierarchicalTopics: Visually Exploring Large Text Collections Using Topic Hierarchies"
  },
  "254": {
    "Abstract": "Continuous Parallel Coordinates (CPC) are a contemporary visualization technique in order to combine several scalar fields, given over a common domain. They facilitate a continuous view for parallel coordinates by considering a smooth scalar field instead of a finite number of straight lines. We show that there are feature curves in CPC which appear to be the dominant structures of a CPC. We present methods to extract and classify them and demonstrate their usefulness to enhance the visualization of CPCs. In particular, we show that these feature curves are related to discontinuities in Continuous Scatterplots (CSP). We show this by exploiting a curve-curve duality between parallel and Cartesian coordinates, which is a generalization of the well-known point-line duality. Furthermore, we illustrate the theoretical considerations. Concluding, we discuss relations and aspects of the CPC's\/CSP's features concerning the data analysis.",
    "Authors": "Lehmann, D.J.;Theisel, H.",
    "Clusters": "DataFeaturesAndAttributes;ParallelCoordinates;TopologyBasedTechniques;",
    "DOI": "10.1109\/TVCG.2011.200",
    "Keywords": "visualization;topology;parallel coordinates;features",
    "Keywords_Processed": "visualization;parallel coordinate;feature;topology",
    "Title": "Features in Continuous Parallel Coordinates"
  },
  "129": {
    "Abstract": "We present a method for automatically building typographic maps that merge text and spatial data into a visual representation where text alone forms the graphical features. We further show how to use this approach to visualize spatial data such as traffic density, crime rate, or demographic data. The technique accepts a vector representation of a geographic map and spatializes the textual labels in the space onto polylines and polygons based on user-defined visual attributes and constraints. Our sample implementation runs as a Web service, spatializing shape files from the OpenStreetMap project into typographic maps for any region.",
    "Authors": "Afzal, S.;Maciejewski, R.;Yun Jang;Elmqvist, N.;Ebert, D.S.",
    "Clusters": "GeographyGeospatialVisCartographyTerrainVis;Labeling;SpaceRelatedSpatialDataAndTechniques;TextDocumentTopicAnalysisDataAndTechniques",
    "DOI": "10.1109\/TVCG.2012.264",
    "Keywords": "text visualization;spatial data;geovisualization;label placement",
    "Keywords_Processed": "geovisualization;spatial datum;text visualization;label placement",
    "Title": "Spatial Text Visualization Using Automatic Typographic Maps"
  },
  "14": {
    "Abstract": "Domain-specific database applications tend to contain a sizable number of table-, form-, and report-style views that must each be designed and maintained by a software developer. A significant part of this job is the necessary tweaking of low-level presentation details such as label placements, text field dimensions, list or table styles, and so on. In this paper, we present a horizontally constrained layout management algorithm that automates the display of structured hierarchical data using the traditional visual idioms of hand-designed database UIs: tables, multi-column forms, and outline-style indented lists. We compare our system with pure outline and nested table layouts with respect to space efficiency and readability, the latter with an online user study on 27 subjects. Our layouts are 3.9 and 1.6 times more compact on average than outline layouts and horizontally unconstrained table layouts, respectively, and are as readable as table layouts even for large datasets.",
    "Authors": "Bakke, E.;Karger, D.R.;Miller, R.C.",
    "Clusters": "HierarchicalTreeDataAndTechniques;TabularDataAndTechniques;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2013.137",
    "Keywords": "hierarchy data;nested relations;tabular data;layout management",
    "Keywords_Processed": "tabular datum;nested relation;layout management;hierarchy datum",
    "Title": "Automatic Layout of Structured Hierarchical Reports"
  },
  "314": {
    "Abstract": "Conveying data uncertainty in visualizations is crucial for preventing viewers from drawing conclusions based on untrustworthy data points. This paper proposes a methodology for efficiently generating density plots of uncertain multivariate data sets that draws viewers to preattentively identify values of high certainty while not calling attention to uncertain values. We demonstrate how to augment scatter plots and parallel coordinates plots to incorporate statistically modeled uncertainty and show how to integrate them with existing multivariate analysis techniques, including outlier detection and interactive brushing. Computing high quality density plots can be expensive for large data sets, so we also describe a probabilistic plotting technique that summarizes the data without requiring explicit density plot computation. These techniques have been useful for identifying brain tumors in multivariate magnetic resonance spectroscopy data and we describe how to extend them to visualize ensemble data sets.",
    "Authors": "Feng, D.;Kwock, L.;Yueh Lee;Taylor, R.M.",
    "Clusters": "ChartsDiagramsPlots;InteractionTechniquesGeneral;MultidimensionalMultivariateMultifieldDataAndTechniques;ParallelCoordinates;UncertaintyTechniquesAndVisualization",
    "DOI": "10.1109\/TVCG.2010.176",
    "Keywords": "scatterplot;uncertainty visualization;brushing;multivariate data;parallel coordinates",
    "Keywords_Processed": "multivariate datum;parallel coordinate;brush;scatterplot;uncertainty visualization",
    "Title": "Matching Visual Saliency to Confidence in Plots of Uncertain Data"
  },
  "235": {
    "Abstract": "While it is still most common for information visualization researchers to develop new visualizations from a data-or taskdriven perspective, there is growing interest in understanding the types of visualizations people create by themselves for personal use. As part of this recent direction, we have studied a large collection of whiteboards in a research institution, where people make active use of combinations of words, diagrams and various types of visuals to help them further their thought processes. Our goal is to arrive at a better understanding of the nature of visuals that are created spontaneously during brainstorming, thinking, communicating, and general problem solving on whiteboards. We use the qualitative approaches of open coding, interviewing, and affinity diagramming to explore the use of recognizable and novel visuals, and the interplay between visualization and diagrammatic elements with words, numbers and labels. We discuss the potential implications of our findings on information visualization design.",
    "Authors": "Walny, J.;Carpendale, S.;Riche, N.H.;Venolia, G.;Fawcett, P.",
    "Clusters": "ChartsDiagramsPlots;HumanComputerInteractionHumanFactors;QualitativeEvaluation;",
    "DOI": "10.1109\/TVCG.2011.251",
    "Keywords": "visualization;whiteboard;diagrams;observational study",
    "Keywords_Processed": "visualization;whiteboard;diagram;observational study",
    "Title": "Visual Thinking In Action: Visualizations As Used On Whiteboards"
  },
  "12": {
    "Abstract": "We describe and demonstrate an extensible framework that supports data exploration and provenance in the context of Human Terrain Analysis (HTA). Working closely with defence analysts we extract requirements and a list of features that characterise data analysed at the end of the HTA chain. From these, we select an appropriate non-classified data source with analogous features, and model it as a set of facets. We develop ProveML, an XML-based extension of the Open Provenance Model, using these facets and augment it with the structures necessary to record the provenance of data, analytical process and interpretations. Through an iterative process, we develop and refine a prototype system for Human Terrain Visual Analytics (HTVA), and demonstrate means of storing, browsing and recalling analytical provenance and process through analytic bookmarks in ProveML. We show how these bookmarks can be combined to form narratives that link back to the live data. Throughout the process, we demonstrate that through structured workshops, rapid prototyping and structured communication with intelligence analysts we are able to establish requirements, and design schema, techniques and tools that meet the requirements of the intelligence community. We use the needs and reactions of defence analysts in defining and steering the methods to validate the framework.",
    "Authors": "Walker, R.;Slingsby, A.;Dykes, J.;Kai Xu;Wood, J.;Nguyen, P.H.;Stephens, D.;Wong, B.L.W.;Yongjun Zheng",
    "Clusters": "GeographyGeospatialVisCartographyTerrainVis;InternetWebVisualizationForTheMasses;ProvenanceAndHistory;Storytelling;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/TVCG.2013.132",
    "Keywords": "human terrain analysis;provenance;bookmarks;framework;narrative",
    "Keywords_Processed": "human terrain analysis;framework;narrative;bookmark;provenance",
    "Title": "An Extensible Framework for Provenance in Human Terrain Visual Analytics"
  },
  "96": {
    "Abstract": "All major web mapping services use the web Mercator projection. This is a poor choice for maps of the entire globe or areas of the size of continents or larger countries because the Mercator projection shows medium and higher latitudes with extreme areal distortion and provides an erroneous impression of distances and relative areas. The web Mercator projection is also not able to show the entire globe, as polar latitudes cannot be mapped. When selecting an alternative projection for information visualization, rivaling factors have to be taken into account, such as map scale, the geographic area shown, the map's height-to-width ratio, and the type of cartographic visualization. It is impossible for a single map projection to meet the requirements for all these factors. The proposed composite map projection combines several projections that are recommended in cartographic literature and seamlessly morphs map space as the user changes map scale or the geographic region displayed. The composite projection adapts the map's geometry to scale, to the map's height-to-width ratio, and to the central latitude of the displayed area by replacing projections and adjusting their parameters. The composite projection shows the entire globe including poles; it portrays continents or larger countries with less distortion (optionally without areal distortion); and it can morph to the web Mercator projection for maps showing small regions.",
    "Authors": "Jenny, B.",
    "Clusters": "InternetWebVisualizationForTheMasses;Maps;ProgrammingAlgorithmsAndDataStructures",
    "DOI": "10.1109\/TVCG.2012.192",
    "Keywords": "web mapping;web cartography;web mercator;html5 canvas;multi-scale map;web map projection",
    "Keywords_Processed": "html5 canvas;web mapping;web cartography;web mercator;web map projection;multi scale map",
    "Title": "Adaptive Composite Map Projections"
  },
  "131": {
    "Abstract": "Glyph-based visualization can offer elegant and concise presentation of multivariate information while enhancing speed and ease in visual search experienced by users. As with icon designs, glyphs are usually created based on the designers' experience and intuition, often in a spontaneous manner. Such a process does not scale well with the requirements of applications where a large number of concepts are to be encoded using glyphs. To alleviate such limitations, we propose a new systematic process for glyph design by exploring the parallel between the hierarchy of concept categorization and the ordering of discriminative capacity of visual channels. We examine the feasibility of this approach in an application where there is a pressing need for an efficient and effective means to visualize workflows of biological experiments. By processing thousands of workflow records in a public archive of biological experiments, we demonstrate that a cost-effective glyph design can be obtained by following a process of formulating a taxonomy with the aid of computation, identifying visual channels hierarchically, and defining application-specific abstraction and metaphors.",
    "Authors": "Maguire, E.;Rocca-Serra, P.;Sansone, S.-A.;Davies, J.;Chen, M.",
    "Clusters": "BiologyAndBioinformatics;DesignMethodologiesAndInteractionDesign;GlyphsGlyphBasedTechniques;Taxonomies",
    "DOI": "10.1109\/TVCG.2012.271",
    "Keywords": "taxonomy;design methodologies;bioinformatics visualization;glyph-based techniques",
    "Keywords_Processed": "glyph base technique;taxonomy;bioinformatic visualization;design methodology",
    "Title": "Taxonomy-Based Glyph Design---with a Case Study on Visualizing Workflows of Biological Experiments"
  },
  "116": {
    "Abstract": "In this paper, we propose a new strategy for graph drawing utilizing layouts of many sub-graphs supplied by a large group of people in a crowd sourcing manner. We developed an algorithm based on Laplacian constrained distance embedding to merge subgraphs submitted by different users, while attempting to maintain the topological information of the individual input layouts. To facilitate collection of layouts from many people, a light-weight interactive system has been designed to enable convenient dynamic viewing, modification and traversing between layouts. Compared with other existing graph layout algorithms, our approach can achieve more aesthetic and meaningful layouts with high user preference.",
    "Authors": "Xiaoru Yuan;Limei Che;Yifan Hu;Xin Zhang",
    "Clusters": "DataEditing;DataRegistrationFusionAndIntegration;Engineering;EvaluationGeneral;GraphNetworkDataAndTechniques;MatrixRelatedTechniques",
    "DOI": "10.1109\/TVCG.2012.236",
    "Keywords": "merging;stress model;graph layout;laplacian matrix;editing;force-directed layout;crowdsourcing",
    "Keywords_Processed": "graph layout;laplacian matrix;stress model;merge;edit;force direct layout;crowdsource",
    "Title": "Intelligent Graph Layout Using Many Users' Input"
  },
  "92": {
    "Abstract": "An important feature of networks for many application domains is their community structure. This is because objects within the same community usually have at least one property in common. The investigation of community structure can therefore support the understanding of object attributes from the network topology alone. In real-world systems, objects may belong to several communities at the same time, i.e., communities can overlap. Analyzing fuzzy community memberships is essential to understand to what extent objects contribute to different communities and whether some communities are highly interconnected. We developed a visualization approach that is based on node-link diagrams and supports the investigation of fuzzy communities in weighted undirected graphs at different levels of detail. Starting with the network of communities, the user can continuously drill down to the network of individual nodes and finally analyze the membership distribution of nodes of interest. Our approach uses layout strategies and further visual mappings to graphically encode the fuzzy community memberships. The usefulness of our approach is illustrated by two case studies analyzing networks of different domains: social networking and biological interactions. The case studies showed that our layout and visualization approach helps investigate fuzzy overlapping communities. Fuzzy vertices as well as the different communities to which they belong can be easily identified based on node color and position.",
    "Authors": "Vehlow, C.;Reinhardt, T.;Weiskopf, D.",
    "Clusters": "DataClusteringAndAggregation;GraphNetworkDataAndTechniques;SocialNetworksAndSocialMedia;UncertaintyTechniquesAndVisualization",
    "DOI": "10.1109\/TVCG.2013.232",
    "Keywords": "overlapping community visualization;uncertainty visualization;fuzzy clustering;graph visualization",
    "Keywords_Processed": "overlap community visualization;uncertainty visualization;graph visualization;fuzzy clustering",
    "Title": "Visualizing Fuzzy Overlapping Communities in Networks"
  },
  "255": {
    "Abstract": "A new type of glyph is introduced to visualize unsteady flow with static images, allowing easier analysis of time-dependent phenomena compared to animated visualization. Adopting the visual metaphor of radar displays, this glyph represents flow directions by angles and time by radius in spherical coordinates. Dense seeding of flow radar glyphs on the flow domain naturally lends itself to multi-scale visualization: zoomed-out views show aggregated overviews, zooming-in enables detailed analysis of spatial and temporal characteristics. Uncertainty visualization is supported by extending the glyph to display possible ranges of flow directions. The paper focuses on 2D flow, but includes a discussion of 3D flow as well. Examples from CFD and the field of stochastic hydrogeology show that it is easy to discriminate regions of different spatiotemporal flow behavior and regions of different uncertainty variations in space and time. The examples also demonstrate that parameter studies can be analyzed because the glyph design facilitates comparative visualization. Finally, different variants of interactive GPU-accelerated implementations are discussed.",
    "Authors": "Hlawatsch, M.;Leube, P.;Nowak, W.;Weiskopf, D.",
    "Clusters": "FlowVisualizationDataAndTechniques;GlyphsGlyphBasedTechniques;UncertaintyTechniquesAndVisualization;",
    "DOI": "10.1109\/TVCG.2011.203",
    "Keywords": "visualization;glyph;uncertainty;unsteady flow",
    "Keywords_Processed": "visualization;glyph;uncertainty;unsteady flow",
    "Title": "Flow Radar Glyphs&amp;#8212;Static Visualization of Unsteady Flow with Uncertainty"
  },
  "72": {
    "Abstract": "Business ecosystems are characterized by large, complex, and global networks of firms, often from many different market segments, all collaborating, partnering, and competing to create and deliver new products and services. Given the rapidly increasing scale, complexity, and rate of change of business ecosystems, as well as economic and competitive pressures, analysts are faced with the formidable task of quickly understanding the fundamental characteristics of these interfirm networks. Existing tools, however, are predominantly query- or list-centric with limited interactive, exploratory capabilities. Guided by a field study of corporate analysts, we have designed and implemented dotlink360, an interactive visualization system that provides capabilities to gain systemic insight into the compositional, temporal, and connective characteristics of business ecosystems. dotlink360 consists of novel, multiple connected views enabling the analyst to explore, discover, and understand interfirm networks for a focal firm, specific market segments or countries, and the entire business ecosystem. System evaluation by a small group of prototypical users shows supporting evidence of the benefits of our approach. This design study contributes to the relatively unexplored, but promising area of exploratory information visualization in market research and business strategy.",
    "Authors": "Basole, R.C.;Clear, T.;Mengdie Hu;Mehrotra, H.;Stasko, J.",
    "Clusters": "AnalysisProcessGeneral;BusinessFinanceEconomyManufacturing;DesignStudiesAndCaseStudies;GraphNetworkDataAndTechniques;InteractionTechniquesGeneral",
    "DOI": "10.1109\/TVCG.2013.209",
    "Keywords": "design study;strategic analysis;market research;business ecosystems;network visualization;interaction",
    "Keywords_Processed": "interaction;design study;strategic analysis;network visualization;business ecosystem;market research",
    "Title": "Understanding Interfirm Relationships in Business Ecosystems with Interactive Visualization"
  },
  "75": {
    "Abstract": "Topic modeling has been widely used for analyzing text document collections. Recently, there have been significant advancements in various topic modeling techniques, particularly in the form of probabilistic graphical modeling. State-of-the-art techniques such as Latent Dirichlet Allocation (LDA) have been successfully applied in visual text analytics. However, most of the widely-used methods based on probabilistic modeling have drawbacks in terms of consistency from multiple runs and empirical convergence. Furthermore, due to the complicatedness in the formulation and the algorithm, LDA cannot easily incorporate various types of user feedback. To tackle this problem, we propose a reliable and flexible visual analytics system for topic modeling called UTOPIAN (User-driven Topic modeling based on Interactive Nonnegative Matrix Factorization). Centered around its semi-supervised formulation, UTOPIAN enables users to interact with the topic modeling method and steer the result in a user-driven manner. We demonstrate the capability of UTOPIAN via several usage scenarios with real-world document corpuses such as InfoVis\/VAST paper data set and product review data sets.",
    "Authors": "Jaegul Choo;Changhyun Lee;Reddy, C.K.;Haesun Park",
    "Clusters": "DataClusteringAndAggregation;MachineLearningAndStatistics;NumericalMethodsMathematics;TextDocumentTopicAnalysisDataAndTechniques;",
    "DOI": "10.1109\/TVCG.2013.212",
    "Keywords": "nonnegative matrix factorization;text analytics;latent dirichlet allocation;interactive clustering;visual analytics;topic modeling",
    "Keywords_Processed": "latent dirichlet allocation;visual analytic;nonnegative matrix factorization;text analytic;interactive clustering;topic modeling",
    "Title": "UTOPIAN: User-Driven Topic Modeling Based on Interactive Nonnegative Matrix Factorization"
  },
  "248": {
    "Abstract": "When visualizing tubular 3D structures, external representations are often used for guidance and display, and such views in 2D can often contain occlusions. Virtual dissection methods have been proposed where the entire 3D structure can be mapped to the 2D plane, though these will lose context by straightening curved sections. We present a new method of creating maps of 3D tubular structures that yield a succinct view while preserving the overall geometric structure. Given a dominant view plane for the structure, its curve skeleton is first projected to a 2D skeleton. This 2D skeleton is adjusted to account for distortions in length, modified to remove intersections, and optimized to preserve the shape of the original 3D skeleton. Based on this shaped 2D skeleton, a boundary for the map of the object is obtained based on a slicing path through the structure and the radius around the skeleton. The sliced structure is conformally mapped to a rectangle and then deformed via harmonic mapping to match the boundary placement. This flattened map preserves the general geometric context of a 3D object in a 2D display, and rendering of this flattened map can be accomplished using volumetric ray casting. We have evaluated our method on real datasets of human colon models.",
    "Authors": "Marino, J.;Wei Zeng;Xianfeng Gu;Kaufman, A.",
    "Clusters": "BiomedicalScienceAndMedicine;GeometryBasedTechniques;NumericalMethodsMathematics;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2011.182",
    "Keywords": "volume rendering;conformal mapping;biomedical visualization;medical visualization;geometry-based technique",
    "Keywords_Processed": "volume render;geometry base technique;biomedical visualization;conformal mapping;medical visualization",
    "Title": "Context Preserving Maps of Tubular Structures"
  },
  "42": {
    "Abstract": "Many datasets, such as scientific literature collections, contain multiple heterogeneous facets which derive implicit relations, as well as explicit relational references between data items. The exploration of this data is challenging not only because of large data scales but also the complexity of resource structures and semantics. In this paper, we present PivotSlice, an interactive visualization technique which provides efficient faceted browsing as well as flexible capabilities to discover data relationships. With the metaphor of direct manipulation, PivotSlice allows the user to visually and logically construct a series of dynamic queries over the data, based on a multi-focus and multi-scale tabular view that subdivides the entire dataset into several meaningful parts with customized semantics. PivotSlice further facilitates the visual exploration and sensemaking process through features including live search and integration of online data, graphical interaction histories and smoothly animated visual state transitions. We evaluated PivotSlice through a qualitative lab study with university researchers and report the findings from our observations and interviews. We also demonstrate the effectiveness of PivotSlice using a scenario of exploring a repository of information visualization literature.",
    "Authors": "Jian Zhao;Collins, C.;Chevalier, F.;Balakrishnan, R.",
    "Clusters": "GraphNetworkDataAndTechniques;InteractionTechniquesGeneral;QueriesAndSearch;ZoomingAndNavigationTechniques",
    "DOI": "10.1109\/TVCG.2013.167",
    "Keywords": "network exploration;information visualization;faceted browsing;dynamic query;visual analytics;interaction",
    "Keywords_Processed": "interaction;dynamic query;information visualization;visual analytic;facete browsing;network exploration",
    "Title": "Interactive Exploration of Implicit and Explicit Relations in Faceted Datasets"
  },
  "281": {
    "Abstract": "We present a new technique for providing interpolation within cell-centered Adaptive Mesh Refinement (AMR) data that achieves C0 continuity throughout the 3D domain. Our technique improves on earlier work in that it does not require that adjacent patches differ by at most one refinement level. Our approach takes the dual of each mesh patch and generates \"stitching cells\" on the fly to fill the gaps between dual meshes. We demonstrate applications of our technique with data from Enzo, an AMR cosmological structure formation simulation code. We show ray-cast visualizations that include contributions from particle data (dark matter and stars, also output by Enzo) and gridded hydrodynamic data. We also show results from isosurface studies, including surfaces in regions where adjacent patches differ by more than one refinement level.",
    "Authors": "Moran, P.J.;Ellsworth, D.",
    "Clusters": "AdaptiveProcessingAndRefinement;Interpolation;IsosurfaceAndSurfaceExtractionTechniques;MeshesGridsAndLattices;RaytracingRaycasting;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/TVCG.2011.252",
    "Keywords": "stitching cells;enzo;interpolation;raycasting;isosurface;adaptive mesh refinement;dual meshes",
    "Keywords_Processed": "enzo;adaptive mesh refinement;stitching cell;dual mesh;interpolation;isosurface;raycaste",
    "Title": "Visualization of AMR Data With Multi-Level Dual-Mesh Interpolation"
  },
  "267": {
    "Abstract": "Medical imaging plays a central role in a vast range of healthcare practices. The usefulness of 3D visualizations has been demonstrated for many types of treatment planning. Nevertheless, full access to 3D renderings outside of the radiology department is still scarce even for many image-centric specialties. Our work stems from the hypothesis that this under-utilization is partly due to existing visualization systems not taking the prerequisites of this application domain fully into account. We have developed a medical visualization table intended to better fit the clinical reality. The overall design goals were two-fold: similarity to a real physical situation and a very low learning threshold. This paper describes the development of the visualization table with focus on key design decisions. The developed features include two novel interaction components for touch tables. A user study including five orthopedic surgeons demonstrates that the system is appropriate and useful for this application domain.",
    "Authors": "Lundstrom, C.;Rydell, T.;Forsell, C.;Persson, A.;Ynnerman, A.",
    "Clusters": "BiomedicalScienceAndMedicine;InteractionTechniquesGeneral;LargeAndHighResDisplays",
    "DOI": "10.1109\/TVCG.2011.224",
    "Keywords": "medical visualization;tabletop;multi-touch;treatment planning",
    "Keywords_Processed": "tabletop;multi touch;medical visualization;treatment planning",
    "Title": "Multi-Touch Table System for Medical Visualization: Application to Orthopedic Surgery Planning"
  },
  "168": {
    "Abstract": "In a variety of application areas, the use of simulation steering in decision making is limited at best. Research focusing on this problem suggests that most user interfaces are too complex for the end user. Our goal is to let users create and investigate multiple, alternative scenarios without the need for special simulation expertise. To simplify the specification of parameters, we move from a traditional manipulation of numbers to a sketch-based input approach. Users steer both numeric parameters and parameters with a spatial correspondence by sketching a change onto the rendering. Special visualizations provide immediate visual feedback on how the sketches are transformed into boundary conditions of the simulation models. Since uncertainty with respect to many intertwined parameters plays an important role in planning, we also allow the user to intuitively setup complete value ranges, which are then automatically transformed into ensemble simulations. The interface and the underlying system were developed in collaboration with experts in the field of flood management. The real-world data they have provided has allowed us to construct scenarios used to evaluate the system. These were presented to a variety of flood response personnel, and their feedback is discussed in detail in the paper. The interface was found to be intuitive and relevant, although a certain amount of training might be necessary.",
    "Authors": "Ribicic, H.;Waser, J.;Gurbat, R.;Sadransky, B.;Groller, E.",
    "Clusters": "DesignMethodologiesAndInteractionDesign;EmergencyDisasterManagement;InteractionTechniquesGeneral;Simulation;UncertaintyTechniquesAndVisualization;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/TVCG.2012.261",
    "Keywords": "uncertainty visualization;interaction design;sketch-based steering;flood management;emergency\/disaster management;integrated visualization system;ensemble simulation steering",
    "Keywords_Processed": "ensemble simulation steering;emergency disaster management;flood management;integrate visualization system;sketch base steering;interaction design;uncertainty visualization",
    "Title": "Sketching Uncertainty into Simulations"
  },
  "23": {
    "Abstract": "We present an interface for exploring large design spaces as encountered in simulation-based engineering, design of visual effects, and other tasks that require tuning parameters of computationally-intensive simulations and visually evaluating results. The goal is to enable a style of design with simulations that feels as-direct-as-possible so users can concentrate on creative design tasks. The approach integrates forward design via direct manipulation of simulation inputs (e.g., geometric properties, applied forces) in the same visual space with inverse design via 'tugging' and reshaping simulation outputs (e.g., scalar fields from finite element analysis (FEA) or computational fluid dynamics (CFD)). The interface includes algorithms for interpreting the intent of users' drag operations relative to parameterized models, morphing arbitrary scalar fields output from FEA and CFD simulations, and in-place interactive ensemble visualization. The inverse design strategy can be extended to use multi-touch input in combination with an as-rigid-as-possible shape manipulation to support rich visual queries. The potential of this new design approach is confirmed via two applications: medical device engineering of a vacuum-assisted biopsy device and visual effects design using a physically based flame simulation.",
    "Authors": "Coffey, D.;Chi-Lun Lin;Erdman, A.G.;Keefe, D.F.",
    "Clusters": "InteractionTechniquesGeneral;Simulation;VisualDesignDesignGuidelines",
    "DOI": "10.1109\/TVCG.2013.147",
    "Keywords": "multi-touch;design;simulation;direct manipulation",
    "Keywords_Processed": "multi touch;direct manipulation;simulation;design",
    "Title": "Design by Dragging: An Interface for Creative Forward and Inverse Design with Simulation Ensembles"
  },
  "44": {
    "Abstract": "With the evolution of graphics hardware, high quality global illumination becomes available for real-time volume rendering. Compared to local illumination, global illumination can produce realistic shading effects which are closer to real world scenes, and has proven useful for enhancing volume data visualization to enable better depth and shape perception. However, setting up optimal lighting could be a nontrivial task for average users. There were lighting design works for volume visualization but they did not consider global light transportation. In this paper, we present a lighting design method for volume visualization employing global illumination. The resulting system takes into account view and transfer-function dependent content of the volume data to automatically generate an optimized three-point lighting environment. Our method fully exploits the back light which is not used by previous volume visualization systems. By also including global shadow and multiple scattering, our lighting system can effectively enhance the depth and shape perception of volumetric features of interest. In addition, we propose an automatic tone mapping operator which recovers visual details from overexposed areas while maintaining sufficient contrast in the dark areas. We show that our method is effective for visualizing volume datasets with complex structures. The structural information is more clearly and correctly presented under the automatically generated light sources.",
    "Authors": "Yubo Zhang;Kwan-Liu Ma",
    "Clusters": "ColorColorPerception;Illumination;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2013.172",
    "Keywords": "lighting design;volume rendering;global illumination;tone mapping",
    "Keywords_Processed": "tone mapping;volume render;lighting design;global illumination",
    "Title": "Lighting Design for Globally Illuminated Volume Rendering"
  },
  "86": {
    "Abstract": "As increasing volumes of urban data are captured and become available, new opportunities arise for data-driven analysis that can lead to improvements in the lives of citizens through evidence-based decision making and policies. In this paper, we focus on a particularly important urban data set: taxi trips. Taxis are valuable sensors and information associated with taxi trips can provide unprecedented insight into many different aspects of city life, from economic activity and human behavior to mobility patterns. But analyzing these data presents many challenges. The data are complex, containing geographical and temporal components in addition to multiple variables associated with each trip. Consequently, it is hard to specify exploratory queries and to perform comparative analyses (e.g., compare different regions over time). This problem is compounded due to the size of the data-there are on average 500,000 taxi trips each day in NYC. We propose a new model that allows users to visually query taxi trips. Besides standard analytics queries, the model supports origin-destination queries that enable the study of mobility across the city. We show that this model is able to express a wide range of spatio-temporal queries, and it is also flexible in that not only can queries be composed but also different aggregations and visual representations can be applied, allowing users to explore and compare results. We have built a scalable system that implements this model which supports interactive response times; makes use of an adaptive level-of-detail rendering strategy to generate clutter-free visualization for large results; and shows hidden details to the users in a summary through the use of overlay heat maps. We present a series of case studies motivated by traffic engineers and economists that show how our model and system enable domain experts to perform tasks that were previously unattainable for them.",
    "Authors": "Ferreira, N.;Poco, J.;Vo, H.T.;Freire, J.;Silva, C.T.",
    "Clusters": "AnalysisProcessGeneral;ApplicationsGeneralAndOther;QueriesAndSearch;Traffic",
    "DOI": "10.1109\/TVCG.2013.226",
    "Keywords": "urban data;spatio-temporal queries;visual exploration;taxi movement data",
    "Keywords_Processed": "spatio temporal query;taxi movement datum;urban datum;visual exploration",
    "Title": "Visual Exploration of Big Spatio-Temporal Urban Data: A Study of New York City Taxi Trips"
  },
  "266": {
    "Abstract": "Because of the ever increasing size of output data from scientific simulations, supercomputers are increasingly relied upon to generate visualizations. One use of supercomputers is to generate field lines from large scale flow fields. When generating field lines in parallel, the vector field is generally decomposed into blocks, which are then assigned to processors. Since various regions of the vector field can have different flow complexity, processors will require varying amounts of computation time to trace their particles, causing load imbalance, and thus limiting the performance speedup. To achieve load-balanced streamline generation, we propose a workload-aware partitioning algorithm to decompose the vector field into partitions with near equal workloads. Since actual workloads are unknown beforehand, we propose a workload estimation algorithm to predict the workload in the local vector field. A graph-based representation of the vector field is employed to generate these estimates. Once the workloads have been estimated, our partitioning algorithm is hierarchically applied to distribute the workload to all partitions. We examine the performance of our workload estimation and workload-aware partitioning algorithm in several timings studies, which demonstrates that by employing these methods, better scalability can be achieved with little overhead.",
    "Authors": "Nouanesengsy, B.;Teng-Yok Lee;Han-Wei Shen",
    "Clusters": "FlowVisualizationDataAndTechniques;ParallelSystemsAndParallelProcessing;StreamlinesPathlinesStreaklines;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2011.219",
    "Keywords": "flow visualization;3d vector field visualization;parallel processing;streamlines",
    "Keywords_Processed": "3d vector field visualization;streamline;parallel processing;flow visualization",
    "Title": "Load-Balanced Parallel Streamline Generation on Large Scale Vector fields"
  },
  "81": {
    "Abstract": "How do various topics compete for public attention when they are spreading on social media? What roles do opinion leaders play in the rise and fall of competitiveness of various topics? In this study, we propose an expanded topic competition model to characterize the competition for public attention on multiple topics promoted by various opinion leaders on social media. To allow an intuitive understanding of the estimated measures, we present a timeline visualization through a metaphoric interpretation of the results. The visual design features both topical and social aspects of the information diffusion process by compositing ThemeRiver with storyline style visualization. ThemeRiver shows the increase and decrease of competitiveness of each topic. Opinion leaders are drawn as threads that converge or diverge with regard to their roles in influencing the public agenda change over time. To validate the effectiveness of the visual analysis techniques, we report the insights gained on two collections of Tweets: the 2012 United States presidential election and the Occupy Wall Street movement.",
    "Authors": "Panpan Xu;Yingcai Wu;Enxun Wei;Tai-Quan Peng;Shixia Liu;Zhu, J.J.H.;Huamin Qu",
    "Clusters": "AnalysisProcessGeneral;DiffusionRelatedTechniques;InformationProcessingAndHandling;SocialNetworksAndSocialMedia;TextDocumentTopicAnalysisDataAndTechniques",
    "DOI": "10.1109\/TVCG.2013.221",
    "Keywords": "information propagation;agenda-setting;topic competition;social media visualization;information diffusion",
    "Keywords_Processed": "information diffusion;social medium visualization;information propagation;agenda setting;topic competition",
    "Title": "Visual Analysis of Topic Competition on Social Media"
  },
  "22": {
    "Abstract": "We present a visual analytics solution designed to address prevalent issues in the area of Operational Decision Management (ODM). In ODM, which has its roots in Artificial Intelligence (Expert Systems) and Management Science, it is increasingly important to align business decisions with business goals. In our work, we consider decision models (executable models of the business domain) as ontologies that describe the business domain, and production rules that describe the business logic of decisions to be made over this ontology. Executing a decision model produces an accumulation of decisions made over time for individual cases. We are interested, first, to get insight in the decision logic and the accumulated facts by themselves. Secondly and more importantly, we want to see how the accumulated facts reveal potential divergences between the reality as captured by the decision model, and the reality as captured by the executed decisions. We illustrate the motivation, added value for visual analytics, and our proposed solution and tooling through a business case from the car insurance industry.",
    "Authors": "Broeksema, B.;Baudel, T.;Telea, A.;Crisafulli, P.",
    "Clusters": "MachineLearningAndStatistics;SoftwareVisualization;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/TVCG.2013.146",
    "Keywords": "program analysis;decision support systems;model validation and analysis;multivariate statistics",
    "Keywords_Processed": "decision support system;multivariate statistic;model validation and analysis;program analysis",
    "Title": "Decision Exploration Lab: A Visual Analytics Solution for Decision Management"
  },
  "340": {
    "Abstract": "We extend direct volume rendering with a unified model for generalized isosurfaces, also called interval volumes, allowing a wider spectrum of visual classification. We generalize the concept of scale-invariant opacity-typical for isosurface rendering-to semi-transparent interval volumes. Scale-invariant rendering is independent of physical space dimensions and therefore directly facilitates the analysis of data characteristics. Our model represents sharp isosurfaces as limits of interval volumes and combines them with features of direct volume rendering. Our objective is accurate rendering, guaranteeing that all isosurfaces and interval volumes are visualized in a crack-free way with correct spatial ordering. We achieve simultaneous direct and interval volume rendering by extending preintegration and explicit peak finding with data-driven splitting of ray integration and hybrid computation in physical and data domains. Our algorithm is suitable for efficient parallel processing for interactive applications as demonstrated by our CUDA implementation.",
    "Authors": "Ament, M.;Weiskopf, D.;Carr, H.",
    "Clusters": "ColorColorPerception;DataRegistrationFusionAndIntegration;IsosurfaceAndSurfaceExtractionTechniques;RaytracingRaycasting;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2010.145",
    "Keywords": "direct volume rendering;preintegration;raycasting;isosurface;interval volume;scale-invariant opacity",
    "Keywords_Processed": "preintegration;direct volume render;scale invariant opacity;isosurface;raycaste;interval volume",
    "Title": "Direct Interval Volume Visualization"
  },
  "284": {
    "Abstract": "Visual analysis is widely used to study the behavior of molecules. Of particular interest are the analysis of molecular interactions and the investigation of binding sites. For large molecules, however, it is difficult to detect possible binding sites and paths leading to these sites by pure visual inspection. In this paper, we present new methods for the computation and visualization of potential molecular paths. Using a novel filtering method, we extract the significant paths from the Voronoi diagram of spheres. For the interactive visualization of molecules and their paths, we present several methods using deferred shading and other state-of-theart techniques. To allow for a fast overview of reachable regions of the molecule, we illuminate the molecular surface using a large number of light sources placed on the extracted paths. We also provide a method to compute the extension surface of selected paths and visualize it using the skin surface. Furthermore, we use the extension surface to clip the molecule to allow easy visual tracking of even deeply buried paths. The methods are applied to several proteins to demonstrate their usefulness.",
    "Authors": "Lindow, N.;Baum, D.;Hege, H.-C.",
    "Clusters": "FilteringTechniques;GeometryBasedTechniques;MolecularScienceAndChemistry;ViewDependentVisualization",
    "DOI": "10.1109\/TVCG.2011.259",
    "Keywords": "data filtering;view-dependent visualization;molecular visualization;geometry-based technique",
    "Keywords_Processed": "datum filtering;molecular visualization;geometry base technique;view dependent visualization",
    "Title": "Voronoi-Based Extraction and Visualization of Molecular Paths"
  },
  "355": {
    "Abstract": "This paper presents an interactive visualization tool to study and analyze hyperspectral images (HSI) of historical documents. This work is part of a collaborative effort with the Nationaal Archief of the Netherlands (NAN) and Art Innovation, a manufacturer of hyperspectral imaging hardware designed for old and fragile documents. The NAN is actively capturing HSI of historical documents for use in a variety of tasks related to the analysis and management of archival collections, from ink and paper analysis to monitoring the effects of environmental aging. To assist their work, we have developed a comprehensive visualization tool that offers an assortment of visualization and analysis methods, including interactive spectral selection, spectral similarity analysis, time-varying data analysis and visualization, and selective spectral band fusion. This paper describes our visualization software and how it is used to facilitate the tasks needed by our collaborators. Evaluation feedback from our collaborators on how this tool benefits their work is included.",
    "Authors": "Seon Joo Kim;Shaojie Zhuo;Fanbo Deng;Chi-Wing Fu;Brown, M.S.",
    "Clusters": "AnalysisProcessGeneral;DataRegistrationFusionAndIntegration;ImageBasedDataImageSignalProcessing;TextDocumentTopicAnalysisDataAndTechniques",
    "DOI": "10.1109\/TVCG.2010.172",
    "Keywords": "image fusion;document processing and analysis;data exploration;hyperspectral visualization",
    "Keywords_Processed": "image fusion;hyperspectral visualization;document processing and analysis;datum exploration",
    "Title": "Interactive Visualization of Hyperspectral Images of Historical Documents"
  },
  "276": {
    "Abstract": "We present topological spines-a new visual representation that preserves the topological and geometric structure of a scalar field. This representation encodes the spatial relationships of the extrema of a scalar field together with the local volume and nesting structure of the surrounding contours. Unlike other topological representations, such as contour trees, our approach preserves the local geometric structure of the scalar field, including structural cycles that are useful for exposing symmetries in the data. To obtain this representation, we describe a novel mechanism based on the extraction of extremum graphs-sparse subsets of the Morse-Smale complex that retain the important structural information without the clutter and occlusion problems that arise from visualizing the entire complex directly. Extremum graphs form a natural multiresolution structure that allows the user to suppress noise and enhance topological features via the specification of a persistence range. Applications of our approach include the visualization of 3D scalar fields without occlusion artifacts, and the exploratory analysis of high-dimensional functions.",
    "Authors": "Correa, C.;Lindstrom, P.;Bremer, P.-T.",
    "Clusters": "GraphNetworkDataAndTechniques;ScalarFieldDataTechniques;TopologyBasedTechniques",
    "DOI": "10.1109\/TVCG.2011.244",
    "Keywords": "scalar field topology;extremum graph;topological spine;morse-smale complex",
    "Keywords_Processed": "scalar field topology;extremum graph;topological spine;morse smale complex",
    "Title": "Topological Spines: A Structure-preserving Visual Representation of Scalar fields"
  },
  "183": {
    "Abstract": "Contingency tables summarize the relations between categorical variables and arise in both scientific and business domains. Asymmetrically large two-way contingency tables pose a problem for common visualization methods. The Contingency Wheel has been recently proposed as an interactive visual method to explore and analyze such tables. However, the scalability and readability of this method are limited when dealing with large and dense tables. In this paper we present Contingency Wheel++, new visual analytics methods that overcome these major shortcomings: (1) regarding automated methods, a measure of association based on Pearson's residuals alleviates the bias of the raw residuals originally used, (2) regarding visualization methods, a frequency-based abstraction of the visual elements eliminates overlapping and makes analyzing both positive and negative associations possible, and (3) regarding the interactive exploration environment, a multi-level overview+detail interface enables exploring individual data items that are aggregated in the visualization or in the table using coordinated views. We illustrate the applicability of these new methods with a use case and show how they enable discovering and analyzing nontrivial patterns and associations in large categorical data.",
    "Authors": "Alsallakh, B.;Aigner, W.;Miksch, S.;Groller, E.",
    "Clusters": "LargeScaleDataAndScalability;TabularDataAndTechniques;UserInterfacesGeneral",
    "DOI": "10.1109\/TVCG.2012.254",
    "Keywords": "information interfaces and presentation;contingency table analysis;large categorical data;visual analytics",
    "Keywords_Processed": "contingency table analysis;visual analytic;information interface and presentation;large categorical datum",
    "Title": "Reinventing the Contingency Wheel: Scalable Visual Analytics of Large Categorical Data"
  },
  "83": {
    "Abstract": "Social network analysis (SNA) is becoming increasingly concerned not only with actors and their relations, but also with distinguishing between different types of such entities. For example, social scientists may want to investigate asymmetric relations in organizations with strict chains of command, or incorporate non-actors such as conferences and projects when analyzing coauthorship patterns. Multimodal social networks are those where actors and relations belong to different types, or modes, and multimodal social network analysis (mSNA) is accordingly SNA for such networks. In this paper, we present a design study that we conducted with several social scientist collaborators on how to support mSNA using visual analytics tools. Based on an openended, formative design process, we devised a visual representation called parallel node-link bands (PNLBs) that splits modes into separate bands and renders connections between adjacent ones, similar to the list view in Jigsaw. We then used the tool in a qualitative evaluation involving five social scientists whose feedback informed a second design phase that incorporated additional network metrics. Finally, we conducted a second qualitative evaluation with our social scientist collaborators that provided further insights on the utility of the PNLBs representation and the potential of visual analytics for mSNA.",
    "Authors": "Ghani, S.;Bum Chul Kwon;Seungyoon Lee;Ji Soo Yi;Elmqvist, N.",
    "Clusters": "DesignMethodologiesAndInteractionDesign;DesignStudiesAndCaseStudies;GraphNetworkDataAndTechniques;InteractionTechniquesGeneral;QualitativeEvaluation",
    "DOI": "10.1109\/TVCG.2013.223",
    "Keywords": "multimodal graphs;design study;node-link diagrams;qualitative evaluation;user-centered design;interaction",
    "Keywords_Processed": "interaction;design study;multimodal graph;user center design;qualitative evaluation;node link diagram",
    "Title": "Visual Analytics for Multimodal Social Network Analysis: A Design Study with Social Scientists"
  },
  "123": {
    "Abstract": "We present PivotPaths, an interactive visualization for exploring faceted information resources. During both work and leisure, we increasingly interact with information spaces that contain multiple facets and relations, such as authors, keywords, and citations of academic publications, or actors and genres of movies. To navigate these interlinked resources today, one typically selects items from facet lists resulting in abrupt changes from one subset of data to another. While filtering is useful to retrieve results matching specific criteria, it can be difficult to see how facets and items relate and to comprehend the effect of filter operations. In contrast, the PivotPaths interface exposes faceted relations as visual paths in arrangements that invite the viewer to `take a stroll' through an information space. PivotPaths supports pivot operations as lightweight interaction techniques that trigger gradual transitions between views. We designed the interface to allow for casual traversal of large collections in an aesthetically pleasing manner that encourages exploration and serendipitous discoveries. This paper shares the findings from our iterative design-and-evaluation process that included semi-structured interviews and a two-week deployment of PivotPaths applied to a large database of academic publications.",
    "Authors": "Dork, M.;Riche, N.H.;Ramos, G.;Dumais, S.",
    "Clusters": "AnalysisProcessGeneral;AnimationAndMotion;GraphNetworkDataAndTechniques;InteractionTechniquesGeneral;",
    "DOI": "10.1109\/TVCG.2012.252",
    "Keywords": "information visualization;node-link diagrams;exploratory search;information seeking;interactivity;animation",
    "Keywords_Processed": "interactivity;exploratory search;animation;information visualization;information seek;node link diagram",
    "Title": "PivotPaths: Strolling through Faceted Information Spaces"
  },
  "214": {
    "Abstract": "Visual representations of time-series are useful for tasks such as identifying trends, patterns and anomalies in the data. Many techniques have been devised to make these visual representations more scalable, enabling the simultaneous display of multiple variables, as well as the multi-scale display of time-series of very high resolution or that span long time periods. There has been comparatively little research on how to support the more elaborate tasks associated with the exploratory visual analysis of timeseries, e.g., visualizing derived values, identifying correlations, or discovering anomalies beyond obvious outliers. Such tasks typically require deriving new time-series from the original data, trying different functions and parameters in an iterative manner. We introduce a novel visualization technique called ChronoLenses, aimed at supporting users in such exploratory tasks. ChronoLenses perform on-the-fly transformation of the data points in their focus area, tightly integrating visual analysis with user actions, and enabling the progressive construction of advanced visual analysis pipelines.",
    "Authors": "Jian Zhao;Chevalier, F.;Pietriga, E.;Balakrishnan, R.",
    "Clusters": "AnalysisProcessGeneral;FocusContextTechniques;InteractionTechniquesGeneral;TimeseriesTimeVaryingDataAndTechniques",
    "DOI": "10.1109\/TVCG.2011.195",
    "Keywords": "lens;exploratory visualization;time-series data;focus+context;interaction",
    "Keywords_Processed": "interaction;lens;exploratory visualization;time series datum;focus context",
    "Title": "Exploratory Analysis of Time-Series with ChronoLenses"
  },
  "159": {
    "Abstract": "In this paper, we enable interactive volumetric global illumination by extending photon mapping techniques to handle interactive transfer function (TF) and material editing in the context of volume rendering. We propose novel algorithms and data structures for finding and evaluating parts of a scene affected by these parameter changes, and thus support efficient updates of the photon map. In direct volume rendering (DVR) the ability to explore volume data using parameter changes, such as editable TFs, is of key importance. Advanced global illumination techniques are in most cases computationally too expensive, as they prevent the desired interactivity. Our technique decreases the amount of computation caused by parameter changes, by introducing Historygrams which allow us to efficiently reuse previously computed photon media interactions. Along the viewing rays, we utilize properties of the light transport equations to subdivide a view-ray into segments and independently update them when invalid. Unlike segments of a view-ray, photon scattering events within the volumetric medium needs to be sequentially updated. Using our Historygram approach, we can identify the first invalid photon interaction caused by a property change, and thus reuse all valid photon interactions. Combining these two novel concepts, supports interactive editing of parameters when using volumetric photon mapping in the context of DVR. As a consequence, we can handle arbitrarily shaped and positioned light sources, arbitrary phase functions, bidirectional reflectance distribution functions and multiple scattering which has previously not been possible in interactive DVR.",
    "Authors": "Jonsson, D.;Kronander, J.;Ropinski, T.;Ynnerman, A.",
    "Clusters": "Illumination;SocialNetworksAndSocialMedia;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2012.232",
    "Keywords": "volume rendering;global illumination;photon mapping;participating media",
    "Keywords_Processed": "participate medium;volume render;photon mapping;global illumination",
    "Title": "Historygrams: Enabling Interactive Global Illumination in Direct Volume Rendering using Photon Mapping"
  },
  "301": {
    "Abstract": "We introduce the concept of a Visual Backchannel as a novel way of following and exploring online conversations about large-scale events. Microblogging communities, such as Twitter, are increasingly used as digital backchannels for timely exchange of brief comments and impressions during political speeches, sport competitions, natural disasters, and other large events. Currently, shared updates are typically displayed in the form of a simple list, making it difficult to get an overview of the fast-paced discussions as it happens in the moment and how it evolves over time. In contrast, our Visual Backchannel design provides an evolving, interactive, and multi-faceted visual overview of large-scale ongoing conversations on Twitter. To visualize a continuously updating information stream, we include visual saliency for what is happening now and what has just happened, set in the context of the evolving conversation. As part of a fully web-based coordinated-view system we introduce Topic Streams, a temporally adjustable stacked graph visualizing topics over time, a People Spiral representing participants and their activity, and an Image Cloud encoding the popularity of event photos by size. Together with a post listing, these mutually linked views support cross-filtering along topics, participants, and time ranges. We discuss our design considerations, in particular with respect to evolving visualizations of dynamically changing data. Initial feedback indicates significant interest and suggests several unanticipated uses.",
    "Authors": "Dork, M.;Gruen, D.;Williamson, C.;Carpendale, S.",
    "Clusters": "DatabasesAndDataMining;EventsTrendsOutlierDetectionAnalysisAndVisualization;InternetWebVisualizationForTheMasses;MultipleLinkedCoordinatedViews;SocialNetworksAndSocialMedia;SocialScienceAndHumanities;",
    "DOI": "10.1109\/TVCG.2010.129",
    "Keywords": "information visualization;information retrieval;backchannel;multiple views;microblogging;events;world wide web",
    "Keywords_Processed": "world wide web;information retrieval;information visualization;backchannel;multiple view;microblogge;event",
    "Title": "A Visual Backchannel for Large-Scale Events"
  },
  "322": {
    "Abstract": "Treemaps are space-filling visualizations that make efficient use of limited display space to depict large amounts of hierarchical data. Creating perceptually effective treemaps requires carefully managing a number of design parameters including the aspect ratio and luminance of rectangles. Moreover, treemaps encode values using area, which has been found to be less accurate than judgments of other visual encodings, such as length. We conduct a series of controlled experiments aimed at producing a set of design guidelines for creating effective rectangular treemaps. We find no evidence that luminance affects area judgments, but observe that aspect ratio does have an effect. Specifically, we find that the accuracy of area comparisons suffers when the compared rectangles have extreme aspect ratios or when both are squares. Contrary to common assumptions, the optimal distribution of rectangle aspect ratios within a treemap should include non-squares, but should avoid extremes. We then compare treemaps with hierarchical bar chart displays to identify the data densities at which length-encoded bar charts become less effective than area-encoded treemaps. We report the transition points at which treemaps exhibit judgment accuracy on par with bar charts for both leaf and non-leaf tree nodes. We also find that even at relatively low data densities treemaps result in faster comparisons than bar charts. Based on these results, we present a set of guidelines for the effective use of treemaps and suggest alternate approaches for treemap layout.",
    "Authors": "Kong, N.;Heer, J.;Agrawala, M.",
    "Clusters": "EvaluationGeneral;HierarchicalTreeDataAndTechniques;Perception;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2010.186",
    "Keywords": "mechanical turk;treemap;graphical perception;rectangular area;visualization;visual encoding;experiment",
    "Keywords_Processed": "visualization;rectangular area;treemap;visual encoding;graphical perception;experiment;mechanical turk",
    "Title": "Perceptual Guidelines for Creating Rectangular Treemaps"
  },
  "236": {
    "Abstract": "Image analysis algorithms are often highly parameterized and much human input is needed to optimize parameter settings. This incurs a time cost of up to several days. We analyze and characterize the conventional parameter optimization process for image analysis and formulate user requirements. With this as input, we propose a change in paradigm by optimizing parameters based on parameter sampling and interactive visual exploration. To save time and reduce memory load, users are only involved in the first step - initialization of sampling - and the last step - visual analysis of output. This helps users to more thoroughly explore the parameter space and produce higher quality results. We describe a custom sampling plug-in we developed for CellProfiler - a popular biomedical image analysis framework. Our main focus is the development of an interactive visualization technique that enables users to analyze the relationships between sampled input parameters and corresponding output. We implemented this in a prototype called Paramorama. It provides users with a visual overview of parameters and their sampled values. User-defined areas of interest are presented in a structured way that includes image-based output and a novel layout algorithm. To find optimal parameter settings, users can tag high- and low-quality results to refine their search. We include two case studies to illustrate the utility of this approach.",
    "Authors": "Pretorius, A.J.;Bray, M.-A.P.;Carpenter, A.E.;Ruddle, R.A.",
    "Clusters": "ImageBasedDataImageSignalProcessing;Parameterization;Sampling;",
    "DOI": "10.1109\/TVCG.2011.253",
    "Keywords": "information visualization;sampling;image analysis;visual analytics;parameter space",
    "Keywords_Processed": "information visualization;sample;visual analytic;image analysis;paramet space",
    "Title": "Visualization of Parameter Space for Image Analysis"
  },
  "345": {
    "Abstract": "Insight into the dynamics of blood-flow considerably improves the understanding of the complex cardiovascular system and its pathologies. Advances in MRI technology enable acquisition of 4D blood-flow data, providing quantitative blood-flow velocities over time. The currently typical slice-by-slice analysis requires a full mental reconstruction of the unsteady blood-flow field, which is a tedious and highly challenging task, even for skilled physicians. We endeavor to alleviate this task by means of comprehensive visualization and interaction techniques. In this paper we present a framework for pre-clinical cardiovascular research, providing tools to both interactively explore the 4D blood-flow data and depict the essential blood-flow characteristics. The framework encompasses a variety of visualization styles, comprising illustrative techniques as well as improved methods from the established field of flow visualization. Each of the incorporated styles, including exploded planar reformats, flow-direction highlights, and arrow-trails, locally captures the blood-flow dynamics and may be initiated by an interactively probed vessel cross-section. Additionally, we present the results of an evaluation with domain experts, measuring the value of each of the visualization styles and related rendering parameters.",
    "Authors": "van Pelt, R.;Olivan Bescos, J.;Breeuwer, M.;Clough, R.E.;Groller, E.;ter Haar Romenij, B.;Vilanova, A.",
    "Clusters": "BiomedicalScienceAndMedicine;FlowVisualizationDataAndTechniques;IllustrativeVisualization;InteractionTechniquesGeneral",
    "DOI": "10.1109\/TVCG.2010.153",
    "Keywords": "flow visualization;phase-contrast cine mri;illustrative visualization;probing;4d mri blood-flow",
    "Keywords_Processed": "illustrative visualization;probe;4d mri blood flow;phase contrast cine mri;flow visualization",
    "Title": "Exploration of 4D MRI Blood Flow using Stylistic Visualization"
  },
  "9": {
    "Abstract": "We present a new efficient and scalable method for the high quality reconstruction of the flow map from sparse samples. The flow map describes the transport of massless particles along the flow. As such, it is a fundamental concept in the analysis of transient flow phenomena and all so-called Lagrangian flow visualization techniques require its approximation. The flow map is generally obtained by integrating a dense 1D, 2D, or 3D set of particles across the domain of definition of the flow. Despite its embarrassingly parallel nature, this computation creates a performance bottleneck in the analysis of large-scale datasets that existing adaptive techniques alleviate only partially. Our iterative approximation method significantly improves upon the state of the art by precisely modeling the flow behavior around automatically detected geometric structures embedded in the flow, thus effectively restricting the sampling effort to interesting regions. Our data reconstruction is based on a modified version of Sibson's scattered data interpolation and allows us at each step to offer an intermediate dense approximation of the flow map and to seamlessly integrate regions that will be further refined in subsequent steps. We present a quantitative and qualitative evaluation of our method on different types of flow datasets and offer a detailed comparison with existing techniques.",
    "Authors": "Barakat, S.S.;Tricoche, X.",
    "Clusters": "AdaptiveProcessingAndRefinement;FlowVisualizationDataAndTechniques;GraphNetworkDataAndTechniques;Interpolation;ParallelSystemsAndParallelProcessing;Sampling",
    "DOI": "10.1109\/TVCG.2013.128",
    "Keywords": "edge features;lagrangian flow visualization;adaptive refinement;parallel reconstruction;sparse sampling;flow map;scattered data interpolation",
    "Keywords_Processed": "sparse sampling;flow map;scatter datum interpolation;edge feature;adaptive refinement;lagrangian flow visualization;parallel reconstruction",
    "Title": "Adaptive Refinement of the Flow Map Using Sparse Samples"
  },
  "269": {
    "Abstract": "Multi-material components, which contain metal parts surrounded by plastic materials, are highly interesting for inspection using industrial 3D X-ray computed tomography (3DXCT). Examples of this application scenario are connectors or housings with metal inlays in the electronic or automotive industry. A major problem of this type of components is the presence of metal, which causes streaking artifacts and distorts the surrounding media in the reconstructed volume. Streaking artifacts and dark-band artifacts around metal components significantly influence the material characterization (especially for the plastic components). In specific cases these artifacts even prevent a further analysis. Due to the nature and the different characteristics of artifacts, the development of an efficient artifact-reduction technique in reconstruction-space is rather complicated. In this paper we present a projection-space pipeline for metal-artifacts reduction. The proposed technique first segments the metal in the spatial domain of the reconstructed volume in order to separate it from the other materials. Then metal parts are forward-projected on the set of projections in a way that metal-projection regions are treated as voids. Subsequently the voids, which are left by the removed metal, are interpolated in the 2D projections. Finally, the metal is inserted back into the reconstructed 3D volume during the fusion stage. We present a visual analysis tool, allowing for interactive parameter estimation of the metal segmentation. The results of the proposed artifact-reduction technique are demonstrated on a test part as well as on real world components. For these specimens we achieve a significant reduction of metal artifacts, allowing an enhanced material characterization.",
    "Authors": "Amirkhanov, A.;Heinzl, C.;Reiter, M.;Kastner, J.;Groller, E.",
    "Clusters": "AnalysisProcessGeneral;BiomedicalScienceAndMedicine;MaterialScience",
    "DOI": "10.1109\/TVCG.2011.228",
    "Keywords": "3d x-ray computed tomography;metal-artifact reduction;multi-material components;visual analysis",
    "Keywords_Processed": "multi material component;visual analysis;3d ray compute tomography;metal artifact reduction",
    "Title": "Projection-Based Metal-Artifact Reduction for Industrial 3D X-ray Computed Tomography"
  },
  "341": {
    "Abstract": "The concept of continuous scatterplot (CSP) is a modern visualization technique. The idea is to define a scalar density value based on the map between an n-dimensional spatial domain and an m-dimensional data domain, which describe the CSP space. Usually the data domain is two-dimensional to visually convey the underlying, density coded, data. In this paper we investigate kinds of map-based discontinuities, especially for the practical cases n = m = 2 and n = 3 | m = 2, and we depict relations between them and attributes of the resulting CSP itself. Additionally, we show that discontinuities build critical line structures, and we introduce algorithms to detect them. Further, we introduce a discontinuity-based visualization approach - called contribution map (CM) -which establishes a relationship between the CSP's data domain and the number of connected components in the spatial domain. We show that CMs enhance the CSP-based linking & brushing interaction. Finally, we apply our approaches to a number of synthetic as well as real data sets.",
    "Authors": "Lehmann, D.J.;Theisel, H.",
    "Clusters": "ChartsDiagramsPlots;DataFeaturesAndAttributes;TopologyBasedTechniques;",
    "DOI": "10.1109\/TVCG.2010.146",
    "Keywords": "discontinuity;visualization;scatterplot;topology",
    "Keywords_Processed": "visualization;scatterplot;discontinuity;topology",
    "Title": "Discontinuities in Continuous Scatter Plots"
  },
  "387": {
    "Abstract": "Information foraging and sensemaking with heterogeneous information are context-dependent activities. Thus visual analytics tools to support these activities must incorporate context. But, context is a difficult concept to define, model, and represent. Creating and representing context in support of visually-enabled reasoning about complex problems with complex information is a complementary but different challenge than that addressed in context-aware computing. In the latter, the goal is automated adaptation of the system to meet user needs for applications such as mobile location-based services where information about the location, the user, and the user goals filters what gets presented on a small mobile device. In contrast, for visual analytics-enabled information foraging and sensemaking, the user is likely to take an active role in foraging for the contextual information needed to support sensemaking in relation to some multifaceted problem. In this paper, we address the challenges of constructing and representing context within visual interfaces that support analytical reasoning in crisis management and humanitarian relief. The challenges stem from the diverse forms of information that can provide context and difficulty in defining and operationalizing context itself. Here, we pay particular attention to document foraging to support construction of the geographic and historical context within which monitoring and sensemaking can be carried out. Specifically, we present the concept of geo-historical context (GHC) and outline an empirical assessment of both the concept and its implementation in the Context Discovery Application, a web-based tool that supports document foraging and sensemaking.",
    "Authors": "Tomaszewski, B.;MacEachren, A.M.",
    "Clusters": "AnalysisProcessGeneral;Cognition;DataAcquisitionAndManagement;FocusContextTechniques;TextDocumentTopicAnalysisDataAndTechniques;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/VAST.2010.5652895",
    "Keywords": "context;sensemaking;geographic information retrieval;mapping;foraging;text analysis",
    "Keywords_Processed": "text analysis;sensemake;context;forage;mapping;geographic information retrieval",
    "Title": "Geo-historical context support for information foraging and sensemaking: Conceptual model, implementation, and assessment"
  },
  "305": {
    "Abstract": "Pixel-based visualization is a popular method of conveying large amounts of numerical data graphically. Application scenarios include business and finance, bioinformatics and remote sensing. In this work, we examined how the usability of such visual representations varied across different tasks and block resolutions. The main stimuli consisted of temporal pixel-based visualization with a white-red color map, simulating monthly temperature variation over a six-year period. In the first study, we included 5 separate tasks to exert different perceptual loads. We found that performance varied considerably as a function of task, ranging from 75% correct in low-load tasks to below 40% in high-load tasks. There was a small but consistent effect of resolution, with the uniform patch improving performance by around 6% relative to higher block resolution. In the second user study, we focused on a high-load task for evaluating month-to-month changes across different regions of the temperature range. We tested both CIE L*u*v* and RGB color spaces. We found that the nature of the change-evaluation errors related directly to the distance between the compared regions in the mapped color space. We were able to reduce such errors by using multiple color bands for the same data range. In a final study, we examined more fully the influence of block resolution on performance, and found block resolution had a limited impact on the effectiveness of pixel-based visualization.",
    "Authors": "Borgo, R.;Proctor, K.;Chen, M.;J\u00e4nicke, H.;Murray, T.;Thornton, I.M.",
    "Clusters": "DynamicVisualizationVisualizationOfChange;EvaluationGeneral;PixelOrientedEncodings;QueriesAndSearch",
    "DOI": "10.1109\/TVCG.2010.150",
    "Keywords": "user study;change detection;pixel-based visualization;visual search;evaluation",
    "Keywords_Processed": "change detection;user study;visual search;pixel base visualization;evaluation",
    "Title": "Evaluating the impact of task demands and block resolution on the effectiveness of pixel-based visualization"
  },
  "294": {
    "Abstract": "The study of complex activities such as scientific production and software development often require modeling connections among heterogeneous entities including people, institutions and artifacts. Despite numerous advances in algorithms and visualization techniques for understanding such social networks, the process of constructing network models and performing exploratory analysis remains difficult and time-consuming. In this paper we present Orion, a system for interactive modeling, transformation and visualization of network data. Orion's interface enables the rapid manipulation of large graphs-including the specification of complex linking relationships-using simple drag-and-drop operations with desired node types. Orion maps these user interactions to statements in a declarative workflow language that incorporates both relational operators (e.g., selection, aggregation and joins) and network analytics (e.g., centrality measures). We demonstrate how these features enable analysts to flexibly construct and compare networks in domains such as online health communities, academic collaboration and distributed software development.",
    "Authors": "Heer, J.;Perer, A.",
    "Clusters": "DataAcquisitionAndManagement;DataTransformation;GraphNetworkDataAndTechniques;ProgrammingAlgorithmsAndDataStructures;SocialNetworksAndSocialMedia;",
    "DOI": "10.1109\/VAST.2011.6102441",
    "Keywords": "visualization;social network analysis;graph;data management;data transformation;end-user programming",
    "Keywords_Processed": "visualization;end user programming;datum transformation;graph;social network analysis;data management",
    "Title": "Orion: A system for modeling, transformation and visualization of multidimensional heterogeneous networks"
  },
  "264": {
    "Abstract": "In Toponomics, the function protein pattern in cells or tissue (the toponome) is imaged and analyzed for applications in toxicology, new drug development and patient-drug-interaction. The most advanced imaging technique is robot-driven multi-parameter fluorescence microscopy. This technique is capable of co-mapping hundreds of proteins and their distribution and assembly in protein clusters across a cell or tissue sample by running cycles of fluorescence tagging with monoclonal antibodies or other affinity reagents, imaging, and bleaching in situ. The imaging results in complex multi-parameter data composed of one slice or a 3D volume per affinity reagent. Biologists are particularly interested in the localization of co-occurring proteins, the frequency of co-occurrence and the distribution of co-occurring proteins across the cell. We present an interactive visual analysis approach for the evaluation of multi-parameter fluorescence microscopy data in toponomics. Multiple, linked views facilitate the definition of features by brushing multiple dimensions. The feature specification result is linked to all views establishing a focus+context visualization in 3D. In a new attribute view, we integrate techniques from graph visualization. Each node in the graph represents an affinity reagent while each edge represents two co-occurring affinity reagent bindings. The graph visualization is enhanced by glyphs which encode specific properties of the binding. The graph view is equipped with brushing facilities. By brushing in the spatial and attribute domain, the biologist achieves a better understanding of the function protein patterns of a cell. Furthermore, an interactive table view is integrated which summarizes unique fluorescence patterns. We discuss our approach with respect to a cell probe containing lymphocytes and a prostate tissue section.",
    "Authors": "Oeltze, S.;Freiler, W.;Hillert, R.;Doleisch, H.;Preim, B.;Schubert, W.",
    "Clusters": "BiologyAndBioinformatics;GraphNetworkDataAndTechniques;Microscopy;MolecularScienceAndChemistry;",
    "DOI": "10.1109\/TVCG.2011.217",
    "Keywords": "protein interaction;toponomics;fluorescence microscopy;graph visualization;visual analytics",
    "Keywords_Processed": "fluorescence microscopy;toponomic;visual analytic;protein interaction;graph visualization",
    "Title": "Interactive, Graph-based Visual Analysis of High-dimensional, Multi-parameter Fluorescence Microscopy Data in Toponomics"
  },
  "346": {
    "Abstract": "Volume ray-casting with a higher order reconstruction filter and\/or a higher sampling rate has been adopted in direct volume rendering frameworks to provide a smooth reconstruction of the volume scalar and\/or to reduce artifacts when the combined frequency of the volume and transfer function is high. While it enables high-quality volume rendering, it cannot support interactive rendering due to its high computational cost. In this paper, we propose a fast high-quality volume ray-casting algorithm which effectively increases the sampling rate. While a ray traverses the volume, intensity values are uniformly reconstructed using a high-order convolution filter. Additional samplings, referred to as virtual samplings, are carried out within a ray segment from a cubic spline curve interpolating those uniformly reconstructed intensities. These virtual samplings are performed by evaluating the polynomial function of the cubic spline curve via simple arithmetic operations. The min max blocks are refined accordingly for accurate empty space skipping in the proposed method. Experimental results demonstrate that the proposed algorithm, also exploiting fast cubic texture filtering supported by programmable GPUs, offers renderings as good as a conventional ray-casting algorithm using high-order reconstruction filtering at the same sampling rate, while delivering 2.5x to 3.3x rendering speed-up.",
    "Authors": "Byeonghun Lee;Jihye Yun;Jinwook Seo;Byonghyo Shim;Yeong Gil Shin;Bohyoung Kim",
    "Clusters": "CurvesAndCurvature;GpuBasedTechniques;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2010.155",
    "Keywords": "direct volume rendering;curve interpolation;high quality;gpu",
    "Keywords_Processed": "curve interpolation;gpu;high quality;direct volume render",
    "Title": "Fast High-Quality Volume Ray Casting with Virtual Samplings"
  },
  "252": {
    "Abstract": "Direct volume rendering has become a popular method for visualizing volumetric datasets. Even though computers are continually getting faster, it remains a challenge to incorporate sophisticated illumination models into direct volume rendering while maintaining interactive frame rates. In this paper, we present a novel approach for advanced illumination in direct volume rendering based on GPU ray-casting. Our approach features directional soft shadows taking scattering into account, ambient occlusion and color bleeding effects while achieving very competitive frame rates. In particular, multiple dynamic lights and interactive transfer function changes are fully supported. Commonly, direct volume rendering is based on a very simplified discrete version of the original volume rendering integral, including the development of the original exponential extinction into a-blending. In contrast to a-blending forming a product when sampling along a ray, the original exponential extinction coefficient is an integral and its discretization a Riemann sum. The fact that it is a sum can cleverly be exploited to implement volume lighting effects, i.e. soft directional shadows, ambient occlusion and color bleeding. We will show how this can be achieved and how it can be implemented on the GPU.",
    "Authors": "Schlegel, P.;Makhinya, M.;Pajarola, R.",
    "Clusters": "BiologyAndBioinformatics;GpuBasedTechniques;Illumination;Rendering;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2011.198",
    "Keywords": "volume rendering;ambient occlusion;exponential extinction;shadows;gpu raycasting",
    "Keywords_Processed": "volume render;ambient occlusion;shadow;exponential extinction;gpu raycasting",
    "Title": "Extinction-Based Shading and Illumination in GPU Volume Ray-Casting"
  },
  "175": {
    "Abstract": "Metal oxides are important for many technical applications. For example alumina (aluminum oxide) is the most commonly-used ceramic in microelectronic devices thanks to its excellent properties. Experimental studies of these materials are increasingly supplemented with computer simulations. Molecular dynamics (MD) simulations can reproduce the material behavior very well and are now reaching time scales relevant for interesting processes like crack propagation. In this work we focus on the visualization of induced electric dipole moments on oxygen atoms in crack propagation simulations. The straightforward visualization using glyphs for the individual atoms, simple shapes like spheres or arrows, is insufficient for providing information about the data set as a whole. As our contribution we show for the first time that fractional anisotropy values computed from the local neighborhood of individual atoms of MD simulation data depict important information about relevant properties of the field of induced electric dipole moments. Iso surfaces in the field of fractional anisotropy as well as adjustments of the glyph representation allow the user to identify regions of correlated orientation. We present novel and relevant findings for the application domain resulting from these visualizations, like the influence of mechanical forces on the electrostatic properties.",
    "Authors": "Grottel, S.;Beck, P.;Muller, C.;Reina, G.;Roth, J.;Trebin, H.-R.;Ertl, T.",
    "Clusters": "GlyphsGlyphBasedTechniques;PhysicsAndPhysicalSciences;PointBasedDataAndTechniques;TimeseriesTimeVaryingDataAndTechniques",
    "DOI": "10.1109\/TVCG.2012.282",
    "Keywords": "time-varying data;point-based data;visualization in physical sciences and engineering;glyph-based techniques",
    "Keywords_Processed": "point base datum;time vary datum;glyph base technique;visualization in physical science and engineering",
    "Title": "Visualization of Electrostatic Dipoles in Molecular Dynamics of Metal Oxides"
  },
  "237": {
    "Abstract": "Narrative visualizations combine conventions of communicative and exploratory information visualization to convey an intended story. We demonstrate visualization rhetoric as an analytical framework for understanding how design techniques that prioritize particular interpretations in visualizations that \"tell a story\" can significantly affect end-user interpretation. We draw a parallel between narrative visualization interpretation and evidence from framing studies in political messaging, decision-making, and literary studies. Devices for understanding the rhetorical nature of narrative information visualizations are presented, informed by the rigorous application of concepts from critical theory, semiotics, journalism, and political theory. We draw attention to how design tactics represent additions or omissions of information at various levels-the data, visual representation, textual annotations, and interactivity-and how visualizations denote and connote phenomena with reference to unstated viewing conventions and codes. Classes of rhetorical techniques identified via a systematic analysis of recent narrative visualizations are presented, and characterized according to their rhetorical contribution to the visualization. We describe how designers and researchers can benefit from the potentially positive aspects of visualization rhetoric in designing engaging, layered narrative visualizations and how our framework can shed light on how a visualization design prioritizes specific interpretations. We identify areas where future inquiry into visualization rhetoric can improve understanding of visualization interpretation.",
    "Authors": "Hullman, J.;Diakopoulos, N.",
    "Clusters": "SemanticsSemioticsRelatedTechniques;Storytelling;TextDocumentTopicAnalysisDataAndTechniques",
    "DOI": "10.1109\/TVCG.2011.255",
    "Keywords": "semiotics;narrative visualization;connotation;framing effects;denotation;rhetoric",
    "Keywords_Processed": "narrative visualization;frame effect;denotation;semiotic;connotation;rhetoric",
    "Title": "Visualization Rhetoric: Framing Effects in Narrative Visualization"
  },
  "112": {
    "Abstract": "A discourse parser is a natural language processing system which can represent the organization of a document based on a rhetorical structure tree-one of the key data structures enabling applications such as text summarization, question answering and dialogue generation. Computational linguistics researchers currently rely on manually exploring and comparing the discourse structures to get intuitions for improving parsing algorithms. In this paper, we present DAViewer, an interactive visualization system for assisting computational linguistics researchers to explore, compare, evaluate and annotate the results of discourse parsers. An iterative user-centered design process with domain experts was conducted in the development of DAViewer. We report the results of an informal formative study of the system to better understand how the proposed visualization and interaction techniques are used in the real research environment.",
    "Authors": "Jian Zhao;Chevalier, F.;Collins, C.;Balakrishnan, R.",
    "Clusters": "HierarchicalTreeDataAndTechniques;InteractionTechniquesGeneral;SocialScienceAndHumanities;TextDocumentTopicAnalysisDataAndTechniques;",
    "DOI": "10.1109\/TVCG.2012.226",
    "Keywords": "discourse structure;visual analytics;computational linguisitics;tree comparison;interaction",
    "Keywords_Processed": "interaction;tree comparison;computational linguisitic;visual analytic;discourse structure",
    "Title": "Facilitating Discourse Analysis with Interactive Visualization"
  },
  "35": {
    "Abstract": "We propose a novel GPU-based approach to render virtual X-ray projections of deformable tetrahedral meshes. These meshes represent the shape and the internal density distribution of a particular anatomical structure and are derived from statistical shape and intensity models (SSIMs). We apply our method to improve the geometric reconstruction of 3D anatomy (e.g. pelvic bone) from 2D X-ray images. For that purpose, shape and density of a tetrahedral mesh are varied and virtual X-ray projections are generated within an optimization process until the similarity between the computed virtual X-ray and the respective anatomy depicted in a given clinical X-ray is maximized. The OpenGL implementation presented in this work deforms and projects tetrahedral meshes of high resolution (200.000+ tetrahedra) at interactive rates. It generates virtual X-rays that accurately depict the density distribution of an anatomy of interest. Compared to existing methods that accumulate X-ray attenuation in deformable meshes, our novel approach significantly boosts the deformation\/projection performance. The proposed projection algorithm scales better with respect to mesh resolution and complexity of the density distribution, and the combined deformation and projection on the GPU scales better with respect to the number of deformation parameters. The gain in performance allows for a larger number of cycles in the optimization process. Consequently, it reduces the risk of being stuck in a local optimum. We believe that our approach will improve treatments in orthopedics, where 3D anatomical information is essential.",
    "Authors": "Ehlke, M.;Ramm, H.;Lamecker, H.;Hege, H.-C.;Zachow, S.",
    "Clusters": "BiomedicalScienceAndMedicine;DataRegistrationFusionAndIntegration;GpuBasedTechniques;MachineLearningAndStatistics;MeshesGridsAndLattices;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2013.159",
    "Keywords": "statistical shape and intensity models;volume rendering;digitally reconstructed radiographs;gpu acceleration;mesh deformation;image registration",
    "Keywords_Processed": "volume render;image registration;statistical shape and intensity model;mesh deformation;digitally reconstruct radiograph;gpu acceleration",
    "Title": "Fast Generation of Virtual X-ray Images for Reconstruction of 3D Anatomy"
  },
  "2": {
    "Abstract": "We present a prop-based, tangible interface for 3D interactive visualization of thin fiber structures. These data are commonly found in current bioimaging datasets, for example second-harmonic generation microscopy of collagen fibers in tissue. Our approach uses commodity visualization technologies such as a depth sensing camera and low-cost 3D display. Unlike most current uses of these emerging technologies in the games and graphics communities, we employ the depth sensing camera to create a fish-tank sterePoscopic virtual reality system at the scientist's desk that supports tracking of small-scale gestures with objects already found in the work space. We apply the new interface to the problem of interactive exploratory visualization of three-dimensional thin fiber data. A critical task for the visual analysis of these data is understanding patterns in fiber orientation throughout a volume.The interface enables a new, fluid style of data exploration and fiber orientation analysis by using props to provide needed passive-haptic feedback, making 3D interactions with these fiber structures more controlled. We also contribute a low-level algorithm for extracting fiber centerlines from volumetric imaging. The system was designed and evaluated with two biophotonic experts who currently use it in their lab. As compared to typical practice within their field, the new visualization system provides a more effective way to examine and understand the 3D bioimaging datasets they collect.",
    "Authors": "Jackson, B.;Tung Yuen Lau;Schroeder, D.;Toussaint, K.C.;Keefe, D.F.",
    "Clusters": "InteractionTechniquesGeneral;Microscopy;",
    "DOI": "10.1109\/TVCG.2013.121",
    "Keywords": "microscopy visualization;tangible interaction;scientific visualization;3d interaction",
    "Keywords_Processed": "scientific visualization;microscopy visualization;tangible interaction;3d interaction",
    "Title": "A Lightweight Tangible 3D Interface for Interactive Visualization of Thin fiber Structures"
  },
  "242": {
    "Abstract": "Analyzing either high-frequency shape detail or any other 2D fields (scalar or vector) embedded over a 3D geometry is a complex task, since detaching the detail from the overall shape can be tricky. An alternative approach is to move to the 2D space, resolving shape reasoning to easier image processing techniques. In this paper we propose a novel framework for the analysis of 2D information distributed over 3D geometry, based on a locally smooth parametrization technique that allows us to treat local 3D data in terms of image content. The proposed approach has been implemented as a sketch-based system that allows to design with a few gestures a set of (possibly overlapping) parameterizations of rectangular portions of the surface. We demonstrate that, due to the locality of the parametrization, the distortion is under an acceptable threshold, while discontinuities can be avoided since the parametrized geometry is always homeomorphic to a disk. We show the effectiveness of the proposed technique to solve specific Cultural Heritage (CH) tasks: the analysis of chisel marks over the surface of a unfinished sculpture and the local comparison of multiple photographs mapped over the surface of an artwork. For this very difficult task, we believe that our framework and the corresponding tool are the first steps toward a computer-based shape reasoning system, able to support CH scholars with a medium they are more used to.",
    "Authors": "Pietroni, N.;Massimiliano, C.;Cignoni, P.;Scopigno, R.",
    "Clusters": "ImageBasedDataImageSignalProcessing;InteractionTechniquesGeneral;Parameterization;SocialScienceAndHumanities;SurfaceRelatedDataAndTechniques",
    "DOI": "10.1109\/TVCG.2011.165",
    "Keywords": "interactive inspection;surface characterization;image processing;cultural heritage;mesh parameterization",
    "Keywords_Processed": "image processing;cultural heritage;surface characterization;mesh parameterization;interactive inspection",
    "Title": "An Interactive Local Flattening Operator to Support Digital Investigations on Artwork Surfaces"
  },
  "100": {
    "Abstract": "People have difficulty understanding statistical information and are unaware of their wrong judgments, particularly in Bayesian reasoning. Psychology studies suggest that the way Bayesian problems are represented can impact comprehension, but few visual designs have been evaluated and only populations with a specific background have been involved. In this study, a textual and six visual representations for three classic problems were compared using a diverse subject pool through crowdsourcing. Visualizations included area-proportional Euler diagrams, glyph representations, and hybrid diagrams combining both. Our study failed to replicate previous findings in that subjects' accuracy was remarkably lower and visualizations exhibited no measurable benefit. A second experiment confirmed that simply adding a visualization to a textual Bayesian problem is of little help, even when the text refers to the visualization, but suggests that visualizations are more effective when the text is given without numerical values. We discuss our findings and the need for more such experiments to be carried out on heterogeneous populations of non-experts.",
    "Authors": "Micallef, L.;Dragicevic, P.;Fekete, J.",
    "Clusters": "ChartsDiagramsPlots;Cognition;EvaluationGeneral;GlyphsGlyphBasedTechniques;MachineLearningAndStatistics",
    "DOI": "10.1109\/TVCG.2012.199",
    "Keywords": "glyph;base rate fallacy;probabilistic judgment;bayesian reasoning;euler diagrams;crowdsourcing",
    "Keywords_Processed": "probabilistic judgment;crowdsource;glyph;bayesian reasoning;base rate fallacy;euler diagram",
    "Title": "Assessing the Effect of Visualizations on Bayesian Reasoning through Crowdsourcing"
  },
  "138": {
    "Abstract": "When and where is an idea dispersed? Social media, like Twitter, has been increasingly used for exchanging information, opinions and emotions about events that are happening across the world. Here we propose a novel visualization design, \u00e2\u20ac\u0153Whisper\u00e2\u20ac\u009d, for tracing the process of information diffusion in social media in real time. Our design highlights three major characteristics of diffusion processes in social media: the temporal trend, social-spatial extent, and community response of a topic of interest. Such social, spatiotemporal processes are conveyed based on a sunflower metaphor whose seeds are often dispersed far away. In Whisper, we summarize the collective responses of communities on a given topic based on how tweets were retweeted by groups of users, through representing the sentiments extracted from the tweets, and tracing the pathways of retweets on a spatial hierarchical layout. We use an efficient flux line-drawing algorithm to trace multiple pathways so the temporal and spatial patterns can be identified even for a bursty event. A focused diffusion series highlights key roles such as opinion leaders in the diffusion process. We demonstrate how our design facilitates the understanding of when and where a piece of information is dispersed and what are the social responses of the crowd, for large-scale events including political campaigns and natural disasters. Initial feedback from domain experts suggests promising use for today's information consumption and dispersion in the wild.",
    "Authors": "Nan Cao;Yu-Ru Lin;Xiaohua Sun;Lazer, D.;Shixia Liu;Huamin Qu",
    "Clusters": "DiffusionRelatedTechniques;SocialNetworksAndSocialMedia;SpatiotemporalDataAndTechniques;",
    "DOI": "10.1109\/TVCG.2012.291",
    "Keywords": "information visualization;contagion;social media;spatio-temporal patterns;microblogging;information diffusion",
    "Keywords_Processed": "information diffusion;information visualization;contagion;spatio temporal pattern;microblogge;social medium",
    "Title": "Whisper: Tracing the Spatiotemporal Process of Information Diffusion in Real Time"
  },
  "365": {
    "Abstract": "Characteristic curves of vector fields include stream, path, and streak lines. Stream and path lines can be obtained by a simple vector field integration of an autonomous ODE system, i.e., they can be described as tangent curves of a vector field. This facilitates their mathematical analysis including the extraction of core lines around which stream or path lines exhibit swirling motion, or the computation of their curvature for every point in the domain without actually integrating them. Such a description of streak lines is not yet available, which excludes them from most of the feature extraction and analysis tools that have been developed in our community. In this paper, we develop the first description of streak lines as tangent curves of a derived vector field - the streak line vector field - and show how it can be computed from the spatial and temporal gradients of the flow map, i.e., a dense path line integration is required. We demonstrate the high accuracy of our approach by comparing it to solutions where the ground truth is analytically known and to solutions where the ground truth has been obtained using the classic streak line computation. Furthermore, we apply a number of feature extraction and analysis tools to the new streak line vector field including the extraction of cores of swirling streak lines and the computation of streak line curvature fields. These first applications foreshadow the large variety of possible future research directions based on our new mathematical description of streak lines.",
    "Authors": "Weinkauf, T.;Theisel, H.",
    "Clusters": "AlgorithmicPatternFeatureDetectionTracking;FlowVisualizationDataAndTechniques;StreamlinesPathlinesStreaklines;SurfaceRelatedDataAndTechniques",
    "DOI": "10.1109\/TVCG.2010.198",
    "Keywords": "unsteady flow visualization;streak surfaces;feature extraction;streaklines",
    "Keywords_Processed": "streak surface;feature extraction;streakline;unsteady flow visualization",
    "Title": "Streak Lines as Tangent Curves of a Derived Vector field"
  },
  "210": {
    "Abstract": "The node-link diagram is an intuitive and venerable way to depict a graph. To reduce clutter and improve the readability of node-link views, Holten & van Wijk's force-directed edge bundling employs a physical simulation to spatially group graph edges. While both useful and aesthetic, this technique has shortcomings: it bundles spatially proximal edges regardless of direction, weight, or graph connectivity. As a result, high-level directional edge patterns are obscured. We present divided edge bundling to tackle these shortcomings. By modifying the forces in the physical simulation, directional lanes appear as an emergent property of edge direction. By considering graph topology, we only bundle edges related by graph structure. Finally, we aggregate edge weights in bundles to enable more accurate visualization of total bundle weights. We compare visualizations created using our technique to standard force-directed edge bundling, matrix diagrams, and clustered graphs; we find that divided edge bundling leads to visualizations that are easier to interpret and reveal both familiar and previously obscured patterns.",
    "Authors": "Selassie, D.;Heller, B.;Heer, J.",
    "Clusters": "AbstractionSimplificationApproximation;DataClusteringAndAggregation;GraphNetworkDataAndTechniques;Simulation",
    "DOI": "10.1109\/TVCG.2011.190",
    "Keywords": "node-link diagrams;physical simulation;aggregation;graph visualization;edge bundling",
    "Keywords_Processed": "edge bundling;aggregation;physical simulation;graph visualization;node link diagram",
    "Title": "Divided Edge Bundling for Directional Network Data"
  },
  "127": {
    "Abstract": "We present and evaluate a framework for constructing sketchy style information visualizations that mimic data graphics drawn by hand. We provide an alternative renderer for the Processing graphics environment that redefines core drawing primitives including line, polygon and ellipse rendering. These primitives allow higher-level graphical features such as bar charts, line charts, treemaps and node-link diagrams to be drawn in a sketchy style with a specified degree of sketchiness. The framework is designed to be easily integrated into existing visualization implementations with minimal programming modification or design effort. We show examples of use for statistical graphics, conveying spatial imprecision and for enhancing aesthetic and narrative qualities of visualization. We evaluate user perception of sketchiness of areal features through a series of stimulus-response tests in order to assess users' ability to place sketchiness on a ratio scale, and to estimate area. Results suggest relative area judgment is compromised by sketchy rendering and that its influence is dependent on the shape being rendered. They show that degree of sketchiness may be judged on an ordinal scale but that its judgement varies strongly between individuals. We evaluate higher-level impacts of sketchiness through user testing of scenarios that encourage user engagement with data visualization and willingness to critique visualization design. Results suggest that where a visualization is clearly sketchy, engagement may be increased and that attitudes to participating in visualization annotation are more positive. The results of our work have implications for effective information visualization design that go beyond the traditional role of sketching as a tool for prototyping or its use for an indication of general uncertainty.",
    "Authors": "Wood, J.;Isenberg, P.;Isenberg, T.;Dykes, J.;Boukhelifa, N.;Slingsby, A.",
    "Clusters": "ArtAndAestheticsInVisualization;IllustrativeVisualization;InteractionTechniquesGeneral;UncertaintyTechniquesAndVisualization;",
    "DOI": "10.1109\/TVCG.2012.262",
    "Keywords": "non-photorealistic rendering;sketch;hand-drawn;uncertainty;visualization",
    "Keywords_Processed": "visualization;non photorealistic rendering;sketch;uncertainty;hand draw",
    "Title": "Sketchy Rendering for Information Visualization"
  },
  "181": {
    "Abstract": "Visual Analytics is \u00e2\u20ac\u0153the science of analytical reasoning facilitated by visual interactive interfaces\u00e2\u20ac\u009d [70]. The goal of this field is to develop tools and methodologies for approaching problems whose size and complexity render them intractable without the close coupling of both human and machine analysis. Researchers have explored this coupling in many venues: VAST, Vis, InfoVis, CHI, KDD, IUI, and more. While there have been myriad promising examples of human-computer collaboration, there exists no common language for comparing systems or describing the benefits afforded by designing for such collaboration. We argue that this area would benefit significantly from consensus about the design attributes that define and distinguish existing techniques. In this work, we have reviewed 1,271 papers from many of the top-ranking conferences in visual analytics, human-computer interaction, and visualization. From these, we have identified 49 papers that are representative of the study of human-computer collaborative problem-solving, and provide a thorough overview of the current state-of-the-art. Our analysis has uncovered key patterns of design hinging on humanand machine-intelligence affordances, and also indicates unexplored avenues in the study of this area. The results of this analysis provide a common framework for understanding these seemingly disparate branches of inquiry, which we hope will motivate future work in the field.",
    "Authors": "Crouser, R.J.;Chang, R.",
    "Clusters": "HumanComputerInteractionHumanFactors;VisualizationSystemsToolkitsAndEnvironments;VisualizationTheoryModelsAndMethods",
    "DOI": "10.1109\/TVCG.2012.195",
    "Keywords": "human complexity;human computation;framework;theory",
    "Keywords_Processed": "human computation;framework;human complexity;theory",
    "Title": "An Affordance-Based Framework for Human Computation and Human-Computer Collaboration"
  },
  "37": {
    "Abstract": "We present the design of a novel framework for the visual integration, comparison, and exploration of correlations in spatial and non-spatial geriatric research data. These data are in general high-dimensional and span both the spatial, volumetric domain - through magnetic resonance imaging volumes - and the non-spatial domain, through variables such as age, gender, or walking speed. The visual analysis framework blends medical imaging, mathematical analysis and interactive visualization techniques, and includes the adaptation of Sparse Partial Least Squares and iterated Tikhonov Regularization algorithms to quantify potential neurologymobility connections. A linked-view design geared specifically at interactive visual comparison integrates spatial and abstract visual representations to enable the users to effectively generate and refine hypotheses in a large, multidimensional, and fragmented space. In addition to the domain analysis and design description, we demonstrate the usefulness of this approach on two case studies. Last, we report the lessons learned through the iterative design and evaluation of our approach, in particular those relevant to the design of comparative visualization of spatial and non-spatial data.",
    "Authors": "Maries, A.;Mays, N.;Hunt, M.O.;Wong, K.F.;Layton, W.;Boudreau, R.;Rosano, C.;Marai, G.E.",
    "Clusters": "ApplicationsGeneralAndOther;ComparisonComparativeVisualizationAndSimilarity;DesignMethodologiesAndInteractionDesign;DesignStudiesAndCaseStudies;IntegratingSpatialAndNonSpatialDataVisualization;MultidimensionalMultivariateMultifieldDataAndTechniques;TasksTaskRequirementsAnalysis",
    "DOI": "10.1109\/TVCG.2013.161",
    "Keywords": "design study;high-dimensional data;applications of visualization;methodology design;task and requirements analysis;visual comparison;integrating spatial and non-spatial visualization",
    "Keywords_Processed": "task and requirement analysis;design study;methodology design;high dimensional datum;application of visualization;integrate spatial and non spatial visualization;visual comparison",
    "Title": "GRACE: A Visual Comparison Framework for Integrated Spatial and Non-Spatial Geriatric Data"
  },
  "317": {
    "Abstract": "Data visualization is regularly promoted for its ability to reveal stories within data, yet these \u00d4\u00c7\u00a3data stories\u00d4\u00c7\u00d8 differ in important ways from traditional forms of storytelling. Storytellers, especially online journalists, have increasingly been integrating visualizations into their narratives, in some cases allowing the visualization to function in place of a written story. In this paper, we systematically review the design space of this emerging class of visualizations. Drawing on case studies from news media to visualization research, we identify distinct genres of narrative visualization. We characterize these design differences, together with interactivity and messaging, in terms of the balance between the narrative flow intended by the author (imposed by graphical elements and the interface) and story discovery on the part of the reader (often through interactive exploration). Our framework suggests design strategies for narrative visualization, including promising under-explored approaches to journalistic storytelling and educational media.",
    "Authors": "Segel, E.;Heer, J.",
    "Clusters": "ApplicationsGeneralAndOther;DesignMethodologiesAndInteractionDesign;DesignStudiesAndCaseStudies;SocialNetworksAndSocialMedia;Storytelling",
    "DOI": "10.1109\/TVCG.2010.179",
    "Keywords": "narrative visualization;journalism;case study;storytelling;design methods;social data analysis",
    "Keywords_Processed": "case study;narrative visualization;storytelle;journalism;design method;social datum analysis",
    "Title": "Narrative Visualization: Telling Stories with Data"
  },
  "130": {
    "Abstract": "Visualizing trajectory attribute data is challenging because it involves showing the trajectories in their spatio-temporal context as well as the attribute values associated with the individual points of trajectories. Previous work on trajectory visualization addresses selected aspects of this problem, but not all of them. We present a novel approach to visualizing trajectory attribute data. Our solution covers space, time, and attribute values. Based on an analysis of relevant visualization tasks, we designed the visualization solution around the principle of stacking trajectory bands. The core of our approach is a hybrid 2D\/3D display. A 2D map serves as a reference for the spatial context, and the trajectories are visualized as stacked 3D trajectory bands along which attribute values are encoded by color. Time is integrated through appropriate ordering of bands and through a dynamic query mechanism that feeds temporally aggregated information to a circular time display. An additional 2D time graph shows temporal information in full detail by stacking 2D trajectory bands. Our solution is equipped with analytical and interactive mechanisms for selecting and ordering of trajectories, and adjusting the color mapping, as well as coordinated highlighting and dedicated 3D navigation. We demonstrate the usefulness of our novel visualization by three examples related to radiation surveillance, traffic analysis, and maritime navigation. User feedback obtained in a small experiment indicates that our hybrid 2D\/3D solution can be operated quite well.",
    "Authors": "Tominski, C.;Schumann, H.;Andrienko, G.;Andrienko, N.",
    "Clusters": "AnalysisProcessGeneral;AnimationAndMotion;InteractionTechniquesGeneral;SpatiotemporalDataAndTechniques;",
    "DOI": "10.1109\/TVCG.2012.265",
    "Keywords": "spatio-temporal data;exploratory data analysis;visualization;trajectory attribute data;interaction",
    "Keywords_Processed": "visualization;interaction;exploratory datum analysis;spatio temporal datum;trajectory attribute datum",
    "Title": "Stacking-Based Visualization of Trajectory Attribute Data"
  },
  "241": {
    "Abstract": "Color vision deficiency (CVD) affects a high percentage of the population worldwide. When seeing a volume visualization result, persons with CVD may be incapable of discriminating the classification information expressed in the image if the color transfer function or the color blending used in the direct volume rendering is not appropriate. Conventional methods used to address this problem adopt advanced image recoloring techniques to enhance the rendering results frame-by-frame; unfortunately, problematic perceptual results may still be generated. This paper proposes an alternative solution that complements the image recoloring scheme by reconfiguring the components of the direct volume rendering (DVR) pipeline. Our approach optimizes the mapped colors of a transfer function to simulate CVD-friendly effect that is generated by applying the image recoloring to the results with the initial transfer function. The optimization process has a low computational complexity, and only needs to be performed once for a given transfer function. To achieve detail-preserving and perceptually natural semi-transparent effects, we introduce a new color composition mode that works in the color space of dichromats. Experimental results and a pilot study demonstrates that our approach can yield dichromats-friendly and consistent volume visualization in real-time.",
    "Authors": "Weifeng Chen;Wei Chen;Hujun Bao",
    "Clusters": "ColorColorPerception;ImageBasedDataImageSignalProcessing;SegmentationAndClassification;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2011.164",
    "Keywords": "direct volume rendering;image recoloring;dichromacy;volume classification",
    "Keywords_Processed": "dichromacy;image recoloring;volume classification;direct volume render",
    "Title": "An Efficient Direct Volume Rendering Approach for Dichromats"
  },
  "61": {
    "Abstract": "This article presents SoccerStories, a visualization interface to support analysts in exploring soccer data and communicating interesting insights. Currently, most analyses on such data relate to statistics on individual players or teams. However, soccer analysts we collaborated with consider that quantitative analysis alone does not convey the right picture of the game, as context, player positions and phases of player actions are the most relevant aspects. We designed SoccerStories to support the current practice of soccer analysts and to enrich it, both in the analysis and communication stages. Our system provides an overview+detail interface of game phases, and their aggregation into a series of connected visualizations, each visualization being tailored for actions such as a series of passes or a goal attempt. To evaluate our tool, we ran two qualitative user studies on recent games using SoccerStories with data from one of the world's leading live sports data providers. The first study resulted in a series of four articles on soccer tactics, by a tactics analyst, who said he would not have been able to write these otherwise. The second study consisted in an exploratory follow-up to investigate design alternatives for embedding soccer phases into word-sized graphics. For both experiments, we received a very enthusiastic feedback and participants consider further use of SoccerStories to enhance their current workflow.",
    "Authors": "Perin, C.;Vuillemot, R.;Fekete, J.",
    "Clusters": "DataClusteringAndAggregation;KnowledgeDiscovery;SportsVisualization;VisualKnowledgeRepresentationAndExternalization",
    "DOI": "10.1109\/TVCG.2013.192",
    "Keywords": "visual knowledge representation;visual aggregation;sport analytics;visual knowledge discovery",
    "Keywords_Processed": "visual aggregation;visual knowledge discovery;sport analytic;visual knowledge representation",
    "Title": "SoccerStories: A Kick-off for Visual Soccer Analysis"
  },
  "120": {
    "Abstract": "We investigate the cognitive impact of various layout features-symmetry, alignment, collinearity, axis alignment and orthogonality - on the recall of network diagrams (graphs). This provides insight into how people internalize these diagrams and what features should or shouldn't be utilised when designing static and interactive network-based visualisations. Participants were asked to study, remember, and draw a series of small network diagrams, each drawn to emphasise a particular visual feature. The visual features were based on existing theories of perception, and the task enabled visual processing at the visceral level only. Our results strongly support the importance of visual features such as symmetry, collinearity and orthogonality, while not showing any significant impact for node-alignment or parallel edges.",
    "Authors": "Marriott, K.;Purchase, H.;Wybrow, M.;Goncu, C.",
    "Clusters": "ChartsDiagramsPlots;DataFeaturesAndAttributes;EvaluationGeneral;GraphNetworkDataAndTechniques;Perception",
    "DOI": "10.1109\/TVCG.2012.245",
    "Keywords": "graph layout;perceptual theories;network diagrams;visual features;experiment;diagram recall",
    "Keywords_Processed": "graph layout;perceptual theory;visual feature;network diagram;diagram recall;experiment",
    "Title": "Memorability of Visual Features in Network Diagrams"
  },
  "385": {
    "Abstract": "Journalists increasingly turn to social media sources such as Facebook or Twitter to support their coverage of various news events. For large-scale events such as televised debates and speeches, the amount of content on social media can easily become overwhelming, yet still contain information that may aid and augment reporting via individual content items as well as via aggregate information from the crowd's response. In this work we present a visual analytic tool, Vox Civitas, designed to help journalists and media professionals extract news value from large-scale aggregations of social media content around broadcast events. We discuss the design of the tool, present the text analysis techniques used to enable the presentation, and provide details on the visual and interaction design. We provide an exploratory evaluation based on a user study in which journalists interacted with the system to explore and report on a dataset of over one hundred thousand twitter messages collected during the U.S. State of the Union presidential address in 2010.",
    "Authors": "Diakopoulos, N.;Naaman, M.;Kivran-Swaine, F.",
    "Clusters": "ApplicationsGeneralAndOther;Cognition;SocialNetworksAndSocialMedia",
    "DOI": "10.1109\/VAST.2010.5652922",
    "Keywords": "social media;computer-assisted reporting;sensemaking;computational journalism",
    "Keywords_Processed": "computer assist reporting;sensemake;social medium;computational journalism",
    "Title": "Diamonds in the rough: Social media visual analytics for journalistic inquiry"
  },
  "104": {
    "Abstract": "We present a novel technique-Compressed Adjacency Matrices-for visualizing gene regulatory networks. These directed networks have strong structural characteristics: out-degrees with a scale-free distribution, in-degrees bound by a low maximum, and few and small cycles. Standard visualization techniques, such as node-link diagrams and adjacency matrices, are impeded by these network characteristics. The scale-free distribution of out-degrees causes a high number of intersecting edges in node-link diagrams. Adjacency matrices become space-inefficient due to the low in-degrees and the resulting sparse network. Compressed adjacency matrices, however, exploit these structural characteristics. By cutting open and rearranging an adjacency matrix, we achieve a compact and neatly-arranged visualization. Compressed adjacency matrices allow for easy detection of subnetworks with a specific structure, so-called motifs, which provide important knowledge about gene regulatory networks to domain experts. We summarize motifs commonly referred to in the literature, and relate them to network analysis tasks common to the visualization domain. We show that a user can easily find the important motifs in compressed adjacency matrices, and that this is hard in standard adjacency matrix and node-link diagrams. We also demonstrate that interaction techniques for standard adjacency matrices can be used for our compressed variant. These techniques include rearrangement clustering, highlighting, and filtering.",
    "Authors": "Dinkla, K.;Westenberg, M.A.;van Wijk, J.J.",
    "Clusters": "Genetics;GraphNetworkDataAndTechniques;MatrixRelatedTechniques",
    "DOI": "10.1109\/TVCG.2012.208",
    "Keywords": "scale-free;adjacency matrix;networks;gene regulation",
    "Keywords_Processed": "network;scale free;gene regulation;adjacency matrix",
    "Title": "Compressed Adjacency Matrices: Untangling Gene Regulatory Networks"
  },
  "216": {
    "Abstract": "Geodemographic classifiers characterise populations by categorising geographical areas according to the demographic and lifestyle characteristics of those who live within them. The dimension-reducing quality of such classifiers provides a simple and effective means of characterising population through a manageable set of categories, but inevitably hides heterogeneity, which varies within and between the demographic categories and geographical areas, sometimes systematically. This may have implications for their use, which is widespread in government and commerce for planning, marketing and related activities. We use novel interactive graphics to delve into OAC - a free and open geodemographic classifier that classifies the UK population in over 200,000 small geographical areas into 7 super-groups, 21 groups and 52 sub-groups. Our graphics provide access to the original 41 demographic variables used in the classification and the uncertainty associated with the classification of each geographical area on-demand. It also supports comparison geographically and by category. This serves the dual purpose of helping understand the classifier itself leading to its more informed use and providing a more comprehensive view of population in a comprehensible manner. We assess the impact of these interactive graphics on experienced OAC users who explored the details of the classification, its uncertainty and the nature of between - and within - class variation and then reflect on their experiences. Visualization of the complexities and subtleties of the classification proved to be a thought-provoking exercise both confirming and challenging users' understanding of population, the OAC classifier and the way it is used in their organisations. Users identified three contexts for which the techniques were deemed useful in the context of local government, confirming the validity of the proposed methods.",
    "Authors": "Slingsby, A.;Dykes, J.;Wood, J.",
    "Clusters": "GeographyGeospatialVisCartographyTerrainVis;SegmentationAndClassification;UncertaintyTechniquesAndVisualization",
    "DOI": "10.1109\/TVCG.2011.197",
    "Keywords": "uncertainty;oac;geodemographics;classification;cartography",
    "Keywords_Processed": "uncertainty;oac;geodemographic;classification;cartography",
    "Title": "Exploring Uncertainty in Geodemographics with Interactive Graphics"
  },
  "320": {
    "Abstract": "Interactive visualization requires the translation of data into a screen space of limited resolution. While currently ignored by most visualization models, this translation entails a loss of information and the introduction of a number of artifacts that can be useful, (e.g., aggregation, structures) or distracting (e.g., over-plotting, clutter) for the analysis. This phenomenon is observed in parallel coordinates, where overlapping lines between adjacent axes form distinct patterns, representing the relation between variables they connect. However, even for a small number of dimensions, the challenge is to effectively convey the relationships for all combinations of dimensions. The size of the dataset and a large number of dimensions only add to the complexity of this problem. To address these issues, we propose Pargnostics, parallel coordinates diagnostics, a model based on screen-space metrics that quantify the different visual structures. Pargnostics metrics are calculated for pairs of axes and take into account the resolution of the display as well as potential axis inversions. Metrics include the number of line crossings, crossing angles, convergence, overplotting, etc. To construct a visualization view, the user can pick from a ranked display showing pairs of coordinate axes and the structures between them, or examine all possible combinations of axes at once in a matrix display. Picking the best axes layout is an NP-complete problem in general, but we provide a way of automatically optimizing the display according to the user's preferences based on our metrics and model.",
    "Authors": "Dasgupta, A.;Kosara, R.",
    "Clusters": "DisplaysGeneral;EvaluationMetricsAndBenchmarks;ParallelCoordinates;VisualizationTheoryModelsAndMethods",
    "DOI": "10.1109\/TVCG.2010.184",
    "Keywords": "display optimization;visualization models;parallel coordinates;metrics",
    "Keywords_Processed": "parallel coordinate;metric;visualization model;display optimization",
    "Title": "Pargnostics: Screen-Space Metrics for Parallel Coordinates"
  },
  "231": {
    "Abstract": "Generation of synthetic datasets is a common practice in many research areas. Such data is often generated to meet specific needs or certain conditions that may not be easily found in the original, real data. The nature of the data varies according to the application area and includes text, graphs, social or weather data, among many others. The common process to create such synthetic datasets is to implement small scripts or programs, restricted to small problems or to a specific application. In this paper we propose a framework designed to generate high dimensional datasets. Users can interactively create and navigate through multi dimensional datasets using a suitable graphical user-interface. The data creation is driven by statistical distributions based on a few user-defined parameters. First, a grounding dataset is created according to given inputs, and then structures and trends are included in selected dimensions and orthogonal projection planes. Furthermore, our framework supports the creation of complex non-orthogonal trends and classified datasets. It can successfully be used to create synthetic datasets simulating important trends as multidimensional clusters, correlations and outliers.",
    "Authors": "Albuquerque, G.;Lowe, T.;Magnor, M.",
    "Clusters": "DataAcquisitionAndManagement;InteractionTechniquesGeneral;MultidimensionalMultivariateMultifieldDataAndTechniques",
    "DOI": "10.1109\/TVCG.2011.237",
    "Keywords": "multivariate data;high-dimensional data;synthetic data generation;interaction",
    "Keywords_Processed": "multivariate datum;high dimensional datum;interaction;synthetic datum generation",
    "Title": "Synthetic Generation of High-Dimensional Datasets"
  },
  "45": {
    "Abstract": "Rankings are a popular and universal approach to structuring otherwise unorganized collections of items by computing a rank for each item based on the value of one or more of its attributes. This allows us, for example, to prioritize tasks or to evaluate the performance of products relative to each other. While the visualization of a ranking itself is straightforward, its interpretation is not, because the rank of an item represents only a summary of a potentially complicated relationship between its attributes and those of the other items. It is also common that alternative rankings exist which need to be compared and analyzed to gain insight into how multiple heterogeneous attributes affect the rankings. Advanced visual exploration tools are needed to make this process efficient. In this paper we present a comprehensive analysis of requirements for the visualization of multi-attribute rankings. Based on these considerations, we propose LineUp - a novel and scalable visualization technique that uses bar charts. This interactive technique supports the ranking of items based on multiple heterogeneous attributes with different scales and semantics. It enables users to interactively combine attributes and flexibly refine parameters to explore the effect of changes in the attribute combination. This process can be employed to derive actionable insights as to which attributes of an item need to be modified in order for its rank to change. Additionally, through integration of slope graphs, LineUp can also be used to compare multiple alternative rankings on the same set of items, for example, over time or across different attribute combinations. We evaluate the effectiveness of the proposed multi-attribute visualization technique in a qualitative study. The study shows that users are able to successfully solve complex ranking tasks in a short period of time.",
    "Authors": "Gratzl, S.;Lex, A.;Gehlenborg, N.;Pfister, H.;Streit, M.",
    "Clusters": "ChartsDiagramsPlots;DataFacetsAndTechniques;EvaluationGeneral;InteractionTechniquesGeneral;MultidimensionalMultivariateMultifieldDataAndTechniques;Ranking",
    "DOI": "10.1109\/TVCG.2013.173",
    "Keywords": "ranking;stacked bar charts;scoring;multi-factorial;multi-attribute;multi-faceted;ranking visualization",
    "Keywords_Processed": "multi factorial;multi attribute;score;rank;stack bar chart;multi faceted;rank visualization",
    "Title": "LineUp: Visual Analysis of Multi-Attribute Rankings"
  },
  "176": {
    "Abstract": "A fundamental characteristic of fluid flow is that it causes mixing: introduce a dye into a flow, and it will disperse. Mixing can be used as a method to visualize and characterize flow. Because mixing is a process that occurs over time, it is a 4D problem that presents a challenge for computation, visualization, and analysis. Motivated by a mixing problem in geophysics, we introduce a combination of methods to analyze, transform, and finally visualize mixing in simulations of convection in a self-gravitating 3D spherical shell representing convection in the Earth's mantle. Geophysicists use tools such as the finite element model CitcomS to simulate convection, and introduce massless, passive tracers to model mixing. The output of geophysical flow simulation is hard to analyze for domain experts because of overall data size and complexity. In addition, information overload and occlusion are problems when visualizing a whole-earth model. To address the large size of the data, we rearrange the simulation data using intelligent indexing for fast file access and efficient caching. To address information overload and interpret mixing, we compute tracer concentration statistics, which are used to characterize mixing in mantle convection models. Our visualization uses a specially tailored version of Direct Volume Rendering. The most important adjustment is the use of constant opacity. Because of this special area of application, i. e. the rendering of a spherical shell, many computations for volume rendering can be optimized. These optimizations are essential to a smooth animation of the time-dependent simulation data. Our results show how our system can be used to quickly assess the simulation output and test hypotheses regarding Earth's mantle convection. The integrated processing pipeline helps geoscientists to focus on their main task of analyzing mantle homogenization.",
    "Authors": "Schroder, S.;Peterson, J.A.;Obermaier, H.;Kellogg, L.H.;Joy, K.I.;Hagen, H.",
    "Clusters": "BiomedicalScienceAndMedicine;EarthSpaceAndEnvironmentalSciences;FlowVisualizationDataAndTechniques;LargeScaleDataAndScalability;PhysicsAndPhysicalSciences",
    "DOI": "10.1109\/TVCG.2012.283",
    "Keywords": "flow visualization;tracer concentration;earth mantle;convection;large data system;geophysics",
    "Keywords_Processed": "earth mantle;large datum system;tracer concentration;geophysic;convection;flow visualization",
    "Title": "Visualization of Flow Behavior in Earth Mantle Convection"
  },
  "290": {
    "Abstract": "We propose a visual analytics procedure for analyzing movement data, i.e., recorded tracks of moving objects. It is oriented to a class of problems where it is required to determine significant places on the basis of certain types of events occurring repeatedly in movement data. The procedure consists of four major steps: (1) event extraction from trajectories; (2) event clustering and extraction of relevant places; (3) spatio-temporal aggregation of events or trajectories; (4) analysis of the aggregated data. All steps are scalable with respect to the amount of the data under analysis. We demonstrate the use of the procedure by example of two real-world problems requiring analysis at different spatial scales.",
    "Authors": "Andrienko, G.;Andrienko, N.;Hurter, C.;Rinzivillo, S.;Wrobel, S.",
    "Clusters": "AnimationAndMotion;DataClusteringAndAggregation;SpaceRelatedSpatialDataAndTechniques;SpatiotemporalDataAndTechniques",
    "DOI": "10.1109\/VAST.2011.6102454",
    "Keywords": "spatio-temporal clustering;spatial clustering;spatio-temporal data;movement;spatial events;trajectory",
    "Keywords_Processed": "spatio temporal datum;spatial event;trajectory;spatial clustering;spatio temporal clustering;movement",
    "Title": "From movement tracks through events to places: Extracting and characterizing significant places from mobility data"
  },
  "64": {
    "Abstract": "Storyline visualizations, which are useful in many applications, aim to illustrate the dynamic relationships between entities in a story. However, the growing complexity and scalability of stories pose great challenges for existing approaches. In this paper, we propose an efficient optimization approach to generating an aesthetically appealing storyline visualization, which effectively handles the hierarchical relationships between entities over time. The approach formulates the storyline layout as a novel hybrid optimization approach that combines discrete and continuous optimization. The discrete method generates an initial layout through the ordering and alignment of entities, and the continuous method optimizes the initial layout to produce the optimal one. The efficient approach makes real-time interactions (e.g., bundling and straightening) possible, thus enabling users to better understand and track how the story evolves. Experiments and case studies are conducted to demonstrate the effectiveness and usefulness of the optimization approach.",
    "Authors": "Shixia Liu;Yingcai Wu;Enxun Wei;Mengchen Liu;Yang Liu",
    "Clusters": "InteractionTechniquesGeneral;LevelOfDetail;Optimization;Storytelling",
    "DOI": "10.1109\/TVCG.2013.196",
    "Keywords": "user interaction;optimization;level-of-detail;storytelling;storylines",
    "Keywords_Processed": "user interaction;optimization;storyline;storytelle;level of detail",
    "Title": "StoryFlow: Tracking the Evolution of Stories"
  },
  "263": {
    "Abstract": "This paper presents a novel framework for visualizing volumetric data specified on complex polyhedral grids, without the need to perform any kind of a priori tetrahedralization. These grids are composed of polyhedra that often are non-convex and have an arbitrary number of faces, where the faces can be non-planar with an arbitrary number of vertices. The importance of such grids in state-of-the-art simulation packages is increasing rapidly. We propose a very compact, face-based data structure for representing such meshes for visualization, called two-sided face sequence lists (TSFSL), as well as an algorithm for direct GPU-based ray-casting using this representation. The TSFSL data structure is able to represent the entire mesh topology in a 1D TSFSL data array of face records, which facilitates the use of efficient 1D texture accesses for visualization. In order to scale to large data sizes, we employ a mesh decomposition into bricks that can be handled independently, where each brick is then composed of its own TSFSL array. This bricking enables memory savings and performance improvements for large meshes. We illustrate the feasibility of our approach with real-world application results, by visualizing highly complex polyhedral data from commercial state-of-the-art simulation packages.",
    "Authors": "Muigg, P.;Hadwiger, M.;Doleisch, H.;Groller, E.",
    "Clusters": "GpuBasedTechniques;MeshesGridsAndLattices;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2011.216",
    "Keywords": "gpu-based visualization;polyhedral grids;volume rendering;unstructured grid",
    "Keywords_Processed": "volume render;polyhedral grid;gpu base visualization;unstructured grid",
    "Title": "Interactive Volume Visualization of General Polyhedral Grids"
  },
  "392": {
    "Abstract": "These current studies explored the impact of individual differences in personality factors on interface interaction and learning performance behaviors in both an interactive visualization and a menu-driven web table in two studies. Participants were administered 3 psychometric measures designed to assess Locus of Control, Extraversion, and Neuroticism. Participants were then asked to complete multiple procedural learning tasks in each interface. Results demonstrated that all three measures predicted completion times. Additionally, results analyses demonstrated personality factors also predicted the number of insights participants reported while completing the tasks in each interface. We discuss how these findings advance our ongoing research in the Personal Equation of Interaction.",
    "Authors": "Green, T.M.;Fisher, B.",
    "Clusters": "Cognition;VisualizationTheoryModelsAndMethods",
    "DOI": "10.1109\/VAST.2010.5653587",
    "Keywords": "cognition and perception theory;embodied cognition;visualization taxonomies and models;visual analytics",
    "Keywords_Processed": "visualization taxonomy and model;visual analytic;embody cognition;cognition and perception theory",
    "Title": "Towards the Personal Equation of Interaction: The impact of personality factors on visual analytics interface interaction"
  },
  "353": {
    "Abstract": "We introduce a flexible technique for interactive exploration of vector field data through classification derived from user-specified feature templates. Our method is founded on the observation that, while similar features within the vector field may be spatially disparate, they share similar neighborhood characteristics. Users generate feature-based visualizations by interactively highlighting well-accepted and domain specific representative feature points. Feature exploration begins with the computation of attributes that describe the neighborhood of each sample within the input vector field. Compilation of these attributes forms a representation of the vector field samples in the attribute space. We project the attribute points onto the canonical 2D plane to enable interactive exploration of the vector field using a painting interface. The projection encodes the similarities between vector field points within the distances computed between their associated attribute points. The proposed method is performed at interactive rates for enhanced user experience and is completely flexible as showcased by the simultaneous identification of diverse feature types.",
    "Authors": "Daniels II, J.;Anderson, E.W.;Nonato, L.G.;Silva, C.T.",
    "Clusters": "AlgorithmicPatternFeatureDetectionTracking;DataClusteringAndAggregation;InteractionTechniquesGeneral;MultidimensionalMultivariateMultifieldDataAndTechniques;VectorFieldsDataAndTechniques",
    "DOI": "10.1109\/TVCG.2010.170",
    "Keywords": "user interaction;high-dimensional data;vector field;feature classification;data clustering",
    "Keywords_Processed": "user interaction;vector field;feature classification;high dimensional datum;datum clustering",
    "Title": "Interactive Vector field Feature Identification"
  },
  "297": {
    "Abstract": "Asynchronous Collaborative Visual Analytics (ACVA) leverages group sensemaking by releasing the constraints on when, where, and who works collaboratively. A significant task to be addressed before ACVA can reach its full potential is effective common ground construction, namely the process in which users evaluate insights from individual work to develop a shared understanding of insights and collectively pool them. This is challenging due to the lack of instant communication and scale of collaboration in ACVA. We propose a novel visual analytics approach that automatically gathers, organizes, and summarizes insights to form common ground with reduced human effort. The rich set of visualization and interaction techniques provided in our approach allows users to effectively and flexibly control the common ground construction and review, explore, and compare insights in detail. A working prototype of the approach has been implemented. We have conducted a case study and a user study to demonstrate its effectiveness.",
    "Authors": "Yang Chen;Alsakran, J.;Barlowe, S.;Jing Yang;Ye Zhao",
    "Clusters": "AnalysisProcessGeneral;CollaborativeVisualization;MultidimensionalMultivariateMultifieldDataAndTechniques;",
    "DOI": "10.1109\/VAST.2011.6102447",
    "Keywords": "asynchronous collaboration;multi-dimensional visualization;insight;visual analytics",
    "Keywords_Processed": "visual analytic;insight;multi dimensional visualization;asynchronous collaboration",
    "Title": "Supporting effective common ground construction in Asynchronous Collaborative Visual Analytics"
  },
  "4": {
    "Abstract": "We present an integrated camera motion design and path generation system for building volume data animations. Creating animations is an essential task in presenting complex scientific visualizations. Existing visualization systems use an established animation function based on keyframes selected by the user. This approach is limited in providing the optimal in-between views of the data. Alternatively, computer graphics and virtual reality camera motion planning is frequently focused on collision free movement in a virtual walkthrough. For semi-transparent, fuzzy, or blobby volume data the collision free objective becomes insufficient. Here, we provide a set of essential criteria focused on computing camera paths to establish effective animations of volume data. Our dynamic multi-criteria solver coupled with a force-directed routing algorithm enables rapid generation of camera paths. Once users review the resulting animation and evaluate the camera motion, they are able to determine how each criterion impacts path generation. In this paper, we demonstrate how incorporating this animation approach with an interactive volume visualization system reduces the effort in creating context-aware and coherent animations. This frees the user to focus on visualization tasks with the objective of gaining additional insight from the volume data.",
    "Authors": "Wei-Hsien Hsu;Yubo Zhang;Kwan-Liu Ma",
    "Clusters": "AnimationAndMotion;CamerasCameraViewsAndProjections;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2013.123",
    "Keywords": "animation;visualization;volume rendering;camera motion planning",
    "Keywords_Processed": "visualization;volume render;camera motion planning;animation",
    "Title": "A Multi-Criteria Approach to Camera Motion Design for Volume Data Animation"
  },
  "376": {
    "Abstract": "In this paper, we introduce a novel application of volume modeling techniques on laser Benign Prostatic Hyperplasia (BPH) therapy simulation. The core technique in our system is an algorithm for simulating the tissue vaporization process by laser heating. Different from classical volume CSG operations, our technique takes experimental data as the guidance to determine the vaporization amount so that only a specified amount of tissue is vaporized in each time. Our algorithm uses a predictor-corrector strategy. First, we apply the classical CSG algorithm on a tetrahedral grid based distance field to estimate the vaporized tissue amount. Then, a volume-correction phase is applied on the distance field. To improve the performance, we further propose optimization approaches for efficient implementation.",
    "Authors": "Zhang, N.;Xiangmin Zhou;Yunhe Shen;Sweet, R.",
    "Clusters": "BiomedicalScienceAndMedicine;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2010.221",
    "Keywords": "volume csg;medical simulation;large benign prostatic hyperplasia simulator;controlled-volume vaporization;volume modeling",
    "Keywords_Processed": "control volume vaporization;medical simulation;large benign prostatic hyperplasia simulator;volume csg;volume model",
    "Title": "Volumetric Modeling in Laser BPH Therapy Simulation"
  },
  "381": {
    "Abstract": "Events that happened in the past are important for understanding the ongoing processes, predicting future developments, and making informed decisions. Significant and\/or interesting events tend to attract many people. Some people leave traces of their attendance in the form of computer-processable data, such as records in the databases of mobile phone operators or photos on photo sharing web sites. We developed a suite of visual analytics methods for reconstructing past events from these activity traces. Our tools combine geocomputations, interactive geovisualizations and statistical methods to enable integrated analysis of the spatial, temporal, and thematic components of the data, including numeric attributes and texts. We demonstrate the utility of our approach on two large real data sets, mobile phone calls in Milano during 9 days and flickr photos made on British Isles during 5 years.",
    "Authors": "Andrienko, G.;Andrienko, N.;Mladenov, M.;Mock, M.;Politz, C.",
    "Clusters": "EventsTrendsOutlierDetectionAnalysisAndVisualization;GeographyGeospatialVisCartographyTerrainVis;LargeScaleDataAndScalability;SpatiotemporalDataAndTechniques;TimeseriesTimeVaryingDataAndTechniques",
    "DOI": "10.1109\/VAST.2010.5652478",
    "Keywords": "spatio-temporal data;event detection;time-series analysis;scalable visualization;geovisualization",
    "Keywords_Processed": "spatio temporal datum;time series analysis;geovisualization;event detection;scalable visualization",
    "Title": "Discovering bits of place histories from people's activity traces"
  },
  "27": {
    "Abstract": "We explore the effectiveness of visualizing dense directed graphs by replacing individual edges with edges connected to 'modules'-or groups of nodes-such that the new edges imply aggregate connectivity. We only consider techniques that offer a lossless compression: that is, where the entire graph can still be read from the compressed version. The techniques considered are: a simple grouping of nodes with identical neighbor sets; Modular Decomposition which permits internal structure in modules and allows them to be nested; and Power Graph Analysis which further allows edges to cross module boundaries. These techniques all have the same goal-to compress the set of edges that need to be rendered to fully convey connectivity-but each successive relaxation of the module definition permits fewer edges to be drawn in the rendered graph. Each successive technique also, we hypothesize, requires a higher degree of mental effort to interpret. We test this hypothetical trade-off with two studies involving human participants. For Power Graph Analysis we propose a novel optimal technique based on constraint programming. This enables us to explore the parameter space for the technique more precisely than could be achieved with a heuristic. Although applicable to many domains, we are motivated by-and discuss in particular-the application to software dependency analysis.",
    "Authors": "Dwyer, T.;Riche, N.H.;Marriott, K.;Mears, C.",
    "Clusters": "GraphNetworkDataAndTechniques",
    "DOI": "10.1109\/TVCG.2013.151",
    "Keywords": "networks;modular decomposition;directed graphs;power graph analysis",
    "Keywords_Processed": "modular decomposition;network;direct graph;power graph analysis",
    "Title": "Edge Compression Techniques for Visualization of Dense Directed Graphs"
  },
  "70": {
    "Abstract": "Traditional sketch-based image or video search systems rely on machine learning concepts as their core technology. However, in many applications, machine learning alone is impractical since videos may not be semantically annotated sufficiently, there may be a lack of suitable training data, and the search requirements of the user may frequently change for different tasks. In this work, we develop a visual analytics systems that overcomes the shortcomings of the traditional approach. We make use of a sketch-based interface to enable users to specify search requirement in a flexible manner without depending on semantic annotation. We employ active machine learning to train different analytical models for different types of search requirements. We use visualization to facilitate knowledge discovery at the different stages of visual analytics. This includes visualizing the parameter space of the trained model, visualizing the search space to support interactive browsing, visualizing candidature search results to support rapid interaction for active learning while minimizing watching videos, and visualizing aggregated information of the search results. We demonstrate the system for searching spatiotemporal attributes from sports video to identify key instances of the team and player performance.",
    "Authors": "Legg, P.A.;Chung, D.H.S.;Parry, M.L.;Bown, R.;Jones, M.;Griffiths, I.W.;Chen, M.",
    "Clusters": "DataClusteringAndAggregation;KnowledgeDiscovery;MachineLearningAndStatistics;MultimediaImageVideoMusic",
    "DOI": "10.1109\/TVCG.2013.207",
    "Keywords": "multimedia visualization;visual knowledge discovery;data clustering;machine learning",
    "Keywords_Processed": "machine learning;multimedia visualization;visual knowledge discovery;datum clustering",
    "Title": "Transformation of an Uncertain Video Search Pipeline to a Sketch-Based Visual Analytics Loop"
  },
  "1": {
    "Abstract": "Knowledge about visualization tasks plays an important role in choosing or building suitable visual representations to pursue them. Yet, tasks are a multi-faceted concept and it is thus not surprising that the many existing task taxonomies and models all describe different aspects of tasks, depending on what these task descriptions aim to capture. This results in a clear need to bring these different aspects together under the common hood of a general design space of visualization tasks, which we propose in this paper. Our design space consists of five design dimensions that characterize the main aspects of tasks and that have so far been distributed across different task descriptions. We exemplify its concrete use by applying our design space in the domain of climate impact research. To this end, we propose interfaces to our design space for different user roles (developers, authors, and end users) that allow users of different levels of expertise to work with it.",
    "Authors": "Schulz, H.;Nocke, T.;Heitzler, M.;Schumann, H.",
    "Clusters": "DesignMethodologiesAndInteractionDesign;EarthSpaceAndEnvironmentalSciences;Taxonomies;VisualDesignDesignGuidelines",
    "DOI": "10.1109\/TVCG.2013.120",
    "Keywords": "design space;visualization recommendation;task taxonomy;climate impact research",
    "Keywords_Processed": "design space;task taxonomy;climate impact research;visualization recommendation",
    "Title": "A Design Space of Visualization Tasks"
  },
  "111": {
    "Abstract": "Event sequence data is common in many domains, ranging from electronic medical records (EMRs) to sports events. Moreover, such sequences often result in measurable outcomes (e.g., life or death, win or loss). Collections of event sequences can be aggregated together to form event progression pathways. These pathways can then be connected with outcomes to model how alternative chains of events may lead to different results. This paper describes the Outflow visualization technique, designed to (1) aggregate multiple event sequences, (2) display the aggregate pathways through different event states with timing and cardinality, (3) summarize the pathways' corresponding outcomes, and (4) allow users to explore external factors that correlate with specific pathway state transitions. Results from a user study with twelve participants show that users were able to learn how to use Outflow easily with limited training and perform a range of tasks both accurately and rapidly.",
    "Authors": "Wongsuphasawat, K.;Gotz, D.",
    "Clusters": "BiomedicalScienceAndMedicine;StateRelatedDataTechniques;TimeseriesTimeVaryingDataAndTechniques;",
    "DOI": "10.1109\/TVCG.2012.225",
    "Keywords": "information visualization;temporal event sequences;state diagram;state transitions;outflow",
    "Keywords_Processed": "state transition;temporal event sequence;information visualization;outflow;state diagram",
    "Title": "Exploring Flow, Factors, and Outcomes of Temporal Event Sequences with the Outflow Visualization"
  },
  "141": {
    "Abstract": "The process of surface perception is complex and based on several influencing factors, e.g., shading, silhouettes, occluding contours, and top down cognition. The accuracy of surface perception can be measured and the influencing factors can be modified in order to decrease the error in perception. This paper presents a novel concept of how a perceptual evaluation of a visualization technique can contribute to its redesign with the aim of improving the match between the distal and the proximal stimulus. During analysis of data from previous perceptual studies, we observed that the slant of 3D surfaces visualized on 2D screens is systematically underestimated. The visible trends in the error allowed us to create a statistical model of the perceived surface slant. Based on this statistical model we obtained from user experiments, we derived a new shading model that uses adjusted surface normals and aims to reduce the error in slant perception. The result is a shape-enhancement of visualization which is driven by an experimentally-founded statistical model. To assess the efficiency of the statistical shading model, we repeated the evaluation experiment and confirmed that the error in perception was decreased. Results of both user experiments are publicly-available datasets.",
    "Authors": "Solteszova, V.;Turkay, C.;Price, M.C.;Viola, I.",
    "Clusters": "EvaluationGeneral;MachineLearningAndStatistics;Perception;Rendering;SurfaceRelatedDataAndTechniques",
    "DOI": "10.1109\/TVCG.2012.188",
    "Keywords": "shading;surface slant;perception;evaluation;statistical analysis",
    "Keywords_Processed": "shade;perception;surface slant;evaluation;statistical analysis",
    "Title": "A Perceptual-Statistics Shading Model"
  },
  "117": {
    "Abstract": "Visual comparison is an intrinsic part of interactive data exploration and analysis. The literature provides a large body of existing solutions that help users accomplish comparison tasks. These solutions are mostly of visual nature and custom-made for specific data. We ask the question if a more general support is possible by focusing on the interaction aspect of comparison tasks. As an answer to this question, we propose a novel interaction concept that is inspired by real-world behavior of people comparing information printed on paper. In line with real-world interaction, our approach supports users (1) in interactively specifying pieces of graphical information to be compared, (2) in flexibly arranging these pieces on the screen, and (3) in performing the actual comparison of side-by-side and overlapping arrangements of the graphical information. Complementary visual cues and add-ons further assist users in carrying out comparison tasks. Our concept and the integrated interaction techniques are generally applicable and can be coupled with different visualization techniques. We implemented an interactive prototype and conducted a qualitative user study to assess the concept's usefulness in the context of three different visualization techniques. The obtained feedback indicates that our interaction techniques mimic the natural behavior quite well, can be learned quickly, and are easy to apply to visual comparison tasks.",
    "Authors": "Tominski, C.;Forsell, C.;Johansson, J.",
    "Clusters": "ComparisonComparativeVisualizationAndSimilarity;HumanComputerInteractionHumanFactors;InteractionTechniquesGeneral;",
    "DOI": "10.1109\/TVCG.2012.237",
    "Keywords": "natural interaction;visualization;human-computer interaction;visual comparison;interaction",
    "Keywords_Processed": "visualization;interaction;natural interaction;human computer interaction;visual comparison",
    "Title": "Interaction Support for Visual Comparison Inspired by Natural Behavior"
  },
  "98": {
    "Abstract": "Comparing slopes is a fundamental graph reading task and the aspect ratio chosen for a plot influences how easy these comparisons are to make. According to Banking to 45\u00c2\u00b0, a classic design guideline first proposed and studied by Cleveland et al., aspect ratios that center slopes around 45\u00c2\u00b0 minimize errors in visual judgments of slope ratios. This paper revisits this earlier work. Through exploratory pilot studies that expand Cleveland et al.'s experimental design, we develop an empirical model of slope ratio estimation that fits more extreme slope ratio judgments and two common slope ratio estimation strategies. We then run two experiments to validate our model. In the first, we show that our model fits more generally than the one proposed by Cleveland et al. and we find that, in general, slope ratio errors are not minimized around 45\u00c2\u00b0. In the second experiment, we explore a novel hypothesis raised by our model: that visible baselines can substantially mitigate errors made in slope judgments. We conclude with an application of our model to aspect ratio selection.",
    "Authors": "Talbot, J.;Gerth, J.;Hanrahan, P.",
    "Clusters": "Perception;VectorFieldsDataAndTechniques;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2012.196",
    "Keywords": "slope perception;orientation resolution;aspect ratio selection;banking to 45 degrees",
    "Keywords_Processed": "orientation resolution;aspect ratio selection;banking to 45 degree;slope perception",
    "Title": "An Empirical Model of Slope Ratio Comparisons"
  },
  "330": {
    "Abstract": "In many common data analysis scenarios the data elements are logically grouped into sets. Venn and Euler style diagrams are a common visual representation of such set membership where the data elements are represented by labels or glyphs and sets are indicated by boundaries surrounding their members. Generating such diagrams automatically such that set regions do not intersect unless the corresponding sets have a non-empty intersection is a difficult problem. Further, it may be impossible in some cases if regions are required to be continuous and convex. Several approaches exist to draw such set regions using more complex shapes, however, the resulting diagrams can be difficult to interpret. In this paper we present two novel approaches for simplifying a complex collection of intersecting sets into a strict hierarchy that can be more easily automatically arranged and drawn (Figure 1). In the first approach, we use compact rectangular shapes for drawing each set, attempting to improve the readability of the set intersections. In the second approach, we avoid drawing intersecting set regions by duplicating elements belonging to multiple sets. We compared both of our techniques to the traditional non-convex region technique using five readability tasks. Our results show that the compact rectangular shapes technique was often preferred by experimental subjects even though the use of duplications dramatically improves the accuracy and performance time for most of our tasks. In addition to general set representation our techniques are also applicable to visualization of networks with intersecting clusters of nodes.",
    "Authors": "Riche, N.H.;Dwyer, T.",
    "Clusters": "ChartsDiagramsPlots;GraphNetworkDataAndTechniques;SetRelatedDataTechniques;",
    "DOI": "10.1109\/TVCG.2010.210",
    "Keywords": "information visualization;euler diagrams;set visualization;graph visualization",
    "Keywords_Processed": "graph visualization;information visualization;set visualization;euler diagram",
    "Title": "Untangling Euler Diagrams"
  },
  "17": {
    "Abstract": "Sets of simulation runs based on parameter and model variation, so-called ensembles, are increasingly used to model physical behaviors whose parameter space is too large or complex to be explored automatically. Visualization plays a key role in conveying important properties in ensembles, such as the degree to which members of the ensemble agree or disagree in their behavior. For ensembles of time-varying vector fields, there are numerous challenges for providing an expressive comparative visualization, among which is the requirement to relate the effect of individual flow divergence to joint transport characteristics of the ensemble. Yet, techniques developed for scalar ensembles are of little use in this context, as the notion of transport induced by a vector field cannot be modeled using such tools. We develop a Lagrangian framework for the comparison of flow fields in an ensemble. Our techniques evaluate individual and joint transport variance and introduce a classification space that facilitates incorporation of these properties into a common ensemble visualization. Variances of Lagrangian neighborhoods are computed using pathline integration and Principal Components Analysis. This allows for an inclusion of uncertainty measurements into the visualization and analysis approach. Our results demonstrate the usefulness and expressiveness of the presented method on several practical examples.",
    "Authors": "Hummel, M.;Obermaier, H.;Garth, C.;Joy, K.I.",
    "Clusters": "ComparisonComparativeVisualizationAndSimilarity;DimensionalityReduction;FlowVisualizationDataAndTechniques;MachineLearningAndStatistics;Simulation;TimeseriesTimeVaryingDataAndTechniques;",
    "DOI": "10.1109\/TVCG.2013.141",
    "Keywords": "time-varying;variance;comparison;visualization;flow field;lagrangian;principal component analysis;ensemble",
    "Keywords_Processed": "visualization;comparison;flow field;time vary;variance;ensemble;principal component analysis;lagrangian",
    "Title": "Comparative Visual Analysis of Lagrangian Transport in CFD Ensembles"
  },
  "79": {
    "Abstract": "For preserving the grotto wall paintings and protecting these historic cultural icons from the damage and deterioration in nature environment, a visual analytics framework and a set of tools are proposed for the discovery of degradation patterns. In comparison with the traditional analysis methods that used restricted scales, our method provides users with multi-scale analytic support to study the problems on site, cave, wall and particular degradation area scales, through the application of multidimensional visualization techniques. Several case studies have been carried out using real-world wall painting data collected from a renowned World Heritage site, to verify the usability and effectiveness of the proposed method. User studies and expert reviews were also conducted through by domain experts ranging from scientists such as microenvironment researchers, archivists, geologists, chemists, to practitioners such as conservators, restorers and curators.",
    "Authors": "Jiawan Zhang;Kai Kang;Dajian Liu;Ye Yuan;Yanli, E.",
    "Clusters": "ApplicationsGeneralAndOther;ArtAndAestheticsInVisualization;SocialScienceAndHumanities;",
    "DOI": "10.1109\/TVCG.2013.219",
    "Keywords": "wall paintings;cultural heritage;visual analytics;degradation",
    "Keywords_Processed": "visual analytic;cultural heritage;wall painting;degradation",
    "Title": "Vis4Heritage: Visual Analytics Approach on Grotto Wall Painting Degradations"
  },
  "359": {
    "Abstract": "Shading is an important feature for the comprehension of volume datasets, but is difficult to implement accurately. Current techniques based on pre-integrated direct volume rendering approximate the volume rendering integral by ignoring non-linear gradient variations between front and back samples, which might result in cumulated shading errors when gradient variations are important and \/ or when the illumination function features high frequencies. In this paper, we explore a simple approach for pre-integrated volume rendering with non-linear gradient interpolation between front and back samples. We consider that the gradient smoothly varies along a quadratic curve instead of a segment in-between consecutive samples. This not only allows us to compute more accurate shaded pre-integrated look-up tables, but also allows us to more efficiently process shading amplifying effects, based on gradient filtering. An interesting property is that the pre-integration tables we use remain two-dimensional as for usual pre-integrated classification. We conduct experiments using a full hardware approach with the Blinn-Phong illumination model as well as with a non-photorealistic illumination model.",
    "Authors": "Guetat, A.;Ancel, A.;Marchesin, S.;Dischler, J.-M.",
    "Clusters": "DataAcquisitionAndManagement;RaytracingRaycasting;Rendering;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2010.187",
    "Keywords": "pre-integration;volume rendering;raycasting;phong shading",
    "Keywords_Processed": "volume render;raycaste;phong shading;pre integration",
    "Title": "Pre-Integrated Volume Rendering with Non-Linear Gradient Interpolation"
  },
  "257": {
    "Abstract": "We present a GPU-based ray-tracing system for the accurate and interactive visualization of cut-surfaces through 3D simulations of physical processes created from spectral\/hp high-order finite element methods. When used by the numerical analyst to debug the solver, the ability for the imagery to precisely reflect the data is critical. In practice, the investigator interactively selects from a palette of visualization tools to construct a scene that can answer a query of the data. This is effective as long as the implicit contract of image quality between the individual and the visualization system is upheld. OpenGL rendering of scientific visualizations has worked remarkably well for exploratory visualization for most solver results. This is due to the consistency between the use of first-order representations in the simulation and the linear assumptions inherent in OpenGL (planar fragments and color-space interpolation). Unfortunately, the contract is broken when the solver discretization is of higher-order. There have been attempts to mitigate this through the use of spatial adaptation and\/or texture mapping. These methods do a better job of approximating what the imagery should be but are not exact and tend to be view-dependent. This paper introduces new rendering mechanisms that specifically deal with the kinds of native data generated by high-order finite element solvers. The exploratory visualization tools are reassessed and cast in this system with the focus on image accuracy. This is accomplished in a GPU setting to ensure interactivity.",
    "Authors": "Nelson, B.;Haimes, R.;Kirby, R.M.",
    "Clusters": "GpuBasedTechniques;IsosurfaceAndSurfaceExtractionTechniques;NumericalMethodsMathematics",
    "DOI": "10.1109\/TVCG.2011.206",
    "Keywords": "gpu raytracing;high-order finite elements;gpu-based root-finding;cut-surface extraction;spectral\/hp elements;cut-plane extraction",
    "Keywords_Processed": "spectral hp element;gpu base root finding;cut plane extraction;high order finite element;gpu raytracing;cut surface extraction",
    "Title": "GPU-Based Interactive Cut-Surface Extraction From High-Order finite Element fields"
  },
  "132": {
    "Abstract": "In this paper, we present the DeepTree exhibit, a multi-user, multi-touch interactive visualization of the Tree of Life. We developed DeepTree to facilitate collaborative learning of evolutionary concepts. We will describe an iterative process in which a team of computer scientists, learning scientists, biologists, and museum curators worked together throughout design, development, and evaluation. We present the importance of designing the interactions and the visualization hand-in-hand in order to facilitate active learning. The outcome of this process is a fractal-based tree layout that reduces visual complexity while being able to capture all life on earth; a custom rendering and navigation engine that prioritizes visual appeal and smooth fly-through; and a multi-user interface that encourages collaborative exploration while offering guided discovery. We present an evaluation showing that the large dataset encouraged free exploration, triggers emotional responses, and facilitates visitor engagement and informal learning.",
    "Authors": "Block, F.;Horn, M.S.;Phillips, B.C.;Diamond, J.;Evans, E.M.;Chia Shen",
    "Clusters": "CollaborativeVisualization;Education;HierarchicalTreeDataAndTechniques;InteractionTechniquesGeneral",
    "DOI": "10.1109\/TVCG.2012.272",
    "Keywords": "multi-touch interaction;large tree visualizations;informal science education;collaborative learning",
    "Keywords_Processed": "collaborative learning;large tree visualization;informal science education;multi touch interaction",
    "Title": "The DeepTree Exhibit: Visualizing the Tree of Life to Facilitate Informal Learning"
  },
  "312": {
    "Abstract": "Many of the pressing questions in information visualization deal with how exactly a user reads a collection of visual marks as information about relationships between entities. Previous research has suggested that people see parts of a visualization as objects, and may metaphorically interpret apparent physical relationships between these objects as suggestive of data relationships. We explored this hypothesis in detail in a series of user experiments. Inspired by the concept of implied dynamics in psychology, we first studied whether perceived gravity acting on a mark in a scatterplot can lead to errors in a participant's recall of the mark's position. The results of this study suggested that such position errors exist, but may be more strongly influenced by attraction between marks. We hypothesized that such apparent attraction may be influenced by elements used to suggest relationship between objects, such as connecting lines, grouping elements, and visual similarity. We further studied what visual elements are most likely to cause this attraction effect, and whether the elements that best predicted attraction errors were also those which suggested conceptual relationships most strongly. Our findings show a correlation between attraction errors and intuitions about relatedness, pointing towards a possible mechanism by which the perception of visual marks becomes an interpretation of data relationships.",
    "Authors": "Ziemkiewicz, C.;Kosara, R.",
    "Clusters": "Cognition;LaboratoryStudies;VisualizationTheoryModelsAndMethods",
    "DOI": "10.1109\/TVCG.2010.174",
    "Keywords": "perceptual cognition;laboratory studies;visualization models;cognition theory",
    "Keywords_Processed": "laboratory study;perceptual cognition;visualization model;cognition theory",
    "Title": "Laws of Attraction: From Perceptual Forces to Conceptual Similarity"
  },
  "187": {
    "Abstract": "Eye movement analysis is gaining popularity as a tool for evaluation of visual displays and interfaces. However, the existing methods and tools for analyzing eye movements and scanpaths are limited in terms of the tasks they can support and effectiveness for large data and data with high variation. We have performed an extensive empirical evaluation of a broad range of visual analytics methods used in analysis of geographic movement data. The methods have been tested for the applicability to eye tracking data and the capability to extract useful knowledge about users' viewing behaviors. This allowed us to select the suitable methods and match them to possible analysis tasks they can support. The paper describes how the methods work in application to eye tracking data and provides guidelines for method selection depending on the analysis tasks.",
    "Authors": "Andrienko, G.;Andrienko, N.;Burch, M.;Weiskopf, D.",
    "Clusters": "AnimationAndMotion;EvaluationGeneral;",
    "DOI": "10.1109\/TVCG.2012.276",
    "Keywords": "trajectory analysis;movement data;eye tracking;visual analytics",
    "Keywords_Processed": "visual analytic;movement datum;eye tracking;trajectory analysis",
    "Title": "Visual Analytics Methodology for Eye Movement Studies"
  },
  "133": {
    "Abstract": "Current interfaces for common information visualizations such as bar graphs, line graphs, and scatterplots usually make use of the WIMP (Windows, Icons, Menus and a Pointer) interface paradigm with its frequently discussed problems of multiple levels of indirection via cascading menus, dialog boxes, and control panels. Recent advances in interface capabilities such as the availability of pen and touch interaction challenge us to re-think this and investigate more direct access to both the visualizations and the data they portray. We conducted a Wizard of Oz study to explore applying pen and touch interaction to the creation of information visualization interfaces on interactive whiteboards without implementing a plethora of recognizers. Our wizard acted as a robust and flexible pen and touch recognizer, giving participants maximum freedom in how they interacted with the system. Based on our qualitative analysis of the interactions our participants used, we discuss our insights about pen and touch interactions in the context of learnability and the interplay between pen and touch gestures. We conclude with suggestions for designing pen and touch enabled interactive visualization interfaces.",
    "Authors": "Walny, J.;Bongshin Lee;Johns, P.;Riche, N.H.;Carpendale, S.",
    "Clusters": "AnalysisProcessGeneral;EvaluationGeneral;HumanComputerInteractionHumanFactors;InteractionTechniquesGeneral",
    "DOI": "10.1109\/TVCG.2012.275",
    "Keywords": "data exploration;whiteboard;wizard of oz;pen and touch;interaction",
    "Keywords_Processed": "interaction;whiteboard;pen and touch;wizard of oz;datum exploration",
    "Title": "Understanding Pen and Touch Interaction for Data Exploration on Interactive Whiteboards"
  },
  "7": {
    "Abstract": "We present an assessment of the state and historic development of evaluation practices as reported in papers published at the IEEE Visualization conference. Our goal is to reflect on a meta-level about evaluation in our community through a systematic understanding of the characteristics and goals of presented evaluations. For this purpose we conducted a systematic review of ten years of evaluations in the published papers using and extending a coding scheme previously established by Lam et al. [2012]. The results of our review include an overview of the most common evaluation goals in the community, how they evolved over time, and how they contrast or align to those of the IEEE Information Visualization conference. In particular, we found that evaluations specific to assessing resulting images and algorithm performance are the most prevalent (with consistently 80-90% of all papers since 1997). However, especially over the last six years there is a steady increase in evaluation methods that include participants, either by evaluating their performances and subjective feedback or by evaluating their work practices and their improved analysis and reasoning capabilities using visual tools. Up to 2010, this trend in the IEEE Visualization conference was much more pronounced than in the IEEE Information Visualization conference which only showed an increasing percentage of evaluation through user performance and experience testing. Since 2011, however, also papers in IEEE Information Visualization show such an increase of evaluations of work practices and analysis as well as reasoning using visual tools. Further, we found that generally the studies reporting requirements analyses and domain-specific work practices are too informally reported which hinders cross-comparison and lowers external validity.",
    "Authors": "Isenberg, T.;Isenberg, P.;Jian Chen;Sedlmair, M.;Moller, T.",
    "Clusters": "EvaluationGeneral;",
    "DOI": "10.1109\/TVCG.2013.126",
    "Keywords": "validation;information visualization;scientific visualization;visualization;systematic review;evaluation",
    "Keywords_Processed": "visualization;validation;information visualization;systematic review;scientific visualization;evaluation",
    "Title": "A Systematic Review on the Practice of Evaluating Visualization"
  },
  "144": {
    "Abstract": "Existing methods for analyzing separation of streamlines are often restricted to a finite time or a local area. In our paper we introduce a new method that complements them by allowing an infinite-time-evaluation of steady planar vector fields. Our algorithm unifies combinatorial and probabilistic methods and introduces the concept of separation in time-discrete Markov-Chains. We compute particle distributions instead of the streamlines of single particles. We encode the flow into a map and then into a transition matrix for each time direction. Finally, we compare the results of our grid-independent algorithm to the popular Finite-Time-Lyapunov-Exponents and discuss the discrepancies.",
    "Authors": "Reich, W.;Scheuermann, G.",
    "Clusters": "AlgorithmicPatternFeatureDetectionTracking;FlowVisualizationDataAndTechniques;UncertaintyTechniquesAndVisualization;VectorFieldsDataAndTechniques",
    "DOI": "10.1109\/TVCG.2012.198",
    "Keywords": "flow visualization;vector field topology;uncertainty;feature extraction",
    "Keywords_Processed": "vector field topology;uncertainty;feature extraction;flow visualization",
    "Title": "Analysis of Streamline Separation at Infinity Using Time-Discrete Markov Chains"
  },
  "173": {
    "Abstract": "The study of aerosol composition for air quality research involves the analysis of high-dimensional single particle mass spectrometry data. We describe, apply, and evaluate a novel interactive visual framework for dimensionality reduction of such data. Our framework is based on non-negative matrix factorization with specifically defined regularization terms that aid in resolving mass spectrum ambiguity. Thereby, visualization assumes a key role in providing insight into and allowing to actively control a heretofore elusive data processing step, and thus enabling rapid analysis meaningful to domain scientists. In extending existing black box schemes, we explore design choices for visualizing, interacting with, and steering the factorization process to produce physically meaningful results. A domain-expert evaluation of our system performed by the air quality research experts involved in this effort has shown that our method and prototype admits the finding of unambiguous and physically correct lower-dimensional basis transformations of mass spectrometry data at significantly increased speed and a higher degree of ease.",
    "Authors": "Engel, D.;Greff, K.;Garth, C.;Bein, K.;Wexler, A.;Hamann, B.;Hagen, H.",
    "Clusters": "DimensionalityReduction;MatrixRelatedTechniques;MultidimensionalMultivariateMultifieldDataAndTechniques;PhysicsAndPhysicalSciences;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2012.280",
    "Keywords": "visual encodings of numerical error metrics;dimension reduction;matrix factorization;multi-dimensional visualization;mass spectrometry data",
    "Keywords_Processed": "multi dimensional visualization;mass spectrometry datum;matrix factorization;visual encoding of numerical error metric;dimension reduction",
    "Title": "Visual Steering and Verification of Mass Spectrometry Data Factorization in Air Quality Research"
  },
  "299": {
    "Abstract": "As people continue to author and share increasing amounts of information in social media, the opportunity to leverage such information for relationship discovery tasks increases. In this paper, we describe a set of systems that mine, aggregate, and infer a social graph from social media inside an enterprise, resulting in over 73 million relationships between 450,000 people. We then describe SaNDVis, a novel visual analytics tool that supports people-centric tasks like expertise location, team building, and team coordination in the enterprise. We also provide details of a 12-month-long, large-scale deployment to almost 1,800 users from which we extract dominant use cases from log and interview data. By integrating social position, evidence, and facets into SaNDVis, we demonstrate how users can use a visual analytics tool to reflect on existing relationships as well as build new relationships in an enterprise setting.",
    "Authors": "Perer, A.;Guy, I.;Uziel, E.;Ronen, I.;Jacovi, M.",
    "Clusters": "KnowledgeDiscovery;SocialNetworksAndSocialMedia",
    "DOI": "10.1109\/VAST.2011.6102443",
    "Keywords": "information discovery;social visualization;social data mining;social networks",
    "Keywords_Processed": "social visualization;information discovery;social datum mining;social network",
    "Title": "Visual social network analytics for relationship discovery in the enterprise"
  },
  "108": {
    "Abstract": "For information visualization researchers, eye tracking has been a useful tool to investigate research participants' underlying cognitive processes by tracking their eye movements while they interact with visual techniques. We used an eye tracker to better understand why participants with a variant of a tabular visualization called `SimulSort' outperformed ones with a conventional table and typical one-column sorting feature (i.e., Typical Sorting). The collected eye-tracking data certainly shed light on the detailed cognitive processes of the participants; SimulSort helped with decision-making tasks by promoting efficient browsing behavior and compensatory decision-making strategies. However, more interestingly, we also found unexpected eye-tracking patterns with Simul- Sort. We investigated the cause of the unexpected patterns through a crowdsourcing-based study (i.e., Experiment 2), which elicited an important limitation of the eye tracking method: incapability of capturing peripheral vision. This particular result would be a caveat for other visualization researchers who plan to use an eye tracker in their studies. In addition, the method to use a testing stimulus (i.e., influential column) in Experiment 2 to verify the existence of such limitations would be useful for researchers who would like to verify their eye tracking results.",
    "Authors": "Sung-Hee Kim;Zhihua Dong;Hanjun Xian;Upatising, B.;Ji Soo Yi",
    "Clusters": "EvaluationGeneral;Perception;QuantitativeEvaluation;ReasoningProblemSolvingAndDecisionMaking;",
    "DOI": "10.1109\/TVCG.2012.215",
    "Keywords": "visualized decision making;limitations;crowdsourcing;peripheral vision;eye tracking;quantitative empirical study",
    "Keywords_Processed": "visualize decision making;limitation;quantitative empirical study;eye tracking;peripheral vision;crowdsource",
    "Title": "Does an Eye Tracker Tell the Truth about Visualizations?: findings while Investigating Visualizations for Decision Making"
  },
  "26": {
    "Abstract": "For high-dimensional data, this work proposes two novel visual exploration methods to gain insights into the data aspect and the dimension aspect of the data. The first is a Dimension Projection Matrix, as an extension of a scatterplot matrix. In the matrix, each row or column represents a group of dimensions, and each cell shows a dimension projection (such as MDS) of the data with the corresponding dimensions. The second is a Dimension Projection Tree, where every node is either a dimension projection plot or a Dimension Projection Matrix. Nodes are connected with links and each child node in the tree covers a subset of the parent node's dimensions or a subset of the parent node's data items. While the tree nodes visualize the subspaces of dimensions or subsets of the data items under exploration, the matrix nodes enable cross-comparison between different combinations of subspaces. Both Dimension Projection Matrix and Dimension Project Tree can be constructed algorithmically through automation, or manually through user interaction. Our implementation enables interactions such as drilling down to explore different levels of the data, merging or splitting the subspaces to adjust the matrix, and applying brushing to select data clusters. Our method enables simultaneously exploring data correlation and dimension correlation for data with high dimensions.",
    "Authors": "Xiaoru Yuan;Donghao Ren;Zuchao Wang;Cong Guo",
    "Clusters": "DimensionalityReduction;HierarchicalTreeDataAndTechniques;InteractionTechniquesGeneral;MatrixRelatedTechniques;MultidimensionalMultivariateMultifieldDataAndTechniques",
    "DOI": "10.1109\/TVCG.2013.150",
    "Keywords": "user interaction;high-dimensional data;hierarchy visualization;matrix;sub-dimensional space;tree;subspace",
    "Keywords_Processed": "user interaction;matrix;high dimensional datum;hierarchy visualization;subspace;sub dimensional space;tree",
    "Title": "Dimension Projection Matrix\/Tree: Interactive Subspace Visual Exploration and Analysis of High Dimensional Data"
  },
  "378": {
    "Abstract": "In risk assessment applications well informed decisions are made based on huge amounts of multi-dimensional data. In many domains not only the risk of a wrong decision, but in particular the trade-off between the costs of possible decisions are of utmost importance. In this paper we describe a framework tightly integrating interactive visual exploration with machine learning to support the decision making process. The proposed approach uses a series of interactive 2D visualizations of numeric and ordinal data combined with visualization of classification models. These series of visual elements are further linked to the classifier's performance visualized using an interactive performance curve. An interactive decision point on the performance curve allows the decision maker to steer the classification model and instantly identify the critical, cost changing data elements, in the various linked visualizations. The critical data elements are represented as images in order to trigger associations related to the knowledge of the expert. In this context the data visualization and classification results are not only linked together, but are also linked back to the classification model. Such a visual analytics framework allows the user to interactively explore the costs of his decisions for different settings of the model and accordingly use the most suitable classification model and make more informed and reliable decisions. A case study on data from the Forensic Psychiatry domain reveals the usefulness of the suggested approach.",
    "Authors": "Migut, M.;Worring, M.",
    "Clusters": "AnalysisProcessGeneral;MachineLearningAndStatistics;MultidimensionalMultivariateMultifieldDataAndTechniques;SegmentationAndClassification;",
    "DOI": "10.1109\/VAST.2010.5652398",
    "Keywords": "decision boundary visualization;classification;multi-dimensional space;interactive visual exploration;visual analytics",
    "Keywords_Processed": "visual analytic;interactive visual exploration;classification;multi dimensional space;decision boundary visualization",
    "Title": "Visual exploration of classification models for risk assessment"
  },
  "106": {
    "Abstract": "Design studies are an increasingly popular form of problem-driven visualization research, yet there is little guidance available about how to do them effectively. In this paper we reflect on our combined experience of conducting twenty-one design studies, as well as reading and reviewing many more, and on an extensive literature review of other field work methods and methodologies. Based on this foundation we provide definitions, propose a methodological framework, and provide practical guidance for conducting design studies. We define a design study as a project in which visualization researchers analyze a specific real-world problem faced by domain experts, design a visualization system that supports solving this problem, validate the design, and reflect about lessons learned in order to refine visualization design guidelines. We characterize two axes - a task clarity axis from fuzzy to crisp and an information location axis from the domain expert's head to the computer - and use these axes to reason about design study contributions, their suitability, and uniqueness from other approaches. The proposed methodological framework consists of 9 stages: learn, winnow, cast, discover, design, implement, deploy, reflect, and write. For each stage we provide practical guidance and outline potential pitfalls. We also conducted an extensive literature survey of related methodological approaches that involve a significant amount of qualitative field work, and compare design study methodology to that of ethnography, grounded theory, and action research.",
    "Authors": "Sedlmair, M.;Meyer, M.;Munzner, T.",
    "Clusters": "DesignMethodologiesAndInteractionDesign;DesignStudiesAndCaseStudies;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/TVCG.2012.213",
    "Keywords": "methodology;visualization;design study;framework",
    "Keywords_Processed": "visualization;design study;framework;methodology",
    "Title": "Design Study Methodology: Reflections from the Trenches and the Stacks"
  },
  "71": {
    "Abstract": "We present a study of linear interpolation when applied to uncertain data. Linear interpolation is a key step for isosurface extraction algorithms, and the uncertainties in the data lead to non-linear variations in the geometry of the extracted isosurface. We present an approach for deriving the probability density function of a random variable modeling the positional uncertainty in the isosurface extraction. When the uncertainty is quantified by a uniform distribution, our approach provides a closed-form characterization of the mentioned random variable. This allows us to derive, in closed form, the expected value as well as the variance of the level-crossing position. While the former quantity is used for constructing a stable isosurface for uncertain data, the latter is used for visualizing the positional uncertainties in the expected isosurface level crossings on the underlying grid.",
    "Authors": "Athawale, T.;Entezari, A.",
    "Clusters": "Interpolation;IsosurfaceAndSurfaceExtractionTechniques;UncertaintyTechniquesAndVisualization",
    "DOI": "10.1109\/TVCG.2013.208",
    "Keywords": "isosurface extraction;uncertainty quantification;linear interpolation;marching cubes",
    "Keywords_Processed": "uncertainty quantification;march cube;linear interpolation;isosurface extraction",
    "Title": "Uncertainty Quantification in Linear Interpolation for Isosurface Extraction"
  },
  "99": {
    "Abstract": "In written and spoken communications, figures of speech (e.g., metaphors and synecdoche) are often used as an aid to help convey abstract or less tangible concepts. However, the benefits of using rhetorical illustrations or embellishments in visualization have so far been inconclusive. In this work, we report an empirical study to evaluate hypotheses that visual embellishments may aid memorization, visual search and concept comprehension. One major departure from related experiments in the literature is that we make use of a dual-task methodology in our experiment. This design offers an abstraction of typical situations where viewers do not have their full attention focused on visualization (e.g., in meetings and lectures). The secondary task introduces \u00e2\u20ac\u0153divided attention\u00e2\u20ac\u009d, and makes the effects of visual embellishments more observable. In addition, it also serves as additional masking in memory-based trials. The results of this study show that visual embellishments can help participants better remember the information depicted in visualization. On the other hand, visual embellishments can have a negative impact on the speed of visual search. The results show a complex pattern as to the benefits of visual embellishments in helping participants grasp key concepts from visualization.",
    "Authors": "Borgo, R.;Abdul-Rahman, A.;Mohamed, F.;Grant, P.W.;Reppa, I.;Floridi, L.;Chen, M.",
    "Clusters": "Cognition;EvaluationGeneral;GlyphsGlyphBasedTechniques;QueriesAndSearch;VisualDesignDesignGuidelines;VisualizationTheoryModelsAndMethods",
    "DOI": "10.1109\/TVCG.2012.197",
    "Keywords": "evaluation;icons;cognition;working memory;visual embellishments;visual search;long-term memory;metaphors",
    "Keywords_Processed": "long term memory;icon;visual search;work memory;cognition;metaphor;visual embellishment;evaluation",
    "Title": "An Empirical Study on Using Visual Embellishments in Visualization"
  },
  "215": {
    "Abstract": "Providing effective feedback on resource consumption in the home is a key challenge of environmental conservation efforts. One promising approach for providing feedback about residential energy consumption is the use of ambient and artistic visualizations. Pervasive computing technologies enable the integration of such feedback into the home in the form of distributed point-of-consumption feedback devices to support decision-making in everyday activities. However, introducing these devices into the home requires sensitivity to the domestic context. In this paper we describe three abstract visualizations and suggest four design requirements that this type of device must meet to be effective: pragmatic, aesthetic, ambient, and ecological. We report on the findings from a mixed methods user study that explores the viability of using ambient and artistic feedback in the home based on these requirements. Our findings suggest that this approach is a viable way to provide resource use feedback and that both the aesthetics of the representation and the context of use are important elements that must be considered in this design space.",
    "Authors": "Rodgers, J.;Bartram, L.",
    "Clusters": "AmbientVisualization;ArtAndAestheticsInVisualization;DistributedSystemsAndGridEnvironments;EarthSpaceAndEnvironmentalSciences;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2011.196",
    "Keywords": "ambient visualization;informative art;casual information visualization;sustainability;distributed visualization",
    "Keywords_Processed": "ambient visualization;distribute visualization;casual information visualization;informative art;sustainability",
    "Title": "Exploring Ambient and Artistic Visualization for Residential Energy Use Feedback"
  },
  "30": {
    "Abstract": "Biological pathway maps are highly relevant tools for many tasks in molecular biology. They reduce the complexity of the overall biological network by partitioning it into smaller manageable parts. While this reduction of complexity is their biggest strength, it is, at the same time, their biggest weakness. By removing what is deemed not important for the primary function of the pathway, biologists lose the ability to follow and understand cross-talks between pathways. Considering these cross-talks is, however, critical in many analysis scenarios, such as judging effects of drugs. In this paper we introduce Entourage, a novel visualization technique that provides contextual information lost due to the artificial partitioning of the biological network, but at the same time limits the presented information to what is relevant to the analyst's task. We use one pathway map as the focus of an analysis and allow a larger set of contextual pathways. For these context pathways we only show the contextual subsets, i.e., the parts of the graph that are relevant to a selection. Entourage suggests related pathways based on similarities and highlights parts of a pathway that are interesting in terms of mapped experimental data. We visualize interdependencies between pathways using stubs of visual links, which we found effective yet not obtrusive. By combining this approach with visualization of experimental data, we can provide domain experts with a highly valuable tool. We demonstrate the utility of Entourage with case studies conducted with a biochemist who researches the effects of drugs on pathways. We show that the technique is well suited to investigate interdependencies between pathways and to analyze, understand, and predict the effect that drugs have on different cell types.",
    "Authors": "Lex, A.;Partl, C.;Kalkofen, D.;Streit, M.;Gratzl, S.;Wassermann, A.M.;Schmalstieg, D.;Pfister, H.",
    "Clusters": "BiologyAndBioinformatics;GraphNetworkDataAndTechniques;MolecularScienceAndChemistry;SetRelatedDataTechniques",
    "DOI": "10.1109\/TVCG.2013.154",
    "Keywords": "pathway visualization;subsets;biomolecular data;graph;biological networks",
    "Keywords_Processed": "pathway visualization;biological network;graph;subset;biomolecular datum",
    "Title": "Entourage: Visualizing Relationships between Biological Pathways using Contextual Subsets"
  },
  "186": {
    "Abstract": "Visual analytics emphasizes the interplay between visualization, analytical procedures performed by computers and human perceptual and cognitive activities. Human reasoning is an important element in this context. There are several theories in psychology and HCI explaining open-ended and exploratory reasoning. Five of these theories (sensemaking theories, gestalt theories, distributed cognition, graph comprehension theories and skill-rule-knowledge models) are described in this paper. We discuss their relevance for visual analytics. In order to do this more systematically, we developed a schema of categories relevant for visual analytics research and evaluation. All these theories have strengths but also weaknesses in explaining interaction with visual analytics systems. A possibility to overcome the weaknesses would be to combine two or more of these theories.",
    "Authors": "Pohl, M.;Smuc, M.;Mayr, E.",
    "Clusters": "Cognition;DesignMethodologiesAndInteractionDesign;KnowledgeDiscovery;ReasoningProblemSolvingAndDecisionMaking",
    "DOI": "10.1109\/TVCG.2012.273",
    "Keywords": "cognitive theory;problem solving;interaction design;visual knowledge discovery;reasoning",
    "Keywords_Processed": "reason;problem solve;visual knowledge discovery;interaction design;cognitive theory",
    "Title": "The User Puzzle---Explaining the Interaction with Visual Analytics Systems"
  },
  "39": {
    "Abstract": "We present a first investigation into hybrid-image visualization for data analysis in large-scale viewing environments. Hybrid-image visualizations blend two different visual representations into a single static view, such that each representation can be perceived at a different viewing distance. Our work is motivated by data analysis scenarios that incorporate one or more displays with sufficiently large size and resolution to be comfortably viewed by different people from various distances. Hybrid-image visualizations can be used, in particular, to enhance overview tasks from a distance and detail-in-context tasks when standing close to the display. By using a perception-based blending approach, hybrid-image visualizations make two full-screen visualizations accessible without tracking viewers in front of a display. We contribute a design space, discuss the perceptual rationale for our work, provide examples, and introduce a set of techniques and tools to aid the design of hybrid-image visualizations.",
    "Authors": "Isenberg, P.;Dragicevic, P.;Willett, W.;Bezerianos, A.;Fekete, J.",
    "Clusters": "CollaborativeVisualization;ImageBasedDataImageSignalProcessing;LargeAndHighResDisplays;MultiScaleDataTechniques;",
    "DOI": "10.1109\/TVCG.2013.163",
    "Keywords": "large displays;hybrid images;collaboration;visualization;multi-scale",
    "Keywords_Processed": "visualization;hybrid image;collaboration;multi scale;large display",
    "Title": "Hybrid-Image Visualization for Large Viewing Environments"
  },
  "271": {
    "Abstract": "The field of visualization has addressed navigation of very large datasets, usually meshes and volumes. Significantly less attention has been devoted to the issues surrounding navigation of very large images. In the last few years the explosive growth in the resolution of camera sensors and robotic image acquisition techniques has widened the gap between the display and image resolutions to three orders of magnitude or more. This paper presents the first steps towards navigation of very large images, particularly landscape images, from an interactive visualization perspective. The grand challenge in navigation of very large images is identifying regions of potential interest. In this paper we outline a three-step approach. In the first step we use multi-scale saliency to narrow down the potential areas of interest. In the second step we outline a method based on statistical signatures to further cull out regions of high conformity. In the final step we allow a user to interactively identify the exceptional regions of high interest that merit further attention. We show that our approach of progressive elicitation is fast and allows rapid identification of regions of interest. Unlike previous work in this area, our approach is scalable and computationally reasonable on very large images. We validate the results of our approach by comparing them to user-tagged regions of interest on several very large landscape images from the Internet.",
    "Authors": "Cheuk Yiu Ip;Varshney, A.",
    "Clusters": "EventsTrendsOutlierDetectionAnalysisAndVisualization;ImageBasedDataImageSignalProcessing;InteractionTechniquesGeneral;LargeScaleDataAndScalability;Perception",
    "DOI": "10.1109\/TVCG.2011.231",
    "Keywords": "image saliency;interactive visualization;guided interaction;scene perception;anomaly detection;very large scale images",
    "Keywords_Processed": "image saliency;very large scale image;guide interaction;scene perception;interactive visualization;anomaly detection",
    "Title": "Saliency-Assisted Navigation of Very Large Landscape Images"
  },
  "201": {
    "Abstract": "The relationship between candidates' position on a ballot paper and vote rank is explored in the case of 5000 candidates for the UK 2010 local government elections in the Greater London area. This design study uses hierarchical spatially arranged graphics to represent two locations that affect candidates at very different scales: the geographical areas for which they seek election and the spatial location of their names on the ballot paper. This approach allows the effect of position bias to be assessed; that is, the degree to which the position of a candidate's name on the ballot paper influences the number of votes received by the candidate, and whether this varies geographically. Results show that position bias was significant enough to influence rank order of candidates, and in the case of many marginal electoral wards, to influence who was elected to government. Position bias was observed most strongly for Liberal Democrat candidates but present for all major political parties. Visual analysis of classification of candidate names by ethnicity suggests that this too had an effect on votes received by candidates, in some cases overcoming alphabetic name bias. The results found contradict some earlier research suggesting that alphabetic name bias was not sufficiently significant to affect electoral outcome and add new evidence for the geographic and ethnicity influences on voting behaviour. The visual approach proposed here can be applied to a wider range of electoral data and the patterns identified and hypotheses derived from them could have significant implications for the design of ballot papers and the conduct of fair elections.",
    "Authors": "Wood, J.;Badawood, D.;Dykes, J.;Slingsby, A.",
    "Clusters": "ApplicationsGeneralAndOther;Cognition;GeographyGeospatialVisCartographyTerrainVis;HierarchicalTreeDataAndTechniques;SocialScienceAndHumanities",
    "DOI": "10.1109\/TVCG.2011.174",
    "Keywords": "democracy;hierarchy;election;bias;treemap;governance;voting;geovisualization",
    "Keywords_Processed": "hierarchy;treemap;governance;election;democracy;geovisualization;bias;voting",
    "Title": "BallotMaps: Detecting Name Bias in Alphabetically Ordered Ballot Papers"
  },
  "217": {
    "Abstract": "Multivariate data visualization is a classic topic, for which many solutions have been proposed, each with its own strengths and weaknesses. In standard solutions the structure of the visualization is fixed, we explore how to give the user more freedom to define visualizations. Our new approach is based on the usage of Flexible Linked Axes: The user is enabled to define a visualization by drawing and linking axes on a canvas. Each axis has an associated attribute and range, which can be adapted. Links between pairs of axes are used to show data in either scatter plot- or Parallel Coordinates Plot-style. Flexible Linked Axes enable users to define a wide variety of different visualizations. These include standard methods, such as scatter plot matrices, radar charts, and PCPs [11]; less well known approaches, such as Hyperboxes [1], TimeWheels [17], and many-to-many relational parallel coordinate displays [14]; and also custom visualizations, consisting of combinations of scatter plots and PCPs. Furthermore, our method allows users to define composite visualizations that automatically support brushing and linking. We have discussed our approach with ten prospective users, who found the concept easy to understand and highly promising.",
    "Authors": "Claessen, J.H.T.;van Wijk, J.J.",
    "Clusters": "ChartsDiagramsPlots;MultidimensionalMultivariateMultifieldDataAndTechniques;ParallelCoordinates;",
    "DOI": "10.1109\/TVCG.2011.201",
    "Keywords": "multivariate data;visualization;scatterplot;parallel coordinates plot",
    "Keywords_Processed": "multivariate datum;scatterplot;parallel coordinate plot;visualization",
    "Title": "Flexible Linked Axes for Multivariate Data Visualization"
  },
  "213": {
    "Abstract": "Node-link diagrams are an effective and popular visualization approach for depicting hierarchical structures and for showing parent-child relationships. In this paper, we present the results of an eye tracking experiment investigating traditional, orthogonal, and radial node-link tree layouts as a piece of empirical basis for choosing between those layouts. Eye tracking was used to identify visual exploration behaviors of participants that were asked to solve a typical hierarchy exploration task by inspecting a static tree diagram: finding the least common ancestor of a given set of marked leaf nodes. To uncover exploration strategies, we examined fixation points, duration, and saccades of participants' gaze trajectories. For the non-radial diagrams, we additionally investigated the effect of diagram orientation by switching the position of the root node to each of the four main orientations. We also recorded and analyzed correctness of answers as well as completion times in addition to the eye movement data. We found out that traditional and orthogonal tree layouts significantly outperform radial tree layouts for the given task. Furthermore, by applying trajectory analysis techniques we uncovered that participants cross-checked their task solution more often in the radial than in the non-radial layouts.",
    "Authors": "Burch, M.;Konevtsova, N.;Heinrich, J.;Hoeferlin, M.;Weiskopf, D.",
    "Clusters": "EvaluationGeneral;GraphNetworkDataAndTechniques;HierarchicalTreeDataAndTechniques",
    "DOI": "10.1109\/TVCG.2011.193",
    "Keywords": "user study;hierarchy visualization;eye tracking;node-link layout",
    "Keywords_Processed": "node link layout;hierarchy visualization;eye tracking;user study",
    "Title": "Evaluation of Traditional, Orthogonal, and Radial Tree Diagrams by an Eye Tracking Study"
  },
  "380": {
    "Abstract": "Visualization of multi-dimensional data is challenging due to the number of complex correlations that may be present in the data but that are difficult to be visually identified. One of the main causes for this problem is the inherent loss of information that occurs when high-dimensional data is projected into 2D or 3D. Although 2D scatterplots are ubiquitous due to their simplicity and familiarity, there are not a lot of variations on their basic metaphor. In this paper, we present a new way of visualizing multidimensional data using scatterplots. We extend 2D scatterplots using sensitivity coefficients to highlight local variation of one variable with respect to another. When applied to a scatterplot, these sensitivities can be understood as velocities, and the resulting visualization resembles a flow field. We also present a number of operations, based on flow-field analysis, that help users navigate, select and cluster points in an efficient manner. We show the flexibility and generality of this approach using a number of multidimensional data sets across different domains.",
    "Authors": "Yu-Hsuan Chan;Correa, C.;Kwan-Liu Ma",
    "Clusters": "DataTransformation;DimensionalityReduction;MachineLearningAndStatistics;UncertaintyTechniquesAndVisualization",
    "DOI": "10.1109\/VAST.2010.5652460",
    "Keywords": "uncertainty;data transformation;principal component analysis;model fitting",
    "Keywords_Processed": "datum transformation;uncertainty;model fit;principal component analysis",
    "Title": "Flow-based scatterplots for sensitivity analysis"
  },
  "18": {
    "Abstract": "This paper presents ConnectomeExplorer, an application for the interactive exploration and query-guided visual analysis of large volumetric electron microscopy (EM) data sets in connectomics research. Our system incorporates a knowledge-based query algebra that supports the interactive specification of dynamically evaluated queries, which enable neuroscientists to pose and answer domain-specific questions in an intuitive manner. Queries are built step by step in a visual query builder, building more complex queries from combinations of simpler queries. Our application is based on a scalable volume visualization framework that scales to multiple volumes of several teravoxels each, enabling the concurrent visualization and querying of the original EM volume, additional segmentation volumes, neuronal connectivity, and additional meta data comprising a variety of neuronal data attributes. We evaluate our application on a data set of roughly one terabyte of EM data and 750 GB of segmentation data, containing over 4,000 segmented structures and 1,000 synapses. We demonstrate typical use-case scenarios of our collaborators in neuroscience, where our system has enabled them to answer specific scientific questions using interactive querying and analysis on the full-size data for the first time.",
    "Authors": "Beyer, J.;Al-Awami, A.;Kasthuri, N.;Lichtman, J.;Pfister, H.;Hadwiger, M.",
    "Clusters": "KnowledgeDiscovery;LargeScaleDataAndScalability;NeurosciencesAndBrainVisualization;QueriesAndSearch",
    "DOI": "10.1109\/TVCG.2013.142",
    "Keywords": "query algebra;petascale volume analysis;neuroscience;connectomics;visual knowledge discovery",
    "Keywords_Processed": "neuroscience;petascale volume analysis;query algebra;connectomic;visual knowledge discovery",
    "Title": "ConnectomeExplorer: Query-Guided Visual Analysis of Large Volumetric Neuroscience Data"
  },
  "68": {
    "Abstract": "Spatial organization has been proposed as a compelling approach to externalizing the sensemaking process. However, there are two ways in which space can be provided to the user: by creating a physical workspace that the user can interact with directly, such as can be provided by a large, high-resolution display, or through the use of a virtual workspace that the user navigates using virtual navigation techniques such as zoom and pan. In this study we explicitly examined the use of spatial sensemaking techniques within these two environments. The results demonstrate that these two approaches to providing sensemaking space are not equivalent, and that the greater embodiment afforded by the physical workspace changes how the space is perceived and used, leading to increased externalization of the sensemaking process.",
    "Authors": "Andrews, C.;North, C.",
    "Clusters": "Cognition;LargeAndHighResDisplays;ZoomingAndNavigationTechniques",
    "DOI": "10.1109\/TVCG.2013.205",
    "Keywords": "embodiment;sensemaking;physical navigation;visual analytics;large and high-resolution display",
    "Keywords_Processed": "large and high resolution display;sensemake;visual analytic;physical navigation;embodiment",
    "Title": "The Impact of Physical Navigation on Spatial Organization for Sensemaking"
  },
  "270": {
    "Abstract": "We present a quasi interpolation framework that attains the optimal approximation-order of Voronoi splines for reconstruction of volumetric data sampled on general lattices. The quasi interpolation framework of Voronoi splines provides an unbiased reconstruction method across various lattices. Therefore this framework allows us to analyze and contrast the sampling-theoretic performance of general lattices, using signal reconstruction, in an unbiased manner. Our quasi interpolation methodology is implemented as an efficient FIR filter that can be applied online or as a preprocessing step. We present visual and numerical experiments that demonstrate the improved accuracy of reconstruction across lattices, using the quasi interpolation framework.",
    "Authors": "Mirzargar, M.;Entezari, A.",
    "Clusters": "CurvesAndCurvature;Interpolation;VolumeRenderingModelingAndVisualization;VoronoiBasedTechniques",
    "DOI": "10.1109\/TVCG.2011.230",
    "Keywords": "quasi interpolation;volume visualization;box spline;voronoi spline",
    "Keywords_Processed": "box spline;voronoi spline;quasi interpolation;volume visualization",
    "Title": "Quasi Interpolation With Voronoi Splines"
  },
  "363": {
    "Abstract": "In many applications of Direct Volume Rendering (DVR) the importance of a certain material or feature is highly dependent on its relative spatial location. For instance, in the medical diagnostic procedure, the patient's symptoms often lead to specification of features, tissues and organs of particular interest. One such example is pockets of gas which, if found inside the body at abnormal locations, are a crucial part of a diagnostic visualization. This paper presents an approach that enhances DVR transfer function design with spatial localization based on user specified material dependencies. Semantic expressions are used to define conditions based on relations between different materials, such as only render iodine uptake when close to liver. The underlying methods rely on estimations of material distributions which are acquired by weighing local neighborhoods of the data against approximations of material likelihood functions. This information is encoded and used to influence rendering according to the user's specifications. The result is improved focus on important features by allowing the user to suppress spatially less-important data. In line with requirements from actual clinical DVR practice, the methods do not require explicit material segmentation that would be impossible or prohibitively time-consuming to achieve in most real cases. The scheme scales well to higher dimensions which accounts for multi-dimensional transfer functions and multivariate data. Dual-Energy Computed Tomography, an important new modality in radiology, is used to demonstrate this scalability. In several examples we show significantly improved focus on clinically important aspects in the rendered images.",
    "Authors": "Lindholm, S.;Ljung, P.;Lundstrom, C.;Persson, A.;Ynnerman, A.",
    "Clusters": "GraphNetworkDataAndTechniques;SpaceRelatedSpatialDataAndTechniques;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2010.195",
    "Keywords": "direct volume rendering;spatial conditioning;neighborhood meta-data;transfer function",
    "Keywords_Processed": "transfer function;neighborhood meta datum;spatial conditioning;direct volume render",
    "Title": "Spatial Conditioning of Transfer Functions Using Local Material Distributions"
  },
  "334": {
    "Abstract": "Most images used in visualization are computed with the planar pinhole camera. This classic camera model has important advantages such as simplicity, which enables efficient software and hardware implementations, and similarity to the human eye, which yields images familiar to the user. However, the planar pinhole camera has only a single viewpoint, which limits images to parts of the scene to which there is direct line of sight. In this paper we introduce the curved ray camera to address the single viewpoint limitation. Rays are C1-continuous curves that bend to circumvent occluders. Our camera is designed to provide a fast 3-D point projection operation, which enables interactive visualization. The camera supports both 3-D surface and volume datasets. The camera is a powerful tool that enables seamless integration of multiple perspectives for overcoming occlusions in visualization while minimizing distortions.",
    "Authors": "Jian Cui;Rosen, P.;Popescu, V.;Hoffmann, C.",
    "Clusters": "CamerasCameraViewsAndProjections;InteractionTechniquesGeneral;OcclusionProblemsTechniques;RaytracingRaycasting;ViewDependentVisualization",
    "DOI": "10.1109\/TVCG.2010.127",
    "Keywords": "interactive visualization;curved rays;camera model;multiperspective visualization;alleviating occlusions",
    "Keywords_Processed": "alleviate occlusion;multiperspective visualization;camera model;curve ray;interactive visualization",
    "Title": "A Curved Ray Camera for Handling Occlusions through Continuous Multiperspective Visualization"
  },
  "273": {
    "Abstract": "Study of symmetric or repeating patterns in scalar fields is important in scientific data analysis because it gives deep insights into the properties of the underlying phenomenon. Though geometric symmetry has been well studied within areas like shape processing, identifying symmetry in scalar fields has remained largely unexplored due to the high computational cost of the associated algorithms. We propose a computationally efficient algorithm for detecting symmetric patterns in a scalar field distribution by analysing the topology of level sets of the scalar field. Our algorithm computes the contour tree of a given scalar field and identifies subtrees that are similar. We define a robust similarity measure for comparing subtrees of the contour tree and use it to group similar subtrees together. Regions of the domain corresponding to subtrees that belong to a common group are extracted and reported to be symmetric. Identifying symmetry in scalar fields finds applications in visualization, data exploration, and feature detection. We describe two applications in detail: symmetry-aware transfer function design and symmetry-aware isosurface extraction.",
    "Authors": "Thomas, D.M.;Natarajan, V.",
    "Clusters": "DataAndAnalysisMetrics;IsosurfaceAndSurfaceExtractionTechniques;ScalarFieldDataTechniques;TopologyBasedTechniques;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2011.236",
    "Keywords": "transfer function design;contour tree;isosurface extraction;similarity measure;scalar field symmetry;persistence",
    "Keywords_Processed": "contour tree;similarity measure;isosurface extraction;persistence;scalar field symmetry;transfer function design",
    "Title": "Symmetry in Scalar field Topology"
  },
  "32": {
    "Abstract": "Conflicting results are reported in the literature on whether dynamic visualizations are more effective than static visualizations for learning and mastering 3-D tasks, and only a few investigations have considered the influence of the spatial abilities of the learners. In a study with 117 participants, we compared the benefit of static vs. dynamic visualization training tools on learners with different spatial abilities performing a typical 3-D task (specifically, creating orthographic projections of a 3-D object). We measured the spatial abilities of the participants using the Mental Rotation Test (MRT) and classified participants into two groups (high and low abilities) to examine how the participants' abilities predicted change in performance after training with static versus dynamic training tools. Our results indicate that: 1) visualization training programs can help learners to improve 3-D task performance, 2) dynamic visualizations provide no advantages over static visualizations that show intermediate steps, 3) training programs are more beneficial for individuals with low spatial abilities than for individuals with high spatial abilities, and 4) training individuals with high spatial abilities using dynamic visualizations provides little benefit.",
    "Authors": "Froese, M.-E.;Tory, M.;Evans, G.-W.;Shrikhande, K.",
    "Clusters": "ApplicationsGeneralAndOther;CamerasCameraViewsAndProjections;Cognition;EvaluationGeneral;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2013.156",
    "Keywords": "orthographic projection;training;cad;3d visualization;spatial ability;evaluation",
    "Keywords_Processed": "evaluation;training;3d visualization;spatial ability;cad;orthographic projection",
    "Title": "Evaluation of Static and Dynamic Visualization Training Approaches for Users with Different Spatial Abilities"
  },
  "265": {
    "Abstract": "The unguided visual exploration of volumetric data can be both a challenging and a time-consuming undertaking. Identifying a set of favorable vantage points at which to start exploratory expeditions can greatly reduce this effort and can also ensure that no important structures are being missed. Recent research efforts have focused on entropy-based viewpoint selection criteria that depend on scalar values describing the structures of interest. In contrast, we propose a viewpoint suggestion pipeline that is based on feature-clustering in high-dimensional space. We use gradient\/normal variation as a metric to identify interesting local events and then cluster these via k-means to detect important salient composite features. Next, we compute the maximum possible exposure of these composite feature for different viewpoints and calculate a 2D entropy map parameterized in longitude and latitude to point out promising view orientations. Superimposed onto an interactive track-ball interface, users can then directly use this entropy map to quickly navigate to potentially interesting viewpoints where visibility-based transfer functions can be employed to generate volume renderings that minimize occlusions. To give full exploration freedom to the user, the entropy map is updated on the fly whenever a view has been selected, pointing to new and promising but so far unseen view directions. Alternatively, our system can also use a set-cover optimization algorithm to provide a minimal set of views needed to observe all features. The views so generated could then be saved into a list for further inspection or into a gallery for a summary presentation.",
    "Authors": "Ziyi Zheng;Ahmed, N.;Mueller, K.",
    "Clusters": "BiologyAndBioinformatics;DataClusteringAndAggregation;InformationTheory;NumericalMethodsMathematics;ViewDependentVisualization;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2011.218",
    "Keywords": "ant colony optimization;k-means;entropy;direct volume rendering;view suggestion;set-cover problem",
    "Keywords_Processed": "entropy;mean;view suggestion;set cover problem;direct volume render;ant colony optimization",
    "Title": "iView: A Feature Clustering Framework for Suggesting Informative Views in Volume Visualization"
  },
  "366": {
    "Abstract": "Symmetric second-order tensor fields play a central role in scientific and biomedical studies as well as in image analysis and feature-extraction methods. The utility of displaying tensor field samples has driven the development of visualization techniques that encode the tensor shape and orientation into the geometry of a tensor glyph. With some exceptions, these methods work only for positive-definite tensors (i.e. having positive eigenvalues, such as diffusion tensors). We expand the scope of tensor glyphs to all symmetric second-order tensors in two and three dimensions, gracefully and unambiguously depicting any combination of positive and negative eigenvalues. We generalize a previous method of superquadric glyphs for positive-definite tensors by drawing upon a larger portion of the superquadric shape space, supplemented with a coloring that indicates the tensor's quadratic form. We show that encoding arbitrary eigenvalue sign combinations requires design choices that differ fundamentally from those in previous work on traceless tensors (arising in the study of liquid crystals). Our method starts with a design of 2-D tensor glyphs guided by principles of symmetry and continuity, and creates 3-D glyphs that include the 2-D glyphs in their axis-aligned cross-sections. A key ingredient of our method is a novel way of mapping from the shape space of three-dimensional symmetric second-order tensors to the unit square. We apply our new glyphs to stress tensors from mechanics, geometry tensors and Hessians from image analysis, and rate-of-deformation tensors in computational fluid dynamics.",
    "Authors": "Schultz, T.;Kindlmann, G.",
    "Clusters": "GlyphsGlyphBasedTechniques;TensorDataAndTechniques",
    "DOI": "10.1109\/TVCG.2010.199",
    "Keywords": "stress tensor;glyph design;geometry tensors;rate-of-deformation tensors;tensor glyphs",
    "Keywords_Processed": "stress tensor;geometry tensor;tensor glyph;glyph design;rate of deformation tensor",
    "Title": "Superquadric Glyphs for Symmetric Second-Order Tensors"
  },
  "95": {
    "Abstract": "Recently there has been increasing research interest in displaying graphs with curved edges to produce more readable visualizations. While there are several automatic techniques, little has been done to evaluate their effectiveness empirically. In this paper we present two experiments studying the impact of edge curvature on graph readability. The goal is to understand the advantages and disadvantages of using curved edges for common graph tasks compared to straight line segments, which are the conventional choice for showing edges in node-link diagrams. We included several edge variations: straight edges, edges with different curvature levels, and mixed straight and curved edges. During the experiments, participants were asked to complete network tasks including determination of connectivity, shortest path, node degree, and common neighbors. We also asked the participants to provide subjective ratings of the aesthetics of different edge types. The results show significant performance differences between the straight and curved edges and clear distinctions between variations of curved edges.",
    "Authors": "Kai Xu;Rooney, C.;Passmore, P.;Dong-Han Ham;Nguyen, P.H.",
    "Clusters": "CurvesAndCurvature;EvaluationGeneral;GraphNetworkDataAndTechniques;",
    "DOI": "10.1109\/TVCG.2012.189",
    "Keywords": "visualization;curved edges;evaluation;graph",
    "Keywords_Processed": "visualization;graph;curve edge;evaluation",
    "Title": "A User Study on Curved Edges in Graph Visualization"
  },
  "251": {
    "Abstract": "Multi-valued data sets are increasingly common, with the number of dimensions growing. A number of multi-variate visualization techniques have been presented to display such data. However, evaluating the utility of such techniques for general data sets remains difficult. Thus most techniques are studied on only one data set. Another criticism that could be levied against previous evaluations of multi-variate visualizations is that the task doesn't require the presence of multiple variables. At the same time, the taxonomy of tasks that users may perform visually is extensive. We designed a task, trend localization, that required comparison of multiple data values in a multi-variate visualization. We then conducted a user study with this task, evaluating five multivariate visualization techniques from the literature (Brush Strokes, Data-Driven Spots, Oriented Slivers, Color Blending, Dimensional Stacking) and juxtaposed grayscale maps. We report the results and discuss the implications for both the techniques and the task.",
    "Authors": "Livingston, M.A.;Decker, J.W.",
    "Clusters": "EvaluationGeneral;MultidimensionalMultivariateMultifieldDataAndTechniques;TasksTaskRequirementsAnalysis;",
    "DOI": "10.1109\/TVCG.2011.194",
    "Keywords": "user study;multivariate visualization;visual task design;visual analytics",
    "Keywords_Processed": "multivariate visualization;visual analytic;user study;visual task design",
    "Title": "Evaluation of Trend Localization with Multi-Variate Visualizations"
  },
  "10": {
    "Abstract": "We present ambient scattering as a preintegration method for scattering on mesoscopic scales in direct volume rendering. Far-range scattering effects usually provide negligible contributions to a given location due to the exponential attenuation with increasing distance. This motivates our approach to preintegrating multiple scattering within a finite spherical region around any given sample point. To this end, we solve the full light transport with a Monte-Carlo simulation within a set of spherical regions, where each region may have different material parameters regarding anisotropy and extinction. This precomputation is independent of the data set and the transfer function, and results in a small preintegration table. During rendering, the look-up table is accessed for each ray sample point with respect to the viewing direction, phase function, and material properties in the spherical neighborhood of the sample. Our rendering technique is efficient and versatile because it readily fits in existing ray marching algorithms and can be combined with local illumination and volumetric ambient occlusion. It provides interactive volumetric scattering and soft shadows, with interactive control of the transfer function, anisotropy parameter of the phase function, lighting conditions, and viewpoint. A GPU implementation demonstrates the benefits of ambient scattering for the visualization of different types of data sets, with respect to spatial perception, high-quality illumination, translucency, and rendering speed.",
    "Authors": "Ament, M.;Sadlo, F.;Weiskopf, D.",
    "Clusters": "Illumination;Rendering;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2013.129",
    "Keywords": "gradient-free shading;volume illumination;direct volume rendering;preintegrated light transport;ambient scattering",
    "Keywords_Processed": "direct volume render;preintegrate light transport;ambient scattering;gradient free shading;volume illumination",
    "Title": "Ambient Volume Scattering"
  },
  "377": {
    "Abstract": "In this paper we present World Lines as a novel interactive visualization that provides complete control over multiple heterogeneous simulation runs. In many application areas, decisions can only be made by exploring alternative scenarios. The goal of the suggested approach is to support users in this decision making process. In this setting, the data domain is extended to a set of alternative worlds where only one outcome will actually happen. World Lines integrate simulation, visualization and computational steering into a single unified system that is capable of dealing with the extended solution space. World Lines represent simulation runs as causally connected tracks that share a common time axis. This setup enables users to interfere and add new information quickly. A World Line is introduced as a visual combination of user events and their effects in order to present a possible future. To quickly find the most attractive outcome, we suggest World Lines as the governing component in a system of multiple linked views and a simulation component. World Lines employ linking and brushing to enable comparative visual analysis of multiple simulations in linked views. Analysis results can be mapped to various visual variables that World Lines provide in order to highlight the most compelling solutions. To demonstrate this technique we present a flooding scenario and show the usefulness of the integrated approach to support informed decision making.",
    "Authors": "Waser, J.;Fuchs, R.;Ribicic, H.;Schindler, B.;Bloschl, G.;Groller, E.",
    "Clusters": "ApplicationsGeneralAndOther;FlowVisualizationDataAndTechniques;ParticleVisualizationAndTechniques;ReasoningProblemSolvingAndDecisionMaking;Simulation",
    "DOI": "10.1109\/TVCG.2010.223",
    "Keywords": "decision making;simulation steering;computational fluid dynamics;smoothed particle hydrodynamics;parallel worlds;problem solving environments",
    "Keywords_Processed": "simulation steering;decision make;smoothed particle hydrodynamic;problem solve environment;computational fluid dynamic;parallel world",
    "Title": "World Lines"
  },
  "324": {
    "Abstract": "Electronic test and measurement systems are becoming increasingly sophisticated in order to match the increased complexity and ultra-high speed of the devices under test. A key feature in many such instruments is a vastly increased capacity for storage of digital signals. Storage of 109 time points or more is now possible. At the same time, the typical screens on such measurement devices are relatively small. Therefore, these instruments can only render an extremely small fraction of the complete signal at any time. SignalLens uses a Focus+Context approach to provide a means of navigating to and inspecting low-level signal details in the context of the entire signal trace. This approach provides a compact visualization suitable for embedding into the small displays typically provided by electronic measurement instruments. We further augment this display with computed tracks which display time-aligned computed properties of the signal. By combining and filtering these computed tracks it is possible to easily and quickly find computationally detected features in the data which are often obscured by the visual compression required to render the large data sets on a small screen. Further, these tracks can be viewed in the context of the entire signal trace as well as visible high-level signal features. Several examples using real-world electronic measurement data are presented, which demonstrate typical use cases and the effectiveness of the design.",
    "Authors": "Kincaid, R.",
    "Clusters": "EvaluationGeneral;FocusContextTechniques;ImageBasedDataImageSignalProcessing;PhysicsAndPhysicalSciences",
    "DOI": "10.1109\/TVCG.2010.193",
    "Keywords": "lens;electronic signal;test and measurement;focus+context;signal processing",
    "Keywords_Processed": "electronic signal;signal processing;test and measurement;lens;focus context",
    "Title": "SignalLens: Focus+Context Applied to Electronic Time Series"
  },
  "124": {
    "Abstract": "For many applications involving time series data, people are often interested in the changes of item values over time as well as their ranking changes. For example, people search many words via search engines like Google and Bing every day. Analysts are interested in both the absolute searching number for each word as well as their relative rankings. Both sets of statistics may change over time. For very large time series data with thousands of items, how to visually present ranking changes is an interesting challenge. In this paper, we propose RankExplorer, a novel visualization method based on ThemeRiver to reveal the ranking changes. Our method consists of four major components: 1) a segmentation method which partitions a large set of time series curves into a manageable number of ranking categories; 2) an extended ThemeRiver view with embedded color bars and changing glyphs to show the evolution of aggregation values related to each ranking category over time as well as the content changes in each ranking category; 3) a trend curve to show the degree of ranking changes over time; 4) rich user interactions to support interactive exploration of ranking changes. We have applied our method to some real time series data and the case studies demonstrate that our method can reveal the underlying patterns related to ranking changes which might otherwise be obscured in traditional visualizations.",
    "Authors": "Conglei Shi;Weiwei Cui;Shixia Liu;Panpan Xu;Wei Chen;Huamin Qu",
    "Clusters": "InteractionTechniquesGeneral;Ranking;TimeseriesTimeVaryingDataAndTechniques;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2012.253",
    "Keywords": "time-series data;ranking change;themeriver;interaction",
    "Keywords_Processed": "interaction;rank change;time series datum;themeriver",
    "Title": "RankExplorer: Visualization of Ranking Changes in Large Time Series Data"
  },
  "261": {
    "Abstract": "Large scale and structurally complex volume datasets from high-resolution 3D imaging devices or computational simulations pose a number of technical challenges for interactive visual analysis. In this paper, we present the first integration of a multiscale volume representation based on tensor approximation within a GPU-accelerated out-of-core multiresolution rendering framework. Specific contributions include (a) a hierarchical brick-tensor decomposition approach for pre-processing large volume data, (b) a GPU accelerated tensor reconstruction implementation exploiting CUDA capabilities, and (c) an effective tensor-specific quantization strategy for reducing data transfer bandwidth and out-of-core memory footprint. Our multiscale representation allows for the extraction, analysis and display of structural features at variable spatial scales, while adaptive level-of-detail rendering methods make it possible to interactively explore large datasets within a constrained memory footprint. The quality and performance of our prototype system is evaluated on large structurally complex datasets, including gigabyte-sized micro-tomographic volumes.",
    "Authors": "Suter, S.K.;Iglesias Guitian, J.A.;Marton, F.;Agus, M.;Elsener, A.;Zollikofer, C.P.E.;Gopi, M.;Gobbetti, E.;Pajarola, R.",
    "Clusters": "GpuBasedTechniques;MultiScaleDataTechniques;Rendering;TensorDataAndTechniques;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2011.214",
    "Keywords": "multi-scale;multi-resolution rendering;interactive volume visualization;gpu\/cuda;tensor reconstruction",
    "Keywords_Processed": "tensor reconstruction;multi resolution render;multi scale;interactive volume visualization;gpu cuda",
    "Title": "Interactive Multiscale Tensor Reconstruction for Multiresolution Volume Visualization"
  },
  "258": {
    "Abstract": "Percutaneous radiofrequency ablation (RFA) is becoming a standard minimally invasive clinical procedure for the treatment of liver tumors. However, planning the applicator placement such that the malignant tissue is completely destroyed, is a demanding task that requires considerable experience. In this work, we present a fast GPU-based real-time approximation of the ablation zone incorporating the cooling effect of liver vessels. Weighted distance fields of varying RF applicator types are derived from complex numerical simulations to allow a fast estimation of the ablation zone. Furthermore, the heat-sink effect of the cooling blood flow close to the applicator's electrode is estimated by means of a preprocessed thermal equilibrium representation of the liver parenchyma and blood vessels. Utilizing the graphics card, the weighted distance field incorporating the cooling blood flow is calculated using a modular shader framework, which facilitates the real-time visualization of the ablation zone in projected slice views and in volume rendering. The proposed methods are integrated in our software assistant prototype for planning RFA therapy. The software allows the physician to interactively place virtual RF applicator models. The real-time visualization of the corresponding approximated ablation zone facilitates interactive evaluation of the tumor coverage in order to optimize the applicator's placement such that all cancer cells are destroyed by the ablation.",
    "Authors": "Rieder, C.;Kroeger, T.;Schumann, C.;Hahn, H.K.",
    "Clusters": "BiomedicalScienceAndMedicine;EarthSpaceAndEnvironmentalSciences;GpuBasedTechniques;InteractionTechniquesGeneral;VisualizationTechniquesAndToolsGeneral;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2011.207",
    "Keywords": "volume rendering;distance field;ablation zone visualization;gpu;radiofrequency ablation;interaction",
    "Keywords_Processed": "interaction;volume render;distance field;radiofrequency ablation;ablation zone visualization;gpu",
    "Title": "GPU-based Real-Time Approximation of the Ablation Zone for Radiofrequency Ablation"
  },
  "161": {
    "Abstract": "Planetary topography is the result of complex interactions between geological processes, of which faulting is a prominent component. Surface-rupturing earthquakes cut and move landforms which develop across active faults, producing characteristic surface displacements across the fault. Geometric models of faults and their associated surface displacements are commonly applied to reconstruct these offsets to enable interpretation of the observed topography. However, current 2D techniques are limited in their capability to convey both the three-dimensional kinematics of faulting and the incremental sequence of events required by a given reconstruction. Here we present a real-time system for interactive retro-deformation of faulted topography to enable reconstruction of fault displacement within a high-resolution (sub 1m\/pixel) 3D terrain visualization. We employ geometry shaders on the GPU to intersect the surface mesh with fault-segments interactively specified by the user and transform the resulting surface blocks in realtime according to a kinematic model of fault motion. Our method facilitates a human-in-the-loop approach to reconstruction of fault displacements by providing instant visual feedback while exploring the parameter space. Thus, scientists can evaluate the validity of traditional point-to-point reconstructions by visually examining a smooth interpolation of the displacement in 3D. We show the efficacy of our approach by using it to reconstruct segments of the San Andreas fault, California as well as a graben structure in the Noctis Labyrinthus region on Mars.",
    "Authors": "Westerteiger, R.;Compton, T.;Bernadin, T.;Cowgill, E.;Gwinner, K.;Hamann, B.;Gerndt, A.;Hagen, H.",
    "Clusters": "GeographyGeospatialVisCartographyTerrainVis;InteractionTechniquesGeneral;MeshesGridsAndLattices;Simulation",
    "DOI": "10.1109\/TVCG.2012.239",
    "Keywords": "mesh deformation;terrain rendering;fault simulation;interactive",
    "Keywords_Processed": "mesh deformation;interactive;terrain render;fault simulation",
    "Title": "Interactive Retro-Deformation of Terrain for Reconstructing 3D Fault Displacements"
  },
  "135": {
    "Abstract": "Uncertainty can arise in any stage of a visual analytics process, especially in data-intensive applications with a sequence of data transformations. Additionally, throughout the process of multidimensional, multivariate data analysis, uncertainty due to data transformation and integration may split, merge, increase, or decrease. This dynamic characteristic along with other features of uncertainty pose a great challenge to effective uncertainty-aware visualization. This paper presents a new framework for modeling uncertainty and characterizing the evolution of the uncertainty information through analytical processes. Based on the framework, we have designed a visual metaphor called uncertainty flow to visually and intuitively summarize how uncertainty information propagates over the whole analysis pipeline. Our system allows analysts to interact with and analyze the uncertainty information at different levels of detail. Three experiments were conducted to demonstrate the effectiveness and intuitiveness of our design.",
    "Authors": "Yingcai Wu;Guo-Xun Yuan;Kwan-Liu Ma",
    "Clusters": "UncertaintyTechniquesAndVisualization",
    "DOI": "10.1109\/TVCG.2012.285",
    "Keywords": "error ellipsoids;uncertainty visualization;uncertainty propagation;uncertainty fusion;uncertainty quantification",
    "Keywords_Processed": "uncertainty fusion;uncertainty quantification;error ellipsoid;uncertainty visualization;uncertainty propagation",
    "Title": "Visualizing Flow of Uncertainty through Analytical Processes"
  },
  "275": {
    "Abstract": "Blood flow and derived data are essential to investigate the initiation and progression of cerebral aneurysms as well as their risk of rupture. An effective visual exploration of several hemodynamic attributes like the wall shear stress (WSS) and the inflow jet is necessary to understand the hemodynamics. Moreover, the correlation between focus-and-context attributes is of particular interest. An expressive visualization of these attributes and anatomic information requires appropriate visualization techniques to minimize visual clutter and occlusions. We present the FLOWLENS as a focus-and-context approach that addresses these requirements. We group relevant hemodynamic attributes to pairs of focus-and-context attributes and assign them to different anatomic scopes. For each scope, we propose several FLOWLENS visualization templates to provide a flexible visual filtering of the involved hemodynamic pairs. A template consists of the visualization of the focus attribute and the additional depiction of the context attribute inside the lens. Furthermore, the FLOWLENS supports local probing and the exploration of attribute changes over time. The FLOWLENS minimizes visual cluttering, occlusions, and provides a flexible exploration of a region of interest. We have applied our approach to seven representative datasets, including steady and unsteady flow data from CFD simulations and 4D PC-MRI measurements. Informal user interviews with three domain experts confirm the usefulness of our approach.",
    "Authors": "Gasteiger, R.;Neugebauer, M.;Beuing, O.;Preim, B.",
    "Clusters": "BiomedicalScienceAndMedicine;FlowVisualizationDataAndTechniques;FocusContextTechniques;IllustrativeVisualization",
    "DOI": "10.1109\/TVCG.2011.243",
    "Keywords": "aneurysm;focus+context;flow visualization;illustrative rendering",
    "Keywords_Processed": "illustrative render;aneurysm;focus context;flow visualization",
    "Title": "The FLOWLENS: A Focus-and-Context Visualization Approach for Exploration of Blood Flow in Cerebral Aneurysms"
  },
  "291": {
    "Abstract": "There are a growing number of machine learning algorithms which operate on graphs. Example applications for these algorithms include predicting which customers will recommend products to their friends in a viral marketing campaign using a customer network, predicting the topics of publications in a citation network, or predicting the political affiliations of people in a social network. It is important for an analyst to have tools to help compare the output of these machine learning algorithms. In this work, we present G-PARE, a visual analytic tool for comparing two uncertain graphs, where each uncertain graph is produced by a machine learning algorithm which outputs probabilities over node labels. G-PARE provides several different views which allow users to obtain a global overview of the algorithms output, as well as focused views that show subsets of nodes of interest. By providing an adaptive exploration environment, G-PARE guides the users to places in the graph where two algorithms predictions agree and places where they disagree. This enables the user to follow cascades of misclassifications by comparing the algorithms outcome with the ground truth. After describing the features of G-PARE, we illustrate its utility through several use cases based on networks from different domains.",
    "Authors": "Sharara, H.;Sopan, A.;Namata, G.;Getoor, L.;Singh, L.",
    "Clusters": "ComparisonComparativeVisualizationAndSimilarity;MachineLearningAndStatistics;UncertaintyTechniquesAndVisualization",
    "DOI": "10.1109\/VAST.2011.6102442",
    "Keywords": "uncertain graphs;uncertainty visualization;comparative analysis;model comparison",
    "Keywords_Processed": "comparative analysis;uncertain graph;uncertainty visualization;model comparison",
    "Title": "G-PARE: A visual analytic tool for comparative analysis of uncertain graphs"
  },
  "54": {
    "Abstract": "In many applications, data tables contain multi-valued attributes that often store the memberships of the table entities to multiple sets such as which languages a person masters, which skills an applicant documents, or which features a product comes with. With a growing number of entities, the resulting element-set membership matrix becomes very rich of information about how these sets overlap. Many analysis tasks targeted at set-typed data are concerned with these overlaps as salient features of such data. This paper presents Radial Sets, a novel visual technique to analyze set memberships for a large number of elements. Our technique uses frequency-based representations to enable quickly finding and analyzing different kinds of overlaps between the sets, and relating these overlaps to other attributes of the table entities. Furthermore, it enables various interactions to select elements of interest, find out if they are over-represented in specific sets or overlaps, and if they exhibit a different distribution for a specific attribute compared to the rest of the elements. These interactions allow formulating highly-expressive visual queries on the elements in terms of their set memberships and attribute values. As we demonstrate via two usage scenarios, Radial Sets enable revealing and analyzing a multitude of overlapping patterns between large sets, beyond the limits of state-of-the-art techniques.",
    "Authors": "Alsallakh, B.;Aigner, W.;Miksch, S.;Hauser, H.",
    "Clusters": "LargeScaleDataAndScalability;MultidimensionalMultivariateMultifieldDataAndTechniques;SetRelatedDataTechniques;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2013.184",
    "Keywords": "overlapping sets;visualization technique;multi-valued attributes;scalability;set-typed data",
    "Keywords_Processed": "multi value attribute;visualization technique;scalability;set type datum;overlap set",
    "Title": "Radial Sets: Interactive Visual Analysis of Large Overlapping Sets"
  },
  "63": {
    "Abstract": "We introduce a visual analytics method to analyze eye movement data recorded for dynamic stimuli such as video or animated graphics. The focus lies on the analysis of data of several viewers to identify trends in the general viewing behavior, including time sequences of attentional synchrony and objects with strong attentional focus. By using a space-time cube visualization in combination with clustering, the dynamic stimuli and associated eye gazes can be analyzed in a static 3D representation. Shot-based, spatiotemporal clustering of the data generates potential areas of interest that can be filtered interactively. We also facilitate data drill-down: the gaze points are shown with density-based color mapping and individual scan paths as lines in the space-time cube. The analytical process is supported by multiple coordinated views that allow the user to focus on different aspects of spatial and temporal information in eye gaze data. Common eye-tracking visualization techniques are extended to incorporate the spatiotemporal characteristics of the data. For example, heat maps are extended to motion-compensated heat maps and trajectories of scan paths are included in the space-time visualization. Our visual analytics approach is assessed in a qualitative users study with expert users, which showed the usefulness of the approach and uncovered that the experts applied different analysis strategies supported by the system.",
    "Authors": "Kurzhals, K.;Weiskopf, D.",
    "Clusters": "DataClusteringAndAggregation;DesignMethodologiesAndInteractionDesign;EvaluationGeneral;SpatiotemporalDataAndTechniques;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2013.194",
    "Keywords": "spatio-temporal clustering;space-time cube;motion-compensated heatmap;eye tracking;dynamic areas of interest",
    "Keywords_Processed": "space time cube;motion compensate heatmap;eye tracking;dynamic area of interest;spatio temporal clustering",
    "Title": "Space-Time Visual Analytics of Eye-Tracking Data for Dynamic Stimuli"
  },
  "278": {
    "Abstract": "A fundamental challenge for time-varying volume data analysis and visualization is the lack of capability to observe and track data change or evolution in an occlusion-free, controllable, and adaptive fashion. In this paper, we propose to organize a timevarying data set into a hierarchy of states. By deriving transition probabilities among states, we construct a global map that captures the essential transition relationships in the time-varying data. We introduce the TransGraph, a graph-based representation to visualize hierarchical state transition relationships. The TransGraph not only provides a visual mapping that abstracts data evolution over time in different levels of detail, but also serves as a navigation tool that guides data exploration and tracking. The user interacts with the TransGraph and makes connection to the volumetric data through brushing and linking. A set of intuitive queries is provided to enable knowledge extraction from time-varying data. We test our approach with time-varying data sets of different characteristics and the results show that the TransGraph can effectively augment our ability in understanding time-varying data.",
    "Authors": "Yi Gu;Chaoli Wang",
    "Clusters": "HierarchicalTreeDataAndTechniques;StateRelatedDataTechniques;TimeseriesTimeVaryingDataAndTechniques;TransitionsAndMorphing;UserInterfacesGeneral",
    "DOI": "10.1109\/TVCG.2011.246",
    "Keywords": "user interface;transition relationship;time-varying visualization;hierarchical representation;states",
    "Keywords_Processed": "transition relationship;user interface;hierarchical representation;time vary visualization;state",
    "Title": "TransGraph: Hierarchical Exploration of Transition Relationships in Time-Varying Volumetric Data"
  },
  "318": {
    "Abstract": "Statistical data associated with geographic regions is nowadays globally available in large amounts and hence automated methods to visually display these data are in high demand. There are several well-established thematic map types for quantitative data on the ratio-scale associated with regions: choropleth maps, cartograms, and proportional symbol maps. However, all these maps suffer from limitations, especially if large data values are associated with small regions. To overcome these limitations, we propose a novel type of quantitative thematic map, the necklace map. In a necklace map, the regions of the underlying two-dimensional map are projected onto intervals on a one-dimensional curve (the necklace) that surrounds the map regions. Symbols are scaled such that their area corresponds to the data of their region and placed without overlap inside the corresponding interval on the necklace. Necklace maps appear clear and uncluttered and allow for comparatively large symbol sizes. They visualize data sets well which are not proportional to region sizes. The linear ordering of the symbols along the necklace facilitates an easy comparison of symbol sizes. One map can contain several nested or disjoint necklaces to visualize clustered data. The advantages of necklace maps come at a price: the association between a symbol and its region is weaker than with other types of maps. Interactivity can help to strengthen this association if necessary. We present an automated approach to generate necklace maps which allows the user to interactively control the final symbol placement. We validate our approach with experiments using various data sets and maps.",
    "Authors": "Speckmann, B.;Verbeek, K.",
    "Clusters": "GeographyGeospatialVisCartographyTerrainVis;Maps;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2010.180",
    "Keywords": "necklace maps;automated cartography;geographic visualization;proportional symbol maps",
    "Keywords_Processed": "geographic visualization;necklace map;automate cartography;proportional symbol map",
    "Title": "Necklace Maps"
  },
  "361": {
    "Abstract": "Graphics artists commonly employ physically-based simulation for the generation of effects such as smoke, explosions, and similar phenomena. The task of finding the correct parameters for a desired result, however, is difficult and time-consuming as current tools provide little to no guidance. In this paper, we present a new approach for the visual exploration of such parameter spaces. Given a three-dimensional scene description, we utilize sampling and spatio-temporal clustering techniques to generate a concise overview of the achievable variations and their temporal evolution. Our visualization system then allows the user to explore the simulation space in a goal-oriented manner. Animation sequences with a set of desired characteristics can be composed using a novel search-by-example approach and interactive direct volume rendering is employed to provide instant visual feedback.",
    "Authors": "Bruckner, S.;Moller, T.",
    "Clusters": "AnalysisProcessGeneral;ApplicationsGeneralAndOther;DataClusteringAndAggregation;TimeseriesTimeVaryingDataAndTechniques",
    "DOI": "10.1109\/TVCG.2010.190",
    "Keywords": "visual exploration;visual effects;clustering;time-dependent volume data",
    "Keywords_Processed": "time dependent volume datum;clustering;visual effect;visual exploration",
    "Title": "Result-Driven Exploration of Simulation Parameter Spaces for Visual Effects Design"
  },
  "162": {
    "Abstract": "This paper presents the first volume visualization system that scales to petascale volumes imaged as a continuous stream of high-resolution electron microscopy images. Our architecture scales to dense, anisotropic petascale volumes because it: (1) decouples construction of the 3D multi-resolution representation required for visualization from data acquisition, and (2) decouples sample access time during ray-casting from the size of the multi-resolution hierarchy. Our system is designed around a scalable multi-resolution virtual memory architecture that handles missing data naturally, does not pre-compute any 3D multi-resolution representation such as an octree, and can accept a constant stream of 2D image tiles from the microscopes. A novelty of our system design is that it is visualization-driven: we restrict most computations to the visible volume data. Leveraging the virtual memory architecture, missing data are detected during volume ray-casting as cache misses, which are propagated backwards for on-demand out-of-core processing. 3D blocks of volume data are only constructed from 2D microscope image tiles when they have actually been accessed during ray-casting. We extensively evaluate our system design choices with respect to scalability and performance, compare to previous best-of-breed systems, and illustrate the effectiveness of our system for real microscopy data from neuroscience.",
    "Authors": "Hadwiger, M.;Beyer, J.;Won-Ki Jeong;Pfister, H.",
    "Clusters": "ApplicationsGeneralAndOther;LargeScaleDataAndScalability;Microscopy;NeurosciencesAndBrainVisualization",
    "DOI": "10.1109\/TVCG.2012.240",
    "Keywords": "high-throughput imaging;high-resolution microscopy;neuroscience;petascale volume exploration",
    "Keywords_Processed": "petascale volume exploration;high throughput imaging;high resolution microscopy;neuroscience",
    "Title": "Interactive Volume Exploration of Petascale Microscopy Data Streams Using a Visualization-Driven Virtual Memory Approach"
  },
  "304": {
    "Abstract": "We investigate the design of declarative, domain-specific languages for constructing interactive visualizations. By separating specification from execution, declarative languages can simplify development, enable unobtrusive optimization, and support retargeting across platforms. We describe the design of the Protovis specification language and its implementation within an object-oriented, statically-typed programming language (Java). We demonstrate how to support rich visualizations without requiring a toolkit-specific data model and extend Protovis to enable declarative specification of animated transitions. To support cross-platform deployment, we introduce rendering and event-handling infrastructures decoupled from the runtime platform, letting designers retarget visualization specifications (e.g., from desktop to mobile phone) with reduced effort. We also explore optimizations such as runtime compilation of visualization specifications, parallelized execution, and hardware-accelerated rendering. We present benchmark studies measuring the performance gains provided by these optimizations and compare performance to existing Java-based visualization tools, demonstrating scalability improvements exceeding an order of magnitude.",
    "Authors": "Heer, J.;Bostock, M.",
    "Clusters": "Optimization;ProgrammingAlgorithmsAndDataStructures;UserInterfacesGeneral;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/TVCG.2010.144",
    "Keywords": "information visualization;user interface;optimization;toolkits;declarative languages;domain-specific languages",
    "Keywords_Processed": "optimization;user interface;domain specific language;information visualization;declarative language;toolkit",
    "Title": "Declarative Language Design for Interactive Visualization"
  },
  "331": {
    "Abstract": "Understanding the diversity of a set of multivariate objects is an important problem in many domains, including ecology, college admissions, investing, machine learning, and others. However, to date, very little work has been done to help users achieve this kind of understanding. Visual representation is especially appealing for this task because it offers the potential to allow users to efficiently observe the objects of interest in a direct and holistic way. Thus, in this paper, we attempt to formalize the problem of visualizing the diversity of a large (more than 1000 objects), multivariate (more than 5 attributes) data set as one worth deeper investigation by the information visualization community. In doing so, we contribute a precise definition of diversity, a set of requirements for diversity visualizations based on this definition, and a formal user study design intended to evaluate the capacity of a visual representation for communicating diversity information. Our primary contribution, however, is a visual representation, called the Diversity Map, for visualizing diversity. An evaluation of the Diversity Map using our study design shows that users can judge elements of diversity consistently and as or more accurately than when using the only other representation specifically designed to visualize diversity.",
    "Authors": "Pham, T.;Hess, R.;Ju, C.;Zhang, E.;Metoyer, R.",
    "Clusters": "CategoricalDataAndTechniques;DataFeaturesAndAttributes;EvaluationGeneral;MultidimensionalMultivariateMultifieldDataAndTechniques;",
    "DOI": "10.1109\/TVCG.2010.216",
    "Keywords": "information visualization;categorical data;diversity;multivariate data;evaluation",
    "Keywords_Processed": "multivariate datum;information visualization;categorical datum;diversity;evaluation",
    "Title": "Visualization of Diversity in Large Multivariate Data Sets"
  },
  "3": {
    "Abstract": "Many application domains deal with multi-variate data that consist of both categorical and numerical information. Small-multiple displays are a powerful concept for comparing such data by juxtaposition. For comparison by overlay or by explicit encoding of computed differences, however, a specification of references is necessary. In this paper, we present a formal model for defining semantically meaningful comparisons between many categories in a small-multiple display. Based on pivotized data that are hierarchically partitioned by the categories assigned to the x and y axis of the display, we propose two alternatives for structure-based comparison within this hierarchy. With an absolute reference specification, categories are compared to a fixed reference category. With a relative reference specification, in contrast, a semantic ordering of the categories is considered when comparing them either to the previous or subsequent category each. Both reference specifications can be defined at multiple levels of the hierarchy (including aggregated summaries), enabling a multitude of useful comparisons. We demonstrate the general applicability of our model in several application examples using different visualizations that compare data by overlay or explicit encoding of differences.",
    "Authors": "Kehrer, J.;Piringer, H.;Berger, W.;Groller, E.",
    "Clusters": "CategoricalDataAndTechniques;ComparisonComparativeVisualizationAndSimilarity;VisualEncodingAndLayoutGeneral;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2013.122",
    "Keywords": "trellis displays;categorical data;comparative visualization;small multiples",
    "Keywords_Processed": "trelli display;comparative visualization;small multiple;categorical datum",
    "Title": "A Model for Structure-Based Comparison of Many Categories in Small-Multiple Displays"
  },
  "364": {
    "Abstract": "Special relativistic visualization offers the possibility of experiencing the optical effects of traveling near the speed of light, including apparent geometric distortions as well as Doppler and searchlight effects. Early high-quality computer graphics images of relativistic scenes were created using offline, computationally expensive CPU-side 4D ray tracing. Alternate approaches such as image-based rendering and polygon-distortion methods are able to achieve interactivity, but exhibit inferior visual quality due to sampling artifacts. In this paper, we introduce a hybrid rendering technique based on polygon distortion and local ray tracing that facilitates interactive high-quality visualization of multiple objects moving at relativistic speeds in arbitrary directions. The method starts by calculating tight image-space footprints for the apparent triangles of the 3D scene objects. The final image is generated using a single image-space ray tracing step incorporating Doppler and searchlight effects. Our implementation uses GPU shader programming and hardware texture filtering to achieve high rendering speed.",
    "Authors": "M\u00c3\u00bcller, T.;Grottel, S.;Weiskopf, D.",
    "Clusters": "ApplicationsGeneralAndOther;GpuBasedTechniques;Illumination;NumericalMethodsMathematics;PhysicsAndPhysicalSciences",
    "DOI": "10.1109\/TVCG.2010.196",
    "Keywords": "aberration of light;illumination;gpu raytracing;poincare transformation;doppler effect;searchlight effect;special relativity",
    "Keywords_Processed": "searchlight effect;special relativity;illumination;gpu raytracing;aberration of light;doppler effect;poincare transformation",
    "Title": "Special Relativistic Visualization by Local Ray Tracing"
  },
  "298": {
    "Abstract": "We have observed increasing interest in visual analytics tools and their applications in investigative analysis. Despite the growing interest and substantial studies regarding the topic, understanding the major roadblocks of using such tools from novice users' perspectives is still limited. Therefore, we attempted to identify such \u00e2\u20ac\u0153visual analytic roadblocks\u00e2\u20ac\u009d for novice users in an investigative analysis scenario. To achieve this goal, we reviewed the existing models, theories, and frameworks that could explain the cognitive processes of human-visualization interaction in investigative analysis. Then, we conducted a qualitative experiment with six novice participants, using a slightly modified version of pair analytics, and analyzed the results through the open-coding method. As a result, we came up with four visual analytic roadblocks and explained these roadblocks using existing cognitive models and theories. We also provided design suggestions to overcome these roadblocks.",
    "Authors": "Bum Chul Kwon;Fisher, B.;Ji Soo Yi",
    "Clusters": "AnalysisProcessGeneral;Cognition;QualitativeEvaluation;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/VAST.2011.6102435",
    "Keywords": "qualitative experiment;visual analytics;investigative analysis;cognitive model;roadblock;framework",
    "Keywords_Processed": "investigative analysis;framework;qualitative experiment;visual analytic;cognitive model;roadblock",
    "Title": "Visual analytic roadblocks for novice investigators"
  },
  "369": {
    "Abstract": "Most multidimensional projection techniques rely on distance (dissimilarity) information between data instances to embed high-dimensional data into a visual space. When data are endowed with Cartesian coordinates, an extra computational effort is necessary to compute the needed distances, making multidimensional projection prohibitive in applications dealing with interactivity and massive data. The novel multidimensional projection technique proposed in this work, called Part-Linear Multidimensional Projection (PLMP), has been tailored to handle multivariate data represented in Cartesian high-dimensional spaces, requiring only distance information between pairs of representative samples. This characteristic renders PLMP faster than previous methods when processing large data sets while still being competitive in terms of precision. Moreover, knowing the range of variation for data instances in the high-dimensional space, we can make PLMP a truly streaming data projection technique, a trait absent in previous methods.",
    "Authors": "Paulovich, F.V.;Silva, C.T.;Nonato, L.G.",
    "Clusters": "DatabasesAndDataMining;DimensionalityReduction;StreamingDataAndTechniques",
    "DOI": "10.1109\/TVCG.2010.207",
    "Keywords": "dimension reduction;visual data mining;projection methods;streaming technique",
    "Keywords_Processed": "stream technique;projection method;dimension reduction;visual datum mining",
    "Title": "Two-Phase Mapping for Projecting Massive Data Sets"
  },
  "142": {
    "Abstract": "Geoscientific modeling and simulation helps to improve our understanding of the complex Earth system. During the modeling process, validation of the geoscientific model is an essential step. In validation, it is determined whether the model output shows sufficient agreement with observation data. Measures for this agreement are called goodness of fit. In the geosciences, analyzing the goodness of fit is challenging due to its manifold dependencies: 1) The goodness of fit depends on the model parameterization, whose precise values are not known. 2) The goodness of fit varies in space and time due to the spatio-temporal dimension of geoscientific models. 3) The significance of the goodness of fit is affected by resolution and preciseness of available observational data. 4) The correlation between goodness of fit and underlying modeled and observed values is ambiguous. In this paper, we introduce a visual analysis concept that targets these challenges in the validation of geoscientific models - specifically focusing on applications where observation data is sparse, unevenly distributed in space and time, and imprecise, which hinders a rigorous analytical approach. Our concept, developed in close cooperation with Earth system modelers, addresses the four challenges by four tailored visualization components. The tight linking of these components supports a twofold interactive drill-down in model parameter space and in the set of data samples, which facilitates the exploration of the numerous dependencies of the goodness of fit. We exemplify our visualization concept for geoscientific modeling of glacial isostatic adjustments in the last 100,000 years, validated against sea levels indicators - a prominent example for sparse and imprecise observation data. An initial use case and feedback from Earth system modelers indicate that our visualization concept is a valuable complement to the range of validation methods.",
    "Authors": "Unger, A.;Schulte, S.;Klemann, V.;Dransch, D.",
    "Clusters": "EarthSpaceAndEnvironmentalSciences;MachineLearningAndStatistics;MultipleLinkedCoordinatedViews;SpatiotemporalDataAndTechniques",
    "DOI": "10.1109\/TVCG.2012.190",
    "Keywords": "spatio-temporal visualization;earth science visualization;model validation;sea level indicators;coordinated & multiple views",
    "Keywords_Processed": "spatio temporal visualization;earth science visualization;model validation;sea level indicator;coordinate multiple view",
    "Title": "A Visual Analysis Concept for the Validation of Geoscientific Simulation Models"
  },
  "82": {
    "Abstract": "Model selection in time series analysis is a challenging task for domain experts in many application areas such as epidemiology, economy, or environmental sciences. The methodology used for this task demands a close combination of human judgement and automated computation. However, statistical software tools do not adequately support this combination through interactive visual interfaces. We propose a Visual Analytics process to guide domain experts in this task. For this purpose, we developed the TiMoVA prototype that implements this process based on user stories and iterative expert feedback on user experience. The prototype was evaluated by usage scenarios with an example dataset from epidemiology and interviews with two external domain experts in statistics. The insights from the experts' feedback and the usage scenarios show that TiMoVA is able to support domain experts in model selection tasks through interactive visual interfaces with short feedback cycles.",
    "Authors": "Bogl, M.;Aigner, W.;Filzmoser, P.;Lammarsch, T.;Miksch, S.;Rind, A.",
    "Clusters": "InteractionTechniquesGeneral;MachineLearningAndStatistics;MultipleLinkedCoordinatedViews;TimeseriesTimeVaryingDataAndTechniques;",
    "DOI": "10.1109\/TVCG.2013.222",
    "Keywords": "model selection;visual interaction;visual analytics;time-series analysis;coordinated & multiple views",
    "Keywords_Processed": "visual interaction;time series analysis;visual analytic;coordinate multiple view;model selection",
    "Title": "Visual Analytics for Model Selection in Time Series Analysis"
  },
  "208": {
    "Abstract": "Traditional layered graph depictions such as flow charts are in wide use. Yet as graphs grow more complex, these depictions can become difficult to understand. Quilts are matrix-based depictions for layered graphs designed to address this problem. In this research, we first improve Quilts by developing three design alternatives, and then compare the best of these alternatives to better-known node-link and matrix depictions. A primary weakness in Quilts is their depiction of skip links, links that do not simply connect to a succeeding layer. Therefore in our first study, we compare Quilts using color-only, text-only, and mixed (color and text) skip link depictions, finding that path finding with the color-only depiction is significantly slower and less accurate, and that in certain cases, the mixed depiction offers an advantage over the text-only depiction. In our second study, we compare Quilts using the mixed depiction to node-link diagrams and centered matrices. Overall results show that users can find paths through graphs significantly faster with Quilts (46.6 secs) than with node-link (58.3 secs) or matrix (71.2 secs) diagrams. This speed advantage is still greater in large graphs (e.g. in 200 node graphs, 55.4 secs vs. 71.1 secs for node-link and 84.2 secs for matrix depictions).",
    "Authors": "Juhee Bae;Watson, B.",
    "Clusters": "GraphNetworkDataAndTechniques;MatrixRelatedTechniques",
    "DOI": "10.1109\/TVCG.2011.187",
    "Keywords": "matrix-based depiction;graph drawing;layered graphs;node-link diagram",
    "Keywords_Processed": "graph drawing;matrix base depiction;layer graph;node link diagram",
    "Title": "Developing and Evaluating Quilts for the Depiction of Large Layered Graphs"
  },
  "65": {
    "Abstract": "Maintaining an awareness of collaborators' actions is critical during collaborative work, including during collaborative visualization activities. Particularly when collaborators are located at a distance, it is important to know what everyone is working on in order to avoid duplication of effort, share relevant results in a timely manner and build upon each other's results. Can a person's brushing actions provide an indication of their queries and interests in a data set? Can these actions be revealed to a collaborator without substantially disrupting their own independent work? We designed a study to answer these questions in the context of distributed collaborative visualization of tabular data. Participants in our study worked independently to answer questions about a tabular data set, while simultaneously viewing brushing actions of a fictitious collaborator, shown directly within a shared workspace. We compared three methods of presenting the collaborator's actions: brushing & linking (i.e. highlighting exactly what the collaborator would see), selection (i.e. showing only a selected item), and persistent selection (i.e. showing only selected items but having them persist for some time). Our results demonstrated that persistent selection enabled some awareness of the collaborator's activities while causing minimal interference with independent work. Other techniques were less effective at providing awareness, and brushing & linking caused substantial interference. These findings suggest promise for the idea of exploiting natural brushing actions to provide awareness in collaborative work.",
    "Authors": "Hajizadeh, A.H.;Tory, M.;Leung, R.",
    "Clusters": "AmbientVisualization;Cognition;CollaborativeVisualization;EvaluationGeneral;InteractionTechniquesGeneral;MultipleLinkedCoordinatedViews",
    "DOI": "10.1109\/TVCG.2013.197",
    "Keywords": "user study;linked views;awareness;collaboration;attentionally ambient visualization;brushing and linking",
    "Keywords_Processed": "user study;collaboration;link view;brush and linking;awareness;attentionally ambient visualization",
    "Title": "Supporting Awareness through Collaborative Brushing and Linking of Tabular Data"
  },
  "85": {
    "Abstract": "This paper is concerned with the creation of 'macros' in workflow visualization as a support tool to increase the efficiency of data curation tasks. We propose computation of candidate macros based on their usage in large collections of workflows in data repositories. We describe an efficient algorithm for extracting macro motifs from workflow graphs. We discovered that the state transition information, used to identify macro candidates, characterizes the structural pattern of the macro and can be harnessed as part of the visual design of the corresponding macro glyph. This facilitates partial automation and consistency in glyph design applicable to a large set of macro glyphs. We tested this approach against a repository of biological data holding some 9,670 workflows and found that the algorithmically generated candidate macros are in keeping with domain expert expectations.",
    "Authors": "Maguire, E.;Rocca-Serra, P.;Sansone, S.-A.;Davies, J.;Chen, M.",
    "Clusters": "GlyphsGlyphBasedTechniques;StateRelatedDataTechniques;VisualPatternFeatureDetectionAndTracking;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2013.225",
    "Keywords": "motif detection;glyph generation;workflow visualization;glyph-based visualization;state-transition-based algorithm",
    "Keywords_Processed": "state transition base algorithm;workflow visualization;motif detection;glyph generation;glyph base visualization",
    "Title": "Visual Compression of Workflow Visualizations with Automated Detection of Macro Motifs"
  },
  "137": {
    "Abstract": "While intuitive time-series visualizations exist for common datasets, student course history data is difficult to represent using traditional visualization techniques due its concurrent nature. A visual composition process is developed and applied to reveal trends across various groupings. By working closely with educators, analytic strategies and techniques are developed to leverage the visualization composition to reveal unknown trends in the data. Furthermore, clustering algorithms are developed to group common course-grade histories for further analysis. Lastly, variations of the composition process are implemented to reveal subtle differences in the underlying data. These analytic tools and techniques enabled educators to confirm expected trends and to discover new ones.",
    "Authors": "Trimm, D.;Rheingans, P.;desJardins, M.",
    "Clusters": "DataClusteringAndAggregation;EvaluationGeneral;VisualDesignDesignGuidelines",
    "DOI": "10.1109\/TVCG.2012.288",
    "Keywords": "student performance analysis;clustering;visualization composition;aggregate visualization",
    "Keywords_Processed": "clustering;aggregate visualization;visualization composition;student performance analysis",
    "Title": "Visualizing Student Histories Using Clustering and Composition"
  },
  "250": {
    "Abstract": "An instant and quantitative assessment of spatial distances between two objects plays an important role in interactive applications such as virtual model assembly, medical operation planning, or computational steering. While some research has been done on the development of distance-based measures between two objects, only very few attempts have been reported to visualize such measures in interactive scenarios. In this paper we present two different approaches for this purpose, and we investigate the effectiveness of these approaches for intuitive 3D implant positioning in a medical operation planning system. The first approach uses cylindrical glyphs to depict distances, which smoothly adapt their shape and color to changing distances when the objects are moved. This approach computes distances directly on the polygonal object representations by means of ray\/triangle mesh intersection. The second approach introduces a set of slices as additional geometric structures, and uses color coding on surfaces to indicate distances. This approach obtains distances from a precomputed distance field of each object. The major findings of the performed user study indicate that a visualization that can facilitate an instant and quantitative analysis of distances between two objects in interactive 3D scenarios is demanding, yet can be achieved by including additional monocular cues into the visualization.",
    "Authors": "Dick, C.;Burgkart, R.;Westermann, R.",
    "Clusters": "BiomedicalScienceAndMedicine;GlyphsGlyphBasedTechniques;VisualEncodingAndLayoutGeneral;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2011.189",
    "Keywords": "distance visualization;glyph;distance field;implant planning;biomedical visualization",
    "Keywords_Processed": "distance field;distance visualization;biomedical visualization;glyph;implant planning",
    "Title": "Distance Visualization for Interactive 3D Implant Planning"
  },
  "74": {
    "Abstract": "This research aims to develop design guidelines for systems that support investigators and analysts in the exploration and assembly of evidence and inferences. We focus here on the problem of identifying candidate 'influencers' within a community of practice. To better understand this problem and its related cognitive and interaction needs, we conducted a user study using a system called INVISQUE (INteractive Visual Search and QUery Environment) loaded with content from the ACM Digital Library. INVISQUE supports search and manipulation of results over a freeform infinite 'canvas'. The study focuses on the representations user create and their reasoning process. It also draws on some pre-established theories and frameworks related to sense-making and cognitive work in general, which we apply as a 'theoretical lenses' to consider findings and articulate solutions. Analysing the user-study data in the light of these provides some understanding of how the high-level problem of identifying key players within a domain can translate into lower-level questions and interactions. This, in turn, has informed our understanding of representation and functionality needs at a level of description which abstracts away from the specifics of the problem at hand to the class of problems of interest. We consider the study outcomes from the perspective of implications for design.",
    "Authors": "Kodagoda, N.;Attfield, S.;Wong, B.L.W.;Rooney, C.;Choudhury, S.",
    "Clusters": "AnalysisProcessGeneral;Cognition;EvaluationGeneral;InteractionTechniquesGeneral;ProgrammingAlgorithmsAndDataStructures;UserInterfacesGeneral",
    "DOI": "10.1109\/TVCG.2013.211",
    "Keywords": "reasoning;interface design;sensemaking;dataframe mode;visual analytics;analysis;evaluation;interaction",
    "Keywords_Processed": "interaction;interface design;reason;sensemake;visual analytic;dataframe mode;analysis;evaluation",
    "Title": "Using Interactive Visual Reasoning to Support Sense-Making: Implications for Design"
  },
  "16": {
    "Abstract": "We propose a new colon flattening algorithm that is efficient, shape-preserving, and robust to topological noise. Unlike previous approaches, which require a mandatory topological denoising to remove fake handles, our algorithm directly flattens the colon surface without any denoising. In our method, we replace the original Euclidean metric of the colon surface with a heat diffusion metric that is insensitive to topological noise. Using this heat diffusion metric, we then solve a Laplacian equation followed by an integration step to compute the final flattening. We demonstrate that our method is shape-preserving and the shape of the polyps are well preserved. The flattened colon also provides an efficient way to enhance the navigation and inspection in virtual colonoscopy. We further show how the existing colon registration pipeline is made more robust by using our colon flattening. We have tested our method on several colon wall surfaces and the experimental results demonstrate the robustness and the efficiency of our method.",
    "Authors": "Gurijala, K.C.;Rui Shi;Wei Zeng;Xianfeng Gu;Kaufman, A.",
    "Clusters": "BiomedicalScienceAndMedicine;PhysicsAndPhysicalSciences;ShapeRelatedTechniques;TopologyBasedTechniques;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2013.139",
    "Keywords": "volume rendering;colon flattening;shape-preserving mapping;virtual colonoscopy;heat diffusion;topological noise",
    "Keywords_Processed": "volume render;virtual colonoscopy;topological noise;heat diffusion;colon flatten;shape preserve mapping",
    "Title": "Colon Flattening Using Heat Diffusion Riemannian Metric"
  },
  "167": {
    "Abstract": "The most important resources to fulfill today's energy demands are fossil fuels, such as oil and natural gas. When exploiting hydrocarbon reservoirs, a detailed and credible model of the subsurface structures is crucial in order to minimize economic and ecological risks. Creating such a model is an inverse problem: reconstructing structures from measured reflection seismics. The major challenge here is twofold: First, the structures in highly ambiguous seismic data are interpreted in the time domain. Second, a velocity model has to be built from this interpretation to match the model to depth measurements from wells. If it is not possible to obtain a match at all positions, the interpretation has to be updated, going back to the first step. This results in a lengthy back and forth between the different steps, or in an unphysical velocity model in many cases. This paper presents a novel, integrated approach to interactively creating subsurface models from reflection seismics. It integrates the interpretation of the seismic data using an interactive horizon extraction technique based on piecewise global optimization with velocity modeling. Computing and visualizing the effects of changes to the interpretation and velocity model on the depth-converted model on the fly enables an integrated feedback loop that enables a completely new connection of the seismic data in time domain and well data in depth domain. Using a novel joint time\/depth visualization, depicting side-by-side views of the original and the resulting depth-converted data, domain experts can directly fit their interpretation in time domain to spatial ground truth data. We have conducted a domain expert evaluation, which illustrates that the presented workflow enables the creation of exact subsurface models much more rapidly than previous approaches.",
    "Authors": "Hollt, T.;Freiler, W.;Gschwantner, F.;Doleisch, H.;Heinemann, G.;Hadwiger, M.",
    "Clusters": "EarthSpaceAndEnvironmentalSciences;VisualEncodingAndLayoutGeneral;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2012.259",
    "Keywords": "exploded views;seismic interpretation;seismic visualization;volume deformation",
    "Keywords_Processed": "volume deformation;seismic visualization;seismic interpretation;explode view",
    "Title": "SeiVis: An Interactive Visual Subsurface Modeling Application"
  },
  "77": {
    "Abstract": "Scientists use DNA sequence differences between an individual's genome and a standard reference genome to study the genetic basis of disease. Such differences are called sequence variants, and determining their impact in the cell is difficult because it requires reasoning about both the type and location of the variant across several levels of biological context. In this design study, we worked with four analysts to design a visualization tool supporting variant impact assessment for three different tasks. We contribute data and task abstractions for the problem of variant impact assessment, and the carefully justified design and implementation of the Variant View tool. Variant View features an information-dense visual encoding that provides maximal information at the overview level, in contrast to the extensive navigation required by currently-prevalent genome browsers. We provide initial evidence that the tool simplified and accelerated workflows for these three tasks through three case studies. Finally, we reflect on the lessons learned in creating and refining data and task abstractions that allow for concise overviews of sprawling information spaces that can reduce or remove the need for the memory-intensive use of navigation.",
    "Authors": "Ferstay, J.A.;Nielsen, C.B.;Munzner, T.",
    "Clusters": "BiologyAndBioinformatics;DesignStudiesAndCaseStudies;Genetics;",
    "DOI": "10.1109\/TVCG.2013.214",
    "Keywords": "information visualization;design study;genetic variants;bioinformatics",
    "Keywords_Processed": "bioinformatic;design study;information visualization;genetic variant",
    "Title": "Variant View: Visualizing Sequence Variants in their Gene Context"
  },
  "351": {
    "Abstract": "Histology is the study of the structure of biological tissue using microscopy techniques. As digital imaging technology advances, high resolution microscopy of large tissue volumes is becoming feasible; however, new interactive tools are needed to explore and analyze the enormous datasets. In this paper we present a visualization framework that specifically targets interactive examination of arbitrarily large image stacks. Our framework is built upon two core techniques: display-aware processing and GPU-accelerated texture compression. With display-aware processing, only the currently visible image tiles are fetched and aligned on-the-fly, reducing memory bandwidth and minimizing the need for time-consuming global pre-processing. Our novel texture compression scheme for GPUs is tailored for quick browsing of image stacks. We evaluate the usability of our viewer for two histology applications: digital pathology and visualization of neural structure at nanoscale-resolution in serial electron micrographs.",
    "Authors": "Won-Ki Jeong;Schneider, J.;Turney, S.G.;Faulkner-Jones, B.E.;Meyer, D.;Westermann, R.;Reid, R.C.;Lichtman, J.;Pfister, H.",
    "Clusters": "BiomedicalScienceAndMedicine;GpuBasedTechniques;LargeAndHighResDisplays;Textures",
    "DOI": "10.1109\/TVCG.2010.168",
    "Keywords": "texture compression;biomedical image processing;gigapixel viewer;gpu",
    "Keywords_Processed": "texture compression;gpu;biomedical image processing;gigapixel viewer",
    "Title": "Interactive Histology of Large-Scale Biomedical Image Stacks"
  },
  "8": {
    "Abstract": "We present a framework for acuity-driven visualization of super-high resolution image data on gigapixel displays. Tiled display walls offer a large workspace that can be navigated physically by the user. Based on head tracking information, the physical characteristics of the tiled display and the formulation of visual acuity, we guide an out-of-core gigapixel rendering scheme by delivering high levels of detail only in places where it is perceivable to the user. We apply this principle to gigapixel image rendering through adaptive level of detail selection. Additionally, we have developed an acuity-driven tessellation scheme for high-quality Focus-and-Context (F+C) lenses that significantly reduces visual artifacts while accurately capturing the underlying lens function. We demonstrate this framework on the Reality Deck, an immersive gigapixel display. We present the results of a user study designed to quantify the impact of our acuity-driven rendering optimizations in the visual exploration process. We discovered no evidence suggesting a difference in search task performance between our framework and naive rendering of gigapixel resolution data, while realizing significant benefits in terms of data transfer overhead. Additionally, we show that our acuity-driven tessellation scheme offers substantially increased frame rates when compared to naive pre-tessellation, while providing indistinguishable image quality.",
    "Authors": "Papadopoulos, C.;Kaufman, A.",
    "Clusters": "FocusContextTechniques;LargeAndHighResDisplays;Perception",
    "DOI": "10.1109\/TVCG.2013.127",
    "Keywords": "gigapixel visualization;focus+context;gigapixel display;visual acuity;reality deck",
    "Keywords_Processed": "visual acuity;gigapixel display;gigapixel visualization;focus context;reality deck",
    "Title": "Acuity-Driven Gigapixel Visualization"
  },
  "188": {
    "Abstract": "Performing exhaustive searches over a large number of text documents can be tedious, since it is very hard to formulate search queries or define filter criteria that capture an analyst's information need adequately. Classification through machine learning has the potential to improve search and filter tasks encompassing either complex or very specific information needs, individually. Unfortunately, analysts who are knowledgeable in their field are typically not machine learning specialists. Most classification methods, however, require a certain expertise regarding their parametrization to achieve good results. Supervised machine learning algorithms, in contrast, rely on labeled data, which can be provided by analysts. However, the effort for labeling can be very high, which shifts the problem from composing complex queries or defining accurate filters to another laborious task, in addition to the need for judging the trained classifier's quality. We therefore compare three approaches for interactive classifier training in a user study. All of the approaches are potential candidates for the integration into a larger retrieval system. They incorporate active learning to various degrees in order to reduce the labeling effort as well as to increase effectiveness. Two of them encompass interactive visualization for letting users explore the status of the classifier in context of the labeled documents, as well as for judging the quality of the classifier in iterative feedback loops. We see our work as a step towards introducing user controlled classification methods in addition to text search and filtering for increasing recall in analytics scenarios involving large corpora.",
    "Authors": "Heimerl, F.;Koch, S.;Bosch, H.;Ertl, T.",
    "Clusters": "DatabasesAndDataMining;EvaluationGeneral;HumanComputerInteractionHumanFactors;MachineLearningAndStatistics;SegmentationAndClassification;",
    "DOI": "10.1109\/TVCG.2012.277",
    "Keywords": "information retrieval;user evaluation;active learning;classification;human-computer interaction;visual analytics",
    "Keywords_Processed": "information retrieval;human computer interaction;user evaluation;active learning;visual analytic;classification",
    "Title": "Visual Classifier Training for Text Document Retrieval"
  },
  "60": {
    "Abstract": "Presenting and communicating insights to an audience-telling a story-is one of the main goals of data exploration. Even though visualization as a storytelling medium has recently begun to gain attention, storytelling is still underexplored in information visualization and little research has been done to help people tell their stories with data. To create a new, more engaging form of storytelling with data, we leverage and extend the narrative storytelling attributes of whiteboard animation with pen and touch interactions. We present SketchStory, a data-enabled digital whiteboard that facilitates the creation of personalized and expressive data charts quickly and easily. SketchStory recognizes a small set of sketch gestures for chart invocation, and automatically completes charts by synthesizing the visuals from the presenter-provided example icon and binding them to the underlying data. Furthermore, SketchStory allows the presenter to move and resize the completed data charts with touch, and filter the underlying data to facilitate interactive exploration. We conducted a controlled experiment for both audiences and presenters to compare SketchStory with a traditional presentation system, Microsoft PowerPoint. Results show that the audience is more engaged by presentations done with SketchStory than PowerPoint. Eighteen out of 24 audience participants preferred SketchStory to PowerPoint. Four out of five presenter participants also favored SketchStory despite the extra effort required for presentation.",
    "Authors": "Bongshin Lee;Kazi, R.H.;Smith, G.",
    "Clusters": "InteractionTechniquesGeneral;Storytelling;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2013.191",
    "Keywords": "data presentation;sketch;visualization;pen and touch;storytelling;interaction",
    "Keywords_Processed": "visualization;interaction;pen and touch;storytelle;sketch;datum presentation",
    "Title": "SketchStory: Telling More Engaging Stories with Data through Freeform Sketching"
  },
  "382": {
    "Abstract": "The massive amount of financial time series data that originates from the stock market generates large amounts of complex data of high interest. However, adequate solutions that can effectively handle the information in order to gain insight and to understand the market mechanisms are rare. In this paper, we present two techniques and applications that enable the user to interactively analyze large amounts of time series data in real-time in order to get insight into the development of assets, market sectors, countries, and the financial market as a whole. The first technique allows users to quickly analyze combinations of single assets, market sectors as well as countries, compare them to each other, and to visually discover the periods of time where market sectors and countries get into turbulence. The second application clusters a selection of large amounts of financial time series data according to their similarity, and analyzes the distribution of the assets among market sectors. This allows users to identify the characteristic graphs which are representative for the development of a particular market sector, and also to identify the assets which behave considerably differently compared to other assets in the same sector. Both applications allow the user to perform investigative exploration techniques and interactive visual analysis in real-time.",
    "Authors": "Ziegler, H.;Jenny, M.;Gruse, T.;Keim, D.A.",
    "Clusters": "AnalysisProcessGeneral;BusinessFinanceEconomyManufacturing;TimeseriesTimeVaryingDataAndTechniques;",
    "DOI": "10.1109\/VAST.2010.5652530",
    "Keywords": "exploratory data analysis;time-series data;time-series clustering;visual analytics;financial information visualization",
    "Keywords_Processed": "time series clustering;financial information visualization;visual analytic;time series datum;exploratory datum analysis",
    "Title": "Visual market sector analysis for financial time series data"
  },
  "262": {
    "Abstract": "Better understanding of hemodynamics conceivably leads to improved diagnosis and prognosis of cardiovascular diseases. Therefore, an elaborate analysis of the blood-flow in heart and thoracic arteries is essential. Contemporary MRI techniques enable acquisition of quantitative time-resolved flow information, resulting in 4D velocity fields that capture the blood-flow behavior. Visual exploration of these fields provides comprehensive insight into the unsteady blood-flow behavior, and precedes a quantitative analysis of additional blood-flow parameters. The complete inspection requires accurate segmentation of anatomical structures, encompassing a time-consuming and hard-to-automate process, especially for malformed morphologies. We present a way to avoid the laborious segmentation process in case of qualitative inspection, by introducing an interactive virtual probe. This probe is positioned semi-automatically within the blood-flow field, and serves as a navigational object for visual exploration. The difficult task of determining position and orientation along the view-direction is automated by a fitting approach, aligning the probe with the orientations of the velocity field. The aligned probe provides an interactive seeding basis for various flow visualization approaches. We demonstrate illustration-inspired particles, integral lines and integral surfaces, conveying distinct characteristics of the unsteady blood-flow. Lastly, we present the results of an evaluation with domain experts, valuing the practical use of our probe and flow visualization techniques.",
    "Authors": "van Pelt, R.;Olivan Bescos, J.;Breeuwer, M.;Clough, R.E.;Groller, E.;ter Haar Romenij, B.;Vilanova, A.",
    "Clusters": "BiomedicalScienceAndMedicine;FlowVisualizationDataAndTechniques;IllustrativeVisualization;ImageBasedDataImageSignalProcessing;InteractionTechniquesGeneral",
    "DOI": "10.1109\/TVCG.2011.215",
    "Keywords": "flow visualization;phase-contrast cine mri;illustrative visualization;probing;multivalued images",
    "Keywords_Processed": "illustrative visualization;probe;multivalue image;phase contrast cine mri;flow visualization",
    "Title": "Interactive Virtual Probing of 4D MRI Blood-Flow"
  },
  "19": {
    "Abstract": "Ensembles of numerical simulations are used in a variety of applications, such as meteorology or computational solid mechanics, in order to quantify the uncertainty or possible error in a model or simulation. Deriving robust statistics and visualizing the variability of an ensemble is a challenging task and is usually accomplished through direct visualization of ensemble members or by providing aggregate representations such as an average or pointwise probabilities. In many cases, the interesting quantities in a simulation are not dense fields, but are sets of features that are often represented as thresholds on physical or derived quantities. In this paper, we introduce a generalization of boxplots, called contour boxplots, for visualization and exploration of ensembles of contours or level sets of functions. Conventional boxplots have been widely used as an exploratory or communicative tool for data analysis, and they typically show the median, mean, confidence intervals, and outliers of a population. The proposed contour boxplots are a generalization of functional boxplots, which build on the notion of data depth. Data depth approximates the extent to which a particular sample is centrally located within its density function. This produces a center-outward ordering that gives rise to the statistical quantities that are essential to boxplots. Here we present a generalization of functional data depth to contours and demonstrate methods for displaying the resulting boxplots for two-dimensional simulation data in weather forecasting and computational fluid dynamics.",
    "Authors": "Whitaker, R.T.;Mirzargar, M.;Kirby, R.M.",
    "Clusters": "ChartsDiagramsPlots;MachineLearningAndStatistics;Simulation;UncertaintyTechniquesAndVisualization;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2013.143",
    "Keywords": "uncertainty visualization;order statistics;band depth;boxplots;ensemble visualization",
    "Keywords_Processed": "ensemble visualization;boxplot;order statistic;band depth;uncertainty visualization",
    "Title": "Contour Boxplots: A Method for Characterizing Uncertainty in Feature Sets from Simulation Ensembles"
  },
  "114": {
    "Abstract": "Lineups [4, 28] have been established as tools for visual testing similar to standard statistical inference tests, allowing us to evaluate the validity of graphical findings in an objective manner. In simulation studies [12] lineups have been shown as being efficient: the power of visual tests is comparable to classical tests while being much less stringent in terms of distributional assumptions made. This makes lineups versatile, yet powerful, tools in situations where conditions for regular statistical tests are not or cannot be met. In this paper we introduce lineups as a tool for evaluating the power of competing graphical designs. We highlight some of the theoretical properties and then show results from two studies evaluating competing designs: both studies are designed to go to the limits of our perceptual abilities to highlight differences between designs. We use both accuracy and speed of evaluation as measures of a successful design. The first study compares the choice of coordinate system: polar versus cartesian coordinates. The results show strong support in favor of cartesian coordinates in finding fast and accurate answers to spotting patterns. The second study is aimed at finding shift differences between distributions. Both studies are motivated by data problems that we have recently encountered, and explore using simulated data to evaluate the plot designs under controlled conditions. Amazon Mechanical Turk (MTurk) is used to conduct the studies. The lineups provide an effective mechanism for objectively evaluating plot designs.",
    "Authors": "Hofmann, H.;Follett, L.;Majumder, M.;Cook, D.",
    "Clusters": "ComparisonComparativeVisualizationAndSimilarity;HypothesisFormingTestingAndVisualEvidence;VisualEncodingAndLayoutGeneral;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2012.230",
    "Keywords": "efficiency of displays;visual inference;lineups;power comparison",
    "Keywords_Processed": "power comparison;efficiency of display;lineup;visual inference",
    "Title": "Graphical Tests for Power Comparison of Competing Designs"
  },
  "185": {
    "Abstract": "Visual analytic tools aim to support the cognitively demanding task of sensemaking. Their success often depends on the ability to leverage capabilities of mathematical models, visualization, and human intuition through flexible, usable, and expressive interactions. Spatially clustering data is one effective metaphor for users to explore similarity and relationships between information, adjusting the weighting of dimensions or characteristics of the dataset to observe the change in the spatial layout. Semantic interaction is an approach to user interaction in such spatializations that couples these parametric modifications of the clustering model with users' analytic operations on the data (e.g., direct document movement in the spatialization, highlighting text, search, etc.). In this paper, we present results of a user study exploring the ability of semantic interaction in a visual analytic prototype, ForceSPIRE, to support sensemaking. We found that semantic interaction captures the analytical reasoning of the user through keyword weighting, and aids the user in co-creating a spatialization based on the user's reasoning and intuition.",
    "Authors": "Endert, A.;Fiaux, P.;North, C.",
    "Clusters": "Cognition;InteractionTechniquesGeneral;",
    "DOI": "10.1109\/TVCG.2012.260",
    "Keywords": "user interaction;sensemaking;visualization;visual analytics;analytic reasoning",
    "Keywords_Processed": "user interaction;visualization;sensemake;visual analytic;analytic reasoning",
    "Title": "Semantic Interaction for Sensemaking: Inferring Analytical Reasoning for Model Steering"
  },
  "90": {
    "Abstract": "Analysis of dynamic object deformations such as cardiac motion is of great importance, especially when there is a necessity to visualize and compare the deformation behavior across subjects. However, there is a lack of effective techniques for comparative visualization and assessment of a collection of motion data due to its 4-dimensional nature, i.e., timely varying three-dimensional shapes. From the geometric point of view, the motion change can be considered as a function defined on the 2D manifold of the surface. This paper presents a novel classification and visualization method based on a medial surface shape space, in which two novel shape descriptors are defined, for discriminating normal and abnormal human heart deformations as well as localizing the abnormal motion regions. In our medial surface shape space, the geodesic distance connecting two points in the space measures the similarity between their corresponding medial surfaces, which can quantify the similarity and disparity of the 3D heart motions. Furthermore, the novel descriptors can effectively localize the inconsistently deforming myopathic regions on the left ventricle. An easy visualization of heart motion sequences on the projected space allows users to distinguish the deformation differences. Our experimental results on both synthetic and real imaging data show that this method can automatically classify the healthy and myopathic subjects and accurately detect myopathic regions on the left ventricle, which outperforms other conventional cardiac diagnostic methods.",
    "Authors": "Taimouri, V.;Jing Hua",
    "Clusters": "BiomedicalScienceAndMedicine;ComparisonComparativeVisualizationAndSimilarity;ShapeRelatedTechniques;SurfaceRelatedDataAndTechniques",
    "DOI": "10.1109\/TVCG.2013.230",
    "Keywords": "shape space;medial surface;comparative visualization;left ventricle diagnosis",
    "Keywords_Processed": "medial surface;leave ventricle diagnosis;shape space;comparative visualization",
    "Title": "Visualization of Shape Motions in Shape Space"
  },
  "370": {
    "Abstract": "Although direct volume rendering is established as a powerful tool for the visualization of volumetric data, efficient and reliable feature detection is still an open topic. Usually, a tradeoff between fast but imprecise classification schemes and accurate but time-consuming segmentation techniques has to be made. Furthermore, the issue of uncertainty introduced with the feature detection process is completely neglected by the majority of existing approaches.In this paper we propose a guided probabilistic volume segmentation approach that focuses on the minimization of uncertainty. In an iterative process, our system continuously assesses uncertainty of a random walker-based segmentation in order to detect regions with high ambiguity, to which the user's attention is directed to support the correction of potential misclassifications. This reduces the risk of critical segmentation errors and ensures that information about the segmentation's reliability is conveyed to the user in a dependable way. In order to improve the efficiency of the segmentation process, our technique does not only take into account the volume data to be segmented, but also enables the user to incorporate classification information. An interactive workflow has been achieved by implementing the presented system on the GPU using the OpenCL API. Our results obtained for several medical data sets of different modalities, including brain MRI and abdominal CT, demonstrate the reliability and efficiency of our approach.",
    "Authors": "Prassni, J.-S.;Ropinski, T.;Hinrichs, K.",
    "Clusters": "ImageBasedDataImageSignalProcessing;SegmentationAndClassification;UncertaintyTechniquesAndVisualization",
    "DOI": "10.1109\/TVCG.2010.208",
    "Keywords": "random walker;classification;uncertainty;volume segmentation",
    "Keywords_Processed": "volume segmentation;uncertainty;classification;random walker",
    "Title": "Uncertainty-Aware Guided Volume Segmentation"
  },
  "245": {
    "Abstract": "Parameterization of complex surfaces constitutes a major means of visualizing highly convoluted geometric structures as well as other properties associated with the surface. It also enables users with the ability to navigate, orient, and focus on regions of interest within a global view and overcome the occlusions to inner concavities. In this paper, we propose a novel area-preserving surface parameterization method which is rigorous in theory, moderate in computation, yet easily extendable to surfaces of non-disc and closed-boundary topologies. Starting from the distortion induced by an initial parameterization, an area restoring diffeomorphic flow is constructed as a Lie advection of differential 2-forms along the manifold, which yields equality of the area elements between the domain and the original surface at its final state. Existence and uniqueness of result are assured through an analytical derivation. Based upon a triangulated surface representation, we also present an efficient algorithm in line with discrete differential modeling. As an exemplar application, the utilization of this method for the effective visualization of brain cortical imaging modalities is presented. Compared with conformal methods, our method can reveal more subtle surface patterns in a quantitative manner. It, therefore, provides a competitive alternative to the existing parameterization techniques for better surface-based analysis in various scenarios.",
    "Authors": "Guangyu Zou;Jiaxi Hu;Xianfeng Gu;Jing Hua",
    "Clusters": "NumericalMethodsMathematics;SurfaceRelatedDataAndTechniques",
    "DOI": "10.1109\/TVCG.2011.171",
    "Keywords": "lie advection;differential forms;area-preserving surface parameterization;surface visualization",
    "Keywords_Processed": "surface visualization;differential form;lie advection;area preserve surface parameterization",
    "Title": "Authalic Parameterization of General Surfaces Using Lie Advection"
  },
  "244": {
    "Abstract": "Asymmetric tensor field visualization can provide important insight into fluid flows and solid deformations. Existing techniques for asymmetric tensor fields focus on the analysis, and simply use evenly-spaced hyperstreamlines on surfaces following eigenvectors and dual-eigenvectors in the tensor field. In this paper, we describe a hybrid visualization technique in which hyperstreamlines and elliptical glyphs are used in real and complex domains, respectively. This enables a more faithful representation of flow behaviors inside complex domains. In addition, we encode tensor magnitude, an important quantity in tensor field analysis, using the density of hyperstreamlines and sizes of glyphs. This allows colors to be used to encode other important tensor quantities. To facilitate quick visual exploration of the data from different viewpoints and at different resolutions, we employ an efficient image-space approach in which hyperstreamlines and glyphs are generated quickly in the image plane. The combination of these techniques leads to an efficient tensor field visualization system for domain scientists. We demonstrate the effectiveness of our visualization technique through applications to complex simulated engine fluid flow and earthquake deformation data. Feedback from domain expert scientists, who are also co-authors, is provided.",
    "Authors": "Guoning Chen;Palke, D.;Zhongzang Lin;Yeh, H.;Vincent, P.;Laramee, R.S.;Zhang, E.",
    "Clusters": "GlyphsGlyphBasedTechniques;StreamlinesPathlinesStreaklines;TensorDataAndTechniques;VectorFieldsDataAndTechniques;ViewDependentVisualization",
    "DOI": "10.1109\/TVCG.2011.170",
    "Keywords": "hyperstreamline placement;vector field;asymmetric tensor fields;glyph packing;view-dependent",
    "Keywords_Processed": "vector field;hyperstreamline placement;asymmetric tensor field;glyph packing;view dependent",
    "Title": "Asymmetric Tensor field Visualization for Surfaces"
  },
  "140": {
    "Abstract": "In the last decades cosmological N-body dark matter simulations have enabled ab initio studies of the formation of structure in the Universe. Gravity amplified small density fluctuations generated shortly after the Big Bang, leading to the formation of galaxies in the cosmic web. These calculations have led to a growing demand for methods to analyze time-dependent particle based simulations. Rendering methods for such N-body simulation data usually employ some kind of splatting approach via point based rendering primitives and approximate the spatial distributions of physical quantities using kernel interpolation techniques, common in SPH (Smoothed Particle Hydrodynamics)-codes. This paper proposes three GPU-assisted rendering approaches, based on a new, more accurate method to compute the physical densities of dark matter simulation data. It uses full phase-space information to generate a tetrahedral tessellation of the computational domain, with mesh vertices defined by the simulation's dark matter particle positions. Over time the mesh is deformed by gravitational forces, causing the tetrahedral cells to warp and overlap. The new methods are well suited to visualize the cosmic web. In particular they preserve caustics, regions of high density that emerge, when several streams of dark matter particles share the same location in space, indicating the formation of structures like sheets, filaments and halos. We demonstrate the superior image quality of the new approaches in a comparison with three standard rendering techniques for N-body simulation data.",
    "Authors": "Kaehler, R.;Hahn, O.;Abel, T.",
    "Clusters": "AstronomyAstrophysics;MeshesGridsAndLattices",
    "DOI": "10.1109\/TVCG.2012.187",
    "Keywords": "n-body simulations;tetrahedral grid;dark matter;astrophysics",
    "Keywords_Processed": "body simulation;tetrahedral grid;astrophysic;dark matter",
    "Title": "A Novel Approach to Visualizing Dark Matter Simulations"
  },
  "206": {
    "Abstract": "Data-Driven Documents (D3) is a novel representation-transparent approach to visualization for the web. Rather than hide the underlying scenegraph within a toolkit-specific abstraction, D3 enables direct inspection and manipulation of a native representation: the standard document object model (DOM). With D3, designers selectively bind input data to arbitrary document elements, applying dynamic transforms to both generate and modify content. We show how representational transparency improves expressiveness and better integrates with developer tools than prior approaches, while offering comparable notational efficiency and retaining powerful declarative components. Immediate evaluation of operators further simplifies debugging and allows iterative development. Additionally, we demonstrate how D3 transforms naturally enable animation and interaction with dramatic performance improvements over intermediate representations.",
    "Authors": "Bostock, M.;Ogievetsky, V.;Heer, J.",
    "Clusters": ";UserInterfacesGeneral;VisualizationSystemsToolkitsAndEnvironments;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2011.185",
    "Keywords": "information visualization;toolkits;2d graphics;user interface",
    "Keywords_Processed": "toolkit;user interface;information visualization;2d graphic",
    "Title": "D&#x0B3; Data-Driven Documents"
  },
  "195": {
    "Abstract": "Increasingly, social network datasets contain social attribute information about actors and their relationship. Analyzing such network with social attributes requires making sense of not only its structural features, but also the relationship between social features in attributes and network structures. Existing social network analysis tools are usually weak in supporting complex analytical tasks involving both structural and social features, and often overlook users' needs for sensemaking tools that help to gather, synthesize, and organize information of these features. To address these challenges, we propose a sensemaking framework of social-network visual analytics in this paper. This framework considers both bottom-up processes, which are about constructing new understandings based on collected information, and top-down processes, which concern using prior knowledge to guide information collection, in analyzing social networks from both social and structural perspectives. The framework also emphasizes the externalization of sensemaking processes through interactive visualization. Guided by the framework, we develop a system, SocialNetSense, to support the sensemaking in visual analytics of social networks with social attributes. The example of using our system to analyze a scholar collaboration network shows that our approach can help users gain insight into social networks both structurally and socially, and enhance their process awareness in visual analytics.",
    "Authors": "Liang Gou;Xiaolong Zhang;Airong Luo;Anderson, P.F.",
    "Clusters": "Cognition;SocialNetworksAndSocialMedia;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/VAST.2012.6400558",
    "Keywords": "socialnetsense;sensemaking;visualization;visual analytics;social networks",
    "Keywords_Processed": "socialnetsense;visualization;sensemake;social network;visual analytic",
    "Title": "SocialNetSense: Supporting sensemaking of social and structural features in networks with interactive visualization"
  },
  "5": {
    "Abstract": "The considerable previous work characterizing visualization usage has focused on low-level tasks or interactions and high-level tasks, leaving a gap between them that is not addressed. This gap leads to a lack of distinction between the ends and means of a task, limiting the potential for rigorous analysis. We contribute a multi-level typology of visualization tasks to address this gap, distinguishing why and how a visualization task is performed, as well as what the task inputs and outputs are. Our typology allows complex tasks to be expressed as sequences of interdependent simpler tasks, resulting in concise and flexible descriptions for tasks of varying complexity and scope. It provides abstract rather than domain-specific descriptions of tasks, so that useful comparisons can be made between visualization systems targeted at different application domains. This descriptive power supports a level of analysis required for the generation of new designs, by guiding the translation of domain-specific problems into abstract tasks, and for the qualitative evaluation of visualization usage. We demonstrate the benefits of our approach in a detailed case study, comparing task descriptions from our typology to those derived from related work. We also discuss the similarities and differences between our typology and over two dozen extant classification systems and theoretical frameworks from the literatures of visualization, human-computer interaction, information retrieval, communications, and cartography.",
    "Authors": "Brehmer, M.;Munzner, T.",
    "Clusters": "QualitativeEvaluation;TasksTaskRequirementsAnalysis;VisualizationTheoryModelsAndMethods",
    "DOI": "10.1109\/TVCG.2013.124",
    "Keywords": "qualitative evaluation;typology;task and requirements analysis;visualization models",
    "Keywords_Processed": "task and requirement analysis;qualitative evaluation;visualization model;typology",
    "Title": "A Multi-Level Typology of Abstract Visualization Tasks"
  },
  "11": {
    "Abstract": "Proposals to establish a 'science of interaction' have been forwarded from Information Visualization and Visual Analytics, as well as Cartography, Geovisualization, and GIScience. This paper reports on two studies to contribute to this call for an interaction science, with the goal of developing a functional taxonomy of interaction primitives for map-based visualization. A semi-structured interview study first was conducted with 21 expert interactive map users to understand the way in which map-based visualizations currently are employed. The interviews were transcribed and coded to identify statements representative of either the task the user wished to accomplish (i.e., objective primitives) or the interactive functionality included in the visualization to achieve this task (i.e., operator primitives). A card sorting study then was conducted with 15 expert interactive map designers to organize these example statements into logical structures based on their experience translating client requests into interaction designs. Example statements were supplemented with primitive definitions in the literature and were separated into two sorting exercises: objectives and operators. The objective sort suggested five objectives that increase in cognitive sophistication (identify, compare, rank, associate, & delineate), but exhibited a large amount of variation across participants due to consideration of broader user goals (procure, predict, & prescribe) and interaction operands (space-alone, attributes-in-space, & space-in-time; elementary & general). The operator sort suggested five enabling operators (import, export, save, edit, & annotate) and twelve work operators (reexpress, arrange, sequence, resymbolize, overlay, pan, zoom, reproject, search, filter, retrieve, & calculate). This taxonomy offers an empirically-derived and ecologically-valid structure to inform future research and design on interaction.",
    "Authors": "Roth, R.E.",
    "Clusters": "GeographyGeospatialVisCartographyTerrainVis;InteractionTechniquesGeneral;Maps",
    "DOI": "10.1109\/TVCG.2013.130",
    "Keywords": "interaction primitives;interactive maps;science of interaction;geovisualization;interaction",
    "Keywords_Processed": "interaction;interactive map;geovisualization;science of interaction;interaction primitive",
    "Title": "An Empirically-Derived Taxonomy of Interaction Primitives for Interactive Cartography and Geovisualization"
  },
  "367": {
    "Abstract": "In virtual colonoscopy, CT scans are typically acquired with the patient in both supine (facing up) and prone (facing down) positions. The registration of these two scans is desirable so that the user can clarify situations or confirm polyp findings at a location in one scan with the same location in the other, thereby improving polyp detection rates and reducing false positives. However, this supine-prone registration is challenging because of the substantial distortions in the colon shape due to the patient's change in position. We present an efficient algorithm and framework for performing this registration through the use of conformal geometry to guarantee that the registration is a diffeomorphism (a one-to-one and onto mapping). The taeniae coli and colon flexures are automatically extracted for each supine and prone surface, employing the colon geometry. The two colon surfaces are then divided into several segments using the flexures, and each segment is cut along a taenia coli and conformally flattened to the rectangular domain using holomorphic differentials. The mean curvature is color encoded as texture images, from which feature points are automatically detected using graph cut segmentation, mathematic morphological operations, and principal component analysis. Corresponding feature points are found between supine and prone and are used to adjust the conformal flattening to be quasi-conformal, such that the features become aligned. We present multiple methods of visualizing our results, including 2D flattened rendering, corresponding 3D endoluminal views, and rendering of distortion measurements. We demonstrate the efficiency and efficacy of our registration method by illustrating matched views on both the 2D flattened colon images and in the 3D volume rendered colon endoluminal view. We analytically evaluate the correctness of the results by measuring the distance between features on the registered colons.",
    "Authors": "Wei Zeng;Marino, J.;Chaitanya Gurijala, K.;Xianfeng Gu;Kaufman, A.",
    "Clusters": "BiomedicalScienceAndMedicine;DataRegistrationFusionAndIntegration;GeometryBasedTechniques;NumericalMethodsMathematics",
    "DOI": "10.1109\/TVCG.2010.200",
    "Keywords": "data registration;medical visualization;geometry-based technique;mathematical foundations for visualization",
    "Keywords_Processed": "geometry base technique;mathematical foundation for visualization;medical visualization;datum registration",
    "Title": "Supine and Prone Colon Registration Using Quasi-Conformal Mapping"
  },
  "388": {
    "Abstract": "Insight Externalization (IE) refers to the process of capturing and recording the semantics of insights in decision making and problem solving. To reduce human effort, Automated Insight Externalization (AIE) is desired. Most existing IE approaches achieve automation by capturing events (e.g., clicks and key presses) or actions (e.g., panning and zooming). In this paper, we propose a novel AIE approach named Click2Annotate. It allows semi-automatic insight annotation that captures low-level analytics task results (e.g., clusters and outliers), which have higher semantic richness and abstraction levels than actions and events. Click2Annotate has two significant benefits. First, it reduces human effort required in IE and generates annotations easy to understand. Second, the rich semantic information encoded in the annotations enables various insight management activities, such as insight browsing and insight retrieval. We present a formal user study that proved this first benefit. We also illustrate the second benefit by presenting the novel insight management activities we developed based on Click2Annotate, namely scented insight browsing and faceted insight search.",
    "Authors": "Yang Chen;Barlowe, S.;Jing Yang",
    "Clusters": "AnalysisProcessGeneral;Labeling;MultidimensionalMultivariateMultifieldDataAndTechniques;ReasoningProblemSolvingAndDecisionMaking;",
    "DOI": "10.1109\/VAST.2010.5652885",
    "Keywords": "annotation;decision making;insight management;multi-dimensional visualization;visual analytics",
    "Keywords_Processed": "multi dimensional visualization;decision make;visual analytic;insight management;annotation",
    "Title": "Click2Annotate: Automated Insight Externalization with rich semantics"
  },
  "352": {
    "Abstract": "Streak surfaces are among the most important features to support 3D unsteady flow exploration, but they are also among the computationally most demanding. Furthermore, to enable a feature driven analysis of the flow, one is mainly interested in streak surfaces that show separation profiles and thus detect unstable manifolds in the flow. The computation of such separation surfaces requires to place seeding structures at the separation locations and to let the structures move correspondingly to these locations in the unsteady flow. Since only little knowledge exists about the time evolution of separating streak surfaces, at this time, an automated exploration of 3D unsteady flows using such surfaces is not feasible. Therefore, in this paper we present an interactive approach for the visual analysis of separating streak surfaces. Our method draws upon recent work on the extraction of Lagrangian coherent structures (LCS) and the real-time visualization of streak surfaces on the GPU. We propose an interactive technique for computing ridges in the finite time Lyapunov exponent (FTLE) field at each time step, and we use these ridges as seeding structures to track streak surfaces in the time-varying flow. By showing separation surfaces in combination with particle trajectories, and by letting the user interactively change seeding parameters such as particle density and position, visually guided exploration of separation profiles in 3D is provided. To the best of our knowledge, this is the first time that the reconstruction and display of semantic separable surfaces in 3D unsteady flows can be performed interactively, giving rise to new possibilities for gaining insight into complex flow phenomena.",
    "Authors": "Ferstl, F.;Burger, K.;Theisel, H.;Westermann, R.",
    "Clusters": "AlgorithmicPatternFeatureDetectionTracking;FlowVisualizationDataAndTechniques;GpuBasedTechniques;SurfaceRelatedDataAndTechniques",
    "DOI": "10.1109\/TVCG.2010.169",
    "Keywords": "unsteady flow visualization;feature extraction;streak surface generation;gpu",
    "Keywords_Processed": "feature extraction;gpu;unsteady flow visualization;streak surface generation",
    "Title": "Interactive Separating Streak Surfaces"
  },
  "292": {
    "Abstract": "To make informed decisions, an expert has to reason with multi-dimensional, heterogeneous data and analysis results of these. Items in such datasets are typically represented by features. However, as argued in cognitive science, features do not yield an optimal space for human reasoning. In fact, humans tend to organize complex information in terms of prototypes or known cases rather than in absolute terms. When confronted with unknown data items, humans assess them in terms of similarity to these prototypical elements. Interestingly, an analogues similarity-to-prototype approach, where prototypes are taken from the data, has been successfully applied in machine learning. Combining such a machine learning approach with human prototypical reasoning in a Visual Analytics context requires to integrate similarity-based classification with interactive visualizations. To that end, the data prototypes should be visually represented to trigger direct associations to cases familiar to the domain experts. In this paper, we propose a set of highly interactive visualizations to explore data and classification results in terms of dissimilarities to visually represented prototypes. We argue that this approach not only supports human reasoning processes, but is also suitable to enhance understanding of heterogeneous data. The proposed framework is applied to a risk assessment case study in Forensic Psychiatry.",
    "Authors": "Migut, M.;van Gemert, J.C.;Worring, M.",
    "Clusters": "HumanComputerInteractionHumanFactors;InteractionTechniquesGeneral;SegmentationAndClassification;",
    "DOI": "10.1109\/VAST.2011.6102451",
    "Keywords": "interactive visualization;prototypes;dissimilarity-based classification;visual analytics;dissimilarity-based visualization",
    "Keywords_Processed": "prototype;visual analytic;dissimilarity base visualization;dissimilarity base classification;interactive visualization",
    "Title": "Interactive decision making using dissimilarity to visually represented prototypes"
  },
  "58": {
    "Abstract": "Cardiovascular diseases (CVD) are the leading cause of death worldwide. Their initiation and evolution depends strongly on the blood flow characteristics. In recent years, advances in 4D PC-MRI acquisition enable reliable and time-resolved 3D flow measuring, which allows a qualitative and quantitative analysis of the patient-specific hemodynamics. Currently, medical researchers investigate the relation between characteristic flow patterns like vortices and different pathologies. The manual extraction and evaluation is tedious and requires expert knowledge. Standardized, (semi-)automatic and reliable techniques are necessary to make the analysis of 4D PC-MRI applicable for the clinical routine. In this work, we present an approach for the extraction of vortex flow in the aorta and pulmonary artery incorporating line predicates. We provide an extensive comparison of existent vortex extraction methods to determine the most suitable vortex criterion for cardiac blood flow and apply our approach to ten datasets with different pathologies like coarctations, Tetralogy of Fallot and aneurysms. For two cases we provide a detailed discussion how our results are capable to complement existent diagnosis information. To ensure real-time feedback for the domain experts we implement our method completely on the GPU.",
    "Authors": "Kohler, B.;Gasteiger, R.;Preim, U.;Theisel, H.;Gutberlet, M.;Preim, B.",
    "Clusters": "ApplicationsGeneralAndOther;BiomedicalScienceAndMedicine;FlowVisualizationDataAndTechniques;LineBasedTechniquesAndApproaches",
    "DOI": "10.1109\/TVCG.2013.189",
    "Keywords": "vortex extraction;line predicates;cardiac blood flow;hemodynamics;4d pc-mri",
    "Keywords_Processed": "vortex extraction;hemodynamic;4d pc mri;line predicate;cardiac blood flow",
    "Title": "Semi-Automatic Vortex Extraction in 4D PC-MRI Cardiac Blood Flow Data using Line Predicates"
  },
  "274": {
    "Abstract": "Overlaid reference elements need to be sufficiently visible to effectively relate to the underlying information, but not so obtrusive that they clutter the presentation. We seek to create guidelines for presenting such structures through experimental studies to define boundary conditions for visual intrusiveness. We base our work on the practice of designers, who use transparency to integrate overlaid grids with their underlying imagery. Previous work discovered a useful range of alpha values for black or white grids overlayed on scatterplot images rendered in shades of gray over gray backgrounds of different lightness values. This work compares black grids to blue and red ones on different image types of scatterplots and maps. We expected that the coloured grids over grayscale images would be more visually salient than black ones, resulting in lower alpha values. Instead, we found that there was no significant difference between the boundaries set for red and black grids, but that the boundaries for blue grids were set consistently higher (more opaque). As in our previous study, alpha values are affected by image density rather than image type, and are consistently lower than many default settings. These results have implications for the design of subtle reference structures.",
    "Authors": "Bartram, L.;Cheung, B.;Stone, M.C.",
    "Clusters": "ArtAndAestheticsInVisualization;AutomaticAnalysisVisualizationTechniques;Perception;VisualDesignDesignGuidelines",
    "DOI": "10.1109\/TVCG.2011.242",
    "Keywords": "information visualization;automatic presentation;visual design;computational aesthetics;applied perception",
    "Keywords_Processed": "apply perception;information visualization;visual design;automatic presentation;computational aesthetic",
    "Title": "The Effect of Colour and Transparency on the Perception of Overlaid Grids"
  },
  "207": {
    "Abstract": "Computing and visualizing sets of elements and their relationships is one of the most common tasks one performs when analyzing and organizing large amounts of data. Common representations of sets such as convex or concave geometries can become cluttered and difficult to parse when these sets overlap in multiple or complex ways, e.g., when multiple elements belong to multiple sets. In this paper, we present a design study of a novel set visual representation, LineSets, consisting of a curve connecting all of the set's elements. Our approach to design the visualization differs from traditional methodology used by the InfoVis community. We first explored the potential of the visualization concept by running a controlled experiment comparing our design sketches to results from the state-of-the-art technique. Our results demonstrated that LineSets are advantageous for certain tasks when compared to concave shapes. We discuss an implementation of LineSets based on simple heuristics and present a study demonstrating that our generated curves do as well as human-drawn ones. Finally, we present two applications of our technique in the context of search tasks on a map and community analysis tasks in social networks.",
    "Authors": "Alper, B.;Riche, N.H.;Ramos, G.;Czerwinski, M.",
    "Clusters": "DataClusteringAndAggregation;DataFacetsAndTechniques;GraphNetworkDataAndTechniques;SetRelatedDataTechniques",
    "DOI": "10.1109\/TVCG.2011.186",
    "Keywords": "faceted visualization;clustering;set visualization;graph visualization",
    "Keywords_Processed": "graph visualization;clustering;set visualization;faceted visualization",
    "Title": "Design Study of LineSets, a Novel Set Visualization Technique"
  },
  "169": {
    "Abstract": "Lighting design is a complex, but fundamental, problem in many fields. In volume visualization, direct volume rendering generates an informative image without external lighting, as each voxel itself emits radiance. However, external lighting further improves the shape and detail perception of features, and it also determines the effectiveness of the communication of feature information. The human visual system is highly effective in extracting structural information from images, and to assist it further, this paper presents an approach to structure-aware automatic lighting design by measuring the structural changes between the images with and without external lighting. Given a transfer function and a viewpoint, the optimal lighting parameters are those that provide the greatest enhancement to structural information - the shape and detail information of features are conveyed most clearly by the optimal lighting parameters. Besides lighting goodness, the proposed metric can also be used to evaluate lighting similarity and stability between two sets of lighting parameters. Lighting similarity can be used to optimize the selection of multiple light sources so that different light sources can reveal distinct structural information. Our experiments with several volume data sets demonstrate the effectiveness of the structure-aware lighting design approach. It is well suited to use by novices as it requires little technical understanding of the rendering parameters associated with direct volume rendering.",
    "Authors": "Yubo Tao;Hai Lin;Feng Dong;Chao Wang;Clapworthy, G.;Hujun Bao",
    "Clusters": "ComparisonComparativeVisualizationAndSimilarity;Illumination;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2012.267",
    "Keywords": "structural dissimilarity;lighting similarity;volume rendering;lighting stability;automatic lighting design",
    "Keywords_Processed": "volume render;structural dissimilarity;light stability;lighting similarity;automatic lighting design",
    "Title": "Structure-Aware Lighting Design for Volume Visualization"
  },
  "350": {
    "Abstract": "Stream surfaces are an intuitive approach to represent 3D vector fields. In many cases, however, they are challenging objects to visualize and to understand, due to a high degree of self-occlusion. Despite the need for adequate rendering methods, little work has been done so far in this important research area. In this paper, we present an illustrative rendering strategy for stream surfaces. In our approach, we apply various rendering techniques, which are inspired by the traditional flow illustrations drawn by Dallmann and Abraham & Shaw in the early 1980s. Among these techniques are contour lines and halftoning to show the overall surface shape. Flow direction as well as singularities on the stream surface are depicted by illustrative surface streamlines. ;To go beyond reproducing static text book images, we provide several interaction features, such as movable cuts and slabs allowing an interactive exploration of the flow and insights into subjacent structures, e.g., the inner windings of vortex breakdown bubbles. These methods take only the parameterized stream surface as input, require no further preprocessing, and can be freely combined by the user. We explain the design, GPU-implementation, and combination of the different illustrative rendering and interaction methods and demonstrate the potential of our approach by applying it to stream surfaces from various flow simulations.",
    "Authors": "Born, S.;Wiebel, A.;Friedrich, J.;Scheuermann, G.;Bartz, D.",
    "Clusters": "FlowVisualizationDataAndTechniques;GpuBasedTechniques;IllustrativeVisualization;SurfaceRelatedDataAndTechniques;VectorFieldsDataAndTechniques",
    "DOI": "10.1109\/TVCG.2010.166",
    "Keywords": "flow visualization;3d vector field data;silhouettes;illustrative rendering;gpu techniques;stream surfaces",
    "Keywords_Processed": "3d vector field datum;stream surface;silhouette;illustrative render;gpu technique;flow visualization",
    "Title": "Illustrative Stream Surfaces"
  },
  "46": {
    "Abstract": "As the visualization field matures, an increasing number of general toolkits are developed to cover a broad range of applications. However, no general tool can incorporate the latest capabilities for all possible applications, nor can the user interfaces and workflows be easily adjusted to accommodate all user communities. As a result, users will often chose either substandard solutions presented in familiar, customized tools or assemble a patchwork of individual applications glued through ad-hoc scripts and extensive, manual intervention. Instead, we need the ability to easily and rapidly assemble the best-in-task tools into custom interfaces and workflows to optimally serve any given application community. Unfortunately, creating such meta-applications at the API or SDK level is difficult, time consuming, and often infeasible due to the sheer variety of data models, design philosophies, limits in functionality, and the use of closed commercial systems. In this paper, we present the ManyVis framework which enables custom solutions to be built both rapidly and simply by allowing coordination and communication across existing unrelated applications. ManyVis allows users to combine software tools with complementary characteristics into one virtual application driven by a single, custom-designed interface.",
    "Authors": "Rungta, A.;Summa, B.;Demir, D.;Bremer, P.-T.;Pascucci, V.",
    "Clusters": "ApplicationsGeneralAndOther;MultipleLinkedCoordinatedViews;ProgrammingAlgorithmsAndDataStructures;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/TVCG.2013.174",
    "Keywords": "integrated applications;linked views;visualization environment;macros",
    "Keywords_Processed": "macro;link view;integrate application;visualization environment",
    "Title": "ManyVis: Multiple Applications in an Integrated Visualization Environment"
  },
  "172": {
    "Abstract": "The U.S. Department of Energy's (DOE) Office of Environmental Management (DOE\/EM) currently supports an effort to understand and predict the fate of nuclear contaminants and their transport in natural and engineered systems. Geologists, hydrologists, physicists and computer scientists are working together to create models of existing nuclear waste sites, to simulate their behavior and to extrapolate it into the future. We use visualization as an integral part in each step of this process. In the first step, visualization is used to verify model setup and to estimate critical parameters. High-performance computing simulations of contaminant transport produces massive amounts of data, which is then analyzed using visualization software specifically designed for parallel processing of large amounts of structured and unstructured data. Finally, simulation results are validated by comparing simulation results to measured current and historical field data. We describe in this article how visual analysis is used as an integral part of the decision-making process in the planning of ongoing and future treatment options for the contaminated nuclear waste sites. Lessons learned from visually analyzing our large-scale simulation runs will also have an impact on deciding on treatment measures for other contaminated sites.",
    "Authors": "Meyer, J.;Bethel, E.W.;Horsman, J.L.;Hubbard, S.S.;Krishnan, H.;Romosan, A.;Keating, E.H.;Monroe, L.;Strelitz, R.;Moore, P.;Taylor, G.;Torkian, B.;Johnson, T.C.;Gorton, I.",
    "Clusters": "DataAcquisitionAndManagement;EarthSpaceAndEnvironmentalSciences;HardwareAccellerationAndComputationGeneral;ParallelSystemsAndParallelProcessing;",
    "DOI": "10.1109\/TVCG.2012.278",
    "Keywords": "environmental management;high-performance computing;visual analytics;data management;parallel rendering",
    "Keywords_Processed": "visual analytic;parallel rendering;environmental management;high performance computing;data management",
    "Title": "Visual Data Analysis as an Integral Part of Environmental Management"
  },
  "41": {
    "Abstract": "People typically interact with information visualizations using a mouse. Their physical movement, orientation, and distance to visualizations are rarely used as input. We explore how to use such spatial relations among people and visualizations (i.e., proxemics) to drive interaction with visualizations, focusing here on the spatial relations between a single user and visualizations on a large display. We implement interaction techniques that zoom and pan, query and relate, and adapt visualizations based on tracking of users' position in relation to a large high-resolution display. Alternative prototypes are tested in three user studies and compared with baseline conditions that use a mouse. Our aim is to gain empirical data on the usefulness of a range of design possibilities and to generate more ideas. Among other things, the results show promise for changing zoom level or visual representation with the user's physical distance to a large display. We discuss possible benefits and potential issues to avoid when designing information visualizations that use proxemics.",
    "Authors": "Jakobsen, M.R.;Sahlemariam Haile, Y.;Knudsen, S.;Hornbaek, K.",
    "Clusters": "AnimationAndMotion;DataAndAnalysisMetrics;EvaluationGeneral;InteractionTechniquesGeneral;LargeAndHighResDisplays;VectorFieldsDataAndTechniques",
    "DOI": "10.1109\/TVCG.2013.166",
    "Keywords": "information visualization;distance;user study;large displays;orientation;movement;proxemics;user tracking",
    "Keywords_Processed": "user study;orientation;information visualization;proxemic;large display;user tracking;distance;movement",
    "Title": "Information Visualization and Proxemics: Design Opportunities and Empirical findings"
  },
  "202": {
    "Abstract": "Many well-cited theories for visualization design state that a visual representation should be optimized for quick and immediate interpretation by a user. Distracting elements like decorative \"chartjunk\" or extraneous information are avoided so as not to slow comprehension. Yet several recent studies in visualization research provide evidence that non-efficient visual elements may benefit comprehension and recall on the part of users. Similarly, findings from studies related to learning from visual displays in various subfields of psychology suggest that introducing cognitive difficulties to visualization interaction can improve a user's understanding of important information. In this paper, we synthesize empirical results from cross-disciplinary research on visual information representations, providing a counterpoint to efficiency-based design theory with guidelines that describe how visual difficulties can be introduced to benefit comprehension and recall. We identify conditions under which the application of visual difficulties is appropriate based on underlying factors in visualization interaction like active processing and engagement. We characterize effective graph design as a trade-off between efficiency and learning difficulties in order to provide Information Visualization (InfoVis) researchers and practitioners with a framework for organizing explorations of graphs for which comprehension and recall are crucial. We identify implications of this view for the design and evaluation of information visualizations.",
    "Authors": "Hullman, J.;Adar, E.;Shah, P.",
    "Clusters": "Cognition;HumanComputerInteractionHumanFactors",
    "DOI": "10.1109\/TVCG.2011.175",
    "Keywords": "engagement;desirable difficulites;active processing;individual differences;cognitive efficiency",
    "Keywords_Processed": "active processing;individual difference;cognitive efficiency;desirable difficulite;engagement",
    "Title": "Benefitting InfoVis with Visual Difficulties"
  },
  "25": {
    "Abstract": "Visualization of dynamically changing networks (graphs) is a significant challenge for researchers. Previous work has experimentally compared animation, small multiples, and other techniques, and found trade-offs between these. One potential way to avoid such trade-offs is to combine previous techniques in a hybrid visualization. We present two taxonomies of visualizations of dynamic graphs: one of non-hybrid techniques, and one of hybrid techniques. We also describe a prototype, called DiffAni, that allows a graph to be visualized as a sequence of three kinds of tiles: diff tiles that show difference maps over some time interval, animation tiles that show the evolution of the graph over some time interval, and small multiple tiles that show the graph state at an individual time slice. This sequence of tiles is ordered by time and covers all time slices in the data. An experimental evaluation of DiffAni shows that our hybrid approach has advantages over non-hybrid techniques in certain cases.",
    "Authors": "Rufiange, S.;McGuffin, M.J.",
    "Clusters": "AnimationAndMotion;BiologyAndBioinformatics;ComparisonComparativeVisualizationAndSimilarity;DynamicDataAndTechniques;Taxonomies;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2013.149",
    "Keywords": "dynamic networks;taxonomy;difference map;hybrid visualization;animation;evolution",
    "Keywords_Processed": "difference map;evolution;animation;dynamic network;hybrid visualization;taxonomy",
    "Title": "DiffAni: Visualizing Dynamic Graphs with a Hybrid of Difference Maps and Animation"
  },
  "154": {
    "Abstract": "This paper presents the Element Visualizer (ElVis), a new, open-source scientific visualization system for use with high-order finite element solutions to PDEs in three dimensions. This system is designed to minimize visualization errors of these types of fields by querying the underlying finite element basis functions (e.g., high-order polynomials) directly, leading to pixel-exact representations of solutions and geometry. The system interacts with simulation data through runtime plugins, which only require users to implement a handful of operations fundamental to finite element solvers. The data in turn can be visualized through the use of cut surfaces, contours, isosurfaces, and volume rendering. These visualization algorithms are implemented using NVIDIA's OptiX GPU-based ray-tracing engine, which provides accelerated ray traversal of the high-order geometry, and CUDA, which allows for effective parallel evaluation of the visualization algorithms. The direct interface between ElVis and the underlying data differentiates it from existing visualization tools. Current tools assume the underlying data is composed of linear primitives; high-order data must be interpolated with linear functions as a result. In this work, examples drawn from aerodynamic simulations-high-order discontinuous Galerkin finite element solutions of aerodynamic flows in particular-will demonstrate the superiority of ElVis' pixel-exact approach when compared with traditional linear-interpolation methods. Such methods can introduce a number of inaccuracies in the resulting visualization, making it unclear if visual artifacts are genuine to the solution data or if these artifacts are the result of interpolation errors. Linear methods additionally cannot properly visualize curved geometries (elements or boundaries) which can greatly inhibit developers' debugging efforts. As we will show, pixel-exact visualization exhibits none of these issues, removing the visualization scheme as a source of - ncertainty for engineers using ElVis.",
    "Authors": "Nelson, B.;Liu, E.;Kirby, R.M.;Haimes, R.",
    "Clusters": "ContourCreasesRidgesValleys;FlowVisualizationDataAndTechniques;IsosurfaceAndSurfaceExtractionTechniques;NumericalMethodsMathematics",
    "DOI": "10.1109\/TVCG.2012.218",
    "Keywords": "contours;high-order finite elements;fluid flow simulation;cut-surface extraction;discontinuous galerkin methods;spectral\/hp elements;isosurface",
    "Keywords_Processed": "spectral hp element;high order finite element;fluid flow simulation;discontinuous galerkin method;contour;isosurface;cut surface extraction",
    "Title": "ElVis: A System for the Accurate and Interactive Visualization of High-Order finite Element Solutions"
  },
  "158": {
    "Abstract": "Visual exploration of volumetric datasets to discover the embedded features and spatial structures is a challenging and tedious task. In this paper we present a semi-automatic approach to this problem that works by visually segmenting the intensity-gradient 2D histogram of a volumetric dataset into an exploration hierarchy. Our approach mimics user exploration behavior by analyzing the histogram with the normalized-cut multilevel segmentation technique. Unlike previous work in this area, our technique segments the histogram into a reasonable set of intuitive components that are mutually exclusive and collectively exhaustive. We use information-theoretic measures of the volumetric data segments to guide the exploration. This provides a data-driven coarse-to-fine hierarchy for a user to interactively navigate the volume in a meaningful manner.",
    "Authors": "Cheuk Yiu Ip;Varshney, A.;JaJa, J.",
    "Clusters": "AnalysisProcessGeneral;ImageBasedDataImageSignalProcessing;SegmentationAndClassification;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2012.231",
    "Keywords": "volume exploration;information-guided exploration;volume classification;normalized cut",
    "Keywords_Processed": "normalize cut;information guide exploration;volume classification;volume exploration",
    "Title": "Hierarchical Exploration of Volumes Using Multilevel Segmentation of the Intensity-Gradient Histograms"
  },
  "73": {
    "Abstract": "From financial statistics to nutritional values, we are frequently exposed to quantitative information expressed in measures of either extreme magnitudes or unfamiliar units, or both. A common practice used to comprehend such complex measures is to relate, re-express, and compare them through visual depictions using magnitudes and units that are easier to grasp. Through this practice, we create a new graphic composition that we refer to as a concrete scale. To the best of our knowledge, there are no design guidelines that exist for concrete scales despite their common use in communication, educational, and decision-making settings. We attempt to fill this void by introducing a novel framework that would serve as a practical guide for their analysis and design. Informed by a thorough analysis of graphic compositions involving complex measures and an extensive literature review of scale cognition mechanisms, our framework outlines the design space of various measure relations-specifically relations involving the re-expression of complex measures to more familiar concepts-and their visual representations as graphic compositions.",
    "Authors": "Chevalier, F.;Vuillemot, R.;Gali, G.",
    "Clusters": "Cognition;ComparisonComparativeVisualizationAndSimilarity;MultiScaleDataTechniques;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2013.210",
    "Keywords": "visual notation;scale cognition;visual comparison;graphic composition;concrete scale",
    "Keywords_Processed": "visual notation;scale cognition;concrete scale;graphic composition;visual comparison",
    "Title": "Using Concrete Scales: A Practical Framework for Effective Visual Depiction of Complex Measures"
  },
  "247": {
    "Abstract": "Large observations and simulations in scientific research give rise to high-dimensional data sets that present many challenges and opportunities in data analysis and visualization. Researchers in application domains such as engineering, computational biology, climate study, imaging and motion capture are faced with the problem of how to discover compact representations of highdimensional data while preserving their intrinsic structure. In many applications, the original data is projected onto low-dimensional space via dimensionality reduction techniques prior to modeling. One problem with this approach is that the projection step in the process can fail to preserve structure in the data that is only apparent in high dimensions. Conversely, such techniques may create structural illusions in the projection, implying structure not present in the original high-dimensional data. Our solution is to utilize topological techniques to recover important structures in high-dimensional data that contains non-trivial topology. Specifically, we are interested in high-dimensional branching structures. We construct local circle-valued coordinate functions to represent such features. Subsequently, we perform dimensionality reduction on the data while ensuring such structures are visually preserved. Additionally, we study the effects of global circular structures on visualizations. Our results reveal never-before-seen structures on real-world data sets from a variety of applications.",
    "Authors": "Bei Wang;Summa, B.;Pascucci, V.;Vejdemo-Johansson, M.",
    "Clusters": "DimensionalityReduction;TopologyBasedTechniques;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2011.177",
    "Keywords": "dimension reduction;visualization;topological analysis;circular coordinates",
    "Keywords_Processed": "visualization;dimension reduction;topological analysis;circular coordinate",
    "Title": "Branching and Circular Features in High Dimensional Data"
  },
  "107": {
    "Abstract": "We present an ethnographic study of design differences in visual presentations between academic disciplines. Characterizing design conventions between users and data domains is an important step in developing hypotheses, tools, and design guidelines for information visualization. In this paper, disciplines are compared at a coarse scale between four groups of fields: social, natural, and formal sciences; and the humanities. Two commonplace presentation types were analyzed: electronic slideshows and whiteboard \u00e2\u20ac\u0153chalk talks\u00e2\u20ac\u009d. We found design differences in slideshows using two methods - coding and comparing manually-selected features, like charts and diagrams, and an image-based analysis using PCA called eigenslides. In whiteboard talks with controlled topics, we observed design behaviors, including using representations and formalisms from a participant's own discipline, that suggest authors might benefit from novel assistive tools for designing presentations. Based on these findings, we discuss opportunities for visualization ethnography and human-centered authoring tools for visual information.",
    "Authors": "Gomez, S.R.;Jianu, R.;Ziemkiewicz, C.;Hua Guo;Laidlaw, D.H.",
    "Clusters": "AnalysisProcessGeneral;PresentationProductionAndDissemination;VisualDesignDesignGuidelines",
    "DOI": "10.1109\/TVCG.2012.214",
    "Keywords": "presentations;information visualization;design;visual analysis",
    "Keywords_Processed": "visual analysis;design;information visualization;presentation",
    "Title": "Different Strokes for Different Folks: Visual Presentation Design between Disciplines"
  },
  "389": {
    "Abstract": "This paper highlights the important role that record-keeping (i.e. taking notes and saving charts) plays in collaborative data analysis within the business domain. The discussion of record-keeping is based on observations from a user study in which co-located teams worked on collaborative visual analytics tasks using large interactive wall and tabletop displays. Part of our findings is a collaborative data analysis framework that encompasses note taking as one of the main activities. We observed that record-keeping was a critical activity within the analysis process. Based on our observations, we characterize notes according to their content, scope, and usage, and describe how they fit into a process of collaborative data analysis. We then discuss suggestions for the design of collaborative visual analytics tools.",
    "Authors": "Mahyar, N.;Sarvghad, A.;Tory, M.",
    "Clusters": "CollaborativeVisualization;EvaluationGeneral;LargeAndHighResDisplays;ProvenanceAndHistory;",
    "DOI": "10.1109\/VAST.2010.5652879",
    "Keywords": "wall displays;collaboration;tabletop;note taking;provenance;recording;history",
    "Keywords_Processed": "history;wall display;tabletop;collaboration;note take;recording;provenance",
    "Title": "A closer look at note taking in the co-located collaborative visual analytics process"
  },
  "323": {
    "Abstract": "This design paper presents new guidance for creating map legends in a dynamic environment. Our contribution is a set ofguidelines for legend design in a visualization context and a series of illustrative themes through which they may be expressed. Theseare demonstrated in an applications context through interactive software prototypes. The guidelines are derived from cartographicliterature and in liaison with EDINA who provide digital mapping services for UK tertiary education. They enhance approaches tolegend design that have evolved for static media with visualization by considering: selection, layout, symbols, position, dynamismand design and process. Broad visualization legend themes include: The Ground Truth Legend, The Legend as Statistical Graphicand The Map is the Legend. Together, these concepts enable us to augment legends with dynamic properties that address specificneeds, rethink their nature and role and contribute to a wider re-evaluation of maps as artifacts of usage rather than statements offact. EDINA has acquired funding to enhance their clients with visualization legends that use these concepts as a consequence ofthis work. The guidance applies to the design of a wide range of legends and keys used in cartography and information visualization.",
    "Authors": "Dykes, J.;Wood, J.;Slingsby, A.",
    "Clusters": "GeographyGeospatialVisCartographyTerrainVis;InternetWebVisualizationForTheMasses;VisualDesignDesignGuidelines;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2010.191",
    "Keywords": "online web mapping;legend;visualization;cartography;design;digimap service",
    "Keywords_Processed": "online web mapping;visualization;legend;digimap service;design;cartography",
    "Title": "Rethinking Map Legends with Visualization"
  },
  "128": {
    "Abstract": "Sports analysts live in a world of dynamic games flattened into tables of numbers, divorced from the rinks, pitches, and courts where they were generated. Currently, these professional analysts use R, Stata, SAS, and other statistical software packages for uncovering insights from game data. Quantitative sports consultants seek a competitive advantage both for their clients and for themselves as analytics becomes increasingly valued by teams, clubs, and squads. In order for the information visualization community to support the members of this blossoming industry, it must recognize where and how visualization can enhance the existing analytical workflow. In this paper, we identify three primary stages of today's sports analyst's routine where visualization can be beneficially integrated: 1) exploring a dataspace; 2) sharing hypotheses with internal colleagues; and 3) communicating findings to stakeholders.Working closely with professional ice hockey analysts, we designed and built SnapShot, a system to integrate visualization into the hockey intelligence gathering process. SnapShot employs a variety of information visualization techniques to display shot data, yet given the importance of a specific hockey statistic, shot length, we introduce a technique, the radial heat map. Through a user study, we received encouraging feedback from several professional analysts, both independent consultants and professional team personnel.",
    "Authors": "Pileggi, H.;Stolper, C.D.;Boyle, J.M.;Stasko, J.",
    "Clusters": "HumanComputerInteractionHumanFactors;HypothesisFormingTestingAndVisualEvidence;KnowledgeDiscovery;VisualKnowledgeRepresentationAndExternalization",
    "DOI": "10.1109\/TVCG.2012.263",
    "Keywords": "visual evidence;visual knowledge representation;hypothesis testing;human-computer interaction;visual knowledge discovery",
    "Keywords_Processed": "hypothesis testing;visual evidence;human computer interaction;visual knowledge representation;visual knowledge discovery",
    "Title": "SnapShot: Visualization to Propel Ice Hockey Analytics"
  },
  "375": {
    "Abstract": "In flow simulations the behavior and properties of particle trajectories often depend on the physical geometry contained in the simulated environment. Understanding the flow in and around the geometry itself is an important part of analyzing the data. Previous work has often utilized focus+context rendering techniques, with an emphasis on showing trajectories while simplifying or illustratively rendering the physical areas. Our research instead emphasizes the local relationship between particle paths and geometry by using a projected multi-field visualization technique. The correlation between a particle path and its surrounding area is calculated on-the-fly and displayed in a non-intrusive manner. In addition, we support visual exploration and comparative analysis through the use of linked information visualization, such as manipulatable curve plots and one-on-one similarity plots. Our technique is demonstrated on particle trajectories from a groundwater simulation and a computer room airflow simulation, where the flow of particles is highly influenced by the dense geometry.",
    "Authors": "Jones, C.;Kwan-Liu Ma",
    "Clusters": "FlowVisualizationDataAndTechniques;FocusContextTechniques;MultidimensionalMultivariateMultifieldDataAndTechniques;MultipleLinkedCoordinatedViews",
    "DOI": "10.1109\/TVCG.2010.218",
    "Keywords": "flow visualization;coordinated linked views;focus+context visualization;multi-field visualization",
    "Keywords_Processed": "coordinate link view;focus context visualization;multi field visualization;flow visualization",
    "Title": "Visualizing Flow Trajectories Using Locality-based Rendering and Warped Curve Plots"
  },
  "93": {
    "Abstract": "Distributed systems are complex to develop and administer, and performance problem diagnosis is particularly challenging. When performance degrades, the problem might be in any of the system's many components or could be a result of poor interactions among them. Recent research efforts have created tools that automatically localize the problem to a small number of potential culprits, but research is needed to understand what visualization techniques work best for helping distributed systems developers understand and explore their results. This paper compares the relative merits of three well-known visualization approaches (side-by-side, diff, and animation) in the context of presenting the results of one proven automated localization technique called request-flow comparison. Via a 26-person user study, which included real distributed systems developers, we identify the unique benefits that each approach provides for different problem types and usage modes.",
    "Authors": "Sambasivan, R.R.;Shafer, I.;Mazurek, M.L.;Ganger, G.R.",
    "Clusters": "DistributedSystemsAndGridEnvironments;HumanComputerInteractionHumanFactors;ReasoningProblemSolvingAndDecisionMaking;",
    "DOI": "10.1109\/TVCG.2013.233",
    "Keywords": "visualization;distributed systems;problem diagnosis;human factors",
    "Keywords_Processed": "visualization;problem diagnosis;distribute system;human factor",
    "Title": "Visualizing Request-Flow Comparison to Aid Performance Diagnosis in Distributed Systems"
  },
  "102": {
    "Abstract": "We characterize the design space of the algorithms that sequentially tile a rectangular area with smaller, fixed-surface, rectangles. This space consist of five independent dimensions: Order, Size, Score, Recurse and Phrase. Each of these dimensions describe a particular aspect of such layout tasks. This class of layouts is interesting, because, beyond encompassing simple grids, tables and trees, it also includes all kinds of treemaps involving the placement of rectangles. For instance, Slice and dice, Squarified, Strip and Pivot layouts are various points in this five dimensional space. Many classic statistics visualizations, such as 100% stacked bar charts, mosaic plots and dimensional stacking, are also instances of this class. A few new and potentially interesting points in this space are introduced, such as spiral treemaps and variations on the strip layout. The core algorithm is implemented as a JavaScript prototype that can be used as a layout component in a variety of InfoViz toolkits.",
    "Authors": "Baudel, T.;Broeksema, B.",
    "Clusters": "ChartsDiagramsPlots;DimensionalityReduction;HierarchicalTreeDataAndTechniques;MeshesGridsAndLattices;VisualEncodingAndLayoutGeneral;VisualizationTheoryModelsAndMethods",
    "DOI": "10.1109\/TVCG.2012.205",
    "Keywords": "mosaic plots;grids;visualization models;layout;treemap;dimensional stacking;tables & tree layouts",
    "Keywords_Processed": "grid;treemap;layout;table tree layout;dimensional stacking;visualization model;mosaic plot",
    "Title": "Capturing the Design Space of Sequential Space-filling Layouts"
  },
  "24": {
    "Abstract": "Visualizing symmetric patterns in the data often helps the domain scientists make important observations and gain insights about the underlying experiment. Detecting symmetry in scalar fields is a nascent area of research and existing methods that detect symmetry are either not robust in the presence of noise or computationally costly. We propose a data structure called the augmented extremum graph and use it to design a novel symmetry detection method based on robust estimation of distances. The augmented extremum graph captures both topological and geometric information of the scalar field and enables robust and computationally efficient detection of symmetry. We apply the proposed method to detect symmetries in cryo-electron microscopy datasets and the experiments demonstrate that the algorithm is capable of detecting symmetry even in the presence of significant noise. We describe novel applications that use the detected symmetry to enhance visualization of scalar field data and facilitate their exploration.",
    "Authors": "Thomas, D.M.;Natarajan, V.",
    "Clusters": "AnalysisProcessGeneral;GraphNetworkDataAndTechniques;ScalarFieldDataTechniques;TopologyBasedTechniques;VisualPatternFeatureDetectionAndTracking",
    "DOI": "10.1109\/TVCG.2013.148",
    "Keywords": "data exploration;scalar field visualization;extremum graph;morse decomposition;symmetry detection",
    "Keywords_Processed": "symmetry detection;morse decomposition;extremum graph;scalar field visualization;datum exploration",
    "Title": "Detecting Symmetry in Scalar fields Using Augmented Extremum Graphs"
  },
  "200": {
    "Abstract": "In modeling and analysis of longitudinal social networks, visual exploration is used in particular to complement and inform other methods. The most common graphical representations for this purpose appear to be animations and small multiples of intermediate states, depending on the type of media available. We present an alternative approach based on matrix representation of gestaltlines (a combination of Tufte's sparklines with glyphs based on gestalt theory). As a result, we obtain static, compact, yet data-rich diagrams that support specifically the exploration of evolving dyadic relations and persistent group structure, although at the expense of cross-sectional network views and indirect linkages.",
    "Authors": "Brandes, U.;Nick, B.",
    "Clusters": "GlyphsGlyphBasedTechniques;GraphNetworkDataAndTechniques;KnowledgeDiscovery;SocialNetworksAndSocialMedia;TimeseriesTimeVaryingDataAndTechniques",
    "DOI": "10.1109\/TVCG.2011.169",
    "Keywords": "glyph-based techniques;visual knowledge discovery and representation;time-series data;network visualization;social networks",
    "Keywords_Processed": "visual knowledge discovery and representation;network visualization;glyph base technique;social network;time series datum",
    "Title": "Asymmetric Relations in Longitudinal Social Networks"
  },
  "225": {
    "Abstract": "We present a novel dynamic graph visualization technique based on node-link diagrams. The graphs are drawn side-byside from left to right as a sequence of narrow stripes that are placed perpendicular to the horizontal time line. The hierarchically organized vertices of the graphs are arranged on vertical, parallel lines that bound the stripes; directed edges connect these vertices from left to right. To address massive overplotting of edges in huge graphs, we employ a splatting approach that transforms the edges to a pixel-based scalar field. This field represents the edge densities in a scalable way and is depicted by non-linear color mapping. The visualization method is complemented by interaction techniques that support data exploration by aggregation, filtering, brushing, and selective data zooming. Furthermore, we formalize graph patterns so that they can be interactively highlighted on demand. A case study on software releases explores the evolution of call graphs extracted from the JUnit open source software project. In a second application, we demonstrate the scalability of our approach by applying it to a bibliography dataset containing more than 1.5 million paper titles from 60 years of research history producing a vast amount of relations between title words.",
    "Authors": "Burch, M.;Vehlow, C.;Beck, F.;Diehl, S.;Weiskopf, D.",
    "Clusters": "GraphNetworkDataAndTechniques;ProgrammingAlgorithmsAndDataStructures;SoftwareVisualization",
    "DOI": "10.1109\/TVCG.2011.226",
    "Keywords": "software visualization;graph splatting;dynamic graph visualization;software evolution",
    "Keywords_Processed": "dynamic graph visualization;software visualization;software evolution;graph splatte",
    "Title": "Parallel Edge Splatting for Scalable Dynamic Graph Visualization"
  },
  "160": {
    "Abstract": "Due to the inherent characteristics of the visualization process, most of the problems in this field have strong ties with human cognition and perception. This makes the human brain and sensory system the only truly appropriate evaluation platform for evaluating and fine-tuning a new visualization method or paradigm. However, getting humans to volunteer for these purposes has always been a significant obstacle, and thus this phase of the development process has traditionally formed a bottleneck, slowing down progress in visualization research. We propose to take advantage of the newly emerging field of Human Computation (HC) to overcome these challenges. HC promotes the idea that rather than considering humans as users of the computational system, they can be made part of a hybrid computational loop consisting of traditional computation resources and the human brain and sensory system. This approach is particularly successful in cases where part of the computational problem is considered intractable using known computer algorithms but is trivial to common sense human knowledge. In this paper, we focus on HC from the perspective of solving visualization problems and also outline a framework by which humans can be easily seduced to volunteer their HC resources. We introduce a purpose-driven game titled \u00e2\u20ac\u0153Disguise\u00e2\u20ac\u009d which serves as a prototypical example for how the evaluation of visualization algorithms can be mapped into a fun and addicting activity, allowing this task to be accomplished in an extensive yet cost effective way. Finally, we sketch out a framework that transcends from the pure evaluation of existing visualization methods to the design of a new one.",
    "Authors": "Ahmed, N.;Ziyi Zheng;Mueller, K.",
    "Clusters": "ColorColorPerception;EvaluationGeneral;HumanComputerInteractionHumanFactors;Perception",
    "DOI": "10.1109\/TVCG.2012.234",
    "Keywords": "perception;color blending;evaluation;human computation",
    "Keywords_Processed": "color blending;human computation;evaluation;perception",
    "Title": "Human Computation in Visualization: Using Purpose Driven Games for Robust Evaluation of Visualization Algorithms"
  },
  "47": {
    "Abstract": "This paper describes an advanced visualization method for the analysis of defects in industrial 3D X-Ray Computed Tomography (XCT) data. We present a novel way to explore a high number of individual objects in a dataset, e.g., pores, inclusions, particles, fibers, and cracks demonstrated on the special application area of pore extraction in carbon fiber reinforced polymers (CFRP). After calculating the individual object properties volume, dimensions and shape factors, all objects are clustered into a mean object (MObject). The resulting MObject parameter space can be explored interactively. To do so, we introduce the visualization of mean object sets (MObject Sets) in a radial and a parallel arrangement. Each MObject may be split up into sub-classes by selecting a specific property, e.g., volume or shape factor, and the desired number of classes. Applying this interactive selection iteratively leads to the intended classifications and visualizations of MObjects along the selected analysis path. Hereby the given different scaling factors of the MObjects down the analysis path are visualized through a visual linking approach. Furthermore the representative MObjects are exported as volumetric datasets to serve as input for successive calculations and simulations. In the field of porosity determination in CFRP non-destructive testing practitioners use representative MObjects to improve ultrasonic calibration curves. Representative pores also serve as input for heat conduction simulations in active thermography. For a fast overview of the pore properties in a dataset we propose a local MObjects visualization in combination with a color-coded homogeneity visualization of cells. The advantages of our novel approach are demonstrated using real world CFRP specimens. The results were evaluated through a questionnaire in order to determine the practicality of the MObjects visualization as a supportive tool for domain specialists.",
    "Authors": "Reh, A.;Gusenbauer, C.;Kastner, J.;Groller, E.;Heinzl, C.",
    "Clusters": "BiomedicalScienceAndMedicine;MaterialScience;Parameterization;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/TVCG.2013.177",
    "Keywords": "porosity;parameter space analysis;mobjects;carbon fiber reinforced polymers;3d x-ray computed tomography",
    "Keywords_Processed": "3d ray compute tomography;mobject;paramet space analysis;porosity;carbon fiber reinforce polymer",
    "Title": "MObjects--A Novel Method for the Visualization and Interactive Exploration of Defects in Industrial XCT Data"
  },
  "150": {
    "Abstract": "We introduce a simple, yet powerful method called the Cumulative Heat Diffusion for shape-based volume analysis, while drastically reducing the computational cost compared to conventional heat diffusion. Unlike the conventional heat diffusion process, where the diffusion is carried out by considering each node separately as the source, we simultaneously consider all the voxels as sources and carry out the diffusion, hence the term cumulative heat diffusion. In addition, we introduce a new operator that is used in the evaluation of cumulative heat diffusion called the Volume Gradient Operator (VGO). VGO is a combination of the LBO and a data-driven operator which is a function of the half gradient. The half gradient is the absolute value of the difference between the voxel intensities. The VGO by its definition captures the local shape information and is used to assign the initial heat values. Furthermore, VGO is also used as the weighting parameter for the heat diffusion process. We demonstrate that our approach can robustly extract shape-based features and thus forms the basis for an improved classification and exploration of features based on shape.",
    "Authors": "Gurijala, K.C.;Lei Wang;Kaufman, A.",
    "Clusters": "PhysicsAndPhysicalSciences;SegmentationAndClassification;ShapeRelatedTechniques;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2012.210",
    "Keywords": "shape-based volume analysis;transfer function;classification;heat diffusion;volume gradient operator",
    "Keywords_Processed": "heat diffusion;volume gradient operator;transfer function;shape base volume analysis;classification",
    "Title": "Cumulative Heat Diffusion Using Volume Gradient Operator for Volume Analysis"
  },
  "177": {
    "Abstract": "This paper presents a visualization approach for detecting and exploring similarity in the temporal variation of field data. We provide an interactive technique for extracting correlations from similarity matrices which capture temporal similarity of univariate functions. We make use of the concept to extract periodic and quasiperiodic behavior at single (spatial) points as well as similarity between different locations within a field and also between different data sets. The obtained correlations are utilized for visual exploration of both temporal and spatial relationships in terms of temporal similarity. Our entire pipeline offers visual interaction and inspection, allowing for the flexibility that in particular time-dependent data analysis techniques require. We demonstrate the utility and versatility of our approach by applying our implementation to data from both simulation and measurement.",
    "Authors": "Frey, S.;Sadlo, F.;Ertl, T.",
    "Clusters": "ComparisonComparativeVisualizationAndSimilarity;TimeseriesTimeVaryingDataAndTechniques",
    "DOI": "10.1109\/TVCG.2012.284",
    "Keywords": "comparative visualization;time-dependent fields;similarity analysis;interactive recurrence analysis",
    "Keywords_Processed": "time dependent field;similarity analysis;comparative visualization;interactive recurrence analysis",
    "Title": "Visualization of Temporal Similarity in field Data"
  },
  "296": {
    "Abstract": "Geographically-grounded situational awareness (SA) is critical to crisis management and is essential in many other decision making domains that range from infectious disease monitoring, through regional planning, to political campaigning. Social media are becoming an important information input to support situational assessment (to produce awareness) in all domains. Here, we present a geovisual analytics approach to supporting SA for crisis events using one source of social media, Twitter. Specifically, we focus on leveraging explicit and implicit geographic information for tweets, on developing place-time-theme indexing schemes that support overview+detail methods and that scale analytical capabilities to relatively large tweet volumes, and on providing visual interface methods to enable understanding of place, time, and theme components of evolving situations. Our approach is user-centered, using scenario-based design methods that include formal scenarios to guide design and validate implementation as well as a systematic claims analysis to justify design choices and provide a framework for future testing. The work is informed by a structured survey of practitioners and the end product of Phase-I development is demonstrated \/ validated through implementation in SensePlace2, a map-based, web application initially focused on tweets but extensible to other media.",
    "Authors": "MacEachren, A.M.;Jaiswal, A.;Robinson, A.;Pezanowski, S.;Savelyev, A.;Mitra, P.;Zhang, X.;Blanford, J.",
    "Clusters": "Cognition;DesignMethodologiesAndInteractionDesign;EmergencyDisasterManagement;GeographyGeospatialVisCartographyTerrainVis;SocialNetworksAndSocialMedia;SpatiotemporalDataAndTechniques;TextDocumentTopicAnalysisDataAndTechniques",
    "DOI": "10.1109\/VAST.2011.6102456",
    "Keywords": "crisis management;text analytics;social media analytics;situation awareness;spatio-temporal analysis;scenario-based design;geovisualization",
    "Keywords_Processed": "scenario base design;spatio temporal analysis;geovisualization;situation awareness;crisis management;text analytic;social medium analytic",
    "Title": "SensePlace2: GeoTwitter analytics support for situational awareness"
  },
  "285": {
    "Abstract": "We present the visual analysis of a biologically inspired CFD simulation of the deformable flapping wings of a dragonfly as it takes off and begins to maneuver, using vortex detection and integration-based flow lines. The additional seed placement and perceptual challenges introduced by having multiple dynamically deforming objects in the highly unsteady 3D flow domain are addressed. A brief overview of the high speed photogrammetry setup used to capture the dragonfly takeoff, parametric surfaces used for wing reconstruction, CFD solver and underlying flapping flight theory is presented to clarify the importance of several unsteady flight mechanisms, such as the leading edge vortex, that are captured visually. A novel interactive seed placement method is used to simplify the generation of seed curves that stay in the vicinity of relevant flow phenomena as they move with the flapping wings. This method allows a user to define and evaluate the quality of a seed's trajectory over time while working with a single time step. The seed curves are then used to place particles, streamlines and generalized streak lines. The novel concept of flowing seeds is also introduced in order to add visual context about the instantaneous vector fields surrounding smoothly animate streak lines. Tests show this method to be particularly effective at visually capturing vortices that move quickly or that exist for a very brief period of time. In addition, an automatic camera animation method is used to address occlusion issues caused when animating the immersed wing boundaries alongside many geometric flow lines. Each visualization method is presented at multiple time steps during the up-stroke and down-stroke to highlight the formation, attachment and shedding of the leading edge vortices in pairs of wings. Also, the visualizations show evidence of wake capture at stroke reversal which suggests the existence of previously unknown unsteady lift generation mechanisms that are unique to qua- wing insects.",
    "Authors": "Koehler, C.;Wischgoll, T.;Haibo Dong;Gaston, Z.",
    "Clusters": "BiologyAndBioinformatics;FlowVisualizationDataAndTechniques;StreamlinesPathlinesStreaklines",
    "DOI": "10.1109\/TVCG.2011.260",
    "Keywords": "flow visualization;vortex visualization;unsteady flow;streamlines;flowing seed points;streaklines;insect flight",
    "Keywords_Processed": "flow seed point;streamline;unsteady flow;insect flight;vortex visualization;streakline;flow visualization",
    "Title": "Vortex Visualization in Ultra Low Reynolds Number Insect Flight"
  },
  "348": {
    "Abstract": "We present the design and evaluation of FI3D, a direct-touch data exploration technique for 3D visualization spaces. The exploration of three-dimensional data is core to many tasks and domains involving scientific visualizations. Thus, effective data navigation techniques are essential to enable comprehension, understanding, and analysis of the information space. While evidence exists that touch can provide higher-bandwidth input, somesthetic information that is valuable when interacting with virtual worlds, and awareness when working in collaboration, scientific data exploration in 3D poses unique challenges to the development of effective data manipulations. We present a technique that provides touch interaction with 3D scientific data spaces in 7 DOF. This interaction does not require the presence of dedicated objects to constrain the mapping, a design decision important for many scientific datasets such as particle simulations in astronomy or physics. We report on an evaluation that compares the technique to conventional mouse-based interaction. Our results show that touch interaction is competitive in interaction speed for translation and integrated interaction, is easy to learn and use, and is preferred for exploration and wayfinding tasks. To further explore the applicability of our basic technique for other types of scientific visualizations we present a second case study, adjusting the interaction to the illustrative visualization of fiber tracts of the brain and the manipulation of cutting planes in this context.",
    "Authors": "Lingyun Yu;Svetachov, P.;Isenberg, P.;Everts, M.H.;Isenberg, T.",
    "Clusters": "EvaluationGeneral;IllustrativeVisualization;InteractionTechniquesGeneral;LargeAndHighResDisplays;ZoomingAndNavigationTechniques",
    "DOI": "10.1109\/TVCG.2010.157",
    "Keywords": "direct-touch interaction;illustrative visualization;wall displays;evaluation;3d navigation and exploration",
    "Keywords_Processed": "direct touch interaction;3d navigation and exploration;illustrative visualization;wall display;evaluation",
    "Title": "FI3D: Direct-Touch Interaction for the Exploration of 3D Scientific Visualization Spaces"
  },
  "69": {
    "Abstract": "Time-oriented data play an essential role in many Visual Analytics scenarios such as extracting medical insights from collections of electronic health records or identifying emerging problems and vulnerabilities in network traffic. However, many software libraries for Visual Analytics treat time as a flat numerical data type and insufficiently tackle the complexity of the time domain such as calendar granularities and intervals. Therefore, developers of advanced Visual Analytics designs need to implement temporal foundations in their application code over and over again. We present TimeBench, a software library that provides foundational data structures and algorithms for time-oriented data in Visual Analytics. Its expressiveness and developer accessibility have been evaluated through application examples demonstrating a variety of challenges with time-oriented data and long-term developer studies conducted in the scope of research and student projects.",
    "Authors": "Rind, A.;Lammarsch, T.;Aigner, W.;Alsallakh, B.;Miksch, S.",
    "Clusters": "TimeseriesTimeVaryingDataAndTechniques;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/TVCG.2013.206",
    "Keywords": "information visualization;toolkits;software infrastructure;time;visual analytics;temporal data",
    "Keywords_Processed": "time;information visualization;visual analytic;toolkit;temporal datum;software infrastructure",
    "Title": "TimeBench: A Data Model and Software Library for Visual Analytics of Time-Oriented Data"
  },
  "211": {
    "Abstract": "Mobile users of maps typically need detailed information about their surroundings plus some context information about remote places. In order to avoid that the map partly gets too dense, cartographers have designed mapping functions that enlarge a user-defined focus region - such functions are sometimes called fish-eye projections. The extra map space occupied by the enlarged focus region is compensated by distorting other parts of the map. We argue that, in a map showing a network of roads relevant to the user, distortion should preferably take place in those areas where the network is sparse. Therefore, we do not apply a predefined mapping function. Instead, we consider the road network as a graph whose edges are the road segments. We compute a new spatial mapping with a graph-based optimization approach, minimizing the square sum of distortions at edges. Our optimization method is based on a convex quadratic program (CQP); CQPs can be solved in polynomial time. Important requirements on the output map are expressed as linear inequalities. In particular, we show how to forbid edge crossings. We have implemented our method in a prototype tool. For instances of different sizes, our method generated output maps that were far less distorted than those generated with a predefined fish-eye projection. Future work is needed to automate the selection of roads relevant to the user. Furthermore, we aim at fast heuristics for application in real-time systems.",
    "Authors": "Haunert, J.-H.;Sering, L.",
    "Clusters": "FocusContextTechniques;GeographyGeospatialVisCartographyTerrainVis;GraphNetworkDataAndTechniques;Maps;Optimization;ProgrammingAlgorithmsAndDataStructures",
    "DOI": "10.1109\/TVCG.2011.191",
    "Keywords": "graph drawing;fisheye view;optimization;quadratic programming;cartography;schematic maps",
    "Keywords_Processed": "optimization;graph drawing;fisheye view;quadratic programming;cartography;schematic map",
    "Title": "Drawing Road Networks with Focus Regions"
  },
  "233": {
    "Abstract": "Network data often contain important attributes from various dimensions such as social affiliations and areas of expertise in a social network. If such attributes exhibit a tree structure, visualizing a compound graph consisting of tree and network structures becomes complicated. How to visually reveal patterns of a network over a tree has not been fully studied. In this paper, we propose a compound graph model, TreeNet, to support visualization and analysis of a network at multiple levels of aggregation over a tree. We also present a visualization design, TreeNetViz, to offer the multiscale and cross-scale exploration and interaction of a TreeNet graph. TreeNetViz uses a Radial, Space-Filling (RSF) visualization to represent the tree structure, a circle layout with novel optimization to show aggregated networks derived from TreeNet, and an edge bundling technique to reduce visual complexity. Our circular layout algorithm reduces both total edge-crossings and edge length and also considers hierarchical structure constraints and edge weight in a TreeNet graph. These experiments illustrate that the algorithm can reduce visual cluttering in TreeNet graphs. Our case study also shows that TreeNetViz has the potential to support the analysis of a compound graph by revealing multiscale and cross-scale network patterns.",
    "Authors": "Liang Gou;Xiaolong Zhang",
    "Clusters": "GraphNetworkDataAndTechniques;MultiScaleDataTechniques;VisualizationSystemsToolkitsAndEnvironments",
    "DOI": "10.1109\/TVCG.2011.247",
    "Keywords": "network and tree;multi-scale and cross-scale;visualization;treenetviz;compound graphs",
    "Keywords_Processed": "visualization;treenetviz;network and tree;multi scale and cross scale;compound graph",
    "Title": "TreeNetViz: Revealing Patterns of Networks over Tree Structures"
  },
  "103": {
    "Abstract": "Classifying a set of objects into clusters can be done in numerous ways, producing different results. They can be visually compared using contingency tables [27], mosaicplots [13], fluctuation diagrams [15], tableplots [20] , (modified) parallel coordinates plots [28], Parallel Sets plots [18] or circos diagrams [19]. Unfortunately the interpretability of all these graphical displays decreases rapidly with the numbers of categories and clusterings. In his famous book A Semiology of Graphics [5] Bertin writes \u00e2\u20ac\u0153the discovery of an ordered concept appears as the ultimate point in logical simplification since it permits reducing to a single instant the assimilation of series which previously required many instants of study\u00e2\u20ac\u009d. Or in more everyday language, if you use good orderings you can see results immediately that with other orderings might take a lot of effort. This is also related to the idea of effect ordering [12], that data should be organised to reflect the effect you want to observe. This paper presents an efficient algorithm based on Bertin's idea and concepts related to Kendall's t [17], which finds informative joint orders for two or more nominal classification variables. We also show how these orderings improve the various displays and how groups of corresponding categories can be detected using a top-down partitioning algorithm. Different clusterings based on data on the environmental performance of cars sold in Germany are used for illustration. All presented methods are available in the R package extracat which is used to compute the optimized orderings for the example dataset.",
    "Authors": "Pilhofer, A.;Gribov, A.;Unwin, A.",
    "Clusters": "ChartsDiagramsPlots;Optimization;SegmentationAndClassification;TimeseriesTimeVaryingDataAndTechniques",
    "DOI": "10.1109\/TVCG.2012.207",
    "Keywords": "order optimization;classification;fluctuation diagrams;seriation",
    "Keywords_Processed": "classification;fluctuation diagram;seriation;order optimization",
    "Title": "Comparing Clusterings Using Bertin's Idea"
  },
  "205": {
    "Abstract": "We consider moving objects as multivariate time-series. By visually analyzing the attributes, patterns may appear that explain why certain movements have occurred. Density maps as proposed by Scheepens et al. [25] are a way to reveal these patterns by means of aggregations of filtered subsets of trajectories. Since filtering is often not sufficient for analysts to express their domain knowledge, we propose to use expressions instead. We present a flexible architecture for density maps to enable custom, versatile exploration using multiple density fields. The flexibility comes from a script, depicted in this paper as a block diagram, which defines an advanced computation of a density field. We define six different types of blocks to create, compose, and enhance trajectories or density fields. Blocks are customized by means of expressions that allow the analyst to model domain knowledge. The versatility of our architecture is demonstrated with several maritime use cases developed with domain experts. Our approach is expected to be useful for the analysis of objects in other domains.",
    "Authors": "Scheepens, R.;Willems, N.;van de Wetering, H.;Andrienko, G.;Andrienko, N.;van Wijk, J.J.",
    "Clusters": "AnimationAndMotion;GeographyGeospatialVisCartographyTerrainVis;ImageBasedDataImageSignalProcessing;MachineLearningAndStatistics;MultidimensionalMultivariateMultifieldDataAndTechniques",
    "DOI": "10.1109\/TVCG.2011.181",
    "Keywords": "geographical information systems;kernel density estimation;raster maps;multivariate data;trajectory",
    "Keywords_Processed": "multivariate datum;raster map;geographical information system;trajectory;kernel density estimation",
    "Title": "Composite Density Maps for Multivariate Trajectories"
  },
  "246": {
    "Abstract": "In this paper we present a framework to define transfer functions from a target distribution provided by the user. A target distribution can reflect the data importance, or highly relevant data value interval, or spatial segmentation. Our approach is based on a communication channel between a set of viewpoints and a set of bins of a volume data set, and it supports 1D as well as 2D transfer functions including the gradient information. The transfer functions are obtained by minimizing the informational divergence or Kullback-Leibler distance between the visibility distribution captured by the viewpoints and a target distribution selected by the user. The use of the derivative of the informational divergence allows for a fast optimization process. Different target distributions for 1D and 2D transfer functions are analyzed together with importance-driven and view-based techniques.",
    "Authors": "Ruiz, M.;Bardera, A.;Boada, I.;Viola, I.;Feixas, M.;Sbert, M.",
    "Clusters": "InformationTheory;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2011.173",
    "Keywords": "information theory;transfer function;informational divergence;kullback-leibler distance",
    "Keywords_Processed": "kullback leibler distance;transfer function;informational divergence;information theory",
    "Title": "Automatic Transfer Functions Based on Informational Divergence"
  },
  "21": {
    "Abstract": "We enhance a user-centered design process with techniques that deliberately promote creativity to identify opportunities for the visualization of data generated by a major energy supplier. Visualization prototypes developed in this way prove effective in a situation whereby data sets are largely unknown and requirements open - enabling successful exploration of possibilities for visualization in Smart Home data analysis. The process gives rise to novel designs and design metaphors including data sculpting. It suggests: that the deliberate use of creativity techniques with data stakeholders is likely to contribute to successful, novel and effective solutions; that being explicit about creativity may contribute to designers developing creative solutions; that using creativity techniques early in the design process may result in a creative approach persisting throughout the process. The work constitutes the first systematic visualization design for a data rich source that will be increasingly important to energy suppliers and consumers as Smart Meter technology is widely deployed. It is novel in explicitly employing creativity techniques at the requirements stage of visualization design and development, paving the way for further use and study of creativity methods in visualization design.",
    "Authors": "Goodwin, S.;Dykes, J.;Jones, S.;Dillingham, I.;Dove, G.;Duffy, A.;Kachkaev, A.;Slingsby, A.;Wood, J.",
    "Clusters": "ApplicationsGeneralAndOther;DesignMethodologiesAndInteractionDesign;EarthSpaceAndEnvironmentalSciences;",
    "DOI": "10.1109\/TVCG.2013.145",
    "Keywords": "smart home;energy consumption;creativity techniques;visualization;user-centered design",
    "Keywords_Processed": "visualization;energy consumption;user center design;creativity technique;smart home",
    "Title": "Creative User-Centered Visualization Design for Energy Analysts and Modelers"
  },
  "113": {
    "Abstract": "Reading a visualization can involve a number of tasks such as extracting, comparing or aggregating numerical values. Yet, most of the charts that are published in newspapers, reports, books, and on the Web only support a subset of these tasks. In this paper we introduce graphical overlays-visual elements that are layered onto charts to facilitate a larger set of chart reading tasks. These overlays directly support the lower-level perceptual and cognitive processes that viewers must perform to read a chart. We identify five main types of overlays that support these processes; the overlays can provide (1) reference structures such as gridlines, (2) highlights such as outlines around important marks, (3) redundant encodings such as numerical data labels, (4) summary statistics such as the mean or max and (5) annotations such as descriptive text for context. We then present an automated system that applies user-chosen graphical overlays to existing chart bitmaps. Our approach is based on the insight that generating most of these graphical overlays only requires knowing the properties of the visual marks and axes that encode the data, but does not require access to the underlying data values. Thus, our system analyzes the chart bitmap to extract only the properties necessary to generate the desired overlay. We also discuss techniques for generating interactive overlays that provide additional controls to viewers. We demonstrate several examples of each overlay type for bar, pie and line charts.",
    "Authors": "Kong, N.;Agrawala, M.",
    "Clusters": "Cognition;Perception;VisualizationTechniquesAndToolsGeneral",
    "DOI": "10.1109\/TVCG.2012.229",
    "Keywords": "graph comprehension;visualization;overlays;graphical perception",
    "Keywords_Processed": "visualization;graphical perception;overlay;graph comprehension",
    "Title": "Graphical Overlays: Using Layered Elements to Aid Chart Reading"
  },
  "219": {
    "Abstract": "We introduce a focus+context method to visualize a complicated metro map of a modern city on a small displaying area. The context of our work is with regard the popularity of mobile devices. The best route to the destination, which can be obtained from the arrival time of trains, is highlighted. The stations on the route enjoy larger spaces, whereas the other stations are rendered smaller and closer to fit the whole map into a screen. To simplify the navigation and route planning for visitors, we formulate various map characteristics such as octilinear transportation lines and regular station distances into energy terms. We then solve for the optimal layout in a least squares sense. In addition, we label the names of stations that are on the route of a passenger according to human preferences, occlusions, and consistencies of label positions using the graph cuts method. Our system achieves real-time performance by being able to report instant information because of the carefully designed energy terms. We apply our method to layout a number of metro maps and show the results and timing statistics to demonstrate the feasibility of our technique.",
    "Authors": "Yu-Shuen Wang;Ming-Te Chi",
    "Clusters": "FocusContextTechniques;Labeling;Maps;Optimization;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2011.205",
    "Keywords": "optimization;focus+context visualization;graph labeling;octilinear layout;metro map",
    "Keywords_Processed": "optimization;metro map;octilinear layout;focus context visualization;graph labeling",
    "Title": "Focus+Context Metro Maps"
  },
  "110": {
    "Abstract": "This paper reports on a between-subject, comparative online study of three information visualization demonstrators that each displayed the same dataset by way of an identical scatterplot technique, yet were different in style in terms of visual and interactive embellishment. We validated stylistic adherence and integrity through a separate experiment in which a small cohort of participants assigned our three demonstrators to predefined groups of stylistic examples, after which they described the styles with their own words. From the online study, we discovered significant differences in how participants execute specific interaction operations, and the types of insights that followed from them. However, in spite of significant differences in apparent usability, enjoyability and usefulness between the style demonstrators, no variation was found on the self-reported depth, expert-rated depth, confidence or difficulty of the resulting insights. Three different methods of insight analysis have been applied, revealing how style impacts the creation of insights, ranging from higher-level pattern seeking to a more reflective and interpretative engagement with content, which is what underlies the patterns. As this study only forms the first step in determining how the impact of style in information visualization could be best evaluated, we propose several guidelines and tips on how to gather, compare and categorize insights through an online evaluation study, particularly in terms of analyzing the concise, yet wide variety of insights and observations in a trustworthy and reproducable manner.",
    "Authors": "Vande Moere, A.;Tomitsch, M.;Wimmer, C.;Christoph, B.;Grechenig, T.",
    "Clusters": "ArtAndAestheticsInVisualization;EvaluationGeneral;IllustrativeVisualization;VisualDesignDesignGuidelines",
    "DOI": "10.1109\/TVCG.2012.221",
    "Keywords": "online study;aesthetics;style;visualization;user experience;design;evaluation",
    "Keywords_Processed": "visualization;style;design;user experience;online study;evaluation;aesthetic",
    "Title": "Evaluating the Effect of Style in Information Visualization"
  },
  "374": {
    "Abstract": "Interactivity is key to exploration of volume data. Interactivity may be hindered due to many factors, e.g. large data size,high resolution or complexity of a data set, or an expensive rendering algorithm. We present a novel framework for visualizing volumedata that enables interactive exploration using proxy images, without accessing the original 3D data. Data exploration using directvolume rendering requires multiple (often redundant) accesses to possibly large amounts of data. The notion of visualization by proxyrelies on the ability to defer operations traditionally used for exploring 3D data to a more suitable intermediate representation forinteraction - proxy images. Such operations include view changes, transfer function exploration, and relighting. While previous workhas addressed specific interaction needs, we provide a complete solution that enables real-time interaction with large data sets andhas low hardware and storage requirements.",
    "Authors": "Tikhonova, A.;Correa, C.;Kwan-Liu Ma",
    "Clusters": "CamerasCameraViewsAndProjections;ImageBasedDataImageSignalProcessing;InteractionTechniquesGeneral;VolumeRenderingModelingAndVisualization",
    "DOI": "10.1109\/TVCG.2010.215",
    "Keywords": "image-based rendering;volume visualization;volume distortion camera;deferred interaction",
    "Keywords_Processed": "image base render;volume visualization;defer interaction;volume distortion camera",
    "Title": "Visualization by Proxy: A Novel Framework for Deferred Interaction with Volume Data"
  },
  "327": {
    "Abstract": "A standard approach for visualizing multivariate networks is to use one or more multidimensional views (for example, scatterplots) for selecting nodes by various metrics, possibly coordinated with a node-link view of the network. In this paper, we present three novel approaches for achieving a tighter integration of these views through hybrid techniques for multidimensional visualization, graph selection and layout. First, we present the FlowVizMenu, a radial menu containing a scatterplot that can be popped up transiently and manipulated with rapid, fluid gestures to select and modify the axes of its scatterplot. Second, the FlowVizMenu can be used to steer an attribute-driven layout of the network, causing certain nodes of a node-link diagram to move toward their corresponding positions in a scatterplot while others can be positioned manually or by force-directed layout. Third, we describe a novel hybrid approach that combines a scatterplot matrix (SPLOM) and parallel coordinates called the Parallel Scatterplot Matrix (P-SPLOM), which can be used to visualize and select features within the network. We also describe a novel arrangement of scatterplots called the Scatterplot Staircase (SPLOS) that requires less space than a traditional scatterplot matrix. Initial user feedback is reported.",
    "Authors": "Viau, C.;McGuffin, M.J.;Chiricota, Y.;Jurisica, I.",
    "Clusters": "ChartsDiagramsPlots;GraphNetworkDataAndTechniques;ParallelCoordinates;UserInterfacesGeneral;VisualEncodingAndLayoutGeneral",
    "DOI": "10.1109\/TVCG.2010.205",
    "Keywords": "attribute-driven layout;interactive graph drawing;scatterplot matrix;radial menus;parallel coordinates;network layout",
    "Keywords_Processed": "parallel coordinate;attribute drive layout;radial menu;scatterplot matrix;network layout;interactive graph drawing",
    "Title": "The FlowVizMenu and Parallel Scatterplot Matrix: Hybrid Multidimensional Visualizations for Network Exploration"
  },
  "13": {
    "Abstract": "We present a novel area-preservation mapping\/flattening method using the optimal mass transport technique, based on the Monge-Brenier theory. Our optimal transport map approach is rigorous and solid in theory, efficient and parallel in computation, yet general for various applications. By comparison with the conventional Monge-Kantorovich approach, our method reduces the number of variables from O(n2) to O(n), and converts the optimal mass transport problem to a convex optimization problem, which can now be efficiently carried out by Newton's method. Furthermore, our framework includes the area weighting strategy that enables users to completely control and adjust the size of areas everywhere in an accurate and quantitative way. Our method significantly reduces the complexity of the problem, and improves the efficiency, flexibility and scalability during visualization. Our framework, by combining conformal mapping and optimal mass transport mapping, serves as a powerful tool for a broad range of applications in visualization and graphics, especially for medical imaging. We provide a variety of experimental results to demonstrate the efficiency, robustness and efficacy of our novel framework.",
    "Authors": "Xin Zhao;Zhengyu Su;Gu, X.;Kaufman, A.;Jian Sun;Jie Gao;Feng Luo",
    "Clusters": "ApplicationsGeneralAndOther;GeometricModeling;Maps;SurfaceRelatedDataAndTechniques",
    "DOI": "10.1109\/TVCG.2013.135",
    "Keywords": "monge-brenier theory;surface flattening;area-preservation mapping;visualization and graphics applications;optimal transport map",
    "Keywords_Processed": "optimal transport map;surface flattening;monge brenier theory;visualization and graphic application;area preservation mapping",
    "Title": "Area-Preservation Mapping using Optimal Mass Transport"
  },
  "288": {
    "Abstract": "Cellular biology deals with studying the behavior of cells. Current time-lapse imaging microscopes help us capture the progress of experiments at intervals that allow for understanding of the dynamic and kinematic behavior of the cells. On the other hand, these devices generate such massive amounts of data (250GB of data per experiment) that manual sieving of data to identify interesting patterns becomes virtually impossible. In this paper we propose an end-to-end system to analyze time-lapse images of the cultures of human neural stem cells (hNSC), that includes an image processing system to analyze the images to extract all the relevant geometric and statistical features within and between images, a database management system to manage and handle queries on the data, a visual analytic system to navigate through the data, and a visual query system to explore different relationships and correlations between the parameters. In each stage of the pipeline we make novel algorithmic and conceptual contributions, and the entire system design is motivated by many different yet unanswered exploratory questions pursued by our neurobiologist collaborators. With a few examples we show how such abstract biological queries can be analyzed and answered by our system.",
    "Authors": "Kulkarni, I.;Mistry, S.Y.;Cummings, B.;Gopi, M.",
    "Clusters": "AnalysisProcessGeneral;BiologyAndBioinformatics;DataAcquisitionAndManagement;NeurosciencesAndBrainVisualization;QueriesAndSearch;ZoomingAndNavigationTechniques",
    "DOI": "10.1109\/VAST.2011.6102459",
    "Keywords": "navigation;neuroscience;stem cell segmentation;exploration;cell imaging;query processing;visual analytics;data management;tracking",
    "Keywords_Processed": "neuroscience;navigation;query processing;tracking;cell image;visual analytic;exploration;stem cell segmentation;data management",
    "Title": "A visual navigation system for querying neural stem cell imaging data"
  }
}