{"0":{"Abstract":"With the recent advances in deep learning, neural network models have obtained state-of-the-art performances for many linguistic tasks in natural language processing. However, this rapid progress also brings enormous challenges. The opaque nature of a neural network model leads to hard-to-debug-systems and difficult-to-interpret mechanisms. Here, we introduce a visualization system that, through a tight yet flexible integration between visualization elements and the underlying model, allows a user to interrogate the model by perturbing the input, internal state, and prediction while observing changes in other parts of the pipeline. We use the natural language inference problem as an example to illustrate how a perturbation-driven paradigm can help domain experts assess the potential limitation of a model, probe its inner states, and interpret and form hypotheses about fundamental model mechanisms such as attention.","Authors":"S. Liu; Z. Li; T. Li; V. Srikumar; V. Pascucci; P. Bremer","DOI":"10.1109\/TVCG.2018.2865230","Keywords":"Natural Language Processing;Interpretable Machine Learning;Natural Language Inference;Attention Visualization","Title":"NLIZE: A Perturbation-Driven Visual Interrogation Tool for Analyzing and Interpreting Natural Language Inference Models","Keywords_Processed":"Natural Language Inference;attention visualization;interpretable machine Learning;Natural Language Processing","Keyword_Vector":[0.0591348064,-0.0222395461,-0.0066894001,-0.0176574328,-0.0055326998,-0.0154153841,-0.0189591235,-0.001256785,-0.0114752311,-0.0006775845,-0.0081190639,0.0030108858,0.0038391824,-0.0197878852,0.0025792301,-0.0225925835,0.0047889015,0.0043825862,0.0136154227,0.0025944814,-0.0171359089,0.0025706256,-0.0014774055,0.0197222619,0.0164965122,0.000925145,0.0599870922,-0.0114462446,-0.0199291905,0.0186906935,0.0018423803,0.0270749541,-0.0119119943,-0.0353152953,-0.0021835291,0.0402623355,-0.0087561202,-0.0050249367,0.0113009199,0.001880062,0.0141651288,-0.0172131277,0.0232213988,0.0439670851,0.0359077501,0.0248022592,0.0452936445,0.0397018885,-0.0243674238,-0.0277028229,-0.0044377605,-0.0256220174],"Abstract_Vector":[0.1725821648,-0.0794243285,-0.0060339536,0.0457862036,-0.093184763,0.0741076346,0.060975841,0.0356217793,-0.0871737385,0.0046748038,0.0451144409,-0.049330274,-0.0110519121,0.0258112925,0.0589344587,0.0292401906,0.0851811773,0.1156284759,-0.0030498362,-0.1288676159,0.0567542642,0.0339742865,0.0072683523,0.0356095413,-0.0073178436,-0.0137624918,0.069213112,-0.0315425152,0.0496095377,0.0529957224,-0.0190613798,-0.0299204521,0.0450440103,0.0189618205,-0.0369048966,-0.0544771665,0.0256669552,-0.0329851359,0.0246717068,-0.0192743725,-0.0455131688,0.0199101679,-0.0328683642,-0.0053633663,-0.0057843998,0.0239729632,0.0382962747,0.0540485986,0.0034469658,0.002988541,-0.0052458554,-0.03095806,-0.0241753541,0.005325483,-0.0096176083,0.0240046756,0.0092938889,0.0028065292,0.0271913658,0.0315506363,0.0410347576,0.0015624194,0.0133313254,-0.025196648,-0.0237888782,-0.0051946255,0.0417830097,0.0299861249,-0.0483841945,0.0345337747,-0.0112472079,0.0171882702,-0.0401267351,-0.0269355887,0.000000472,0.0401481044,-0.0015616655,-0.0155582727,0.0366276635,-0.0287188422,0.0199076261,0.0024177217,0.0157520226,-0.0119547915,0.0298306974,0.0038151855,-0.04973845,0.0342774453,-0.0351930143,-0.0037344619,0.0130175362,-0.0023045961,0.0231466921,-0.0256835445,-0.0211576863,0.001862677,0.0105290915,0.0191772542,-0.0117396624,0.0208590553,0.0074315594,0.0119695515,-0.0278371452,-0.028371886,0.009866283,-0.0437858655,0.0133567896,0.0065012016,0.0185680065,0.0040099797,-0.0355987219,0.0178103496,0.0299475367,0.0493093551,-0.0111849359,-0.0135094035]},"1":{"Abstract":"Complex networks require effective tools and visualizations for their analysis and comparison. Clique communities have been recognized as a powerful concept for describing cohesive structures in networks. We propose an approach that extends the computation of clique communities by considering persistent homology, a topological paradigm originally introduced to characterize and compare the global structure of shapes. Our persistence-based algorithm is able to detect clique communities and to keep track of their evolution according to different edge weight thresholds. We use this information to define comparison metrics and a new centrality measure, both reflecting the relevance of the clique communities inherent to the network. Moreover, we propose an interactive visualization tool based on nested graphs that is capable of compactly representing the evolving relationships between communities for different thresholds and clique degrees. We demonstrate the effectiveness of our approach on various network types.","Authors":"B. Rieck; U. Fugacci; J. Lukasczyk; H. Leitte","DOI":"10.1109\/TVCG.2017.2744321","Keywords":"Persistent homology;topological persistence;cliques;complex networks;visual analysis","Title":"Clique Community Persistence: A Topological Visual Analysis Approach for Complex Networks","Keywords_Processed":"persistent homology;complex network;clique;visual analysis;topological persistence","Keyword_Vector":[0.0949804223,-0.1132142288,-0.0837666502,0.0946926498,-0.0493479651,-0.0575102425,0.1150217781,-0.0273896513,-0.0066500361,-0.0077975526,0.0454173617,-0.0143221517,0.0117211757,0.0905919324,-0.0529712285,-0.0032768073,0.0237461569,0.0550474665,0.0095372723,-0.1028502125,-0.0255924165,-0.0226697659,0.0018626749,-0.0535927675,0.1217689963,-0.0040941369,-0.0471694247,0.0331145262,-0.0236358035,-0.0141708955,-0.0750219427,-0.0472930202,0.0707499331,0.019225983,0.0366410533,-0.0364105245,0.0000884065,-0.0308192479,0.0979252458,-0.0067730075,-0.0015673618,-0.0355493534,0.0053653427,-0.0346580785,0.0164605758,-0.0013080894,-0.0155167009,0.0271685867,-0.0788147167,-0.0049506472,-0.0278291954,-0.0001419895],"Abstract_Vector":[0.1566822918,-0.0632148048,0.0292896925,0.1019860909,0.0647163505,0.0667619127,-0.0071714206,-0.0160679434,-0.1022368715,0.0545798785,0.0393905962,-0.0794831897,-0.0451961262,0.0492208091,0.0775045371,-0.0670188805,-0.0104845496,0.0379754655,0.0054547712,-0.0407988895,-0.0310133746,0.0427642844,-0.0175647165,-0.0446263097,-0.0184807326,0.0376178425,0.006709042,-0.006766324,0.0269150708,0.0445594335,0.0117198657,0.0004611833,-0.019565494,-0.022018433,0.0327637515,0.024648212,0.0201816807,0.0266628713,-0.0122171147,0.0314204025,-0.0033610391,0.0468374749,-0.0102629982,-0.0080206823,0.0227385221,-0.0115029821,0.0307401022,-0.009116892,0.0060321206,0.017247535,0.0302521184,0.025481934,-0.0226147594,0.0076956972,-0.0070066575,0.024157364,0.0082570441,-0.0330892795,0.0225038461,0.0360828567,-0.0174290506,0.0319015582,0.0537025162,-0.0064807577,-0.004275494,-0.0228422856,-0.0075024881,-0.0086088307,0.0101356069,0.0080985585,0.0030175644,0.0446110366,0.0051864008,0.0056740454,0.0175831933,0.0090649729,-0.0231684108,-0.0083071159,-0.0317525431,0.0264658849,-0.0426958496,-0.0222454333,-0.0352160694,-0.0056433965,-0.0312516846,-0.009111266,-0.0414138522,-0.0237369229,-0.0186087991,-0.0088879937,-0.0120388754,-0.0137096997,0.0087274559,-0.0132238199,0.0141219842,0.0059038414,-0.0321055579,0.0044976563,0.0217953952,0.0003832898,-0.0203421933,0.0181185992,0.0151891176,-0.0065648908,0.0253647718,0.0312467996,0.0359708353,0.0115844663,-0.0176221322,0.0091488792,-0.0352014859,-0.0088034217,0.0040485044,-0.0002681086,-0.0091704303,-0.0105418232]},"10":{"Abstract":"Users with anomalous behaviors in online communication systems (e.g. email and social medial platforms) are potential threats to society. Automated anomaly detection based on advanced machine learning techniques has been developed to combat this issue; challenges remain, though, due to the difficulty of obtaining proper ground truth for model training and evaluation. Therefore, substantial human judgment on the automated analysis results is often required to better adjust the performance of anomaly detection. Unfortunately, techniques that allow users to understand the analysis results more efficiently, to make a confident judgment about anomalies, and to explore data in their context, are still lacking. In this paper, we propose a novel visual analysis system, TargetVue, which detects anomalous users via an unsupervised learning model and visualizes the behaviors of suspicious users in behavior-rich context through novel visualization designs and multiple coordinated contextual views. Particularly, TargetVue incorporates three new ego-centric glyphs to visually summarize a user's behaviors which effectively present the user's communication activities, features, and social interactions. An efficient layout method is proposed to place these glyphs on a triangle grid, which captures similarities among users and facilitates comparisons of behaviors of different users. We demonstrate the power of TargetVue through its application in a social bot detection challenge using Twitter data, a case study based on email records, and an interview with expert users. Our evaluation shows that TargetVue is beneficial to the detection of users with anomalous communication behaviors.","Authors":"N. Cao; C. Shi; S. Lin; J. Lu; Y. Lin; C. Lin","DOI":"10.1109\/TVCG.2015.2467196","Keywords":"Anomaly Detection;Social Media;Visual Analysis;Anomaly Detection;Social Media;Visual Analysis","Title":"TargetVue: Visual Analysis of Anomalous User Behaviors in Online Communication Systems","Keywords_Processed":"Anomaly Detection;social medium;Visual Analysis","Keyword_Vector":[0.1800409809,0.0008986375,0.1088202768,-0.0278122839,-0.0256875405,-0.0849935231,-0.0968331143,-0.1206346831,-0.0394632694,-0.0764837562,-0.0536823528,0.1107952429,-0.0635353514,0.0127360152,0.1055305489,-0.0899776174,-0.0225572026,-0.0923738344,0.0063694231,-0.0777435081,-0.0178988693,0.019262206,0.004616419,-0.0037266883,-0.0115057765,0.0240243049,0.0301547094,-0.0614891622,0.0153311619,0.0053582081,-0.080990235,0.0409135169,0.0165064846,0.0043947512,0.019042701,0.0524794497,-0.1206052297,0.0543641356,0.0357050482,-0.0892714454,-0.0509896614,-0.0623837743,-0.0672302165,-0.0242307009,0.0073229922,0.0892083181,0.0699205849,-0.0482283388,-0.1028576192,-0.0182182984,0.0189195877,0.0056335791],"Abstract_Vector":[0.2265087034,0.1151726492,0.2051873503,0.0286344635,-0.0875161806,0.0516332801,0.0395094359,-0.1480890465,0.0141782742,0.0013387329,-0.0159894344,-0.0432526473,0.0091167537,-0.059249239,-0.0149420066,0.0410674159,-0.0411084311,-0.0319287954,-0.0138776673,0.0529427911,-0.052380792,0.0056547378,-0.0307238268,0.034550184,-0.0620292185,-0.0637091126,-0.0027943831,-0.0579009332,-0.0420528114,0.0001568003,-0.0619455951,-0.0000975867,0.0357188249,-0.0185352286,-0.0234133958,-0.0342406724,0.0032023859,0.0154330219,-0.0118875722,-0.0123706294,0.0153379359,0.0025209921,0.0990775924,-0.0273964891,0.0233517403,0.0720204243,-0.0341818635,-0.0521145383,-0.0490448294,-0.0546654139,0.0500672853,0.055719822,0.081424613,0.0066732928,-0.0528697167,0.0229056396,0.0340116156,-0.066533416,0.0579379684,-0.0712661122,0.0112957023,0.0445957711,-0.0715166824,-0.0192123295,-0.0530327826,-0.0252893167,0.0252195047,-0.0389554056,0.0031954651,-0.036179305,-0.070158382,0.0368705143,-0.0784809852,0.0343393263,0.0536381459,-0.0045004707,-0.0158298792,-0.047226203,-0.0143143322,0.0006667504,-0.0260660096,0.0162868868,-0.0379770595,0.0279074269,-0.0540372364,-0.0306212043,0.0099025367,-0.0057889061,0.0221408147,-0.0233115479,0.003371872,0.0122797565,0.0250897905,0.023931001,-0.0338626222,-0.0141139207,0.0213336923,0.0020728773,-0.0336414966,-0.0509295397,-0.0603157385,-0.0209535257,-0.0435070224,-0.006047289,-0.03275272,-0.0026401843,-0.0065907708,-0.0139018462,-0.0233416654,0.0169570219,-0.038524863,0.0495406518,0.0055694967,0.001723781,0.0286184775,0.0077588743]},"100":{"Abstract":"Memory performance is often a major bottleneck for high-performance computing (HPC) applications. Deepening memory hierarchies, complex memory management, and non-uniform access times have made memory performance behavior difficult to characterize, and users require novel, sophisticated tools to analyze and optimize this aspect of their codes. Existing tools target only specific factors of memory performance, such as hardware layout, allocations, or access instructions. However, today's tools do not suffice to characterize the complex relationships between these factors. Further, they require advanced expertise to be used effectively. We present MemAxes, a tool based on a novel approach for analytic-driven visualization of memory performance data. MemAxes uniquely allows users to analyze the different aspects related to memory performance by providing multiple visual contexts for a centralized dataset. We define mappings of sampled memory access data to new and existing visual metaphors, each of which enabling a user to perform different analysis tasks. We present methods to guide user interaction by scoring subsets of the data based on known performance problems. This scoring is used to provide visual cues and automatically extract clusters of interest. We designed MemAxes in collaboration with experts in HPC and demonstrate its effectiveness in case studies.","Authors":"A. Gim\u00e9nez; T. Gamblin; I. Jusufi; A. Bhatele; M. Schulz; P. Bremer; B. Hamann","DOI":"10.1109\/TVCG.2017.2718532","Keywords":"Performance visualization;high-performance computing;memory visualization","Title":"MemAxes: Visualization and Analytics for Characterizing Complex Memory Performance Behaviors","Keywords_Processed":"memory visualization;high performance computing;performance visualization","Keyword_Vector":[0.1173303015,0.0050611555,0.0289169996,0.0640669489,0.1484852977,-0.0486376051,-0.0893353966,0.0927975638,-0.0877972789,0.0655473196,-0.1131580307,0.0407704795,0.0211479139,0.0259389506,0.1045618425,-0.0198268438,0.0628238407,0.0339793972,0.003340423,-0.0250324197,-0.0289766947,0.0543247845,0.0561813504,0.0450976444,0.0363645847,-0.06749245,0.0271244612,-0.0273426949,0.0076604824,-0.0228666401,0.0281920771,0.0297215964,-0.0455881704,-0.043554198,0.0264805768,0.0792753277,0.0745761292,0.1475515594,-0.0094120607,-0.0253956197,0.0040219169,0.0501748966,-0.1251850924,-0.0252464557,-0.0652747097,0.1351792997,-0.0114152799,0.0644633453,0.0138280983,-0.0633693536,-0.0506791874,0.0562386177],"Abstract_Vector":[0.1768166227,-0.0141270075,-0.008002962,-0.0144845256,-0.1220350083,0.1083685695,0.0224949134,0.0368366781,0.0258440527,0.0135910072,-0.0514348352,-0.0432423308,0.0368139082,-0.000300888,0.0039875077,0.0675356942,0.0927571251,0.0701993812,0.0582271696,0.0014329459,0.0181220524,0.0008074258,0.0345822743,0.0491521322,-0.0381028146,-0.0304042968,-0.0418885369,0.0712429667,0.0067999538,0.0131684234,0.0364455029,-0.0567612457,0.0507489003,0.0275234014,0.0131365604,0.0110030559,0.0147037492,-0.0135257536,-0.0411183051,-0.0111984456,0.0724920336,0.0238966905,-0.0289293122,0.0269795997,0.0712790708,0.0546057107,0.0618209917,0.0284658566,0.0549358311,-0.087959187,0.0253176114,0.0193162496,-0.0053314342,-0.0472376341,-0.0092204689,0.0213965988,0.0235338601,-0.0035740939,-0.0458541819,-0.044984482,-0.0106977621,0.0112034177,0.02767938,-0.0126909212,-0.0139904716,0.0039681183,-0.0255929839,-0.019818368,-0.000298233,-0.0338481535,-0.0274320267,0.0414356585,-0.034397636,0.0366082361,-0.0048606687,-0.0201985856,0.014073488,0.0368199486,-0.0525902792,0.0165929856,0.0169526076,-0.02247265,0.0110132925,-0.048965066,-0.0107428495,-0.0188825898,-0.0195122166,-0.0178562143,-0.023294445,-0.0083979523,-0.0155638176,-0.0392331013,-0.0082832784,0.0213193409,-0.0465410013,-0.0221344723,0.0475762771,-0.0086011123,-0.0188436189,0.027605555,0.043094438,-0.0407470171,-0.0134905743,-0.0371234779,-0.0375748156,-0.0262026757,-0.0090662179,0.0140867514,0.0287215981,-0.020205834,-0.0198645425,-0.0093856081,0.0091504314,0.0239983217,0.042199608,0.0099540458]},"101":{"Abstract":"Previous perceptual research and human factors studies have identified several effective methods for texturing 3D surfaces to ensure that their curvature is accurately perceived by viewers. However, most of these studies examined the application of these techniques to static surfaces. This paper explores the effectiveness of applying these techniques to dynamically changing surfaces. When these surfaces change shape, common texturing methods, such as grids and contours, induce a range of different motion cues, which can draw attention and provide information about the size, shape, and rate of change. A human factors study was conducted to evaluate the relative effectiveness of these methods when applied to dynamically changing pseudo-terrain surfaces. The results indicate that, while no technique is most effective for all cases, contour lines generally perform best, and that the pseudo-contour lines induced by banded color scales convey the same benefits.","Authors":"T. Butkiewicz; A. H. Stevens","DOI":"10.1109\/TVCG.2015.2467962","Keywords":"Structured textures;terrain;deformation;dynamic surfaces;Structured textures;terrain;deformation;dynamic surfaces","Title":"Effectiveness of Structured Textures on Dynamically Changing Terrain-like Surfaces","Keywords_Processed":"terrain;deformation;dynamic surface;structured texture","Keyword_Vector":[0.0886896332,-0.0373121347,0.0236126725,-0.0508411455,0.0226966934,0.0085249831,0.0053125474,0.0147073724,0.010605963,-0.024250398,-0.0055274304,0.0162103702,-0.0179723346,-0.0126085533,-0.005923606,0.0091943644,-0.0087960085,0.0246308547,-0.0047005457,0.0088466242,-0.0207005313,-0.0258538597,-0.0323250164,-0.0114825291,0.0255164632,-0.0030408401,0.023214979,-0.0075486131,-0.0092333212,-0.0117948293,0.0119860594,-0.0065116534,-0.0167586935,0.0301633352,0.0330822432,-0.0024418062,-0.0197916636,-0.0189361317,-0.0431373588,-0.0078189794,-0.0198980569,-0.0023878067,0.0025753977,-0.0040031582,-0.0024870095,-0.0100312018,0.0009949664,0.0219849466,0.0166050546,0.0405835845,-0.0077338984,0.000851639],"Abstract_Vector":[0.1795996549,-0.0902606295,-0.0084186964,-0.0004758584,0.0050338804,-0.0364177104,-0.015153288,-0.0136500108,0.0443342355,0.0197158669,-0.0086355964,0.0193346609,-0.0109961229,0.039071275,0.022698452,-0.0073305316,-0.0179656778,-0.0192049355,-0.0249779385,0.0006883895,0.0176662904,-0.006341361,-0.0149399058,0.013251992,0.0417168263,0.0330015882,0.0130249373,-0.0046628709,-0.0085922001,-0.031998621,0.0199251921,-0.034509642,0.000544391,-0.027232828,-0.0456347155,-0.0232755654,0.005662203,0.0230188453,-0.046485326,-0.0276674512,0.0093402413,0.0387411587,-0.0128195193,-0.0226488026,0.037004765,-0.0433138766,0.0499893614,-0.0018769665,-0.0379096276,-0.0076263762,0.0136631007,0.0200872662,0.0029341248,0.0377802126,-0.0006615777,-0.0479539108,-0.0313056571,0.0449235274,-0.0440675851,0.0181650563,-0.0033776213,0.0434882494,0.0106252914,0.0338206461,-0.059295324,0.0348011371,-0.0625910305,0.023637307,-0.0362767311,0.0220319502,-0.0138572403,-0.0090663084,-0.0445553769,-0.0254035233,-0.0313800944,0.034967785,-0.044752931,0.015767723,0.0075989496,0.023582635,0.0330165232,-0.0297017932,-0.0127398832,0.0639927673,0.0037877862,0.0260583328,-0.0299139602,0.0686038403,0.0467672428,-0.0638736406,-0.0146188453,0.006545439,-0.0092648285,-0.0288550269,0.0000044319,0.0117912006,-0.0065182443,0.0200503374,-0.0054777698,0.0311490123,0.0703507089,-0.0421080551,-0.053261171,0.0113764602,0.0494456068,-0.0292392902,-0.0150707706,-0.0024411482,0.0220933714,-0.0109976901,-0.0202875075,0.0087310595,0.0201656921,0.0097424593,0.046644258,-0.004221864]},"102":{"Abstract":"The differential diagnosis of hereditary disorders is a challenging task for clinicians due to the heterogeneity of phenotypes that can be observed in patients. Existing clinical tools are often text-based and do not emphasize consistency, completeness, or granularity of phenotype reporting. This can impede clinical diagnosis and limit their utility to genetics researchers. Herein, we present PhenoBlocks, a novel visual analytics tool that supports the comparison of phenotypes between patients, or between a patient and the hallmark features of a disorder. An informal evaluation of PhenoBlocks with expert clinicians suggested that the visualization effectively guides the process of differential diagnosis and could reinforce the importance of complete, granular phenotypic reporting.","Authors":"M. Glueck; P. Hamilton; F. Chevalier; S. Breslav; A. Khan; D. Wigdor; M. Brudno","DOI":"10.1109\/TVCG.2015.2467733","Keywords":"Clinical diagnosis;differential hierarchy comparison;ontology;phenomics;phenotype;Clinical diagnosis;differential hierarchy comparison;ontology;genomics;phenomics;phenotype","Title":"PhenoBlocks: Phenotype Comparison Visualizations","Keywords_Processed":"phenomic;ontology;clinical diagnosis;differential hierarchy comparison;genomic;phenotype","Keyword_Vector":[0.034793121,0.0196497584,0.0312577716,0.0212973033,0.0111332642,-0.0241102626,-0.0109117219,-0.0806451623,-0.0372832058,-0.0233400828,0.0216818493,0.022449493,-0.0791150817,0.0536800155,0.0910621426,-0.0851507591,-0.0048484195,-0.0197454648,-0.038223036,0.0124178897,-0.0375552481,0.032417711,0.0247646684,-0.0416820199,0.0070882966,0.061736418,-0.0271378358,0.0083980379,-0.0165162973,0.0447328191,0.0003687728,0.0174105502,-0.0008851616,-0.0115784059,-0.0149839456,-0.0032237002,-0.004704892,0.0060482312,0.0127778012,0.0116060582,0.0185056069,0.0195960839,-0.0178288144,-0.0247689169,-0.0422068483,0.0231067559,-0.0326252301,-0.0368229864,-0.0038939903,0.0125822319,-0.0310440482,-0.0237074207],"Abstract_Vector":[0.108698594,0.0382363943,0.061197715,0.0125222471,-0.0268804356,0.0065848009,0.0008179883,0.0789165832,0.0119730051,0.0262359338,0.0194624245,0.0267722206,-0.0099334807,0.0572934976,-0.0316509338,0.0081280291,-0.0141472841,0.0256464294,-0.0398953638,-0.0539089249,-0.0147240457,-0.0456803537,0.0315554252,-0.009230444,0.0235462434,0.0082586753,-0.0130529345,0.0160328353,-0.0084370649,-0.0071566481,0.0288373728,0.0107338938,0.0046233543,0.0178114449,0.0169256262,-0.0158626142,0.0005329003,0.0290442805,0.0178225284,0.0178906253,0.0518248792,0.0231890966,0.0436520411,0.0177708147,-0.00553685,-0.0294739365,-0.0072290693,0.0507839811,-0.0101109557,0.0075126127,-0.0016425784,-0.013537923,-0.0152713905,0.012779112,-0.0028471184,0.0106078552,0.0488414454,0.0003090786,-0.0088057141,-0.0141336167,0.0112768018,0.0057154493,0.0289673421,-0.0394536045,-0.0215461813,-0.042503299,0.0299081557,-0.0367365105,0.0202230509,0.1004201885,-0.0097128146,0.0199651252,0.0285370293,0.0112794009,0.0345045229,0.0012521704,-0.0136931429,-0.0133911,0.0162308364,-0.0014642795,-0.0195302392,0.0281846398,0.0041520385,-0.0229568571,-0.0391906988,-0.0278347985,-0.0237383886,-0.0139374334,0.0284684389,0.0021190937,0.0484222211,-0.0102465184,-0.0008993337,-0.070252258,0.0235608025,0.0042610357,-0.02932234,0.0040522134,0.0132056197,-0.0244840324,0.002803109,0.001918403,-0.0129688134,0.0227045102,-0.0221957999,-0.0243304435,0.0060120993,-0.011734428,0.0230714689,0.0046367226,0.0036280015,-0.0406910082,-0.0501981235,-0.0070468645,0.0342336609,-0.0160371419]},"103":{"Abstract":"Occlusions are a severe bottleneck for the visualization of large and complex datasets. Conventional images only show dataset elements to which there is a direct line of sight, which significantly limits the information bandwidth of the visualization. Multiperspective visualization is a powerful approach for alleviating occlusions to show more than what is visible from a single viewpoint. However, constructing and rendering multiperspective visualizations is challenging. We present a framework for designing multiperspective focus+context visualizations with great flexibility by manipulating the underlying camera model. The focus region viewpoint is adapted to alleviate occlusions. The framework supports multiperspective visualization in three scenarios. In a first scenario, the viewpoint is altered independently for individual image regions to avoid occlusions. In a second scenario, conventional input images are connected into a multiperspective image. In a third scenario, one or several data subsets of interest (i.e., targets) are visualized where they would be seen in the absence of occluders, as the user navigates or the targets move. The multiperspective images are rendered at interactive rates, leveraging the camera model's fast projection operation. We demonstrate the framework on terrain, urban, and molecular biology geometric datasets, as well as on volume rendered density datasets.","Authors":"M. Wu; V. Popescu","DOI":"10.1109\/TVCG.2015.2443804","Keywords":"Occlusion management;camera models;multiperspective visualization;interactive visualization;focus+context;Occlusion management;camera models;multiperspective visualization;interactive visualization;focus+context","Title":"Multiperspective Focus+Context Visualization","Keywords_Processed":"interactive visualization;occlusion management;camera model;multiperspective visualization;focus context","Keyword_Vector":[0.0485101404,-0.0104835579,0.0189366565,-0.0199216169,0.011205914,0.002787241,-0.0093228787,-0.0015498455,0.0070242772,-0.0168467681,0.0063113593,-0.0018142176,-0.0201835358,-0.0008904949,-0.0013924496,0.0003792659,-0.0080291379,0.0044013773,0.002137426,0.0323731389,-0.0115673226,0.0129184483,-0.0016108267,0.0021823829,0.0124123546,0.0165777509,0.0059696562,0.0163716395,0.0083103324,-0.0023011514,-0.0102602343,0.0111975685,-0.0175782338,0.0114189711,0.0008713377,0.0008847906,0.0113320136,0.0039709555,0.0077831281,-0.0092136598,-0.0196517424,-0.0063622114,0.0044320027,0.0002245125,-0.0088870895,-0.004750525,0.0035005546,0.0042463687,0.0014295338,-0.0076782148,-0.002727719,0.0025389395],"Abstract_Vector":[0.1822969619,-0.084588072,-0.0047568131,-0.0014352174,-0.0528817261,-0.0167448872,-0.0111185845,0.0077714112,-0.0151263208,-0.0276953995,-0.0006143492,0.0004116743,0.0110404825,-0.007363742,-0.0216023647,0.0344110642,-0.0072482217,-0.0087109919,0.0116715162,-0.0088832437,-0.0090294074,0.0145645473,-0.0155552662,0.0056573053,-0.0174023368,-0.0225726599,0.0156364088,-0.018092314,0.0141309108,-0.0003160725,0.0001328176,0.029969494,-0.012325235,-0.01282905,0.0150466795,-0.023596882,-0.0286818286,-0.0269644214,-0.0372273655,-0.0170188444,0.0108733819,0.0310494914,0.0264144039,-0.0171636921,-0.0131311269,0.0229526019,0.0187654943,-0.001919442,-0.0234932481,-0.0202539406,0.0045905274,0.0044420562,0.016213273,-0.0006215468,0.0200500435,-0.0114578699,0.0314045605,0.0015325077,0.0153685498,-0.0076485413,-0.003471208,0.0203500315,0.0163346551,0.0078988218,-0.0318345035,-0.0076421266,-0.0074231841,0.0024395361,-0.045422829,0.004494931,-0.0418086078,0.022502242,-0.0074021551,0.00311518,0.0146623986,0.0336150317,-0.0223953084,0.0180540949,-0.0037271049,-0.0019750014,-0.026641298,-0.0352006607,0.0045693093,-0.0141980303,-0.0274451182,0.0143070051,-0.0254135824,0.0219283234,-0.0158314454,-0.0141836504,0.0045602296,0.0051131402,-0.0083653251,-0.0032035326,0.0188099417,-0.0107521758,0.0092701889,-0.024803068,0.0032899161,-0.0056246431,0.0198489525,0.0056484161,-0.0036097055,0.0344326407,-0.037237848,-0.0437382116,0.0110026014,0.0317523559,-0.0401987298,-0.0265538978,-0.0104718097,-0.0170492076,-0.012845066,0.0306986906,-0.0200152854,0.0029677189]},"104":{"Abstract":"While many VA workflows make use of machine-learned models to support analytical tasks, VA workflows have become increasingly important in understanding and improving Machine Learning (ML) processes. In this paper, we propose an ontology (VIS4ML) for a subarea of VA, namely \u201cVA-assisted ML\u201d. The purpose of VIS4ML is to describe and understand existing VA workflows used in ML as well as to detect gaps in ML processes and the potential of introducing advanced VA techniques to such processes. Ontologies have been widely used to map out the scope of a topic in biology, medicine, and many other disciplines. We adopt the scholarly methodologies for constructing VIS4ML, including the specification, conceptualization, formalization, implementation, and validation of ontologies. In particular, we reinterpret the traditional VA pipeline to encompass model-development workflows. We introduce necessary definitions, rules, syntaxes, and visual notations for formulating VIS4ML and make use of semantic web technologies for implementing it in the Web Ontology Language (OWL). VIS4ML captures the high-level knowledge about previous workflows where VA is used to assist in ML. It is consistent with the established VA concepts and will continue to evolve along with the future developments in VA and ML. While this ontology is an effort for building the theoretical foundation of VA, it can be used by practitioners in real-world applications to optimize model-development workflows by systematically examining the potential benefits that can be brought about by either machine or human capabilities. Meanwhile, VIS4ML is intended to be extensible and will continue to be updated to reflect future advancements in using VA for building high-quality data-analytical models or for building such models rapidly.","Authors":"D. Sacha; M. Kraus; D. A. Keim; M. Chen","DOI":"10.1109\/TVCG.2018.2864838","Keywords":"Visual Analytics;Visualization;Machine Learning;Human-Computer Interaction;Ontology;VIS4ML","Title":"VIS4ML: An Ontology for Visual Analytics Assisted Machine Learning","Keywords_Processed":"VIS4ML;ontology;Human Computer interaction;visualization;visual analytic;Machine Learning","Keyword_Vector":[0.074553613,-0.0560799364,-0.0915260624,0.0666228448,-0.0585541217,-0.1010471529,0.0313340869,0.0197075625,0.0033230597,-0.0301603531,-0.0114412862,0.023235508,-0.0159350033,0.0029974104,0.0057438235,0.0102838658,-0.0499183007,-0.0112702946,-0.0010120234,-0.0224548394,0.007792459,-0.0233887418,0.037605306,0.0138863579,-0.0563328775,0.0202855155,0.0265353054,0.0060662293,-0.0313753739,0.0442093672,0.0094506357,0.0146973074,-0.02110519,0.0116235198,-0.0049276727,0.0554504341,-0.0577032118,-0.0408104071,0.0239158283,-0.0423355486,0.0250133594,0.0323716159,-0.0854105824,-0.060740683,-0.0238036139,-0.0145776801,0.0853559308,-0.00517082,-0.038813088,-0.0062898631,-0.0151619126,0.0055949478],"Abstract_Vector":[0.1469198178,-0.0034934042,-0.0325103669,-0.009390537,-0.0547079617,0.0316573073,0.055887173,0.0230506387,0.0594168127,0.0503064245,-0.0786545717,0.0657234373,-0.0479663544,0.0355088685,-0.0700262528,-0.1380262385,0.0370235869,-0.0418211601,-0.0883477599,0.0295776802,-0.1318712184,0.0709649993,0.3844651694,-0.0561204711,-0.077387348,-0.0996610279,0.0257529654,-0.0619698818,0.1640530287,-0.0016830738,-0.0014708466,0.0259283477,-0.0840280918,0.0329162061,-0.0485565412,-0.0218972893,0.0352266919,0.0304578796,-0.0435192608,-0.0108141412,-0.0314961881,-0.0170671908,0.0104826536,0.0052984768,-0.0847890189,-0.0353221223,-0.0606243119,-0.0409753179,-0.0229826316,0.0216279486,0.052748175,0.0829852332,0.0297707991,-0.0439187768,0.0313274725,-0.0215244004,0.0049437391,-0.0458927524,-0.0211300697,-0.0476115333,-0.0227277592,0.0128440931,0.010527701,-0.0061359933,0.0144804452,0.0304596455,0.0118441944,-0.0104867742,0.0276140573,0.0679977712,0.029659041,0.0161770504,-0.0283303391,0.0093938123,-0.0137776951,0.0221971528,-0.0001510718,-0.0080143769,-0.005956534,0.0425963924,0.0191062959,-0.0203788009,0.0293198085,0.0055136907,-0.030710287,0.0486943122,-0.0139976164,0.0234518433,0.009516284,0.0250634906,-0.0247980347,-0.0387963191,-0.0385242068,0.0512706106,-0.0163505951,-0.0430886418,0.0004035725,-0.0020005347,0.0571645131,-0.0135396547,-0.0040829145,0.0211636216,-0.0016542197,0.0228348427,0.0400225641,-0.0222481477,0.0030241882,-0.0002563941,0.0032322031,-0.0234401633,0.0170603359,-0.0634013461,-0.0219145364,0.0141472934,0.0001680683,-0.00296168]},"105":{"Abstract":"Interactive visualization of streaming points for real-time scatterplots and linear blending of correlation patterns is increasingly becoming the dominant mode of visual analytics for both big data and streaming data from active sensors and broadcasting media. To better visualize and interact with inter-stream patterns, it is generally necessary to smooth out gaps or distortions in the streaming data. Previous approaches either animate the points directly or present a sampled static heat-map. We propose a new approach, called StreamMap, to smoothly blend high-density streaming points and create a visual flow that emphasizes the density pattern distributions. In essence, we present three new contributions for the visualization of high-density streaming points. The first contribution is a density-based method called super kernel density estimation that aggregates streaming points using an adaptive kernel to solve the overlapping problem. The second contribution is a robust density morphing algorithm that generates several smooth intermediate frames for a given pair of frames. The third contribution is a trend representation design that can help convey the flow directions of the streaming points. The experimental results on three datasets demonstrate the effectiveness of StreamMap when dynamic visualization and visual analysis of trend patterns on streaming points are required.","Authors":"C. Li; G. Baciu; Y. Han","DOI":"10.1109\/TVCG.2017.2668409","Keywords":"Information visualization;trend visualization;streaming data;density map;time-varying;scatterplots","Title":"StreamMap: Smooth Dynamic Visualization of High-Density Streaming Points","Keywords_Processed":"density map;trend visualization;time vary;stream datum;scatterplot;information visualization","Keyword_Vector":[0.1200101538,-0.1119231251,-0.1314378104,0.0485838779,-0.0509989674,-0.1595261736,-0.0271052256,0.0338847737,-0.0552603516,0.0247174575,-0.0775358632,0.1112374792,-0.0364146088,0.022639475,-0.0669209119,-0.0072911808,-0.1261843528,-0.0375507429,-0.017644949,0.0037574953,0.1369087135,-0.0284889979,0.0825049618,-0.0043837215,-0.079045189,0.156539319,-0.030177893,0.1092525857,-0.070100139,0.0630200898,-0.0661908594,-0.0968886991,-0.0462977412,0.0620918091,-0.0109310135,0.0391839831,-0.1080857452,-0.1130062351,0.0421427047,-0.1438002923,0.0352754986,0.0332767499,-0.1648818511,-0.0910191265,0.0440242157,0.052272431,0.2176945932,-0.0474614517,-0.088435015,0.0672004636,-0.007571201,0.0685829699],"Abstract_Vector":[0.2021928773,-0.0445406559,-0.0167263754,-0.0046990682,0.0081862353,0.0417472521,-0.0916659413,-0.020064115,0.0153410571,-0.0951564587,-0.0546201985,0.0284532272,-0.0511679511,0.0379129007,0.0564942073,-0.0425817923,-0.0722793879,-0.028652922,-0.0671168625,-0.0120803578,0.0427177793,-0.0510227985,-0.0223375989,-0.0146805133,0.0063022382,0.0113882487,-0.0177054768,-0.0562531398,0.0605409942,0.098869047,0.1007217211,0.0651652387,0.0774459985,0.0154388076,-0.0533337784,0.0395515409,-0.0705623781,-0.0945249403,-0.0518223572,-0.0231275271,-0.1234528782,-0.0623677059,0.08426592,0.1004511917,-0.0125501648,0.0392255267,0.0211358819,0.0816968798,-0.0234518689,-0.0694564673,0.0517022416,0.0132417982,-0.0094212821,0.0798345372,0.0556043283,0.0595523631,0.0645741741,-0.016329956,-0.06125971,-0.0686999963,-0.0240834004,-0.0294977964,-0.0062180409,0.0138883827,-0.0387681271,-0.001370699,-0.0221952263,-0.0004716187,-0.0409161977,-0.0027119499,-0.0244354156,-0.0069895927,-0.0214146243,0.017159807,0.0093645611,-0.011734248,0.0227784943,0.0012113763,0.0105722952,-0.0075510545,-0.0518286638,0.0115418673,0.0056242416,0.0172860261,0.0219027206,-0.0145183178,-0.0098030013,-0.002588292,-0.0306780076,0.0060273118,0.0549227397,0.0170589015,0.0025243897,0.0029185661,0.0504167233,-0.0332353586,0.0435786044,0.0172387272,-0.0037530449,0.0032347802,0.003526283,-0.0140221534,0.0063514038,0.003767509,0.0209718305,0.0165915776,0.0026416699,-0.0162341194,-0.0198840428,-0.0440518865,0.0297783943,0.0209556233,-0.0141286835,0.0076344523,0.0072504645,-0.0153186477]},"106":{"Abstract":"We apply the alternating direction method of multipliers (ADMM) optimization algorithm to implicit time integration of elastic bodies, and show that the resulting method closely relates to the recently proposed projective dynamics algorithm. However, as ADMM is a general purpose optimization algorithm applicable to a broad range of objective functions, it permits the use of nonlinear constitutive models and hard constraints while retaining the speed, parallelizability, and robustness of projective dynamics. We further extend the algorithm to improve the handling of dynamically changing constraints such as sliding and contact, while maintaining the benefits of a constant, prefactored system matrix. We demonstrate the benefits of our algorithm on several examples that include cloth, collisions, and volumetric deformable bodies with nonlinear elasticity and skin sliding effects.","Authors":"M. Overby; G. E. Brown; J. Li; R. Narain","DOI":"10.1109\/TVCG.2017.2730875","Keywords":"Computer graphics;animation;computer simulation;optimization methods;dynamics","Title":"ADMM $\\supseteq$ Projective Dynamics: Fast Simulation of Hyperelastic Models with Dynamic Constraints","Keywords_Processed":"dynamic;computer graphic;optimization method;computer simulation;animation","Keyword_Vector":[0.1090587753,-0.028936228,0.0254870511,-0.0496330033,0.0145606421,0.0248591992,-0.0120517508,0.0135736789,-0.0058710615,-0.0418867295,-0.0078838044,0.0096761255,-0.0040595736,-0.0161380891,-0.0276657637,0.0150549385,-0.014984624,0.0222718159,-0.0088874313,0.0261380539,-0.0321759119,0.0069190999,-0.013778828,-0.0062016705,0.0060084001,0.0308778648,0.0201439003,0.0142301044,0.0410908366,-0.0146808096,0.0128127171,0.0287105524,-0.0483022925,0.0096949848,0.0158076284,-0.0251394049,0.0024211675,-0.0019821786,0.0121235958,-0.0429551204,-0.0403267223,0.0732465961,0.0132842883,-0.006781384,-0.0100006407,0.0017814204,-0.008519787,0.0436818552,0.0082230568,-0.0340494092,0.0786131217,0.0221804757],"Abstract_Vector":[0.2036270151,-0.119083829,0.0007390243,-0.0119420752,-0.0296531049,-0.0093220762,0.0096038867,0.0053630262,0.0148490684,-0.0037403768,-0.0145793789,0.0304041469,-0.0195215595,0.0212348334,-0.0187680397,-0.0048284429,-0.0353832069,-0.0342870478,0.0001715456,-0.0219075297,0.0054431084,-0.0254422549,-0.0019767997,-0.0556724965,0.0069347442,0.0160119246,0.0016266931,0.0109586918,-0.0001304902,-0.0168290286,-0.0410066704,0.0287520981,-0.0686425682,0.0120122923,-0.0145310976,-0.0083191125,0.0357020051,0.0121515685,0.0183569536,-0.0041871723,-0.031545754,-0.0003343475,-0.0562535774,-0.0082408888,0.0541913548,-0.0006159549,-0.0427389549,-0.0070095414,0.022790933,0.0336509738,-0.0162135356,0.0260606777,-0.0319940736,0.0406926033,0.0214187572,-0.0165493572,0.0237008356,-0.0374102132,0.0020482518,0.0415990851,-0.0033998973,0.0239421199,0.0082708013,0.0007093127,0.0322658175,0.0283930663,-0.0194561045,-0.0226095943,-0.0270703245,-0.0040675655,-0.0184142625,0.0161941962,-0.0177151153,0.0891157001,-0.0039562746,0.0157253315,-0.01960676,0.026141958,0.0022563746,0.0695209476,0.0751928846,0.0652880975,0.0656346642,0.0768267823,0.0032780762,-0.0160641002,-0.0149774038,0.0000999687,0.0072620916,-0.057182703,0.0209725022,0.0234869179,0.0085693594,0.0157561425,0.0143624754,0.0047331498,0.0352594248,0.0124287829,-0.0166691186,0.0257257638,0.0381625052,-0.0387449867,-0.0457834021,0.0115679205,0.0080475515,-0.0115754618,0.0424980391,-0.072630887,-0.0347928045,0.0181040271,-0.045823857,-0.0059113674,-0.0125560836,0.0130112651,-0.0019823471,0.0227942279]},"107":{"Abstract":"We present a semi-automatic approach for stream surface generation. Our approach is based on the conjecture that good seeding curves can be inferred from a set of streamlines. Given a set of densely traced streamlines over the flow field, we design a sketch-based interface that allows users to describe their perceived flow patterns through drawing simple strokes directly on top of the streamline visualization results. Based on the 2D stroke, we identify a 3D seeding curve and generate a stream surface that captures the flow pattern of streamlines at the outermost layer. Then, we remove the streamlines whose patterns are covered by the stream surface. Repeating this process, users can peel the flow by replacing the streamlines with customized surfaces layer by layer. Furthermore, we propose an optimization scheme to identify the optimal seeding curve in the neighborhood of an original seeding curve based on surface quality measures. To support interactive optimization, we design a parallel surface quality estimation strategy that estimates the quality of a seeding curve without generating the surface. Our sketch-based interface leverages an intuitive painting metaphor which most users are familiar with. We present results using multiple data sets to show the effectiveness of our approach.","Authors":"J. Tao; C. Wang","DOI":"10.1109\/TVCG.2017.2750681","Keywords":"Flow visualization;sketch-based interface;human perception;seeding curves;stream surfaces","Title":"Semi-Automatic Generation of Stream Surfaces via Sketching","Keywords_Processed":"sketch base interface;flow visualization;stream surface;human perception;seed curve","Keyword_Vector":[0.1403681939,0.0463671131,-0.0611226257,-0.0632920871,-0.0173737377,-0.0381611607,-0.0698518869,0.0017718054,-0.0791615755,0.0375581122,-0.0206471801,0.123392073,0.0057455556,-0.0258515803,-0.0263294025,0.0671114583,0.0321952302,0.0212557962,0.0415648463,0.0696227716,-0.0872451105,-0.0213362044,-0.017264376,-0.0245578823,-0.018559413,0.0132309426,0.0083126068,-0.0042098234,0.0351554191,-0.0521521843,-0.0233042991,0.0028790393,0.0072830154,-0.0219228989,-0.0405444293,-0.0874221301,0.0564779212,-0.052051932,0.0099436025,-0.0556156184,0.0856610847,-0.0492643578,0.0198654842,0.0319140388,0.0206302106,0.0458141315,0.0377446054,0.0359743441,0.0072770646,-0.0423673387,-0.0174141688,-0.0576721505],"Abstract_Vector":[0.1849870772,0.0076904702,-0.0402519428,-0.0555363777,-0.0533387595,0.0892370011,0.0185934722,0.0431387254,-0.0386433618,0.0351474365,-0.0210660387,-0.0094517914,0.0194405293,-0.0061001363,0.0035358908,0.038806929,0.0398591755,0.016985403,0.038977802,0.0475625516,-0.0367064749,-0.0351292589,-0.0200189595,0.0367050409,-0.0019294841,-0.0017584521,-0.0101450716,0.0613256839,0.0395387915,-0.0463128346,0.0498950562,-0.0151283373,0.0132395903,0.0017772852,-0.0416218399,-0.0127148503,0.0039499668,-0.0165887623,-0.0137133788,0.0111660547,0.0065043807,0.0088054515,0.0012366897,0.0108898388,0.0210479203,0.0088043642,-0.030941541,0.0017136625,0.0033305438,-0.0367375381,-0.0156866709,-0.0347193936,0.02391737,-0.0227209539,0.0492196513,-0.0170203072,-0.0212561495,0.0072076697,-0.0186862057,0.0233644057,-0.0158774048,0.0038630228,-0.031387221,-0.0024925499,-0.0160504345,-0.0313277026,0.0015489745,-0.0400671729,0.0018841467,0.0301878598,-0.0470932146,0.0270625334,0.0448621184,-0.0185274848,-0.0378281248,-0.0524706178,-0.0035528999,0.0370870271,0.036138151,0.0076371426,0.0023444235,-0.0230543967,-0.0139759637,0.0211173538,0.0391800417,0.015357828,-0.0027229255,-0.0428712494,0.0012250026,0.0007256388,0.0094340474,-0.0385324469,-0.0195600868,0.0190516933,-0.0009005174,0.0081953495,0.0107527785,0.0285463707,0.0371132438,-0.0289811548,-0.0188201518,-0.0129022645,-0.020525976,-0.0343611843,-0.000051329,0.0354582788,-0.0433902682,-0.0045887179,0.0224975908,0.0113468162,-0.0053126278,-0.0126855217,0.0261809448,0.020327786,0.0040133823,-0.0126708025]},"108":{"Abstract":"Relief is an art form part way between 3D sculpture and 2D painting. We present a novel approach for generating a texture-mapped high-relief model from a single brush painting. Our aim is to extract the brushstrokes from a painting and generate the individual corresponding relief proxies rather than recovering the exact depth map from the painting, which is a tricky computer vision problem, requiring assumptions that are rarely satisfied. The relief proxies of brushstrokes are then combined together to form a 2.5D high-relief model. To extract brushstrokes from 2D paintings, we apply layer decomposition and stroke segmentation by imposing boundary constraints. The segmented brushstrokes preserve the style of the input painting. By inflation and a displacement map of each brushstroke, the features of brushstrokes are preserved by the resultant high-relief model of the painting. We demonstrate that our approach is able to produce convincing high-reliefs from a variety of paintings(with humans, animals, flowers, etc.). As a secondary application, we show how our brushstroke extraction algorithm could be used for image editing. As a result, our brushstroke extraction algorithm is specifically geared towards paintings with each brushstroke drawn very purposefully, such as Chinese paintings, Rosemailing paintings, etc.","Authors":"Y. Fu; H. Yu; C. Yeh; J. Zhang; T. Lee","DOI":"10.1109\/TVCG.2018.2860004","Keywords":"Brush painting;brushstroke;layer decomposition;displacement mapping;high-relief","Title":"High Relief from Brush Painting","Keywords_Processed":"displacement mapping;brushstroke;layer decomposition;Brush painting;high relief","Keyword_Vector":[0.1487377173,0.1498743684,-0.1128869171,-0.0402715171,-0.0019894747,-0.0533168935,-0.0812098976,-0.0371919935,-0.1494208166,-0.0158247893,0.0545688116,0.0567142861,0.109394859,0.0380357752,0.1493252758,0.0768633929,0.0726445929,0.0775413577,0.1839325864,-0.0127427336,-0.0815916523,0.0945710345,0.1493717984,0.1312809545,-0.0887129939,-0.0395961646,-0.0480617914,0.0119307816,0.0944067037,-0.1120325327,-0.0130815198,0.0068667201,0.0347432944,0.0577435163,0.0293622385,-0.065230909,-0.2042135646,0.0288702597,-0.0534321689,-0.1867683361,-0.048551653,0.2595308848,-0.0232364971,-0.0804394815,-0.0193491436,0.0109407464,0.0919834589,-0.0358448109,-0.1122518473,0.0530077585,0.0002139227,0.0974649737],"Abstract_Vector":[0.2086995678,0.0760963958,-0.1359490279,-0.0211014819,-0.0068265691,0.0293429009,0.0898488955,0.079660904,0.1049782475,-0.0192597101,0.0161572263,-0.0108784054,-0.051257981,-0.0412067071,0.0053294933,-0.0581605133,-0.0052742164,-0.0078993071,-0.0132901337,0.0131637159,0.0056125303,-0.0104580609,0.0621289159,-0.0233332349,-0.0114667918,0.0118291058,0.0402924745,-0.0686564885,0.0102124932,-0.033342119,-0.0288615119,0.0266407132,-0.034750307,-0.0313204767,0.0907735226,-0.0353092531,0.0036602322,-0.0005338787,0.065661854,-0.0157940053,-0.0599706423,0.0214804837,0.0221438688,0.0756641293,0.0261669031,-0.0475265849,-0.0020464667,-0.0378992105,-0.0187039853,-0.0376850059,-0.0613110177,0.0353591977,-0.0017356518,-0.0610252521,0.0310296866,-0.0736104677,0.0007555328,-0.0409489452,-0.0133535814,-0.0127109192,-0.0153718292,-0.0034601266,0.008979589,-0.0421807991,0.0237295295,-0.0047605881,0.0298818767,-0.0462217169,-0.0128049707,0.0277982506,-0.0608142848,-0.036783815,0.0294135706,-0.0062421417,-0.0310205744,-0.0948808206,0.0497667694,0.0334945978,0.0030441701,-0.0368496155,0.0693163606,-0.0096164972,0.0973412502,-0.0105812626,0.0293872889,-0.0297115096,-0.0202483842,-0.01273814,0.0091833458,0.0062903327,0.0230609009,-0.0313150354,-0.0240545266,0.0264305852,0.0049686329,-0.0446283802,0.0428964408,0.0194513898,0.0222855464,-0.0108645912,0.0094909238,0.000421461,-0.004476002,-0.0137348022,-0.0140625186,-0.0275698217,-0.060618346,0.037308653,0.0107262285,-0.0183348343,-0.0406173023,-0.0272481717,0.0069690352,0.0047323764,-0.0460044204,-0.0120254421]},"109":{"Abstract":"To interpret data visualizations, people must determine how visual features map onto concepts. For example, to interpret colormaps, people must determine how dimensions of color (e.g., lightness, hue) map onto quantities of a given measure (e.g., brain activity, correlation magnitude). This process is easier when the encoded mappings in the visualization match people's predictions of how visual features will map onto concepts, their inferred mappings. To harness this principle in visualization design, it is necessary to understand what factors determine people's inferred mappings. In this study, we investigated how inferred color-quantity mappings for colormap data visualizations were influenced by the background color. Prior literature presents seemingly conflicting accounts of how the background color affects inferred color-quantity mappings. The present results help resolve those conflicts, demonstrating that sometimes the background has an effect and sometimes it does not, depending on whether the colormap appears to vary in opacity. When there is no apparent variation in opacity, participants infer that darker colors map to larger quantities (dark-is-more bias). As apparent variation in opacity increases, participants become biased toward inferring that more opaque colors map to larger quantities (opaque-is-more bias). These biases work together on light backgrounds and conflict on dark backgrounds. Under such conflicts, the opaque-is-more bias can negate, or even supersede the dark-is-more bias. The results suggest that if a design goal is to produce colormaps that match people's inferred mappings and are robust to changes in background color, it is beneficial to use colormaps that will not appear to vary in opacity on any background color, and to encode larger quantities in darker colors.","Authors":"K. B. Schloss; C. C. Gramazio; A. T. Silverman; M. L. Parker; A. S. Wang","DOI":"10.1109\/TVCG.2018.2865147","Keywords":"Visual Reasoning;Visual Communication;Colormaps;Color Perception;Visual Encoding;Visual Design","Title":"Mapping Color to Meaning in Colormap Data Visualizations","Keywords_Processed":"visual Encoding;visual reasoning;colormap;visual communication;Color Perception;visual Design","Keyword_Vector":[0.0491657913,0.0097699088,-0.0051384575,0.0129615174,0.0184794983,-0.0319831413,-0.0143241275,-0.0358739538,-0.0691458624,-0.0044818701,0.002641661,0.0252674739,-0.0419302795,0.0577471059,-0.0222153665,-0.0119203817,-0.043944099,-0.0317152168,-0.0047775183,0.0406836209,0.0011980153,0.0041756278,-0.0457108072,0.0873117318,-0.0021790217,-0.070253878,-0.0488139175,-0.0130666931,-0.0658236116,-0.0318875482,0.0328822868,-0.0159400685,-0.0450554554,-0.0006976919,0.0105053607,-0.0014987922,0.0260556199,0.0124272952,0.0048979749,0.0654116617,-0.0141235127,0.0130352137,-0.0846760207,-0.0617176651,-0.0215471301,-0.0396477711,-0.0424915037,0.0535356079,0.0114441872,0.0114124607,-0.0206785544,-0.0534197447],"Abstract_Vector":[0.1823248492,0.0651056722,0.011634741,-0.0007992621,-0.0413161683,0.0581656937,-0.0016199868,-0.0069507184,0.0769867352,0.0237353693,-0.0182900515,0.0341287517,-0.0711252012,0.0281920015,-0.0359270412,-0.0659904382,-0.0372520898,-0.0068994618,0.0096849811,-0.067196381,-0.0213844292,0.0333367965,0.0108087775,0.0567335494,0.0097705383,-0.0165594314,0.0191372464,0.0354904416,-0.0086157381,-0.047822133,0.0202663817,-0.0238099833,-0.054666604,-0.0133017457,-0.0235424985,-0.0297960144,-0.0270366624,-0.0296999243,0.0099031959,0.0324892799,0.0523035716,0.0146263007,0.0226428455,0.0161635963,-0.0534423264,0.0295545683,0.0123950418,0.0759040631,0.0097000761,-0.0065793124,-0.0129059512,0.00789064,-0.0267377903,0.0223423002,0.0506606376,0.0122797616,0.0154773716,-0.0140689048,-0.0294075822,0.0141524604,-0.006703809,-0.0031373856,0.0184584142,-0.0248659263,-0.0058328659,0.0290951871,-0.0244862846,-0.0348528567,-0.0197861509,0.025159197,-0.0095367454,0.0383576251,0.0275640837,-0.002635789,-0.0070165168,0.0085282616,0.0095770061,0.0336831523,0.0155016839,-0.0457751463,0.0192375727,0.0272095378,-0.0285150803,-0.0291591689,-0.0922525762,0.0236321801,0.0071065281,0.0123169205,0.0349056585,0.0061819655,0.0051964816,0.0447845497,0.0568303859,-0.0542043689,-0.0434623232,-0.0088028949,-0.0121628394,-0.0012814835,-0.0449292386,-0.0451749016,-0.0006767684,0.0183108498,-0.0009918498,0.0046066548,-0.0235809172,-0.034440126,-0.0178057353,0.0369270368,0.0111984375,0.0324648246,0.0024183856,0.0212191731,0.0380013993,-0.0204527159,0.0534858832,0.0268512792]},"11":{"Abstract":"The human placenta is essential for the supply of the fetus. To monitor the fetal development, imaging data is acquired using (US). Although it is currently the gold-standard in fetal imaging, it might not capture certain abnormalities of the placenta. (MRI) is a safe alternative for the in utero examination while acquiring the fetus data in higher detail. Nevertheless, there is currently no established procedure for assessing the condition of the placenta and consequently the fetal health. Due to maternal respiration and inherent movements of the fetus during examination, a quantitative assessment of the placenta requires fetal motion compensation, precise placenta segmentation and a standardized visualization, which are challenging tasks. Utilizing advanced motion compensation and automatic segmentation methods to extract the highly versatile shape of the placenta, we introduce a novel visualization technique that presents the fetal and maternal side of the placenta in a standardized way. Our approach enables physicians to explore the placenta even in utero. This establishes the basis for a comparative assessment of multiple placentas to analyze possible pathologic arrangements and to support the research and understanding of this vital organ. Additionally, we propose a three-dimensional structure-aware surface slicing technique in order to explore relevant regions inside the placenta. Finally, to survey the applicability of our approach, we consulted clinical experts in prenatal diagnostics and imaging. We received mainly positive feedback, especially the applicability of our technique for research purposes was appreciated.","Authors":"H. Miao; G. Mistelbauer; A. Karimov; A. Alansary; A. Davidson; D. F. A. Lloyd; M. Damodaram; L. Story; J. Hutter; J. V. Hajnal; M. Rutherford; B. Preim; B. Kainz; M. E. Gr\u00f6ller","DOI":"10.1109\/TVCG.2017.2674938","Keywords":"Placenta;fetal;flattening;structure-aware slicing;peeling","Title":"Placenta Maps: In Utero Placental Health Assessment of the Human Fetus","Keywords_Processed":"peeling;fetal;flatten;Placenta;structure aware slicing","Keyword_Vector":[0.036439319,0.006306395,-0.0050912999,0.03625013,0.0039841588,0.0088817027,0.0018730738,0.0174267891,-0.0284758595,-0.0308319267,-0.003774075,0.0163581698,0.0441255719,0.0161909776,0.0412926427,-0.060725174,0.0142793049,0.0185604915,-0.0154731574,0.0069265448,0.0362060003,-0.0722591712,0.0241525118,-0.0370880552,-0.0136173531,-0.0046799304,0.0461279741,0.0444991597,0.0183884151,-0.0020819086,-0.0012080921,-0.0490083632,0.0001310382,0.0174995812,-0.0300619119,-0.0011807631,0.0206393665,-0.0220829961,0.0479257599,0.0881887432,-0.0107266389,-0.0176383248,-0.0029616351,-0.0279413795,-0.0124796151,-0.0509500969,0.0192413805,0.0546009098,-0.0264536075,0.0111641874,0.0254122851,0.0143446958],"Abstract_Vector":[0.0945127751,0.0322485277,-0.0045359805,0.0091121087,-0.0221269132,0.0216757267,-0.0153793459,0.0013933081,0.0099446985,-0.0145145731,-0.0195527377,-0.0011848972,-0.0354068093,0.0109269181,-0.0026069728,0.018303709,0.0257160264,0.028640509,-0.0002525984,-0.0206205938,0.0257410355,0.0330919555,0.0040841121,0.0023644628,-0.0472308634,-0.0074103798,0.0054350935,0.0266268526,-0.0014332623,0.0135893951,0.0080770847,0.0319331464,-0.0027932823,0.017432022,0.0003289582,-0.0024801437,0.014135009,-0.0275117293,0.018787316,-0.0189831263,-0.0186449586,-0.0090923061,0.0215199288,-0.0215401342,0.0027838444,0.0001864513,0.0278287531,0.0105161701,0.008263026,-0.0093631143,-0.000126187,0.0239343393,0.0074529581,-0.0004111267,-0.0121391692,-0.015462655,0.010408767,0.0110919539,-0.004465178,0.0016655419,-0.0071658509,-0.0105094502,-0.0054807992,-0.0015136563,-0.0102440774,0.0051141836,0.0039504718,0.0245441759,0.0080433867,0.0156762336,-0.0005850741,-0.0035442744,0.0021065085,-0.0033131506,-0.0166450771,-0.0159295891,-0.0071783581,-0.0139277958,0.019556394,-0.0080304755,0.0182035162,0.0280035842,-0.0101718027,-0.0041044041,-0.0044564849,-0.0009164789,0.0065728727,-0.020343157,-0.0090034914,0.0093127688,-0.015468806,0.0121434202,-0.0234911764,-0.0027813686,0.0185663567,0.0124558996,0.0115275241,-0.0013116774,0.0123126259,0.0021745212,-0.0101354755,0.0154081307,0.0094659915,-0.0063734757,-0.0064254209,0.0047285965,-0.0111932727,-0.0054518047,0.0071062758,-0.0077588481,-0.0112917159,-0.0162835807,-0.0032069143,-0.0000993052,-0.0041767456,-0.0257147962]},"110":{"Abstract":"Drones allow exploring dangerous or impassable areas safely from a distant point of view. However, flight control from an egocentric view in narrow or constrained environments can be challenging. Arguably, an exocentric view would afford a better overview and, thus, more intuitive flight control of the drone. Unfortunately, such an exocentric view is unavailable when exploring indoor environments. This paper investigates the potential of drone-augmented human vision, i.e., of exploring the environment and controlling the drone indirectly from an exocentric viewpoint. If used with a see-through display, this approach can simulate X-ray vision to provide a natural view into an otherwise occluded environment. The user's view is synthesized from a three-dimensional reconstruction of the indoor environment using image-based rendering. This user interface is designed to reduce the cognitive load of the drone's flight control. The user can concentrate on the exploration of the inaccessible space, while flight control is largely delegated to the drone's autopilot system. We assess our system with a first experiment showing how drone-augmented human vision supports spatial understanding and improves natural interaction with the drone.","Authors":"O. Erat; W. A. Isop; D. Kalkofen; D. Schmalstieg","DOI":"10.1109\/TVCG.2018.2794058","Keywords":"X-ray;mixed reality;hololens;drone;pick-and-place","Title":"Drone-Augmented Human Vision: Exocentric Control for Drones Exploring Hidden Areas","Keywords_Processed":"ray;drone;mixed reality;hololen;pick and place","Keyword_Vector":[0.2089551015,-0.154700178,-0.1330523825,0.00298145,-0.0038006427,-0.2091152147,0.0525715159,0.0094261003,0.036482462,0.0153761512,-0.0122048926,-0.0634398214,-0.0550149482,-0.0061201133,0.0738794541,-0.0179633406,-0.0830520928,-0.1134519737,0.0267057057,0.0644095188,-0.0059742764,-0.0967925715,-0.0640986367,0.0879710564,0.0168325415,-0.1544800624,0.0272600848,0.0381691763,-0.006356114,-0.0325862968,0.0385880877,-0.0723272738,0.053844934,0.0444595479,0.1628628373,0.0412803701,-0.0899032264,-0.1118358171,-0.0367438461,0.0409180159,0.017142307,0.0057229901,0.0099300959,-0.0253313645,-0.0509996328,-0.0490893366,-0.0028482741,0.0032822219,0.0451386208,-0.062007806,0.0469255045,-0.0387904225],"Abstract_Vector":[0.2165680066,-0.142652926,0.0117307401,0.0258307437,-0.1162789478,0.0284983382,0.0218009387,0.0213062064,-0.1290513939,-0.008908881,-0.0282962811,0.0206278865,-0.1344947351,-0.0443119174,-0.0331409903,0.0730903608,0.0760500965,-0.0390688102,-0.0146405866,-0.0356075988,0.1300955241,-0.0068240811,-0.0109121788,-0.0144121023,-0.037659007,0.0150182931,0.0434604528,-0.0238947133,0.0006335532,-0.0598436107,-0.0314536049,0.020454183,-0.0907684819,0.0152648763,-0.0599879251,0.0323816671,-0.0028498729,-0.0819167563,0.0397663073,-0.0124673018,0.049315165,-0.0664337671,-0.0364335988,0.0443448612,-0.0286628456,0.0695776686,-0.0445534985,-0.011010862,0.0133060861,0.0726619263,0.0246009683,0.011526328,-0.0569090301,0.0040552454,-0.0165689188,-0.0249525675,0.0816057153,-0.0079575335,0.0474225922,-0.0441175546,-0.0086484612,0.0020163471,-0.0303800132,0.0149139495,-0.0354529995,0.0426964129,0.0261461683,0.0008329926,0.052443606,0.0248481117,-0.0255477769,0.0332150581,0.0097860082,-0.0701729443,-0.0418275117,-0.0195835663,0.0009643879,0.0047340693,-0.0391123656,-0.0269609439,0.0144163636,0.046221128,-0.0306754951,0.0479064685,-0.0315173534,-0.0121434013,-0.031060103,0.0710121958,0.01706528,-0.0004428376,0.0013669634,-0.0846555214,-0.0172468516,0.028649924,0.016262811,-0.0271773726,-0.0217299118,0.0106419192,-0.0440802303,0.0415893606,0.0186675757,0.0023971623,0.0281969643,-0.0049948698,-0.0082975064,-0.0096430596,0.0047247891,-0.0010879882,-0.0123176431,-0.0211639359,0.0045785325,0.0567124127,-0.0279593571,-0.0087198721,0.0481687307,-0.0085349129]},"111":{"Abstract":"Regarded as a high-level tactic in soccer, a team formation assigns players different tasks and indicates their active regions on the pitch, thereby influencing the team performance significantly. Analysis of formations in soccer has become particularly indispensable for soccer analysts. However, formations of a team are intrinsically time-varying and contain inherent spatial information. The spatio-temporal nature of formations and other characteristics of soccer data, such as multivariate features, make analysis of formations in soccer a challenging problem. In this study, we closely worked with domain experts to characterize domain problems of formation analysis in soccer and formulated several design goals. We design a novel spatio-temporal visual representation of changes in team formation, allowing analysts to visually analyze the evolution of formations and track the spatial flow of players within formations over time. Based on the new design, we further design and develop ForVizor, a visual analytics system, which empowers users to track the spatio-temporal changes in formation and understand how and why such changes occur. With ForVizor, domain experts conduct formation analysis of two games. Analysis results with insights and useful feedback are summarized in two case studies.","Authors":"Y. Wu; X. Xie; J. Wang; D. Deng; H. Liang; H. Zhang; S. Cheng; W. Chen","DOI":"10.1109\/TVCG.2018.2865041","Keywords":"Soccer data;formation analysis;spatio-temporal visualization","Title":"ForVizor: Visualizing Spatio-Temporal Team Formations in Soccer","Keywords_Processed":"formation analysis;spatio temporal visualization;soccer datum","Keyword_Vector":[0.0952320198,-0.0102643344,-0.0533634542,0.0145472286,0.0616394949,-0.0847182378,-0.0822530174,-0.0199739004,-0.0943155864,0.0435187937,-0.0625146434,0.0993064744,0.0377862593,0.1121005167,-0.0698881954,0.0254482379,-0.0012450592,0.0090605006,-0.0006627245,-0.1365112209,0.1285314347,0.1439835334,-0.1782981106,-0.0411392423,0.0008190789,0.1101761004,-0.0380692649,0.0617073886,0.0900789327,0.035868257,-0.0996945421,-0.0690307809,-0.0078922599,0.0829797777,-0.0334576228,0.0217903468,0.1186787175,-0.0615083093,-0.0651928368,-0.057353772,0.0989548085,0.1872543407,0.0054952464,-0.1047759092,-0.0383465683,-0.0443368119,0.0896826637,0.0319389719,0.0977419014,0.0165468341,-0.0154578793,-0.1077628018],"Abstract_Vector":[0.163752697,-0.0066885853,-0.021360372,0.0041659261,-0.0548243186,0.0286505926,0.0189049827,0.0146607061,0.0114424101,-0.01682518,-0.014508208,-0.0211408208,-0.0268235175,0.0084450168,-0.0000259053,0.038158341,0.0689132938,0.0280526972,-0.0176896371,-0.0271729227,0.0202428724,-0.0146257271,0.0256170585,0.0316950015,0.011401949,0.000036313,0.0285895737,-0.0014259265,0.0012064973,-0.012438889,-0.0194350971,0.0335786136,0.0030644885,0.0540990434,0.0194293406,0.0039193796,-0.007626714,-0.0253843084,-0.015772682,-0.0397014308,-0.0134376432,0.0504830097,0.0129273696,0.0101100945,-0.0133903686,-0.0063088852,0.0208267253,0.0434253432,-0.0044835353,-0.0335088462,-0.0197658968,0.0020770296,0.0117250419,-0.0053171716,0.0093828011,-0.0387027091,0.0275451719,0.0317456207,0.0027665952,0.0031747086,0.0397850413,-0.0341952955,-0.0013901568,0.0112620014,0.0049485724,-0.0425762533,-0.0263124827,-0.0202113228,0.0344821226,0.0246259815,0.0130072363,0.0692873358,-0.0059950968,0.0149272457,-0.0002793617,-0.0124080718,-0.0236076736,0.0108790745,0.002654364,-0.0055946943,0.0223656996,-0.0048432212,-0.0067499159,0.0187177172,-0.0429946606,-0.0101472442,0.0002461524,-0.0109065733,-0.0131404493,0.0045368258,0.0162123559,-0.0058440587,0.010439871,-0.0387948623,-0.0418539222,-0.0018613362,0.0058724916,-0.0137131199,-0.0356380483,0.0153129744,-0.0046289906,0.0273570708,0.0441255297,0.0117948307,0.0091864173,-0.0175855463,0.0176562304,-0.0480250177,-0.027105195,0.0243445741,0.0130903089,-0.030849844,0.0152722754,-0.0339865457,0.002774504,-0.0277734159]},"112":{"Abstract":"Star Coordinates are a popular projection technique in order to analyze and to disclose characteristic patterns of multidimensional data. Unfortunately, the shape, appearance, and distribution of such patterns are strongly affected by the given scaling of the data and can mislead the projection-based data analysis. In an extreme case, patterns might be more related to the choice of scaling than to the data themselves. Thus, we present the LloydRelaxer: a tool to minimize scaling-based effects in Star Coordinates. Our algorithm enforces a scaling configuration for which the data explains the observed patterns better than any scaling of them could do. It does so by an iterative minimizing and optimization process based on Voronoi diagrams and on the Lloyd relaxation within the projection space. We evaluate and test our approach by real benchmark multidimensional data of the UCI data repository.","Authors":"D. J. Lehmann; H. Theisel","DOI":"10.1109\/TVCG.2017.2705189","Keywords":"Information visualization;multidimensional data;scaling;star coordinates;projections","Title":"The LloydRelaxer: An Approach to Minimize Scaling Effects for Multivariate Projections","Keywords_Processed":"multidimensional datum;star coordinate;projection;scale;information visualization","Keyword_Vector":[0.1452230478,0.0225237202,0.2693100771,0.0187913529,-0.1272937563,-0.0726710981,0.0344999998,0.0408887219,0.0517385976,0.0497798896,-0.0610644885,-0.005222477,0.0909918252,0.0062057809,0.0015873905,-0.0301112122,-0.0642207325,0.0001068729,0.0370109921,-0.0237190566,-0.0426084929,0.021716901,-0.0317110164,-0.0008376416,0.0035634784,0.0198337147,0.0166858,-0.036381228,-0.0038605934,-0.0343380645,0.0383730071,-0.0194098042,-0.0343295822,-0.0122550929,-0.0333548079,0.0135557517,-0.0196429489,-0.0172515819,-0.029348441,-0.0318723073,0.013164107,-0.0038415485,-0.0082447751,0.0289762343,0.0106468162,0.0221988506,-0.0247993219,-0.0185333835,-0.0046365753,-0.0087459799,0.0239444897,-0.0050485692],"Abstract_Vector":[0.2136849507,0.0821683724,0.2228107825,-0.0717332879,0.0937985721,-0.0017343212,0.12881472,-0.0655006413,0.0135779563,0.0373675339,-0.0744788505,0.0197739014,-0.0134560982,0.008363096,0.0037012524,0.0083157237,0.0225165997,0.0273693554,-0.0560932321,-0.0191994386,-0.0358381011,-0.0223972128,0.0176830259,-0.0208420399,-0.0286757258,0.0115118417,0.0025818914,-0.0109244178,-0.0210327878,0.0045753979,0.0124390923,-0.0164333367,-0.0113845303,-0.0269716579,0.0005445918,-0.0176017672,0.0253555157,-0.0383892335,0.0153294801,0.0100241017,-0.0066144679,0.0098003285,0.0683373119,0.000858503,-0.0004476779,-0.0441911229,0.0125780392,0.046068933,0.0273067068,0.0241083292,-0.0349939691,-0.011128201,-0.0113893495,0.0396891252,0.0289328398,0.0217272169,0.000314774,-0.0108984449,-0.0247592197,0.0060923729,0.0367802542,0.001637079,0.0230682593,-0.0131423345,0.0220221646,0.0256340379,-0.0271717553,-0.0300106639,0.0030482011,0.0144825387,0.0514165352,-0.0100129395,-0.0027298227,-0.0265825585,-0.0215813422,-0.0458916751,0.0480022918,0.0132795719,-0.0083019508,0.0056546149,-0.0393323712,0.0034603151,0.0376531193,0.0456718113,-0.0327513008,-0.009375496,-0.0232756176,0.017795951,0.0316100512,-0.0215101957,0.0226999766,-0.0316110222,0.0176071854,-0.0518062914,0.0426516361,-0.0006679035,-0.0053178797,0.0354454239,0.0192409891,-0.0347094705,-0.001239107,-0.0156230208,0.0204332685,-0.0405792465,-0.0328119785,0.0128508983,0.0055794138,0.0380972666,0.0143920448,0.0079228371,-0.0441057218,-0.0119777365,-0.012812149,0.0014420682,0.0622972874,-0.011126834]},"113":{"Abstract":"Motion-to-photon latency causes images to sway from side to side in a VR\/AR system, while display persistence causes smearing; both of these are undesirable artifacts. We show that once latency is reduced or eliminated, smearing due to display persistence becomes the dominant visual artifact, even with accurate tracker prediction. We investigate the human perceptual mechanisms responsible for this and we demonstrate a modified 3D rotation display controller architecture for driving a high speed digital display which minimizes latency and persistence. We simulate it in software and we built a testbench based on a very high frame rate (2880 fps 1-bit images) display system mounted on a mechanical rotation gantry which emulates display rotation during head rotation in an HMD.","Authors":"M. Regan; G. S. P. Miller","DOI":"10.1109\/TVCG.2017.2656979","Keywords":"Virtual reality;latency;persistence;display modulation","Title":"The Problem of Persistence with Rotating Displays","Keywords_Processed":"latency;display modulation;virtual reality;persistence","Keyword_Vector":[0.1310949901,-0.0449385912,0.0072813,-0.0545674232,-0.0107959348,-0.002685134,-0.0821862674,0.0112758551,-0.0486982176,-0.0115200291,-0.0346410245,0.0746509708,-0.0163791005,0.0024634578,-0.0952395841,0.0222985037,-0.0866185145,-0.0207662294,-0.0370893198,0.0487705957,0.0824462921,-0.0107887981,0.0088201093,-0.0007434309,0.0040019643,0.1056263475,-0.0256398527,0.0596257988,-0.0064219116,0.0646561291,-0.0928916525,-0.1419650701,-0.0189488406,0.0024672948,0.004919303,-0.013943108,0.0033329075,-0.0514531009,0.0043993943,-0.0251024268,-0.0007037698,0.0353166763,-0.1121236144,0.0149179292,0.0574307944,0.0977069883,0.0146972952,-0.0253646073,-0.0982242766,0.0855700359,0.0224923521,0.096374678],"Abstract_Vector":[0.3014525669,-0.1635057597,-0.031241257,-0.0397047344,-0.0806594956,0.0044514306,0.0037902316,0.0373230306,-0.0133401807,-0.0453922759,0.0048815631,0.0130416957,0.1145367897,-0.0263239937,-0.0267604097,0.0214529973,-0.0324400768,0.0016177431,-0.0280801097,0.0008777977,-0.0096283011,0.0207754014,-0.0530134497,-0.016681645,-0.0332864048,-0.0513705258,-0.0269505608,-0.0635997055,-0.0055330806,0.0160912507,-0.0236075669,-0.0094137479,0.0270553691,0.0090912136,0.0435580362,-0.0273609146,-0.0189412354,-0.0202982106,-0.0077877083,0.0008728529,0.0248462949,0.0054021671,0.0386724408,-0.091694659,-0.0200158745,-0.0101978012,0.0079072752,-0.0216632635,-0.0000136923,-0.0284193969,0.0134478262,0.0040235015,0.0317668769,-0.0036741215,0.0142203429,-0.0239992554,0.0350687186,0.031997198,-0.0233430561,-0.0429659553,-0.0226154044,0.0033891317,0.020878508,-0.0332968677,0.0164337974,0.0439246803,0.0405468914,-0.0012457215,0.0082858872,0.0083193417,-0.0164780087,-0.0031020226,0.0124901352,0.0239406435,0.0137502237,-0.0073534168,-0.029113423,0.0514296098,-0.0487469442,-0.0223310963,-0.0274030095,-0.0156411604,0.0080205428,0.0148043764,0.0026352721,0.0447365715,0.0151268684,-0.0263614667,-0.0054970409,-0.0118991692,-0.0309047199,-0.0042392118,0.0188535432,-0.0288322525,0.0065400389,-0.0027418916,0.0110733956,-0.0208780393,-0.038811658,0.020340464,-0.0077416774,0.0318846541,-0.0049550361,0.0181034896,-0.0293021242,-0.0372739844,0.0109818284,-0.0425170459,0.011181016,-0.009324561,0.0123522462,0.0084573501,-0.0041991081,-0.020747449,0.0016908406,-0.0038397379]},"114":{"Abstract":"The problem of isosurface extraction in uncertain data is an important research problem and may be approached in two ways. One can extract statistics (e.g., mean) from uncertain data points and visualize the extracted field. Alternatively, data uncertainty, characterized by probability distributions, can be propagated through the isosurface extraction process. We analyze the impact of data uncertainty on topology and geometry extraction algorithms. A novel, edge-crossing probability based approach is proposed to predict underlying isosurface topology for uncertain data. We derive a probabilistic version of the midpoint decider that resolves ambiguities that arise in identifying topological configurations. Moreover, the probability density function characterizing positional uncertainty in isosurfaces is derived analytically for a broad class of nonparametric distributions. This analytic characterization can be used for efficient closed-form computation of the expected value and variation in geometry. Our experiments show the computational advantages of our analytic approach over Monte-Carlo sampling for characterizing positional uncertainty. We also show the advantage of modeling underlying error densities in a nonparametric statistical framework as opposed to a parametric statistical framework through our experiments on ensemble datasets and uncertain scalar fields.","Authors":"T. Athawale; E. Sakhaee; A. Entezari","DOI":"10.1109\/TVCG.2015.2467958","Keywords":"Uncertainty quantification;linear interpolation;isosurface extraction;marching cubes;nonparametric statistics;Uncertainty quantification;linear interpolation;isosurface extraction;marching cubes;nonparametric statistics","Title":"Isosurface Visualization of Data with Nonparametric Models for Uncertainty","Keywords_Processed":"march cube;nonparametric statistic;isosurface extraction;uncertainty quantification;linear interpolation","Keyword_Vector":[0.1028839013,-0.0856380862,-0.093386084,0.0456434652,-0.0533755767,-0.0468003376,-0.0327018377,-0.0410811767,-0.0821936408,0.0286386358,-0.024888008,-0.0129489384,0.0345327835,0.2787420068,-0.1687355225,-0.0101950579,0.0176194479,0.0465735494,-0.0245267064,-0.0906050785,0.0311799001,-0.0739069296,-0.0317683793,-0.0213050189,-0.0236685001,0.0393460105,0.0051629555,0.024308162,-0.0427583841,0.0316649501,-0.0374828795,-0.0752684398,0.0017537789,-0.0082682715,0.0035567664,-0.0724010216,0.0760943434,-0.0138727742,0.0019066923,-0.0606192675,-0.0051852597,0.0050278596,0.0036565635,0.048175777,0.0124488617,-0.0019347198,-0.0103042922,-0.0425832058,-0.0040055167,-0.0278174975,0.0077161003,-0.0504375815],"Abstract_Vector":[0.2382572251,-0.1601040142,0.0262012949,-0.004498671,-0.0837501108,-0.008001672,0.0197916131,0.0372567046,-0.0807026862,-0.0108095057,-0.0290594727,0.0059800647,-0.0193383101,-0.0161999005,0.0019038587,-0.0221507238,-0.0590316002,-0.0153667437,-0.0086563055,0.0192483529,0.0255754172,-0.0242840495,0.017763998,-0.0314644621,-0.0478797472,0.0306334877,-0.0279849858,0.0025922245,-0.0731094115,-0.0593079516,0.0280443092,0.0019053124,-0.062777987,-0.0311317669,-0.0456824927,-0.0297685564,0.0168865772,0.0296726983,0.0136803986,-0.0041836303,-0.0328852656,0.0082215827,0.0277667681,-0.0522187337,0.0600302158,-0.0450062133,0.0496709031,0.0243928978,-0.0463166448,0.0413788319,0.0065117327,0.0611584705,0.0190963366,0.0337251943,-0.0116825816,0.0019241367,-0.0073239294,0.0184003684,0.0426997288,0.0331730829,0.0028750762,0.035706666,-0.0338230889,-0.005115943,0.0205384314,0.0073028031,-0.0885374337,0.050043656,0.0004534644,0.0109091259,-0.0150806091,-0.0381123131,0.0252880853,0.0037307764,0.0116157112,-0.0114918887,-0.034467705,0.0113208494,0.0137084005,0.0002256798,0.0313581457,-0.0133481788,0.0359107146,-0.0444450059,0.0117909951,-0.0050739692,0.006415247,-0.0016675268,0.0219080282,0.0022555501,-0.0130839093,0.0238434808,0.0193211817,-0.0150262301,0.0469662255,-0.0049998134,0.0346531701,0.0589371429,-0.0218093723,0.0318352713,-0.0354794873,-0.0038927148,0.0332949027,0.0152913814,0.0255137888,0.0000016981,0.0081297637,0.0107627716,-0.0342647261,0.0075926979,-0.0168162664,0.0304932444,-0.0280528925,-0.0225770737,-0.0102351004,0.0017752147]},"115":{"Abstract":"Bibliographic data such as collections of scientific articles and citation networks have been studied extensively in information visualization and visual analytics research. Powerful systems have been built to support various types of bibliographic analysis, but they require some training and cannot be used to disseminate the insights gained. In contrast, we focused on developing a more accessible visual analytics system, called SurVis, that is ready to disseminate a carefully surveyed literature collection. The authors of a survey may use our Web-based system to structure and analyze their literature database. Later, readers of the survey can obtain an overview, quickly retrieve specific publications, and reproduce or extend the original bibliographic analysis. Our system employs a set of selectors that enable users to filter and browse the literature collection as well as to control interactive visualizations. The versatile selector concept includes selectors for textual search, filtering by keywords and meta-information, selection and clustering of similar publications, and following citation links. Agreement to the selector is represented by word-sized sparkline visualizations seamlessly integrated into the user interface. Based on an analysis of the analytical reasoning process, we derived requirements for the system. We developed the system in a formative way involving other researchers writing literature surveys. A questionnaire study with 14 visual analytics experts confirms that SurVis meets the initially formulated requirements.","Authors":"F. Beck; S. Koch; D. Weiskopf","DOI":"10.1109\/TVCG.2015.2467757","Keywords":"Visual analytics of documents;bibliographic data;dissemination;literature browser;Visual analytics of documents;bibliographic data;dissemination;literature browser","Title":"Visual Analysis and Dissemination of Scientific Literature Collections with SurVis","Keywords_Processed":"visual analytic of document;literature browser;dissemination;bibliographic datum","Keyword_Vector":[0.1762985234,-0.0868709026,-0.095871808,0.1815918893,-0.1099725349,-0.1725742947,0.0779612066,0.0231098378,-0.0048213461,0.021774965,0.0353973863,0.0178095209,0.015109086,0.2015218818,0.0070698632,-0.0011488031,-0.0283526428,0.004739895,0.10610224,-0.0636044857,0.0335556583,0.0169119911,-0.1864209509,0.0457890374,0.0557637953,-0.0693532934,0.0174623665,-0.0274260786,0.020661517,0.0563307863,0.0482622997,-0.0011559924,-0.016317584,0.0698228155,-0.0280789656,0.1836566987,0.0035895559,-0.0666817423,-0.0100166716,0.0091343144,0.0155052578,0.0812388293,-0.0042364819,-0.0601177967,-0.0149263317,-0.0121129656,0.0557395959,0.0251692242,0.0732644782,0.070351062,0.0015043882,-0.0867868933],"Abstract_Vector":[0.313811637,0.043670076,0.0343854319,-0.0246417442,-0.0736552086,0.0768773581,-0.009230041,0.0449145854,0.022265291,-0.0360480241,0.0205549665,-0.0286837875,-0.0797777981,-0.0499774053,-0.0849596012,-0.0823755441,-0.0521151301,0.0480994104,0.0126359807,-0.0497997171,0.0011524535,0.0796436735,-0.0210904084,-0.0535263818,0.0286879444,-0.0148389672,-0.0327890892,0.0493130714,0.0072121515,-0.0627574159,-0.0137416852,-0.0309884862,-0.0470771606,-0.0373005141,-0.0433319309,0.0016215896,-0.0077155352,0.0553055068,0.0100336466,0.0067534537,0.0058947158,0.0194311802,0.0074552967,-0.0053645429,0.0245560112,-0.0462232148,-0.001581499,0.0524415305,-0.0055364018,-0.0397203485,0.0014186084,0.0197010106,-0.0290969726,-0.0370821408,-0.0083494576,0.0017159212,0.0313768908,0.0031619513,0.018093044,0.0683126789,0.0081046482,0.0063718401,-0.0621175343,-0.0148171491,0.0097494235,0.0085471396,0.0197736704,-0.0206935647,0.0331375274,0.0025754434,-0.0157315344,0.0238564574,0.0530069982,-0.0265839978,0.0024555887,0.0575568018,-0.0466858078,0.0021795393,0.0108208544,0.000986874,0.0496745171,0.0159844204,0.0216544593,-0.0044740791,0.023419066,-0.0277477597,-0.002541738,0.000689659,0.0221499283,-0.0306409368,0.0143438262,-0.0645362467,0.0380183195,-0.0790696391,0.0600596943,0.0148382789,-0.0076238119,0.0490037296,-0.0293466346,0.0234893859,-0.0165443321,-0.0390056517,0.0231151207,0.01426215,0.0400498243,-0.0173904215,-0.0099507822,0.0320486805,0.0559888013,-0.0492623525,-0.0792803727,0.0537078415,0.0273906381,-0.0345871676,-0.0770978573,0.0389637809]},"116":{"Abstract":"We empirically evaluate the extent to which people perceive non-constant time and speed encoded on 2D paths. In our graphical perception study, we evaluate nine encodings from the literature for both straight and curved paths. Visualizing time and speed information is a challenge when the x and y axes already encode other data dimensions, for example when plotting a trip on a map. This is particularly true in disciplines such as time-geography and movement analytics that often require visualizing spatio-temporal trajectories. A common approach is to use 2D+time trajectories, which are 2D paths for which time is an additional dimension. However, there are currently no guidelines regarding how to represent time and speed on such paths. Our study results provide InfoVis designers with clear guidance regarding which encodings to use and which ones to avoid; in particular, we suggest using color value to encode speed and segment length to encode time whenever possible.","Authors":"C. Perin; T. Wun; R. Pusch; S. Carpendale","DOI":"10.1109\/TVCG.2017.2743918","Keywords":"Trajectory visualization;visual encoding;movement data;graphical perception;quantitative evaluation","Title":"Assessing the Graphical Perception of Time and Speed on 2D+Time Trajectories","Keywords_Processed":"visual encoding;graphical perception;quantitative evaluation;movement datum;trajectory visualization","Keyword_Vector":[0.3422098411,0.285416571,-0.0962519647,-0.0747536454,-0.1097435221,-0.0295205359,0.0045744123,0.0630511452,-0.1124091995,0.3328664627,-0.237980962,0.0495515347,0.012437666,-0.0200088021,-0.0392557497,-0.0370361995,0.0970406115,-0.0977845261,-0.028023264,-0.0607383113,-0.0996720336,0.1350851347,0.0885051769,0.0354169626,0.1695650549,0.0278506929,0.0449403175,0.0385372548,0.0979007076,0.0652690753,0.028901806,0.0049551096,0.007148256,-0.0616939583,0.0460082367,0.0852836318,-0.0599395833,0.0639484682,0.0259604546,0.0196360873,-0.108246109,-0.0828477855,0.1165641942,-0.0007122692,0.0256904454,0.0614288495,-0.0634825928,0.0238720629,0.0044624903,0.0679936143,-0.0260167015,0.0279165838],"Abstract_Vector":[0.2726935123,-0.0849299805,-0.0591649648,-0.0787133164,-0.0245975305,-0.0204431013,-0.0478776606,-0.0134931155,-0.0613131455,-0.0284944602,-0.0166095631,-0.0050318949,0.0140886677,0.0040777073,-0.0175721969,0.0272281772,-0.0201725131,-0.013143158,0.0019136998,-0.0105019731,-0.0138066447,0.000549957,-0.0245125499,0.0333173956,0.0349887475,-0.0531930906,0.0133118065,-0.0380821719,-0.0688667268,-0.0125370389,0.0091091444,0.0099446685,-0.0265865204,0.02547026,0.0189159645,-0.0397177334,-0.0166166024,-0.0491312335,-0.0188439021,-0.012502471,0.0218880187,0.0023407306,0.0082307405,-0.0608608568,0.0048755952,-0.0081907044,-0.0063518201,0.0060084986,-0.0245293287,0.0172486242,-0.0140414641,0.018753017,0.0444728541,-0.0124691995,0.0374597229,-0.0256520859,0.0410146831,0.0062872854,-0.0505737011,-0.011118402,-0.0100288023,0.0205349549,0.0194259767,0.0032011589,-0.0022379241,-0.0159765608,-0.0120232009,0.0230154096,-0.0688276241,-0.015032416,-0.0236751503,-0.0063230121,0.0198285422,0.0145588512,-0.0473030577,-0.0009616133,-0.0584587655,0.0211083724,-0.0227037474,0.0643812918,-0.0152130675,0.0058983602,0.0492907701,0.0197697869,0.0285477111,-0.0028100966,0.0132729406,0.0126078737,-0.0125084107,-0.0654429052,-0.014950254,-0.0119755207,-0.024629105,-0.0270603417,0.0122166681,-0.0177811708,0.0456940288,-0.0104191064,-0.0167026063,-0.0287519757,-0.0008173574,-0.0300961071,0.0302497858,-0.012340263,-0.0101712657,-0.0031661692,0.0129452727,-0.020745014,0.0260011394,0.0077260582,-0.0141805445,-0.0298820907,-0.0056548748,-0.0201136746,-0.0130261373,0.0434809021]},"117":{"Abstract":"Neural sequence-to-sequence models have proven to be accurate and robust for many sequence prediction tasks, and have become the standard approach for automatic translation of text. The models work with a five-stage blackbox pipeline that begins with encoding a source sequence to a vector space and then decoding out to a new target sequence. This process is now standard, but like many deep learning methods remains quite difficult to understand or debug. In this work, we present a visual analysis tool that allows interaction and \u201cwhat if\u201d-style exploration of trained sequence-to-sequence models through each stage of the translation process. The aim is to identify which patterns have been learned, to detect model errors, and to probe the model with counterfactual scenario. We demonstrate the utility of our tool through several real-world sequence-to-sequence use cases on large-scale models.","Authors":"H. Strobelt; S. Gehrmann; M. Behrisch; A. Perer; H. Pfister; A. M. Rush","DOI":"10.1109\/TVCG.2018.2865044","Keywords":"Explainable AI;Visual Debugging;Visual Analytics;Machine Learning;Deep Learning;NLP","Title":"Seq2seq-Vis: A Visual Debugging Tool for Sequence-to-Sequence Models","Keywords_Processed":"explainable AI;deep Learning;visual Debugging;NLP;visual analytic;Machine Learning","Keyword_Vector":[0.1827391079,-0.208878191,-0.1695323296,0.1902207366,-0.1285571111,-0.1296920483,0.0983420554,-0.0137736928,0.0579690983,0.0538336873,0.0294830039,0.0797776957,0.0036612644,0.0286249775,0.0677601822,0.1198499698,0.0724942135,-0.0302908834,-0.0819568242,0.0658015219,-0.030631238,-0.0660746146,0.009816926,0.0382819094,-0.0234216068,0.045106879,0.0079483407,-0.1033312819,-0.019651392,0.0421340519,-0.0706606125,0.0035244338,0.0053569381,0.0260545427,-0.009419195,0.0234690413,-0.011684298,0.0083990618,0.0364076717,-0.0515108102,-0.0478033733,-0.041710259,-0.0055197666,-0.026190663,-0.0319176094,-0.0771581529,-0.0596139045,-0.0137699524,0.0558397843,0.0014257431,-0.001219202,0.0956335126],"Abstract_Vector":[0.289764399,-0.1369942392,0.0034699997,0.0246632221,0.0360603962,-0.0680125158,0.0325715619,-0.0214091842,-0.0568340803,0.0099198397,0.0547466451,-0.0542608946,-0.1138823672,-0.0360479726,-0.1178660846,0.0267189935,0.0128230983,-0.0721489441,-0.0536099434,0.0487309909,0.0202709436,-0.0111664063,-0.0346319376,0.0307937985,-0.0687303143,0.039886349,-0.0184714564,-0.0262908809,0.0138445315,-0.062764222,0.0726001423,-0.0070791727,-0.0049858964,-0.057160728,0.0495546484,0.0324324104,-0.0327841064,-0.0433426725,-0.026912336,0.0265452098,-0.0214620069,-0.0252963916,-0.0314760572,-0.0252915791,-0.0110843477,0.071800344,-0.0470168247,0.0066739268,0.0322614287,-0.0117933594,-0.0236236159,0.0648125203,0.0201049373,0.027262156,-0.0080291385,0.0338455013,0.0461012394,0.004879569,0.0033048379,-0.0205383229,-0.0164399032,-0.0481587456,-0.0351788726,-0.0425217842,0.0298741095,-0.0340805187,-0.0155010409,-0.0355312142,-0.0326882099,0.0294046866,0.0444717626,0.0245378184,-0.0668958528,-0.0299447305,-0.0011602952,0.0110801166,0.0124081704,-0.0132755625,-0.0292675336,0.0457950811,-0.0522800719,0.0031894241,0.0537632691,0.0024695753,-0.0265699953,0.0303771398,0.0145057922,0.0301024966,-0.0337555295,0.0150536359,-0.0050047111,-0.0313287674,-0.0315621985,-0.0029280866,0.0250794116,-0.015047955,0.0222355095,-0.0139401606,-0.0082069003,-0.0140124083,0.0190800223,0.0212861489,0.0231674997,-0.0217912622,0.068287355,0.0019609657,-0.0190753146,-0.0045113754,-0.0150914058,-0.0160629746,0.0275303093,0.0037895779,0.0261708517,-0.0472043758,0.0129680581,0.0310904791]},"118":{"Abstract":"Ensemble sensitivity analysis (ESA) has been established in the atmospheric sciences as a correlation-based approach to determine the sensitivity of a scalar forecast quantity computed by a numerical weather prediction model to changes in another model variable at a different model state. Its applications include determining the origin of forecast errors and placing targeted observations to improve future forecasts. We - a team of visualization scientists and meteorologists - present a visual analysis framework to improve upon current practice of ESA. We support the user in selecting regions to compute a meaningful target forecast quantity by embedding correlation-based grid-point clustering to obtain statistically coherent regions. The evolution of sensitivity features computed via ESA are then traced through time, by integrating a quantitative measure of feature matching into optical-flow-based feature assignment, and displayed by means of a swipe-path showing the geo-spatial evolution of the sensitivities. Visualization of the internal correlation structure of computed features guides the user towards those features robustly predicting a certain weather event. We demonstrate the use of our method by application to real-world 2D and 3D cases that occurred during the 2016 NAWDEX field campaign, showing the interactive generation of hypothesis chains to explore how atmospheric processes sensitive to each other are interrelated.","Authors":"A. Kumpf; M. Rautenhaus; M. Riemer; R. Westermann","DOI":"10.1109\/TVCG.2018.2864901","Keywords":"Correlation;clustering;tracking;ensemble visualization","Title":"Visual Analysis of the Temporal Evolution of Ensemble Forecast Sensitivities","Keywords_Processed":"ensemble visualization;clustering;correlation;tracking","Keyword_Vector":[0.1128337903,0.2020553341,-0.1043803809,-0.0064952865,-0.0227816974,0.011254543,0.0257881232,-0.0054743004,0.0091930228,0.0114871235,0.0192567829,-0.0231557315,0.0222772636,-0.0017084188,0.008276086,0.0151427672,-0.0141021161,-0.039422198,-0.0181535975,0.0207725415,-0.0152894869,0.0163081637,0.0261520413,-0.0210512971,-0.0183360807,0.0155897207,0.0219192786,-0.0286798279,-0.0025613777,0.0206466673,-0.0251718808,-0.0001741702,0.002375351,0.0231154217,-0.0086645625,-0.0349790529,0.0132624061,-0.0421285791,0.0225522567,0.0012485992,0.0370337059,-0.0059302595,0.030687312,-0.0046623165,-0.0347897679,0.032516076,0.0088322689,0.0774831349,-0.0266885451,-0.0000953248,0.0552139735,-0.0449738494],"Abstract_Vector":[0.1969925767,0.1021628733,-0.1137302529,-0.0667868728,0.0267812032,0.0537725983,0.0032873775,-0.0228756326,-0.0090680909,0.0131837194,-0.004245014,0.014322178,-0.0106516484,-0.0062056168,-0.0011709624,0.0039983354,0.008209929,-0.0431474776,-0.0233755828,-0.0315848201,-0.0201260517,-0.016231678,0.0078397197,0.0004804355,0.0268090209,0.0030916095,0.0508311185,-0.0207520928,-0.0436326579,-0.0115754312,-0.0018417046,-0.013678714,-0.0143793476,-0.0190855694,-0.0215718066,-0.0008195128,0.0504624612,0.00842054,-0.0016202092,0.0352179331,0.0520408166,0.0408252902,-0.0366668323,0.0240936743,-0.0090898401,0.0038353123,-0.0287013386,0.0193718437,0.0079306156,-0.0347573471,-0.0038597215,0.0118298239,-0.0011516116,0.0325723294,0.0146010675,0.0276858282,-0.0308280742,-0.0336731098,0.0042741999,-0.0004253835,-0.0044180375,-0.0539236727,0.0333292102,-0.0077449443,0.0150551459,-0.0115455666,-0.0007251094,-0.0186136837,0.0084757688,-0.056540341,-0.0027803403,0.0065661638,0.0044745928,0.0244963744,-0.0203452627,-0.0171216222,0.0091216376,0.0394026816,-0.0153272379,-0.0089107773,0.0113063219,0.0471300302,0.0404598803,-0.0465462192,-0.0130908162,0.0154877374,-0.0212931955,0.0111420617,0.0008851046,0.0040832315,-0.0058615609,0.0412078294,-0.0046031278,-0.0110446775,0.0144629924,-0.0558720358,0.0062490304,-0.05685056,-0.0252065735,0.033515301,-0.0029731103,0.0050913219,-0.0231954008,0.0240804195,-0.0182580343,-0.0140954228,0.0091936117,-0.031169091,-0.0076196488,0.0099048496,0.002188296,-0.01023339,-0.0184231671,-0.0059655055,0.0465075826,0.0007759057]},"119":{"Abstract":"Selecting a good aspect ratio is crucial for effective 2D diagrams. There are several aspect ratio selection methods for function plots and line charts, but only few can handle general, discrete diagrams such as 2D scatter plots. However, these methods either lack a perceptual foundation or heavily rely on intermediate isoline representations, which depend on choosing the right isovalues and are time-consuming to compute. This paper introduces a general image-based approach for selecting aspect ratios for a wide variety of 2D diagrams, ranging from scatter plots and density function plots to line charts. Our approach is derived from Federer's co-area formula and a line integral representation that enable us to directly construct image-based versions of existing selection methods using density fields. In contrast to previous methods, our approach bypasses isoline computation, so it is faster to compute, while following the perceptual foundation to select aspect ratios. Furthermore, this approach is complemented by an anisotropic kernel density estimation to construct density fields, allowing us to more faithfully characterize data patterns, such as the subgroups in scatterplots or dense regions in time series. We demonstrate the effectiveness of our approach by quantitatively comparing to previous methods and revisiting a prior user study. Finally, we present extensions for ROI banking, multi-scale banking, and the application to image data.","Authors":"Y. Wang; Z. Wang; C. Fu; H. Schmauder; O. Deussen; D. Weiskopf","DOI":"10.1109\/TVCG.2018.2865266","Keywords":"Aspect ratio;image-based method;Federer's co-area formula;density field;anisotropic kernel density estimation","Title":"Image-Based Aspect Ratio Selection","Keywords_Processed":"aspect ratio;image base method;anisotropic kernel density estimation;Federer co area formula;density field","Keyword_Vector":[0.0691478217,0.0787340639,0.0236509075,0.0233387513,0.011753349,-0.088597193,-0.053495944,-0.0642361026,-0.0069998374,-0.1337302267,0.0439226629,0.0425451293,0.2454908545,0.0541133335,0.1746004081,-0.0359405491,0.0756584912,0.0845767074,-0.1048897158,0.0799492529,0.1237584583,-0.2038173411,0.1763914403,-0.0625898067,-0.026948894,-0.0059127863,0.1135295898,0.2673037207,0.1204939865,-0.0000150769,0.0320710381,-0.1794914178,-0.0295990059,-0.0340974967,0.0349742883,0.046010052,0.0781655083,-0.0202836998,0.0989841653,0.1490440406,-0.0537240143,-0.0341472127,-0.060594827,-0.0572784104,-0.0494667874,-0.037739004,0.0251318158,0.0441267586,0.0207736755,-0.0338271728,-0.0182210418,-0.0506125923],"Abstract_Vector":[0.2119168594,0.1309293401,-0.0519861085,0.0309529646,-0.0662977128,0.0667748763,0.0793168322,0.0474015892,0.113156176,-0.065358275,-0.0091875195,-0.0568448355,-0.0686330918,0.0969165702,0.0655649486,-0.0242317601,0.0640676211,-0.0789688057,-0.0722552933,-0.0471540581,0.1871945766,0.1014986101,0.0134591037,0.0744305775,-0.0341431173,-0.0527657095,-0.0414156346,0.0080014153,0.0473781326,-0.0321844979,-0.0435512805,0.1210655337,-0.0088655582,0.0442946709,-0.0150298882,-0.0964151812,0.0136823386,-0.0071942394,0.0430267268,0.0070466093,0.0437603791,0.0031890503,0.0430384274,0.0016257979,-0.032123089,0.0157375847,0.0771656409,-0.0137056932,-0.0237921879,0.0524341309,-0.0236779157,-0.0127204894,0.0060395093,0.0705975704,-0.0276359787,0.0302980341,0.006207853,-0.0377722077,0.0242236894,0.0142683002,0.0623302434,-0.0298261377,-0.0027386178,-0.0636106577,-0.001341825,0.024463121,-0.0097625553,-0.05862106,0.0019990841,-0.0106243794,0.0339321586,0.0553834405,0.02384045,0.0097570509,0.0245853902,0.0302122963,-0.018250617,-0.0395791361,-0.0195338483,0.0404520052,-0.0475498261,-0.0018778968,-0.0274531044,-0.0273637671,-0.0318211772,-0.0049022794,0.0522586139,-0.0009013932,-0.0100176851,-0.0110375357,-0.0084670472,0.0856771004,-0.0039358771,0.0124579431,0.0419947129,0.0155117722,0.0408842187,0.0558419093,0.0081782678,-0.0094006378,-0.0049612836,0.0342578825,0.0528385742,0.0180748109,0.0322511486,-0.0102041034,-0.0115022467,-0.0466356966,0.0290816212,-0.0149621958,-0.0455666417,-0.0146137301,-0.0192314712,0.0152407041,-0.0090250518,-0.0211772735]},"12":{"Abstract":"Ego-network, which represents relationships between a specific individual, i.e., the ego, and people connected to it, i.e., alters, is a critical target to study in social network analysis. Evolutionary patterns of ego-networks along time provide huge insights to many domains such as sociology, anthropology, and psychology. However, the analysis of dynamic ego-networks remains challenging due to its complicated time-varying graph structures, for example: alters come and leave, ties grow stronger and fade away, and alter communities merge and split. Most of the existing dynamic graph visualization techniques mainly focus on topological changes of the entire network, which is not adequate for egocentric analytical tasks. In this paper, we present egoSlider, a visual analysis system for exploring and comparing dynamic ego-networks. egoSlider provides a holistic picture of the data through multiple interactively coordinated views, revealing ego-network evolutionary patterns at three different layers: a macroscopic level for summarizing the entire ego-network data, a mesoscopic level for overviewing specific individuals' ego-network evolutions, and a microscopic level for displaying detailed temporal information of egos and their alters. We demonstrate the effectiveness of egoSlider with a usage scenario with the DBLP publication records. Also, a controlled user study indicates that in general egoSlider outperforms a baseline visualization of dynamic networks for completing egocentric analytical tasks.","Authors":"Y. Wu; N. Pitipornvivat; J. Zhao; S. Yang; G. Huang; H. Qu","DOI":"10.1109\/TVCG.2015.2468151","Keywords":"Egocentric network;dynamic graph;network visualization;glyph-based design;visual analytics;Egocentric network;dynamic graph;network visualization;glyph-based design;visual analytics","Title":"egoSlider: Visual Analysis of Egocentric Network Evolution","Keywords_Processed":"dynamic graph;network visualization;egocentric network;glyph base design;visual analytic","Keyword_Vector":[0.1542938439,-0.1976953872,-0.2374618927,0.1658462694,-0.144603361,-0.2704749106,0.0666592304,0.0456067933,0.0339352956,-0.007196346,-0.0200303957,0.0749236644,-0.0402538325,-0.0508744457,0.0005145523,0.0054578955,-0.078496844,-0.0003490318,-0.0189404501,-0.0259662053,0.0969580817,0.0031641093,0.0379968007,0.010142177,-0.0818076825,0.1108173668,-0.0268610287,0.0157136125,0.0050316162,0.0425219912,0.0179192974,-0.0410582203,-0.0270836199,0.0439656804,-0.0023152524,0.0510509977,-0.0637699901,-0.1005916176,0.0395258464,-0.0817517428,0.0179496749,-0.0076336888,-0.1362296994,-0.0920298391,0.0587309478,0.0121158944,0.1459582763,-0.0084186696,-0.0669926725,0.0778270699,-0.022614873,0.0080338263],"Abstract_Vector":[0.1214584493,-0.0299298916,-0.0008874877,-0.0256776649,-0.0121918316,0.0296638778,-0.0554495645,-0.0141832798,0.0232204167,-0.0572896519,-0.0459059114,0.0488095132,-0.0253570363,0.0832191673,0.0488156615,-0.0548002675,-0.0616065191,-0.0235518916,-0.065078345,-0.0214387959,0.0359088731,-0.0222984131,-0.0263282323,-0.0169379812,0.0045374773,0.0374509884,0.0117129572,-0.0178465122,-0.0101345707,0.0636847697,0.047465317,0.0850760368,0.039330244,0.0483484477,-0.0076754276,0.067402687,-0.0497883591,-0.0454720856,-0.0033321518,-0.0218149497,-0.0639336403,-0.0457804471,0.0430840996,0.0549230467,-0.0178476756,0.0266491061,-0.0283213287,0.0387365433,-0.0081625111,-0.037859699,0.0357652567,-0.0124735863,-0.0149959972,-0.0051015235,0.0057799948,-0.0123435588,-0.0131551366,0.0041490936,-0.0577192113,0.0143035773,-0.0155587504,-0.0259960814,-0.0070931181,0.0339041047,-0.0295928983,0.0063754572,-0.0039680358,0.0285342352,-0.006689866,-0.0193334779,-0.0106193459,-0.0145266773,-0.0426630927,-0.0014565291,-0.012109922,-0.0122883241,-0.018956951,0.0279387506,0.0055288505,-0.0049214676,-0.0267331739,-0.0167504307,0.0174343736,0.0276185624,-0.010336785,0.0034060556,-0.0021996907,0.0014163089,-0.0120690616,0.0047608203,-0.0067973636,0.0008059817,0.0139808337,0.0113996411,-0.0000588389,-0.0171438939,0.013791272,0.0093680411,0.021493738,-0.0037040268,-0.0227608195,-0.022474022,0.0086006034,0.0100555861,0.005930004,-0.0203571914,-0.015802751,-0.0179607569,0.0099121528,-0.0118844626,0.0107618041,-0.0083411783,0.009596023,0.0078171294,0.0193338753,-0.00780146]},"120":{"Abstract":"The rising quantity and complexity of data creates a need to design and optimize data processing pipelines-the set of data processing steps, parameters and algorithms that perform operations on the data. Visualization can support this process but, although there are many examples of systems for visual parameter analysis, there remains a need to systematically assess users' requirements and match those requirements to exemplar visualization methods. This article presents a new characterization of the requirements for pipeline design and optimization. This characterization is based on both a review of the literature and first-hand assessment of eight application case studies. We also match these requirements with exemplar functionality provided by existing visualization tools. Thus, we provide end-users and visualization developers with a way of identifying functionality that addresses data processing problems in an application. We also identify seven future challenges for visualization research that are not met by the capabilities of today's systems.","Authors":"T. von Landesberger; D. W. Fellner; R. A. Ruddle","DOI":"10.1109\/TVCG.2016.2603178","Keywords":"Visualization systems;requirement analysis;data processing pipelines","Title":"Visualization System Requirements for Data Processing Pipeline Design and Optimization","Keywords_Processed":"visualization system;requirement analysis;datum processing pipeline","Keyword_Vector":[0.1191756284,0.1084905399,-0.0484291718,0.0228204694,0.025741201,-0.0199092855,-0.0171704772,0.0054988146,0.0181302521,-0.0331332833,0.0378900835,0.0520798932,0.0448845295,0.0718327335,0.0622663946,0.006785262,-0.0038900102,-0.0537540145,0.0236256938,-0.0732467657,0.0805008531,-0.032291824,-0.0775021236,0.0433280743,-0.0236846118,-0.0355546931,0.0884476606,0.0603668222,0.0668651311,0.0000214749,-0.004623546,-0.0359953126,0.0043646525,-0.0013703686,0.0140218686,0.1329437187,0.019304348,-0.0413169452,0.0151272268,0.0413628044,0.0532971341,0.0438739385,0.017411517,-0.033354268,-0.0518800418,0.0116730508,0.027320807,0.0162484864,0.0207803453,-0.0466985224,-0.0080928063,0.0500961086],"Abstract_Vector":[0.1753087684,0.0287863675,-0.0313985954,-0.0168093079,-0.0265241313,0.0447305554,0.0331463328,0.035780985,0.0736683374,-0.0190439619,-0.0138249659,-0.0043185249,-0.0707520864,-0.0028871451,-0.0049925791,-0.0171324109,0.0341429942,0.0426093141,-0.0147642682,-0.0445569897,0.0227055586,0.0097690586,0.0178752647,0.0008707713,0.0165131656,-0.0036605925,0.0088486793,0.0214746685,-0.0023396134,0.0239675062,-0.0329868906,0.0540137114,0.0013862057,0.0145865061,-0.0173257317,0.0052144067,-0.0059462984,-0.0130296006,0.0063367504,-0.0109096674,0.0139219399,0.0535396033,-0.0064485032,0.0119990731,0.025568892,0.0099330601,-0.0157181533,0.0467071364,0.0004842492,-0.0176909795,-0.023259237,-0.0051152685,-0.0012629066,0.0349419468,0.0110994458,-0.0143925314,-0.0005955938,-0.0084228755,0.0055836908,0.00098916,0.0137476467,-0.0392310214,-0.0231394779,0.0151962151,-0.0166913915,0.0000022561,-0.0490235588,0.0056803936,-0.0025825382,0.0088304104,0.0217009189,0.0410882118,-0.0050730223,0.0126732757,-0.0005271374,-0.0135615294,-0.0116200537,0.0374249038,0.0000802744,-0.0300425004,0.0127979941,0.0082516518,-0.0022167647,0.0355735925,0.0067084073,0.0068900872,-0.0266670859,-0.0457195393,-0.0214555141,-0.0106001458,-0.0208840875,0.0079698195,0.0255039842,0.0200694064,0.0147216432,-0.0256706022,-0.0009234831,-0.0224071733,-0.0037705746,-0.0261610535,-0.0072506176,0.0179340269,0.0172727892,-0.0032469202,-0.0241751589,0.0018513403,0.0093798557,0.0347937991,-0.012160967,-0.0093464641,-0.0028252175,-0.0079962448,0.0026526842,0.0117427009,0.0475756573,0.0283518217]},"121":{"Abstract":"We propose a reinforcement learning approach for real-time exposure control of a mobile camera that is personalizable. Our approach is based on Markov Decision Process (MDP). In the camera viewfinder or live preview mode, given the current frame, our system predicts the change in exposure so as to optimize the trade-off among image quality, fast convergence, and minimal temporal oscillation. We model the exposure prediction function as a fully convolutional neural network that can be trained through Gaussian policy gradient in an end-to-end fashion. As a result, our system can associate scene semantics with exposure values; it can also be extended to personalize the exposure adjustments for a user and device. We improve the learning performance by incorporating an adaptive metering module that links semantics with exposure. This adaptive metering module generalizes the conventional spot or matrix metering techniques. We validate our system using the MIT FiveK [1] and our own datasets captured using iPhone 7 and Google Pixel. Experimental results show that our system exhibits stable real-time behavior while improving visual quality compared to what is achieved through native camera control.","Authors":"H. Yang; B. Wang; N. Vesdapunt; M. Guo; S. B. Kang","DOI":"10.1109\/TVCG.2018.2865555","Keywords":"Auto exposure;reinforcement learning;personalization","Title":"Personalized Exposure Control Using Adaptive Metering and Reinforcement Learning","Keywords_Processed":"personalization;reinforcement learning;auto exposure","Keyword_Vector":[0.1574225985,0.0126848072,0.0235699252,-0.0492399965,-0.0245420399,0.0451858389,-0.0164182706,0.0637192569,-0.0233857543,-0.0714836234,-0.085688852,-0.0149360998,-0.1649378763,0.0461526571,-0.1629616034,0.0285494634,-0.0421951152,-0.0127066866,-0.10273599,0.1184898194,0.0013489305,0.0313324646,0.1003684359,0.1341365229,0.2470088225,0.0405516999,0.1414688546,-0.0024390495,0.2337214869,0.1198202505,0.1033899719,-0.1218117725,0.1167451134,-0.0237904548,-0.0314902743,-0.0539129563,-0.1024975592,0.0526576747,-0.0955088603,0.0124797338,0.0133622788,0.0124450358,0.0441685889,-0.0399140655,0.0357482848,-0.0384386296,-0.0412878327,-0.0056566757,-0.0143486737,0.0226043816,-0.0246640366,-0.0005045681],"Abstract_Vector":[0.2006839767,-0.0936403989,0.0368716792,-0.0372524422,-0.0713306174,0.0009782198,0.0368883813,-0.0350849863,-0.0644999623,-0.2123057657,0.2138740996,0.2954315501,-0.1124577557,0.0100206307,0.1351486806,-0.0823094834,0.1552505465,-0.0257377482,0.0080962851,-0.0144037669,-0.030464608,-0.0279167694,0.0275036865,0.00524328,0.0213138355,-0.0093700006,-0.1000433396,0.0137516317,-0.0074843612,0.0146207021,-0.0271205307,-0.0921574837,-0.0440600162,0.017348462,-0.0010589115,-0.0088960826,0.0236012375,0.0276779182,-0.0005786636,0.0093127468,-0.0226819944,0.06880333,-0.0003091305,-0.0015240456,0.0350811266,-0.0144738031,0.015776462,-0.0366143682,-0.0037709742,-0.0411838961,0.0144067442,0.0064607907,-0.0373677918,-0.0201057678,-0.0531971713,0.0298228128,0.0114710723,0.0053141851,-0.0359152681,-0.0006569162,-0.0087588524,-0.0120432722,-0.0214149791,0.0047323474,0.0286034063,0.0205721333,0.026501211,-0.0236058411,0.0283638956,0.0123176781,0.0402646723,-0.0078869342,-0.0071029893,-0.0042512003,0.004577041,0.0182662034,-0.0166121292,-0.0095483069,-0.0334091592,-0.0232741082,-0.0152330426,0.0032519338,0.0023484375,0.0178892854,-0.0013902102,-0.0048873195,0.0133151479,0.0272395197,0.0162224286,-0.0175949773,-0.0133455472,-0.0112830955,0.0244239384,0.0068195078,0.0474403138,0.0303398591,0.0020394275,0.0472763027,0.0188400982,0.0141168707,-0.0108158582,-0.0335598209,-0.0215554649,0.0038128878,-0.0180214988,-0.0122794638,-0.0256279798,0.001080695,-0.0148355733,-0.0025965321,-0.0079067677,0.0289605913,0.0064966153,-0.0054837594,-0.0117129741,0.0041384724]},"122":{"Abstract":"Understanding and accounting for uncertainty is critical to effectively reasoning about visualized data. However, evaluating the impact of an uncertainty visualization is complex due to the difficulties that people have interpreting uncertainty and the challenge of defining correct behavior with uncertainty information. Currently, evaluators of uncertainty visualization must rely on general purpose visualization evaluation frameworks which can be ill-equipped to provide guidance with the unique difficulties of assessing judgments under uncertainty. To help evaluators navigate these complexities, we present a taxonomy for characterizing decisions made in designing an evaluation of an uncertainty visualization. Our taxonomy differentiates six levels of decisions that comprise an uncertainty visualization evaluation: the behavioral targets of the study, expected effects from an uncertainty visualization, evaluation goals, measures, elicitation techniques, and analysis approaches. Applying our taxonomy to 86 user studies of uncertainty visualizations, we find that existing evaluation practice, particularly in visualization research, focuses on Performance and Satisfaction-based measures that assume more predictable and statistically-driven judgment behavior than is suggested by research on human judgment and decision making. We reflect on common themes in evaluation practice concerning the interpretation and semantics of uncertainty, the use of confidence reporting, and a bias toward evaluating performance as accuracy rather than decision quality. We conclude with a concrete set of recommendations for evaluators designed to reduce the mismatch between the conceptualization of uncertainty in visualization versus other fields.","Authors":"J. Hullman; X. Qiao; M. Correll; A. Kale; M. Kay","DOI":"10.1109\/TVCG.2018.2864889","Keywords":"Uncertainty visualization;user study;subjective confidence;probability distribution","Title":"In Pursuit of Error: A Survey of Uncertainty Visualization Evaluation","Keywords_Processed":"user study;uncertainty visualization;probability distribution;subjective confidence","Keyword_Vector":[0.2084309697,-0.0263437953,0.2293185789,0.0512484972,-0.168349372,-0.0294257318,-0.0153157504,-0.0068021443,0.0741726509,-0.1644061812,-0.0769161319,0.0350898745,0.0065743974,0.0950971477,-0.1099682099,0.0583126055,0.1961168882,-0.1494537498,-0.0459678859,-0.0597597768,0.0209999857,-0.031855647,0.0398631083,0.1456088066,-0.0764513367,-0.0655306384,-0.0477507602,-0.0442884742,-0.120446606,-0.0612644197,-0.0498685378,0.0712325319,0.0657216899,0.0824984486,0.0063026188,-0.0319338799,0.137192006,0.0035396347,0.0029537719,0.0589823903,0.0128466467,-0.0044535,-0.019350093,-0.0597850119,0.0214670719,-0.0180001211,0.0116355326,-0.0173670017,0.0017512196,0.0228126601,0.0270610443,0.064017745],"Abstract_Vector":[0.1993949731,-0.0217369415,0.1730621032,-0.0624761388,0.0526110018,0.0207764699,0.1205450113,-0.0560149003,-0.00846706,-0.0140055199,0.0060254515,-0.0167744626,0.0073090962,0.0159228632,-0.0384844872,0.0259672552,-0.0507011746,0.0286644841,-0.0157008675,-0.0018855079,0.011790077,-0.0009037221,0.0211200091,-0.0339668478,-0.0143096402,-0.0100658176,0.0177915418,-0.0267140694,-0.010811291,-0.0087393573,0.0161705139,-0.0180648509,-0.0234702628,-0.0344262502,0.013072495,-0.0024660975,-0.0199579311,0.0698778881,0.0274177378,-0.0067158297,-0.0430200897,0.0214378149,0.0265572958,0.0008379832,0.0196939763,-0.0476772988,-0.0171782388,0.0363497691,0.0240909654,0.0113848524,0.0109761611,-0.0000073017,-0.0291953736,-0.0006116241,-0.0067180133,0.0143496403,-0.0229757013,0.0114421601,-0.0007892721,0.0434341602,0.0130149452,-0.0029244156,0.0289787194,0.0131125956,0.0154102859,0.0544220267,-0.0353627591,-0.0094593007,-0.0114528651,0.0047292408,0.0004975773,0.0140279045,0.0173079429,0.0183474681,-0.0049452558,-0.0159889726,0.0102714384,0.0296737254,-0.0093196916,-0.0183314707,0.0154793945,0.0032559361,0.0202174673,-0.0248737429,0.0108050361,0.0557903828,0.0021066904,-0.004847679,0.0272087151,0.0082787427,-0.0408725073,-0.0124590269,0.0055911888,0.025555732,0.0404711544,0.0120844539,0.0078543112,-0.0239803405,-0.0228344112,-0.041116404,0.0166293099,0.0000434514,-0.0036739163,0.0062975617,-0.0236472031,-0.0182455347,0.0259306362,0.0012381512,-0.004262675,0.012076887,0.0108435367,0.0099854432,-0.0288405629,0.0039774489,-0.0094520951,0.0236630589]},"123":{"Abstract":"We present Reactive Vega, a system architecture that provides the first robust and comprehensive treatment of declarative visual and interaction design for data visualization. Starting from a single declarative specification, Reactive Vega constructs a dataflow graph in which input data, scene graph elements, and interaction events are all treated as first-class streaming data sources. To support expressive interactive visualizations that may involve time-varying scalar, relational, or hierarchical data, Reactive Vega's dataflow graph can dynamically re-write itself at runtime by extending or pruning branches in a data-driven fashion. We discuss both compile- and run-time optimizations applied within Reactive Vega, and share the results of benchmark studies that indicate superior interactive performance to both D3 and the original, non-reactive Vega system.","Authors":"A. Satyanarayan; R. Russell; J. Hoffswell; J. Heer","DOI":"10.1109\/TVCG.2015.2467091","Keywords":"Information visualization;systems;toolkits;declarative specification;optimization;interaction;streaming data;Information visualization;systems;toolkits;declarative specification;optimization;interaction;streaming data","Title":"Reactive Vega: A Streaming Dataflow Architecture for Declarative Interactive Visualization","Keywords_Processed":"toolkit;interaction;stream datum;system;optimization;declarative specification;information visualization","Keyword_Vector":[0.1754753829,0.0591669706,-0.0103256115,-0.0717204269,-0.0072704896,0.0150349709,-0.0102903426,0.0138593416,0.0202934694,-0.0170446413,-0.0036210661,-0.0074915641,-0.030351123,-0.0083230318,-0.0058595287,0.0431831348,-0.0370926051,-0.0053714113,-0.005719053,0.0101479844,-0.0241143042,-0.0012681116,-0.045375985,0.0025251641,-0.0166515086,0.0146515049,0.0086181382,0.0009829537,0.016581622,-0.0196382135,-0.0281864284,0.0050181693,-0.0208396782,0.0101397552,0.0197501008,0.0169009645,0.0720013713,-0.0114032341,-0.0273795742,0.0289192018,-0.022977807,0.0399042124,0.0579515459,-0.0373159903,-0.0305057575,0.0085952328,-0.0210043348,-0.0035620804,-0.0556052196,0.0116449919,0.0021246574,0.1411768496],"Abstract_Vector":[0.3045045662,0.0170691556,-0.0278682415,-0.0072958467,-0.1026735675,-0.0310555601,0.0277542406,-0.0061429456,0.0639234712,0.0100231776,0.1001397086,-0.0636072444,0.0156974788,-0.0070308128,-0.0376747445,-0.0348915886,0.0151753409,0.0352135549,-0.0363685172,0.009953121,0.0002547473,0.045701104,-0.037296267,0.0315666692,-0.001861517,-0.0349431075,0.0029215194,-0.0083958747,-0.0302810403,-0.034058248,-0.0140302391,0.0492934715,-0.0280751911,0.0072065579,-0.0007387922,0.01514067,-0.0114616172,0.011716503,0.0239468469,-0.0517661843,0.0027945786,0.0045496194,0.0185907988,-0.0160188045,0.0155344271,0.0073906263,0.0239053917,0.0118503902,0.0013664649,0.0102413282,-0.065545046,0.0184363151,0.0037912688,-0.0357958139,-0.0262577032,-0.0273297054,0.0017407539,0.0429390866,-0.0499366279,0.0003207484,-0.0256725534,-0.0119725082,-0.0549339164,0.046645185,-0.0329766447,0.0157734376,0.0049741235,0.026456004,-0.0569146588,0.0442103011,-0.0170105984,0.0096688393,-0.0770083802,0.0075811046,-0.0019822018,-0.0409159767,-0.0154604228,0.0659011742,0.0059305163,-0.0141454069,-0.0037415935,-0.0015991524,0.0522709887,0.0487343707,-0.0226313327,0.025735307,-0.0332474725,0.0114673042,-0.0303051504,-0.0068139594,-0.0390199856,-0.0283905287,-0.0116654048,0.0276959465,-0.0162524605,-0.0358839915,0.026463148,-0.056191945,0.0006393734,-0.0093682897,0.0422801518,-0.0226531714,-0.0087051882,0.0130115054,0.0007879494,-0.0405646282,-0.0119778801,0.0180720537,-0.0352750887,0.0081447994,0.0343118595,-0.0022437556,0.0356206995,-0.0414452473,0.0065237197,0.0101154499]},"124":{"Abstract":"Data with multiple probabilistic labels are common in many situations. For example, a movie may be associated with multiple genres with different levels of confidence. Despite their ubiquity, the problem of visualizing probabilistic labels has not been adequately addressed. Existing approaches often either discard the probabilistic information, or map the data to a low-dimensional subspace where their associations with original labels are obscured. In this paper, we propose a novel visual technique, UnTangle Map, for visualizing probabilistic multi-labels. In our proposed visualization, data items are placed inside a web of connected triangles, with labels assigned to the triangle vertices such that nearby labels are more relevant to each other. The positions of the data items are determined based on the probabilistic associations between items and labels. UnTangle Map provides both (a) an automatic label placement algorithm, and (b) adaptive interactions that allow users to control the label positioning for different information needs. Our work makes a unique contribution by providing an effective way to investigate the relationship between data items and their probabilistic labels, as well as the relationships among labels. Our user study suggests that the visualization effectively helps users discover emergent patterns and compare the nuances of probabilistic information in the data labels.","Authors":"N. Cao; Y. Lin; D. Gotz","DOI":"10.1109\/TVCG.2015.2424878","Keywords":"Visualization;Multidimensional Visualization;Probability Vector;Visualization;multidimensional visualization;probability vector","Title":"UnTangle Map: Visual Analysis of Probabilistic Multi-Label Data","Keywords_Processed":"probability vector;Multidimensional visualization;multidimensional visualization;visualization;Probability Vector","Keyword_Vector":[0.1124825176,0.1338011653,-0.0387210028,-0.013542653,-0.002516439,-0.0033021401,0.0022496776,0.0014691068,-0.0080784916,-0.0240027905,0.0528239671,0.0536059914,-0.0169555381,0.0122411428,0.0963238221,0.0589225131,-0.0932029252,-0.1489875451,0.1151234322,-0.0932739216,0.1318511521,-0.0975216592,0.0206504822,-0.022930196,0.0210542021,0.051047493,0.0152084529,-0.0624493954,0.0073242473,-0.0311544081,-0.0625891579,-0.0097670867,-0.0471448302,-0.0161800529,-0.0792944721,-0.0230358907,-0.0423920266,0.016083386,-0.0112449185,0.0881841227,-0.1011499567,0.0405512104,0.1092280773,-0.0439109988,0.0584925477,0.0519864973,-0.0295729587,0.0658932576,-0.0305080193,-0.0726137131,-0.0371809772,-0.0646705362],"Abstract_Vector":[0.178669721,0.1063368576,-0.1016559927,0.015512237,0.0160796865,0.056993148,0.0494057241,0.0647358001,0.0891787081,-0.0112930479,0.000177099,0.0419936858,-0.0703172529,-0.0931474513,0.01678594,-0.0256321934,-0.0553084823,0.0163185075,-0.0062839314,0.0371430016,0.005268145,0.0038744138,-0.0337200383,-0.0055226184,-0.0323278803,-0.0080323682,0.0085409293,-0.0426092322,0.0051521118,-0.0069551982,0.019941713,-0.021079636,-0.0273773989,-0.0709911385,0.0199117027,0.0017279866,-0.0065899498,0.0115166622,0.0197567815,0.0064268902,-0.0223181834,-0.0094942242,0.0098578652,-0.0152705482,-0.0643396528,0.0006535433,0.005635306,-0.0372801969,0.0159692147,-0.037601248,0.0201964623,-0.0204316595,0.0161359341,-0.0313001128,0.0378623273,-0.060442291,0.0414285231,-0.023354253,0.0117659018,0.0170493938,0.0000127292,0.0282888481,0.0342167263,-0.027639751,0.0023393628,0.0129188837,0.0138702643,0.0025969507,-0.0228351347,-0.0105807875,-0.0057935658,-0.0115747725,-0.001595522,0.028264956,-0.0077177982,-0.0251892231,-0.0266148308,0.011068106,0.0130735355,0.0296991589,-0.0192770138,0.0473139002,0.0189601455,0.0263779177,-0.0226292184,0.0020029111,0.0154347341,0.0052524298,-0.0214463368,0.0187172414,-0.0094061002,-0.0237391975,-0.0058362783,-0.0090786943,0.011033835,0.0065391278,-0.013845071,-0.0063093081,-0.0054844042,0.0318666587,-0.0290794302,0.0208988877,-0.0135932875,-0.0079938462,-0.0094444875,-0.0306707606,0.0101457126,0.0044775987,0.0452418583,-0.0354277923,-0.0298646989,0.0095479037,0.021233532,-0.0129741375,-0.0391216796,0.0099952795]},"125":{"Abstract":"Virtual characters that appear almost photo-realistic have been shown to induce negative responses from viewers in traditional media, such as film and video games. This effect, described as the uncanny valley, is the reason why realism is often avoided when the aim is to create an appealing virtual character. In Virtual Reality, there have been few attempts to investigate this phenomenon and the implications of rendering virtual characters with high levels of realism on user enjoyment. In this paper, we conducted a large-scale experiment on over one thousand members of the public in order to gather information on how virtual characters are perceived in interactive virtual reality games. We were particularly interested in whether different render styles (realistic, cartoon, etc.) would directly influence appeal, or if a character's personality was the most important indicator of appeal. We used a number of perceptual metrics such as subjective ratings, proximity, and attribution bias in order to test our hypothesis. Our main result shows that affinity towards virtual characters is a complex interaction between the character's appearance and personality, and that realism is in fact a positive choice for virtual characters in virtual reality.","Authors":"K. Zibrek; E. Kokkinara; R. Mcdonnell","DOI":"10.1109\/TVCG.2018.2794638","Keywords":"Personality;virtual characters;virtual reality;perception","Title":"The Effect of Realistic Appearance of Virtual Characters in Immersive Environments - Does the Character's Personality Play a Role?","Keywords_Processed":"perception;personality;virtual character;virtual reality","Keyword_Vector":[0.0217622481,0.0049429887,-0.0023555308,0.010968619,0.0133964887,0.0030610716,-0.0026427473,0.0067939001,-0.0082453317,-0.0189472282,0.0103013473,-0.0014195203,-0.0243721071,0.0387070941,0.0031010972,-0.0353177052,-0.0069383539,0.0038876262,-0.0154284867,0.0155915972,0.0163976305,-0.0189815651,0.0032106852,0.0414135435,-0.0116648762,0.0053287793,0.0188246766,-0.0208780204,-0.0000554242,0.0135214127,-0.0112592395,0.00026015,0.0011763036,0.0046433751,0.0131446492,-0.0273555702,-0.0193937314,-0.0181943508,0.0314734918,-0.0700728846,0.0939937886,-0.0377886811,0.0241789825,0.0174546647,-0.0171499907,0.1044070431,-0.049945359,0.1127612066,-0.0298290925,0.0194591582,0.0047840026,0.0238929322],"Abstract_Vector":[0.2017861572,0.0000970842,0.0273995117,-0.0010329035,0.0052422154,-0.0488043115,-0.0567565191,0.0005330142,0.0275083795,-0.0369525061,0.005297549,-0.0144519664,-0.0465339889,-0.0111964645,-0.0173126929,-0.0063267375,-0.0400446918,-0.0153569117,0.0251792814,-0.0467323542,0.0058972173,-0.0005500926,-0.0294307727,0.0075306342,-0.0050349264,-0.0132608522,0.0189640511,0.047312522,0.0706305139,-0.0049578724,-0.0592920746,-0.0371376413,-0.0318283995,-0.0305389235,-0.0632078481,0.0279727119,-0.0161089377,-0.0298520577,-0.0496953907,0.0701390722,0.0243128176,0.0127300446,0.0031854121,0.0200284445,-0.0333295317,-0.0392072655,0.0752566959,-0.0321287367,-0.0378425439,0.0096858538,-0.0466404046,0.0001052604,0.0006344698,-0.0029919211,0.0088860396,0.001721754,-0.0246543125,-0.0009003777,-0.0314772251,-0.0212610579,-0.0089367313,-0.0014275732,0.062464413,0.055492474,0.0195421089,0.0376261064,0.0154073817,0.0389428265,-0.0186796795,0.0597066742,-0.0584053442,-0.0096810179,-0.0222069282,-0.012120442,-0.0039114337,0.0193323252,0.0295503401,-0.0090507002,-0.0309297895,-0.0305286343,-0.0383696586,-0.0240092248,-0.0212452227,-0.0336570289,0.0124747098,0.0210521232,0.0130182296,0.0403443261,-0.0371993517,-0.0711792461,0.0365059129,0.0278264289,0.029452128,-0.0560114877,0.0053551594,-0.0109919626,0.0292346664,0.0036216656,-0.017592492,0.004982287,0.0388988885,-0.0011742018,-0.0044698546,0.0004035165,0.0035754324,0.016459446,0.004870561,-0.0308385167,0.0094634648,0.0131619922,-0.0129485222,0.0313358438,0.0358718873,0.0727524628,-0.0307043858,-0.0132100694]},"126":{"Abstract":"This paper presents a framework for externalizing and analyzing expert knowledge about discrepancies in data through the use of visualization. Grounded in an 18-month design study with global health experts, the framework formalizes the notion of data discrepancies as implicit error, both in global health data and more broadly. We use the term implicit error to describe measurement error that is inherent to and pervasive throughout a dataset, but that isn't explicitly accounted for or defined. Instead, implicit error exists in the minds of experts, is mainly qualitative, and is accounted for subjectively during expert interpretation of the data. Externalizing knowledge surrounding implicit error can assist in synchronizing, validating, and enhancing interpretation, and can inform error analysis and mitigation. The framework consists of a description of implicit error components that are important for downstream analysis, along with a process model for externalizing and analyzing implicit error using visualization. As a second contribution, we provide a rich, reflective, and verifiable description of our research process as an exemplar summary toward the ongoing inquiry into ways of increasing the validity and transferability of design study research.","Authors":"N. Mccurdy; J. Gerdes; M. Meyer","DOI":"10.1109\/TVCG.2018.2864913","Keywords":"implicit error;knowledge externalization;design study","Title":"A Framework for Externalizing Implicit Error Using Visualization","Keywords_Processed":"implicit error;design study;knowledge externalization","Keyword_Vector":[0.0939585861,-0.0532519148,0.0084861271,-0.0568103977,0.0196737473,0.0044639773,0.0451362251,-0.0046919712,0.0002347394,-0.04525274,-0.0069868534,-0.0029708422,-0.0078911432,-0.0406165174,-0.0015107356,0.0309708526,-0.0118856281,0.0576954337,-0.0331025519,-0.0801408599,-0.0299121292,0.0341698912,-0.0194151264,-0.0410480031,0.1651976887,-0.0078912314,0.0208567982,0.0566200963,-0.0610188352,-0.0928638706,-0.0241177818,-0.0470377672,-0.0370300737,0.0356870603,-0.0614811305,-0.022355578,-0.042023226,-0.0004305929,0.1105492551,0.0091151638,0.0307403228,-0.0017702124,0.0222832748,0.0227224548,-0.009387407,-0.0186846497,-0.0079997787,-0.0128320257,-0.0258946707,0.0053939565,-0.0305127629,0.0159781545],"Abstract_Vector":[0.2160518619,-0.1297041931,0.0178180281,0.0397755909,0.0094156801,-0.0032040038,0.0196343767,-0.0150684212,-0.0690675703,0.0900606284,0.0560857238,-0.0825538803,-0.0794401596,0.0743293352,0.0815466945,-0.048209305,0.0012431058,0.0149019844,-0.0091359162,-0.0539059163,-0.017236338,0.038078624,-0.0233525699,-0.0065337694,-0.0264140756,0.0236277898,0.0160794706,-0.0071481197,-0.0449869131,-0.0316120917,0.0168827721,-0.0084519574,-0.0125131227,-0.0008449878,-0.0051754181,0.0038154583,-0.0055021505,0.023167081,-0.0076404762,-0.0058164093,-0.0370156781,0.0385591846,-0.0187672956,-0.0053963588,-0.0150777159,-0.0427049368,0.0164238248,0.0277686809,0.0042265968,-0.01082962,0.0212546723,-0.0128709004,-0.011716198,0.0128292242,-0.0217634746,0.0284869094,0.0146697623,0.00250178,0.005506463,0.0230155413,0.0182151129,0.0090714054,-0.0049412081,-0.0102994147,0.0118757831,0.0183164336,-0.0302496828,0.0172758597,-0.0315356874,0.0240736671,-0.0317484516,-0.0093277513,0.027004272,0.0068505804,0.0293910616,0.0181852958,0.0145286095,-0.0083723066,0.0020623292,0.0219463818,-0.0298883256,0.0032237737,-0.0329158116,-0.0292029922,0.1023096183,0.010235553,-0.0077937982,0.0805279223,-0.0242283401,-0.0345441748,-0.0218031021,-0.0144169057,0.0225046505,-0.0201754674,-0.0478613028,-0.0316423298,0.010530068,-0.0207158948,0.0213030089,-0.0089042827,0.0116398678,0.0367036536,-0.0727202341,-0.0381542642,-0.0261127615,0.0651226298,-0.0065895092,-0.0159808961,0.0362965192,-0.0035545112,-0.0241615897,-0.0099374536,0.0054807888,0.0316558804,0.0124669927,0.0256563536]},"127":{"Abstract":"Identification of early signs of rotating stall is essential for the study of turbine engine stability. With recent advancements of high performance computing, high-resolution unsteady flow fields allow in depth exploration of rotating stall and its possible causes. Performing stall analysis, however, involves significant effort to process large amounts of simulation data, especially when investigating abnormalities across many time steps. In order to assist scientists during the exploration process, we present a visual analytics framework to identify suspected spatiotemporal regions through a comparative visualization so that scientists are able to focus on relevant data in more detail. To achieve this, we propose efficient stall analysis algorithms derived from domain knowledge and convey the analysis results through juxtaposed interactive plots. Using our integrated visualization system, scientists can visually investigate the detected regions for potential stall initiation and further explore these regions to enhance the understanding of this phenomenon. Positive feedback from scientists demonstrate the efficacy of our system in analyzing rotating stall.","Authors":"C. Chen; S. Dutta; X. Liu; G. Heinlein; H. Shen; J. Chen","DOI":"10.1109\/TVCG.2015.2467952","Keywords":"Turbine flow visualization;vortex extraction;anomaly detection;juxtaposition;brushing and linking;time series;Turbine flow visualization;vortex extraction;anomaly detection;juxtaposition;brushing and linking;time series","Title":"Visualization and Analysis of Rotating Stall for Transonic Jet Engine Simulation","Keywords_Processed":"turbine flow visualization;anomaly detection;vortex extraction;juxtaposition;brush and linking;time series","Keyword_Vector":[0.0878212848,-0.0620464741,-0.0100424769,0.046421044,0.0020569883,0.0962674567,0.0293419981,-0.0178980624,-0.0252326061,-0.018138724,0.0274240806,0.0019293194,0.0339867589,-0.04874478,-0.0274002954,-0.0703564507,-0.015270574,-0.0244726159,0.0166733006,0.0091456323,-0.0110796493,-0.0003611709,-0.0080068657,0.0957916397,-0.0042590199,0.0339867131,-0.0059866211,0.0230153238,0.0696472294,0.0156762049,-0.0235399249,0.0237026228,-0.0999099306,0.0736352888,-0.0059076682,-0.0113031717,-0.0023718944,-0.0554118694,0.0216205483,-0.0022747713,0.0078344573,0.0067423552,-0.0344401842,0.0363771719,0.0099341979,-0.0473866608,0.0052279949,0.0065013297,0.0486108289,-0.0122315221,-0.0061953932,-0.0265147834],"Abstract_Vector":[0.2271767681,-0.0399560675,0.0160252421,0.02422055,-0.0004555901,-0.0245801185,-0.0188517934,0.005027977,-0.0061055792,-0.0250609991,-0.0061910689,0.0145117779,-0.0494985017,-0.0107894902,-0.0170408342,0.008368917,0.0211579565,0.0172459967,-0.0211131757,-0.0467424069,-0.0066159947,0.0073758008,0.0079673936,0.0284145251,0.0013611942,0.0224453368,0.0206576664,-0.0111696205,0.026990365,-0.0312011113,0.0226278864,0.0445075741,-0.0420060239,-0.0483620233,-0.0214078143,-0.017241003,0.0368478107,0.0159720949,-0.0006423501,0.0037972924,0.0102092997,-0.0181095168,0.001570293,-0.028170799,-0.0181309801,0.013635772,-0.0056373174,-0.0089525018,0.0033038089,0.0159682756,0.0194236934,0.0222578018,-0.0135846861,0.0107224617,-0.015239851,-0.01649472,0.0104563686,-0.0093826627,0.0108717116,0.0119635117,-0.0309689992,0.0247970902,0.0361896757,-0.002064465,-0.0136882207,-0.0472868904,-0.0427552192,0.0087012204,-0.0392786228,-0.0027200821,0.0056086662,-0.0381636811,0.0102339282,0.0189928869,0.0581124031,-0.0330224,-0.0005163003,-0.0188969779,-0.0301999161,-0.0228861186,-0.0461611558,-0.0545902356,-0.0417505201,0.0110310397,-0.0434788038,-0.0179958953,-0.0033331329,-0.0238397066,-0.0213503847,-0.0358838719,-0.0177022319,0.0026949319,0.0102489984,-0.0008282207,0.0052485334,0.0407414362,0.0148885811,0.0068229452,-0.0280519779,0.0208488464,0.033083251,-0.006781323,0.0077890043,-0.0279647478,-0.0213466975,0.0426471735,0.0141327582,0.027618413,-0.0225376567,0.0210831881,0.011099684,0.0021898444,0.0336235878,-0.0043242556,0.0003587873,0.0340573909]},"128":{"Abstract":"An ensemble is a collection of related datasets, called members, built from a series of runs of a simulation or an experiment. Ensembles are large, temporal, multidimensional, and multivariate, making them difficult to analyze. Another important challenge is visualizing ensembles that vary both in space and time. Initial visualization techniques displayed ensembles with a small number of members, or presented an overview of an entire ensemble, but without potentially important details. Recently, researchers have suggested combining these two directions, allowing users to choose subsets of members to visualization. This manual selection process places the burden on the user to identify which members to explore. We first introduce a static ensemble visualization system that automatically helps users locate interesting subsets of members to visualize. We next extend the system to support analysis and visualization of temporal ensembles. We employ 3D shape comparison, cluster tree visualization, and glyph based visualization to represent different levels of detail within an ensemble. This strategy is used to provide two approaches for temporal ensemble analysis: (1) segment based ensemble analysis, to capture important shape transition time-steps, clusters groups of similar members, and identify common shape changes over time across multiple members; and (2) time-step based ensemble analysis, which assumes ensemble members are aligned in time by combining similar shapes at common time-steps. Both approaches enable users to interactively visualize and analyze a temporal ensemble from different perspectives at different levels of detail. We demonstrate our techniques on an ensemble studying matter transition from hadronic gas to quark-gluon plasma during gold-on-gold particle collisions.","Authors":"L. Hao; C. G. Healey; S. A. Bass","DOI":"10.1109\/TVCG.2015.2468093","Keywords":"Ensemble visualization;Ensemble visualization","Title":"Effective Visualization of Temporal Ensembles","Keywords_Processed":"ensemble visualization","Keyword_Vector":[0.0484442167,0.0187980664,0.0486709513,0.0122728488,0.0070727721,-0.0364909935,-0.0162153584,-0.0767808313,-0.0406625961,-0.079647115,-0.0096952953,0.0366789758,-0.0896583242,0.0297701581,0.0761876412,-0.0647592742,0.0465450276,-0.013244145,-0.0097536876,0.0068683515,0.0036991618,0.0561591772,-0.0100809622,-0.005045909,-0.0308574163,0.072541238,-0.0600588237,-0.0017517475,-0.0353198305,0.0687132075,-0.009847995,-0.0048079822,0.0288771839,-0.0123081442,-0.0451804054,0.0221244413,-0.0094351165,-0.0359806306,-0.0175702823,0.0346171785,-0.019118091,0.0411643342,-0.0301833446,0.0162364182,-0.0636998288,-0.0287962764,-0.0094831691,-0.0209322227,0.0011565755,-0.0005077511,-0.0008037977,-0.0065274291],"Abstract_Vector":[0.1970162406,-0.0141237576,0.0369801076,0.0367712843,-0.0702748347,0.0209418659,0.0324037983,-0.0026831698,0.0147442283,0.0080581054,0.0408447526,-0.0122136245,-0.0015684725,-0.0308830796,-0.0356300934,0.0718085136,0.0232278987,0.068638785,0.0185201388,-0.1078957785,0.0845996465,-0.0926793185,0.0239780727,0.0591327151,0.0829723006,-0.0044528318,0.0216876138,-0.0199396105,0.0330263984,-0.0154067134,-0.0193318403,0.0318477964,-0.03364319,0.0407476056,0.0491219908,-0.0245515674,0.0303780733,-0.0068186421,-0.075636845,-0.0724615377,0.0544481412,-0.0158843923,-0.00861665,-0.0224759478,-0.0121782897,0.0050261116,0.0764759361,0.0051829234,-0.0386155365,-0.046499835,0.0031722808,0.0024488903,0.0223346281,0.002792673,0.0319651125,0.0322496129,0.0563054686,-0.0120386002,0.0344906594,0.0434266234,0.013898329,-0.0153531034,0.0198677793,0.066150831,0.0099304522,-0.0731959262,-0.0176780315,0.0293272818,-0.0103127964,0.0422080359,0.0364560547,0.0472203281,-0.0519250416,-0.0044611463,-0.0016088965,-0.0098715888,-0.0013105124,-0.0264156902,0.0239125545,0.0107392579,0.0006186363,-0.0012750105,0.0276237975,-0.0191562744,-0.0503468825,0.0329816471,-0.01978037,0.0018469898,-0.001108917,0.0635800381,0.0032764986,0.0014024213,-0.0267608417,0.0030100926,-0.0430137162,-0.0069720323,0.0143926357,0.0171229242,0.0300078479,0.0252184824,-0.026770902,-0.0133288052,0.038604416,0.009434308,0.0107172699,-0.0246424158,-0.0133738849,-0.0151183641,0.0002475033,-0.0082604392,-0.0339831372,0.000627599,-0.0036045719,0.0074977049,0.0218779383,0.0194644304]},"129":{"Abstract":"Molecular dynamics (MD) simulations are crucial to investigating important processes in physics and thermodynamics. The simulated atoms are usually visualized as hard spheres with Phong shading, where individual particles and their local density can be perceived well in close-up views. However, for large-scale simulations with 10 million particles or more, the visualization of large fields-of-view usually suffers from strong aliasing artifacts, because the mismatch between data size and output resolution leads to severe under-sampling of the geometry. Excessive super-sampling can alleviate this problem, but is prohibitively expensive. This paper presents a novel visualization method for large-scale particle data that addresses aliasing while enabling interactive high-quality rendering. We introduce the novel concept of screen-space normal distribution functions (S-NDFs) for particle data. S-NDFs represent the distribution of surface normals that map to a given pixel in screen space, which enables high-quality re-lighting without re-rendering particles. In order to facilitate interactive zooming, we cache S-NDFs in a screen-space mipmap (S-MIP). Together, these two concepts enable interactive, scale-consistent re-lighting and shading changes, as well as zooming, without having to re-sample the particle data. We show how our method facilitates the interactive exploration of real-world large-scale MD simulation data in different scenarios.","Authors":"M. Ibrahim; P. Wickenh\u00e4user; P. Rautek; G. Reina; M. Hadwiger","DOI":"10.1109\/TVCG.2017.2743979","Keywords":"Multiresolution Techniques;Point-Based Data;Glyph-based Techniques;Scalability Issues;Molecular Visualization","Title":"Screen-Space Normal Distribution Function Caching for Consistent Multi-Resolution Rendering of Large Particle Data","Keywords_Processed":"molecular visualization;point base Data;multiresolution technique;Glyph base technique;scalability issue","Keyword_Vector":[0.0427606949,0.0196307952,-0.0117255244,0.0373788655,0.0291399343,-0.0146960139,-0.002955079,-0.0086531688,-0.0541078007,-0.0140539228,0.0083790186,0.0300810392,-0.0423289035,0.0384023844,0.0697520586,-0.044396968,-0.0378128692,-0.0314442187,0.0443578026,-0.0330520941,0.0208317319,-0.0485208471,-0.083064258,0.0579465539,-0.0095809581,-0.0371082251,0.0843805033,0.0044541353,-0.0516300743,-0.0026337637,-0.0310061571,0.0001853247,-0.0364665054,-0.0369311616,-0.008916303,-0.0004028546,-0.0318805047,-0.0120788942,-0.0665873112,0.0054122028,0.03746511,-0.0091447502,0.0174970355,-0.0090077095,0.0154063722,-0.0652212157,-0.0118188762,0.0980478722,0.0289995848,-0.007447117,-0.0326180488,-0.0267324415],"Abstract_Vector":[0.2112546226,0.0513960913,-0.0251411353,-0.0018271653,-0.0680465262,0.0913424432,-0.0194050591,0.0185418358,0.080402698,-0.0026354221,-0.0309872188,-0.0182532683,-0.0924621561,-0.0032526498,-0.0697470799,0.0093163242,0.0418204416,0.0797082364,0.0162244518,-0.0485899942,-0.0161075551,0.0624510344,-0.0287270209,-0.0351305573,0.0166875376,-0.0227679296,0.0014528091,0.006623449,-0.0279583843,0.0274646358,0.0383726076,0.0071679996,-0.0152039408,0.0195236762,0.0123892018,-0.0294651013,0.0038305026,-0.00681035,-0.0269398701,0.013718913,0.0019245962,0.0385138151,-0.0075388523,-0.0108012148,0.0169461256,-0.0118672354,0.0956792276,0.0320806994,0.0022440824,0.0084454979,0.0051520315,0.0060871458,-0.0027850828,-0.0195732458,-0.0332578924,0.0063239748,0.0263227921,0.0222933659,0.0132016563,-0.035369401,0.0366914411,0.0111917324,-0.0190017985,-0.0014991939,0.0115235904,0.036586011,0.013164082,-0.022848046,0.0195778607,0.0237427271,-0.0081611297,-0.0172029512,-0.0231649701,-0.0349062153,0.0560204668,0.05617718,-0.0386787232,-0.0138290556,-0.0157070642,0.0095141698,0.0209858719,0.0071829815,0.0234490654,0.0387101156,0.0256012208,0.0049537628,0.0122388017,-0.0341286662,-0.0262715691,0.0189855811,-0.0374799203,0.0226886436,-0.0073786205,-0.0317894502,-0.0211267457,-0.0303469283,-0.0098296608,0.0289017701,-0.0017085595,0.0453873738,-0.0222110994,0.0338856165,0.0114848859,-0.0014056081,0.0251324252,-0.0545869259,-0.0246503136,0.0113029707,0.0089507834,-0.0095607101,-0.0066013583,-0.006077526,0.0170027107,-0.0156381889,0.0227574136,-0.0404353595]},"13":{"Abstract":"Atmospheric fronts play a central role in meteorology, as the boundaries between different air masses and as fundamental features of extra-tropical cyclones. They appear in numerous conceptual model depictions of extra-tropical weather systems. Conceptually, fronts are three-dimensional surfaces in space possessing an innate structural complexity, yet in meteorology, both manual and objective identification and depiction have historically focused on the structure in two dimensions. In this work, we -a team of visualization scientists and meteorologists-propose a novel visualization approach to analyze the three-dimensional structure of atmospheric fronts and related physical and dynamical processes. We build upon existing approaches to objectively identify fronts as lines in two dimensions and extend these to obtain frontal surfaces in three dimensions, using the magnitude of temperature change along the gradient of a moist potential temperature field as the primary identifying factor. We introduce the use of normal curves in the temperature gradient field to visualize a frontal zone (i.e., the transitional zone between the air masses) and the distribution of atmospheric variables in such zones. To enable for the first time a statistical analysis of frontal zones, we present a new approach to obtain the volume enclosed by a zone, by classifying grid boxes that intersect with normal curves emanating from a selected front. We introduce our method by means of an idealized numerical simulation and demonstrate its use with two real-world cases using numerical weather prediction data.","Authors":"M. Kern; T. Hewson; A. Sch\u00e4tler; R. Westermann; M. Rautenhaus","DOI":"10.1109\/TVCG.2018.2864806","Keywords":"Meteorology;Atmospheric Fronts;Feature Detection","Title":"Interactive 3D Visual Analysis of Atmospheric Fronts","Keywords_Processed":"meteorology;feature detection;Atmospheric front","Keyword_Vector":[0.0128212632,-0.0019835673,0.0070842499,0.009995163,0.0113787146,-0.0039038466,-0.0000898832,0.0016024133,-0.0074210278,0.0155720129,-0.0087302081,-0.0000070643,-0.0095565148,-0.0039135021,0.0253971809,-0.0023461386,0.0161671675,0.0018418858,0.0062270334,-0.0040066037,-0.0101528772,0.0022529201,-0.0045509962,0.0077789456,-0.0091229187,0.0041934858,-0.0012250932,-0.0270314088,-0.0028496851,0.0168948771,-0.0033874521,-0.0197062543,0.0156616238,0.031935769,0.0293061487,0.008905837,0.0065782201,-0.0114117461,0.0178213448,0.0000580181,-0.0002239256,0.0345906339,-0.0061449308,0.0315733889,0.0262702772,0.0046903964,-0.0149315778,-0.0405767434,0.0237911161,0.0020271081,0.0154641807,-0.0132886983],"Abstract_Vector":[0.185574832,-0.020599244,0.0299094422,-0.0263902699,-0.0399604681,0.0141500491,-0.0160011568,-0.0141325396,0.0252914013,0.0127508297,0.0506024731,0.006474559,0.0218940227,0.0318761203,-0.0446910769,0.008100725,-0.0435890348,0.0102713623,0.0232077406,-0.0388536879,-0.0270448995,0.0215085801,0.0176770521,0.0129871339,-0.0193339818,0.0339861841,-0.0189969584,0.0206305231,0.0240800761,-0.0202600402,0.0424311324,-0.00111434,-0.0553538206,-0.0068107102,-0.034608208,-0.0354070186,-0.0347392926,0.0000950073,0.0280893301,-0.0162562742,-0.0359136388,0.0312955957,-0.0168389714,0.0088457319,0.0237295429,-0.0215084012,0.0137335708,0.009999791,-0.0197165992,0.0327674975,-0.0409894893,0.0280756094,0.0064860508,0.0194039475,0.0496720989,0.0355895797,0.0143493678,0.0085968345,-0.0133019684,0.0150699692,0.0077737179,0.00256047,-0.0226934874,-0.0397643049,-0.022805428,-0.0136630065,0.0543769274,0.0405094883,-0.0082592977,0.046242466,-0.0156929658,0.0025365876,0.0120374534,0.0024616125,-0.0068480579,0.0028726845,0.0123485098,0.0040107739,0.003555623,-0.008134427,-0.0049625901,0.0397734153,-0.0139046654,-0.002383477,-0.0350710585,-0.0006140586,-0.0245688188,-0.0525968154,0.0385559968,0.0055740958,0.0095914347,0.0243071037,-0.0238597348,-0.0483788274,0.0154064422,0.0151317274,0.0142476374,0.032256068,-0.0108276462,-0.0374672883,-0.0127698666,0.0332473223,0.0013554552,0.021362454,-0.0166186284,-0.0165409346,0.0157906189,-0.0043545748,0.0086193085,0.000718434,0.0031455877,0.0052423339,0.0155759363,-0.0068667411,0.0252010946,-0.0111060675]},"130":{"Abstract":"We present a design study of the TensorFlow Graph Visualizer, part of the TensorFlow machine intelligence platform. This tool helps users understand complex machine learning architectures by visualizing their underlying dataflow graphs. The tool works by applying a series of graph transformations that enable standard layout techniques to produce a legible interactive diagram. To declutter the graph, we decouple non-critical nodes from the layout. To provide an overview, we build a clustered graph using the hierarchical structure annotated in the source code. To support exploration of nested structure on demand, we perform edge bundling to enable stable and responsive cluster expansion. Finally, we detect and highlight repeated structures to emphasize a model's modular composition. To demonstrate the utility of the visualizer, we describe example usage scenarios and report user feedback. Overall, users find the visualizer useful for understanding, debugging, and sharing the structures of their models.","Authors":"K. Wongsuphasawat; D. Smilkov; J. Wexler; J. Wilson; D. Man\u00e9; D. Fritz; D. Krishnan; F. B. Vi\u00e9gas; M. Wattenberg","DOI":"10.1109\/TVCG.2017.2744878","Keywords":"Neural Network;Graph Visualization;Dataflow Graph;Clustered Graph","Title":"Visualizing Dataflow Graphs of Deep Learning Models in TensorFlow","Keywords_Processed":"cluster Graph;Neural Network;Dataflow Graph;Graph visualization","Keyword_Vector":[0.122716642,-0.0395504202,-0.0266793589,0.0056361381,-0.0140997147,0.0727445287,-0.0210669965,0.0013493637,-0.0645566195,0.0434775707,-0.032064119,0.0932710798,0.0254614306,-0.0679452338,-0.0105278784,-0.0304027364,0.0405952025,-0.0402920833,-0.0205825763,0.0175871901,-0.0418265236,-0.0325644329,-0.0679878592,-0.0229664624,0.0440698204,0.0246675352,-0.0079757182,-0.0287233274,-0.0019856194,0.024914,0.0698954369,-0.0217418595,0.0117011145,-0.0243403698,-0.0313975202,0.0294616288,0.0164183399,0.0411173553,0.0132959304,-0.0251919949,-0.0357630874,0.0134269871,0.0290569041,0.047039577,-0.0026599142,0.0096663695,0.0432583978,-0.0566440851,0.0311880968,-0.0258816956,0.0000306989,0.0347354441],"Abstract_Vector":[0.2100417809,-0.1011142233,-0.0196317577,-0.0002296198,-0.0174087363,-0.0448822635,-0.0149665234,-0.0197685338,-0.0005010563,-0.0191231447,0.021612181,0.0089688956,0.0364592501,-0.0031780038,-0.0049851778,-0.0104006205,-0.0402022841,-0.0333493707,0.0013004248,-0.0327552071,-0.0079794316,-0.0055627971,-0.0188997904,-0.008553446,-0.0194582557,-0.0189716805,0.0038765454,-0.0461410617,-0.0276596322,-0.0164504649,0.0124286719,-0.0139829337,0.0034590257,-0.013708938,0.0363535006,-0.0230866715,-0.0161919523,-0.014340999,-0.013076183,-0.0062474125,0.0366966565,0.0024240395,0.0023417018,-0.0260230216,-0.0070000966,-0.000335157,0.0115675656,-0.0089917545,-0.0224123744,-0.0453434578,-0.016857471,0.0342501202,0.0249553916,0.0049067515,0.0030396572,0.0455159377,-0.0254960149,0.0036301802,0.0093422191,0.0225539658,0.0279543145,-0.029451801,-0.0157513303,-0.0001166721,-0.0194703849,-0.0032485493,-0.0245548821,0.0013632267,0.0171005143,0.0433937693,-0.0168833581,-0.0097477332,-0.0258911539,0.0236124097,-0.0034392465,-0.0445715961,-0.013785341,-0.0001386177,-0.0207140727,0.0032639266,-0.0337792281,-0.0134890941,0.0236068714,0.0206509811,-0.0323481655,-0.0133689864,0.0034986337,0.0163863791,-0.022692284,0.0197516872,-0.0009694619,0.0256395048,-0.0389231988,-0.011380819,0.0204194271,-0.029769149,-0.0255036289,0.0046612559,0.0122502711,0.0351617118,0.0275196039,-0.0171638027,0.0041396943,0.0205673644,0.017621758,0.0104075595,-0.0215668209,-0.0091679115,-0.0105921006,-0.0005799137,0.0416758142,0.0276472769,0.0042242321,0.0014001267,0.0067848953,-0.0075828947]},"131":{"Abstract":"This paper studies the problem of how to assess the quality of photographing viewpoints and how to choose good viewpoints for taking photographs of architectures. We achieve this by learning from photographs of world famous landmarks that are available on the Internet and their viewpoint quality ranked by online user annotation. Unlike previous efforts devoted to photo quality assessment which mainly rely on 2D image features, we show in this paper combining 2D image features extracted from images with 3D geometric features computed on the 3D models can result in more reliable evaluation of viewpoint quality. Specifically, we collect a set of photographs for each of 15 world famous architectures as well as their 3D models from the Internet. Viewpoint recovery for images is carried out through an image-model registration process, after which a newly proposed viewpoint clustering strategy is exploited to validate users' viewpoint preferences when photographing landmarks. Finally, we extract a number of 2D and 3D features for each image based on multiple visual and geometric cues and perform viewpoint recommendation by learning from both 2D and 3D features using a specifically designed SVM-2K multi-view learner, achieving superior performance over using solely 2D or 3D features. We show the effectiveness of the proposed approach through extensive experiments. The experiments also demonstrate that our system can be used to recommend viewpoints for rendering textured 3D models of buildings for the use of architectural design, in addition to viewpoint evaluation of photographs and recommendation of viewpoints for photographing architectures in practice.","Authors":"J. He; L. Wang; W. Zhou; H. Zhang; X. Cui; Y. Guo","DOI":"10.1109\/TVCG.2018.2853751","Keywords":"Viewpoint assessment;viewpoint recommendation;learning;image aesthetics","Title":"Viewpoint Assessment and Recommendation for Photographing Architectures","Keywords_Processed":"learn;viewpoint recommendation;viewpoint assessment;image aesthetic","Keyword_Vector":[0.077939985,-0.0559308246,-0.0378056973,-0.0468611472,0.014212407,-0.0592574938,-0.0973382461,-0.0458006302,-0.1103943155,0.1289319062,-0.0967757998,0.1595254375,0.0386230603,-0.0312299501,-0.0449215707,-0.0336886032,0.0367406726,0.0116008116,-0.0557944503,-0.0096654989,0.039837256,0.0276921041,0.0455393173,-0.0610977626,0.0303672083,-0.0206751002,0.0918467509,-0.0688509928,-0.01663539,0.0163970119,-0.0370912987,0.0664227399,-0.0305107163,-0.0156245445,0.0250022029,-0.0053530836,0.0695340532,-0.0095112236,0.0248018452,0.0429080297,-0.0455312965,-0.0563644941,0.0718427251,0.0246813271,-0.0152196864,-0.0249856769,0.0082108605,0.0129496234,0.0426699322,0.0426293817,-0.0222716006,0.0443174197],"Abstract_Vector":[0.1124992444,-0.0222297837,-0.0013274422,-0.0015507405,-0.0202652471,0.0331174795,-0.0077886631,-0.0001750997,0.0293665913,0.014436392,-0.0487414061,0.0483834205,-0.0449311062,0.0223257609,0.0205288321,-0.0115110383,0.0167289862,0.0091856855,-0.0028019858,-0.0228893877,0.006138684,-0.0482380746,0.0165026752,-0.0071971646,-0.0259654404,0.0262753047,-0.0071554754,-0.0012091398,0.0067500048,-0.0292037874,0.0421758144,0.0258854776,-0.0039705574,-0.0013557255,-0.0097783322,-0.0238234939,0.0234105855,-0.0014028291,0.031791801,-0.0044352648,0.0065000857,-0.0180241618,0.0335468376,0.0078781158,-0.044077841,0.0086565568,0.0002948095,-0.0066300093,0.0263815833,-0.0468958464,-0.0007201595,-0.0523094825,-0.0341927422,-0.0324169919,-0.0137633173,-0.006028187,-0.004461992,0.0133082479,-0.0144123821,-0.0261149143,0.0078646966,0.0049897741,0.0126534163,0.0209214583,0.0027418515,-0.0308610633,-0.0143314245,-0.0048362258,-0.0338710384,-0.0173425287,0.0032452151,0.0313115901,0.0017022643,0.0254194093,-0.0110170024,-0.0651601857,0.0683655346,0.0270713109,-0.0238689628,-0.0267088138,0.0061103644,0.018951274,-0.0361688981,0.0336011164,0.0241787731,0.0063692445,-0.0043676177,0.0047968283,-0.0217549452,0.015762488,-0.0111066171,0.0206459143,0.0003140207,0.0236648797,-0.016472575,-0.0246854383,-0.0080117013,-0.038507413,-0.0251864416,-0.0000515157,0.0048112725,-0.0166889532,-0.0027515318,-0.0249378642,0.006186194,0.0156859756,0.0126110863,0.0179594242,0.0038495796,0.0206685608,-0.0176198958,0.0203725565,0.0024450405,-0.0075767484,0.0518615795,0.0304945302]},"132":{"Abstract":"Central to many text analysis methods is the notion of a concept: a set of semantically related keywords characterizing a specific object, phenomenon, or theme. Advances in word embedding allow building a concept from a small set of seed terms. However, naive application of such techniques may result in false positive errors because of the polysemy of natural language. To mitigate this problem, we present a visual analytics system called ConceptVector that guides a user in building such concepts and then using them to analyze documents. Document-analysis case studies with real-world datasets demonstrate the fine-grained analysis provided by ConceptVector. To support the elaborate modeling of concepts, we introduce a bipolar concept model and support for specifying irrelevant words. We validate the interactive lexicon building interface by a user study and expert reviews. Quantitative evaluation shows that the bipolar lexicon generated with our methods is comparable to human-generated ones.","Authors":"D. Park; S. Kim; J. Lee; J. Choo; N. Diakopoulos; N. Elmqvist","DOI":"10.1109\/TVCG.2017.2744478","Keywords":"Text analytics;visual analytics;word embedding;text summarization;text classification;concepts","Title":"ConceptVector: Text Visual Analytics via Interactive Lexicon Building Using Word Embedding","Keywords_Processed":"text classification;word embed;text analytic;text summarization;concept;visual analytic","Keyword_Vector":[0.0628745859,-0.0171910946,0.0124779613,-0.0280021758,0.0068801668,0.0056607649,-0.0081675838,0.00340701,-0.0011614009,-0.0191917648,-0.0028897014,0.0132276163,-0.0011976158,-0.0196848009,-0.0153326417,0.0137899085,-0.0205196672,0.0031364836,-0.0057864511,0.0175719055,-0.0173442432,0.0027949314,-0.0092517987,-0.002331799,0.0095112065,0.0104263804,0.0088595575,0.0149972722,0.0219607228,-0.0109494126,0.0098140479,0.0166443776,-0.0231581731,0.0087728684,0.0138750809,-0.0177989824,0.0017506504,0.0061405196,0.0079674621,-0.0195653305,-0.0234272364,0.0363387376,0.0053747744,0.0049027768,-0.012670834,-0.0015714742,-0.0191321204,0.0252589786,0.0047470541,-0.0245187076,0.0516746161,0.0265818066],"Abstract_Vector":[0.1565328653,-0.0623045796,0.0188666881,0.0135542105,-0.04097326,0.0257507697,-0.0121468353,0.0246759172,-0.0084560106,-0.0109222622,-0.0546382608,0.0777374155,-0.0231749924,-0.028444317,0.0494173157,-0.005737082,-0.0578339761,-0.0342983589,0.0158732253,-0.0148448205,0.0200247528,-0.0104402505,-0.0190306345,-0.0045731342,-0.0206033125,0.0380017495,0.0075853346,-0.0124364864,0.0031296809,-0.0010944912,0.0113816228,0.0022621852,-0.0127089015,0.0177507116,-0.0120060266,-0.0516969114,0.0154039021,0.0116759821,-0.0174287311,0.0131659503,-0.0168752721,0.0606033836,0.0085113405,0.0064169805,-0.0112448376,0.0118413477,-0.0056430768,-0.004133387,0.0080410392,-0.0077549153,-0.0312735119,0.000186084,-0.0189588857,-0.0359060886,-0.0075699162,-0.047050369,0.0161667556,-0.0155450529,0.0142436373,-0.014278002,-0.0003590118,0.0130080178,-0.0245556436,0.0077445082,-0.0024799011,-0.0043045221,-0.0009779367,0.00190489,-0.0352217928,-0.0015134733,-0.0132185082,-0.021391472,0.0031463894,0.021940367,-0.0085375523,0.0109357499,-0.0056635689,0.0209040775,0.0094746965,-0.0119651426,0.0060095681,0.036095966,0.0052576246,-0.0101537767,0.0631642753,0.0008815342,-0.0269879872,0.0099691235,-0.004733011,0.0106861623,0.0255118188,0.0372667233,-0.0355836746,-0.004313629,0.0013641994,0.0061911913,-0.0010178418,0.0053827501,-0.0100822119,-0.0041348824,0.0073825425,-0.0347602825,0.0023998496,0.0165192224,-0.0122505743,0.0186311728,-0.0259627951,0.0172018836,-0.0061492896,0.0073044754,0.0145725227,0.0392753632,-0.0322105068,-0.0270695761,-0.0136811122,0.0029126274]},"133":{"Abstract":"We propose a system to infer binocular disparity from a monocular video stream in real-time. Different from classic reconstruction of physical depth in computer vision, we compute perceptually plausible disparity, that is numerically inaccurate, but results in a very similar overall depth impression with plausible overall layout, sharp edges, fine details and agreement between luminance and disparity. We use several simple monocular cues to estimate disparity maps and confidence maps of low spatial and temporal resolution in real-time. These are complemented by spatially-varying, appearance-dependent and class-specific disparity prior maps, learned from example stereo images. Scene classification selects this prior at runtime. Fusion of prior and cues is done by means of robust MAP inference on a dense spatio-temporal conditional random field with high spatial and temporal resolution. Using normal distributions allows this in constant-time, parallel per-pixel work. We compare our approach to previous 2D-to-3D conversion systems in terms of different metrics, as well as a user study and validate our notion of perceptually plausible disparity.","Authors":"T. Leimk\u00fchler; P. Kellnhofer; T. Ritschel; K. Myszkowski; H. Seidel","DOI":"10.1109\/TVCG.2017.2703612","Keywords":"Depth cues;stereo;image-based rendering;perceptual reasoning;video analysis;viewing algorithms;pixel classification;real-time systems","Title":"Perceptual Real-Time 2D-to-3D Conversion Using Cue Fusion","Keywords_Processed":"image base render;real time system;depth cue;perceptual reasoning;stereo;video analysis;view algorithm;pixel classification","Keyword_Vector":[0.1607835612,-0.0617045773,-0.0005994426,-0.0019195038,-0.0143364704,0.1001884575,-0.0825862178,0.0600954774,0.0522303334,0.0132165204,0.0410354248,0.1587461871,-0.0332994166,-0.0134101094,-0.0564631946,-0.0630408093,-0.0331893906,-0.0388302672,0.0687207123,0.2728276182,0.0947310535,0.0916637108,-0.0276830354,-0.0233275457,0.1143054466,0.0245133958,0.0619926685,0.0428106972,0.0001454698,0.0349241308,-0.1138548772,-0.0451107769,0.0557465957,0.0430251769,-0.0887601061,0.0043890646,0.0366175743,0.0856428406,0.0341297923,-0.0113869706,-0.0968593925,-0.0387796966,-0.0662337029,0.048791302,-0.0325775527,0.1522518789,0.0044381666,0.0344955688,0.003705203,-0.0282560138,-0.0220908154,0.1018945778],"Abstract_Vector":[0.2776787819,-0.1129566874,-0.0462239883,-0.0323075722,0.004248179,-0.1266739394,-0.0478237597,-0.030104775,0.0427993037,-0.0306664174,0.0724152011,-0.0741939783,0.0411426583,-0.0385961223,-0.0214029501,0.033872376,-0.0443315657,-0.0112759729,-0.0121716854,-0.0233628952,0.0384490572,-0.0190748929,0.0095195368,-0.0027543064,-0.0621956748,-0.0411425735,-0.0106731202,0.0023738673,0.0184086533,0.0180710011,-0.0196659295,-0.0194675039,0.0024380571,0.0179421249,0.0137282482,-0.0236187792,0.0231927744,-0.0762897989,0.023798256,0.0056329513,0.0197890322,0.0194796646,0.0195049172,0.0386412772,-0.0474762379,-0.0365888042,0.0319907709,-0.0439134185,0.0000041717,-0.0512294442,-0.0109346897,0.011408379,-0.0133605855,0.0571814346,0.0492158243,-0.0381582439,-0.0020054149,-0.0775882083,0.0189198169,-0.0215529339,0.0065222814,-0.0670608163,-0.0327764274,0.0327475267,-0.0655906857,0.0417010294,-0.0227039271,-0.068849083,-0.0173243165,0.0091889001,-0.0345216318,-0.0071871867,0.021303093,0.0255952936,0.0450693014,-0.0276105001,-0.0044453911,-0.0176700624,-0.0399848893,0.0105164314,0.0345001151,0.0448565878,-0.0289349723,0.0237582109,0.0328478873,-0.0000278895,-0.0744152735,0.0120241779,0.0067392912,0.0109744225,-0.0358881554,-0.0205852456,-0.034733542,0.0257993331,-0.0362688326,0.0137521644,-0.0008845209,-0.0269342674,0.0172896584,-0.0022590974,-0.0350765406,-0.0308759026,-0.0006448209,0.0131117011,-0.0017388142,0.0104083334,-0.0244598271,-0.0010057918,-0.0614143089,-0.0224452082,-0.0334811063,-0.0380674222,-0.0192490519,-0.0626640975,-0.0184601736,0.0368602509]},"134":{"Abstract":"While the Sense of Agency (SoA) has so far been predominantly characterised in VR as a component of the Sense of Embodiment, other communities (e.g., in psychology or neurosciences) have investigated the SoA from a different perspective proposing complementary theories. Yet, despite the acknowledged potential benefits of catching up with these theories a gap remains. This paper first aims to contribute to fill this gap by introducing a theory according to which the SoA can be divided into two components, the feeling and the judgment of agency, and relies on three principles, namely the principles of priority, exclusivity and consistency. We argue that this theory could provide insights on the factors influencing the SoA in VR systems. Second, we propose novel approaches to manipulate the SoA in controlled VR experiments (based on these three principles) as well as to measure the SoA, and more specifically its two components based on neurophysiological markers, using ElectroEncephaloGraphy (EEG). We claim that these approaches would enable us to deepen our understanding of the SoA in VR contexts. Finally, we validate these approaches in an experiment. Our results (N=24) suggest that our approach was successful in manipulating the SoA as the modulation of each of the three principles induced significant decreases of the SoA (measured using questionnaires). In addition, we recorded participants' EEG signals during the VR experiment, and neurophysiological markers of the SoA, potentially reflecting the feeling and judgment of agency specifically, were revealed. Our results also suggest that users' profile, more precisely their Locus of Control (LoC), influences their level of immersion and SoA.","Authors":"C. Jeunet; L. Albert; F. Argelaguet; A. L\u00e9cuyer","DOI":"10.1109\/TVCG.2018.2794598","Keywords":"Sense of Agency;Priority Principle;Consistency Principle;Exclusivity Principle;Feeling of Agency;Judgment of Agency;EEG;Neurophysiological Marker;Pre-Motor Cortex;Right Posterior Parietal Cortex;Locus of Control","Title":"\u201cDo You Feel in Control?\u201d: Towards Novel Approaches to Characterise, Manipulate and Measure the Sense of Agency in Virtual Environments","Keywords_Processed":"EEG;Locus of Control;priority Principle;exclusivity Principle;right Posterior Parietal Cortex;Pre Motor Cortex;sense of Agency;feel of Agency;judgment of Agency;consistency Principle;neurophysiological marker","Keyword_Vector":[0.3112628291,-0.0323609718,0.1373768828,-0.1708026207,-0.0703264899,-0.1281024294,-0.1892591998,-0.0217788365,-0.1342164653,-0.1413020683,-0.0806464512,0.1683776456,0.011418807,0.0089807136,-0.1383334166,0.1133424372,0.0227166688,0.1364299548,0.1222311317,-0.0562878703,-0.0643071737,0.1566763025,0.1484614344,-0.0099716874,-0.2546288174,0.0844088419,0.014626209,0.0198264431,0.0231319618,-0.1699374275,-0.0796018555,-0.1520242644,0.0112381325,-0.018213394,0.0369792681,-0.0856678969,0.029509732,-0.0785979334,-0.0793730065,-0.1048924971,0.052381435,0.0225180806,0.1297191545,-0.0549499706,-0.1306734966,0.0142118395,-0.0001529817,0.0298081112,0.0250920338,0.1230164057,-0.1156626806,-0.0789990219],"Abstract_Vector":[0.2264664723,-0.0843174714,0.0654251057,-0.0192355716,-0.0645112119,0.0193097811,0.0383673852,0.0093928286,-0.007953103,0.0022055561,0.0064951383,0.0162609128,0.0706978079,0.0081759985,-0.0165343227,-0.0392519154,-0.0773672599,0.0099684256,0.0057293108,-0.0292750721,-0.0077134356,-0.0011345471,0.000340537,0.00721331,0.0203278787,0.018364387,-0.0016877488,0.0037982431,-0.0064467276,-0.0416677043,0.0109460258,-0.0121201191,-0.0010552781,-0.0033589021,0.0164834639,0.0186099806,-0.0176246588,0.0250724804,-0.022168473,0.017373017,-0.0135496364,-0.017709377,0.0224269857,-0.0596645581,-0.0190648705,-0.0139297988,0.0223468591,0.0178234677,0.0050892535,-0.0203893422,0.0039305884,-0.012403259,-0.0139596363,-0.0254767826,-0.012804213,-0.0106317196,-0.0041293367,0.0289473279,0.0004916711,0.0229043588,-0.0041442604,-0.0237836795,-0.0045221233,0.0325049936,-0.0007618071,0.0064227221,0.0235619108,-0.007091255,-0.0160161205,0.0040908938,-0.0357000568,0.007409106,0.0391044779,-0.0034189494,0.0020688702,-0.0158213975,-0.0044774446,0.0050698408,-0.005808285,-0.0091862366,-0.000011237,0.0035433618,-0.0151215416,0.0337064238,-0.0030970432,-0.0105750467,0.0245695108,-0.0064322877,-0.0124259979,-0.0381729342,0.0013787782,0.0185184165,0.0339704245,-0.0442049521,-0.0009415917,0.0303253623,0.045304068,-0.0268361023,0.0108044729,-0.0117215395,-0.0250712929,0.0312997748,-0.0044968323,-0.0138482763,-0.0018475218,0.0000751706,0.0395879038,0.0185720539,-0.0227268872,-0.0249671089,0.0302189603,0.0003357931,0.0277675548,-0.0179109063,0.0146403395,0.0110668335]},"135":{"Abstract":"Convolutional neural networks can successfully perform many computer vision tasks on images. For visualization, how do CNNs perform when applied to graphical perception tasks? We investigate this question by reproducing Cleveland and McGill's seminal 1984 experiments, which measured human perception efficiency of different visual encodings and defined elementary perceptual tasks for visualization. We measure the graphical perceptual capabilities of four network architectures on five different visualization tasks and compare to existing and new human performance baselines. While under limited circumstances CNNs are able to meet or outperform human task performance, we find that CNNs are not currently a good model for human graphical perception. We present the results of these experiments to foster the understanding of how CNNs succeed and fail when applied to data visualizations.","Authors":"D. Haehn; J. Tompkin; H. Pfister","DOI":"10.1109\/TVCG.2018.2865138","Keywords":"Machine Perception;Graphical Perception;Deep Learning;Convolutional Neural Networks","Title":"Evaluating \u2018Graphical Perception\u2019 with CNNs","Keywords_Processed":"Graphical Perception;Convolutional Neural network;deep Learning;Machine Perception","Keyword_Vector":[0.1270284052,-0.0519116178,-0.0143955083,-0.080495929,0.054537653,-0.033952561,0.0138890988,-0.0310076398,-0.1110742028,0.0410319877,-0.0838503516,0.1137772578,-0.0235062293,0.0664579477,-0.0639302557,0.0506706687,-0.0650404774,-0.015962031,-0.046615132,-0.0733877659,0.189568903,0.0619273023,-0.1174467251,-0.0162033518,-0.0815365203,0.0971520847,0.0471742398,0.055043158,0.019511561,-0.0551953052,-0.1016344354,-0.1120384967,-0.0275847763,-0.0003123336,-0.0100862149,-0.0742766735,0.0735888241,-0.0527160915,-0.0816223238,-0.0277786663,0.0736908095,0.0676510863,-0.0146787067,-0.0802900485,-0.0132384398,0.061641074,0.0799925143,-0.0860022621,0.0086442203,-0.030986694,-0.0138013125,-0.0798293857],"Abstract_Vector":[0.1572919794,-0.0816568317,0.000461328,0.0819432566,0.0280350884,0.0493887182,-0.0172493966,0.0120771328,0.0018188231,-0.0114239595,-0.0135882452,0.0705187502,0.0518538705,-0.0624029388,0.0552988687,0.0859006726,-0.0635217399,0.0069331326,-0.094170028,-0.0328414775,-0.0312705909,-0.0005627581,-0.0148636808,0.0174118291,-0.0371086899,-0.0478488531,-0.0252374425,0.0201436424,-0.0088830365,-0.0479247474,0.0085121115,-0.0349365602,0.0114725109,0.0145066435,-0.0079837412,0.0338762432,0.032143242,0.0467118787,-0.0112131257,-0.0206536347,0.0584593121,-0.0205434533,0.0204173219,-0.0080479204,0.0350466339,-0.0042727085,0.0280600659,-0.0295350141,-0.0074017948,0.0057897929,-0.00650973,0.0621080116,-0.010994742,0.0451335645,-0.0044733877,-0.0239443505,0.0083328185,0.0282237779,-0.0276890609,0.0099569276,0.0424315823,-0.0284004475,0.0216199911,0.0130583434,-0.0448640604,0.0251672849,0.0136173994,0.0022582176,0.0012330167,-0.0285041942,0.0273833695,0.025385536,0.0178763971,0.0047196039,0.0149846689,0.0118928815,0.0043641861,0.0100167675,0.0319018535,-0.0273147921,0.0070477754,-0.0206373421,0.0133913507,-0.0029184965,-0.0045755844,-0.0356470808,0.0271055027,0.0035610751,-0.038426287,-0.0027839787,-0.022422239,0.0017411332,0.0082867766,0.0041219487,-0.0299755009,-0.018612765,-0.0080247823,0.0032667051,0.031672083,0.0271268004,-0.0067081252,0.0296547269,0.0309063952,0.0115922194,0.0105330981,-0.0142998298,0.0203117665,-0.0223878254,-0.0125513084,0.0195866909,0.0185495466,0.0265206432,-0.0242954961,0.011248146,0.0110346464,0.0050353412]},"136":{"Abstract":"Manual editing of a metro map is essential because many aesthetic and readability demands in map generation cannot be achieved by using a fully automatic method. In addition, a metro map should be updated when new metro lines are developed in a city. Considering that manually designing a metro map is time-consuming and requires expert skills, we present an interactive editing system that considers human knowledge and adjusts the layout to make it consistent with user expectations. In other words, only a few stations are controlled and the remaining stations are relocated by our system. Our system supports both curvilinear and octilinear layouts when creating metro maps. It solves an optimization problem, in which even spaces, route straightness, and maximum included angles at junctions are considered to obtain a curvilinear result. The system then rotates each edge to extend either vertically, horizontally, or diagonally while approximating the station positions provided by users to generate an octilinear layout. Experimental results, quantitative and qualitative evaluations, and user studies show that our editing system is easy to use and allows even non-professionals to design a metro map.","Authors":"Y. Wang; W. Peng","DOI":"10.1109\/TVCG.2015.2430290","Keywords":"Metro Map;Interactive editing;octilinear;least squares optimization;Metro map;interactive editing;octilinear;least squares optimization","Title":"Interactive Metro Map Editing","Keywords_Processed":"Metro map;interactive editing;octilinear;Metro Map;least square optimization","Keyword_Vector":[0.2074155188,-0.1045913198,-0.0279257723,0.1388130601,0.0177240955,0.2417430398,-0.0395416224,0.1550279187,-0.0479874457,-0.0643171576,-0.0836356203,0.0104162975,-0.0400364753,-0.0225279826,0.0022631555,-0.1739280936,-0.066262124,0.0578264014,0.0653292942,0.1550678659,0.0769754865,-0.0266829079,0.0121575971,-0.0224588656,-0.0014410135,-0.0068166727,0.0684356834,-0.0186479904,-0.0526029376,0.0050098635,-0.0688825113,-0.0624096324,0.0121136475,0.086550777,-0.0833384978,-0.0419070507,-0.0122119193,-0.0339888004,0.0116262709,0.0540852791,-0.0739547379,-0.0484495194,0.0515608997,-0.0498747405,0.0318435748,-0.0103258582,-0.0142392319,0.0618090638,-0.0715851431,0.0409637028,0.0022775101,0.0741443867],"Abstract_Vector":[0.2316056355,-0.0607115833,-0.0390225333,-0.0486112626,0.0279072622,-0.1080205829,-0.080151222,-0.0561437968,0.0680776754,-0.0378889485,0.033643071,-0.0710545558,-0.0137676911,-0.0240266159,-0.0010875005,0.0150542363,-0.0511330285,0.0176983107,0.0250123597,-0.0019237933,-0.010242516,0.0113396402,0.044538071,-0.0461135617,-0.0869025376,0.0026635947,0.0097514773,0.0859258616,-0.0063940083,0.0330731765,-0.0231285206,0.0252462953,-0.0419719708,-0.0117284158,0.0415133891,-0.0410745801,-0.0423129693,-0.0641557477,0.0250354765,0.0795332966,0.0449086791,0.0243334027,0.0489091821,0.0119296467,-0.012399826,0.0007229529,0.0041860027,0.0096199972,-0.0164329045,-0.0204289081,-0.0230638999,-0.0383761986,-0.0296353205,0.0108288829,-0.026446686,-0.0761940048,-0.0106751442,-0.0452178481,-0.0313334123,0.0036960897,-0.0201229139,-0.0091822044,-0.0346106719,0.1108651474,0.0649773927,-0.0670750749,0.1753916133,0.0614204936,0.002636191,0.0126849348,0.0454328338,-0.044482923,-0.0284442731,0.0465449275,-0.009331572,0.0027983795,-0.0306335312,-0.0774634715,-0.0448564857,-0.0583701054,-0.0052797239,0.0184876527,-0.0776559724,-0.0595690718,-0.0306726288,-0.0270855467,0.0187864928,0.0410925728,-0.1258727898,-0.1110152271,0.0442613949,-0.0046835691,-0.0011440907,-0.0861510721,0.0090991012,-0.0274086746,0.021826343,0.0060950614,-0.0064210318,-0.0072205385,0.0878793355,-0.0215701232,0.0319538491,-0.0425641215,0.0297590647,0.0858058487,-0.0076736423,-0.0052044217,0.004931977,0.0333211389,-0.0368981765,0.064682625,-0.0256928811,0.0121923744,0.0526544058,0.0096716208]},"137":{"Abstract":"We introduce a novel approach for flame volume reconstruction from videos using inexpensive charge-coupled device (CCD) consumer cameras. The approach includes an economical data capture technique using inexpensive CCD cameras. Leveraging the smear feature of the CCD chip, we present a technique for synchronizing CCD cameras while capturing flame videos from different views. Our reconstruction is based on the radiative transport equation which enables complex phenomena such as emission, extinction, and scattering to be used in the rendering process. Both the color intensity and temperature reconstructions are implemented using the CUDA parallel computing framework, which provides real-time performance and allows visualization of reconstruction results after every iteration. We present the results of our approach using real captured data and physically-based simulated data. Finally, we also compare our approach against the other state-of-the-art flame volume reconstruction methods and demonstrate the efficacy and efficiency of our approach in four different applications: (1) rendering of reconstructed flames in virtual environments, (2) rendering of reconstructed flames in augmented reality, (3) flame stylization, and (4) reconstruction of other semitransparent phenomena.","Authors":"L. Shen; D. Zhu; S. Nadeem; Z. Wang; A. E. Kaufman","DOI":"10.1109\/TVCG.2017.2712688","Keywords":"Flame volume reconstruction;flame rendering;flame videos;radiative transport equation;CCD camera synchronization","Title":"Radiative Transport Based Flame Volume Reconstruction from Videos","Keywords_Processed":"flame video;flame render;flame volume reconstruction;ccd camera synchronization;radiative transport equation","Keyword_Vector":[0.0218917752,0.012856353,-0.0129322129,0.0002806254,0.0071170382,0.0035442107,-0.0006966835,0.0051389799,-0.0061919679,0.0057999535,0.0014749017,0.0073975919,0.0128690059,0.0187350956,-0.0025682061,0.0056818801,-0.0257882532,-0.0539623685,0.0333433277,-0.0243949655,0.0144739226,0.0062889589,-0.024905177,0.014143945,-0.0006873853,0.0158343852,0.0387547783,0.0123654437,0.0145881314,0.0201240562,0.0026311277,-0.0130595817,-0.0274280532,0.0180873313,-0.0126653016,-0.0181224258,-0.0034388566,-0.0369108024,0.0016536874,-0.0238717667,-0.0130402618,0.0600253604,0.0326663938,-0.0002267197,-0.0210385822,0.0146738439,0.0136656141,0.01600226,-0.0063109692,0.0012685564,0.0461494586,-0.0348833757],"Abstract_Vector":[0.1554036297,0.0671825939,-0.0743755163,-0.0498775572,-0.0276160865,0.1240749408,0.0169025913,0.0343918147,0.1123361738,-0.0113427212,-0.0458518819,0.0150735463,-0.0823821675,-0.0281688902,0.0009102107,-0.0134423178,0.0185432986,0.0296892963,0.0156349065,-0.0199861253,-0.011542674,-0.0208730501,-0.0197261762,-0.0550901372,0.0035213614,-0.0100155307,0.0364495477,0.0125982425,-0.0240630475,0.0225187562,0.033120073,-0.0121860457,-0.035604539,-0.0475330961,-0.0172965757,0.0382878237,-0.014927135,0.0065023886,0.00888312,0.0397544616,0.0933136464,0.063162974,-0.0025223654,-0.0028881894,-0.0095654759,0.0144030194,-0.0165617716,0.0026117914,-0.0285099629,-0.0051821754,0.0309852886,-0.0110708698,-0.0108644089,0.0263121956,-0.0103397833,0.0104221328,-0.0146160566,-0.0196490108,-0.0174180088,-0.0409046779,-0.0488049274,-0.031836496,0.0204210001,-0.0647497124,-0.0020526661,0.0178030984,-0.0365343627,0.0318856068,-0.0562732831,-0.0493648955,0.0781175113,0.0083851695,-0.0253056235,-0.0012734624,0.0309596943,-0.0197238471,0.011078708,-0.0203554208,-0.0149249127,-0.0331383537,0.0156878433,-0.0027051961,0.0471293311,-0.0020346925,-0.0240099052,0.0038861242,-0.04165278,0.0180928407,-0.0051329932,0.047864545,0.0012205014,0.0408272587,-0.0061931777,0.0222561055,0.0175615215,-0.0147987321,0.0435762676,-0.0425530743,-0.0081680303,0.0421985459,-0.0250834518,0.0220735211,-0.0502348781,-0.0476950765,0.0215123431,-0.0050072415,0.0073891172,0.0312603631,-0.0708971602,0.0244058774,-0.0320356131,0.0076870742,-0.0119653913,-0.0022384854,0.0501602402,-0.0091660626]},"138":{"Abstract":"Path tracing provides photo-realistic rendering in many applications but intermediate previsualization often suffers from distracting noise. Since the fundamental underlying problem is insufficient samples, we exploit the coherence of the visual signal to reconstruct missing samples, using a low-rank matrix completion framework. We present novel methods to construct low rank matrices for incomplete images including missing pixel, missing sub-pixel, and multi-frame scenarios. A convolutional neural network provides fast pre-completion for initialising missing values, and subsequent weighted nuclear norm minimisation (WNNM) with a parameter adjustment strategy (PAWNNM) efficiently recovers missing values even in high frequency details. The result shows better visual quality than recent methods including compressed sensing based reconstruction.","Authors":"P. Liu; J. Lewis; T. Rhee","DOI":"10.1109\/TVCG.2017.2722414","Keywords":"Previsualisation;path-tracing;sampling and reconstruction;matrix completion;nuclear norm minimization;convolutional neural network","Title":"Low-Rank Matrix Completion to Reconstruct Incomplete Rendering Images","Keywords_Processed":"previsualisation;path tracing;nuclear norm minimization;convolutional neural network;matrix completion;sampling and reconstruction","Keyword_Vector":[0.2583179659,-0.1713864144,-0.1058424932,0.2537762901,0.0945860527,-0.0236107268,0.0286303441,0.2635255619,0.0162361739,-0.0540314829,-0.1178719676,-0.0357834213,-0.0679385907,-0.1123917809,0.0971818396,-0.0383735079,0.0025911865,0.0835836298,-0.0453634677,0.0138990257,-0.0046892299,0.0135965037,0.0547502604,-0.0315412318,-0.0471018143,-0.0439284873,0.0126000506,-0.0533544612,-0.0168358898,-0.0069957272,-0.004445801,0.0214283201,0.0341372578,-0.1019771848,0.0191763918,0.0358821061,-0.0271733467,-0.0404011076,-0.0011747502,-0.0536080649,-0.0307900193,0.0646945558,-0.0816961902,-0.0043807323,0.0418067023,0.0257309999,-0.0373456329,-0.0216329191,-0.0174185787,-0.0204825513,-0.0205437449,-0.0829488855],"Abstract_Vector":[0.2257182932,-0.1247456621,0.0065367993,-0.0013169767,-0.0275841834,-0.0412752908,-0.0292716359,-0.0068761203,-0.008708061,-0.0058568443,-0.0491968799,0.0083287551,-0.082451168,-0.0512035645,0.0068388173,0.0470778695,0.0062754333,-0.0415749344,0.0185943124,0.0303529614,0.0197059005,-0.0621167405,-0.0220640016,0.0040688882,0.0021382092,-0.1033352354,-0.0233941336,-0.0508175683,0.0276292115,0.0472449185,0.0014103677,0.0004209896,-0.002966082,0.0469312284,-0.0266600209,-0.0264783674,-0.0099594105,0.0231284259,0.0301559081,0.0016214274,-0.0441918219,0.0764143538,-0.0571959112,-0.0174551341,-0.0181737327,0.0047018554,0.0249479268,0.0101338484,0.0362113972,0.049802377,0.0312967861,0.0264600324,-0.0117437031,0.0076766042,0.020761766,-0.005969092,0.0049346567,0.0090925041,0.0261592875,-0.0180486116,-0.0724529349,0.0153031897,-0.006130484,-0.0066931067,-0.0282717547,-0.0140792715,0.0008082627,0.0000404635,0.0225140939,0.0085627473,0.0220905115,-0.0021448101,-0.0350989255,0.0112271143,0.006556788,0.014388103,-0.0110044781,0.0110586074,-0.0098153258,-0.0089883243,0.0309255852,-0.0125687959,-0.0012535811,0.0695416259,0.0213061316,-0.0468953051,0.04944532,0.0006394696,0.0312206438,-0.0281653423,0.0660440668,-0.0037209277,-0.0111727995,0.0195925898,-0.0514765012,0.0103645392,-0.0152131444,0.052581107,-0.0040674784,-0.0024844326,0.0598677282,-0.0090150108,0.0275886212,-0.0241518811,-0.0005757584,-0.0194139987,0.0288621548,0.0018547302,0.0235568232,0.002045926,-0.0539927286,-0.0144737114,-0.0318418649,0.0028173228,0.0068558352,0.0119922826]},"139":{"Abstract":"While nothing can be more vivid, immediate and real than our own sensorial experiences, emerging virtual reality technologies are playing with the possibility of being able to share someone else's sensory reality. The Painter Project is a virtual environment where users see a video from a painter's point of view in tandem with a tracked rendering of their own hand while they paint on a physical canvas. The end result is an experiment in superimposition of one experiential reality on top of another, hopefully opening a new window into an artist's creative process. This explorative study tested this virtual environment on stimulating empathy and creativity. The findings indicate potential for this technology as a new expert-novice mentorship simulation.","Authors":"L. J. Gerry","DOI":"10.1109\/TVCG.2017.2657239","Keywords":"Embodied simulations;virtual environments;mixed reality;creativity;empathy;painting","Title":"Paint with Me: Stimulating Creativity and Empathy While Painting with a Painter in Virtual Reality","Keywords_Processed":"empathy;painting;mixed reality;virtual environment;creativity;embody simulation","Keyword_Vector":[0.2092769085,-0.1460539896,-0.0309695919,-0.1032536197,0.0558495213,-0.0014881411,-0.1078015699,0.0154506442,0.0272876408,0.1490089847,-0.0210255069,0.0594381807,0.0328291094,0.1242026024,-0.0601187553,-0.0348830433,0.0674878173,-0.1804698218,0.0515857417,0.3053126659,0.1144596599,0.0989240876,-0.0783160967,-0.1318229743,-0.0241395,0.1104320502,0.0408534503,0.0153632709,0.0046468465,-0.1033535059,-0.0204618015,-0.0192464432,-0.047049834,-0.0486057235,0.0023363135,-0.0422141019,0.0556333464,-0.004189292,0.0358243242,0.0576857675,0.1100575423,0.01645157,0.0312417487,-0.0178833102,0.0183376092,-0.0631052511,-0.0236438991,-0.0446420438,-0.0076002183,0.0697526229,-0.0665577722,0.070778514],"Abstract_Vector":[0.1335110329,-0.0957676493,0.0107180288,0.0174913636,-0.0113428455,0.0471518003,-0.0207532118,-0.0133597314,0.0045011596,-0.016100279,-0.0099593868,0.0213887571,0.0266571633,0.0026994662,0.0083142708,0.0140188946,-0.0362034225,-0.0098241903,0.0276695599,-0.039364712,0.0449423539,-0.0283069171,0.0027521007,-0.0468431211,0.0051191484,-0.0009968332,-0.0090036017,0.0078855114,-0.0208701026,0.0171987343,0.0017643514,-0.0160297507,0.0134259101,0.0237641354,-0.003113981,0.003764061,0.0088430355,-0.0141720331,-0.0009374767,-0.0063208899,-0.0076883028,-0.0073997458,-0.0112317932,-0.0117367918,-0.002810768,-0.0572570237,-0.0286488335,-0.0067126646,0.0020035458,-0.0214148931,-0.0432041933,0.0244030007,0.0223491862,0.0051487231,-0.0094881886,0.0227069745,-0.0415689949,-0.0235176502,0.0061685642,-0.0077874192,-0.0046957342,-0.0349235947,0.0029375284,0.0302791242,-0.0227923564,-0.0047703233,0.0277796023,-0.0020907463,-0.0172746013,-0.0130319951,0.0029390301,-0.0169123173,-0.003489721,0.0287398441,-0.0185729581,-0.0023467877,0.0038755001,0.0326487703,-0.0162105126,-0.0011116437,0.0128851882,-0.0448514301,0.012565168,0.0214664617,-0.0415277209,-0.0079560746,-0.007838531,-0.0240226171,0.0083675947,0.0487483843,-0.048532369,0.0505751645,-0.0043190405,0.0195656524,-0.0646865154,-0.0179874073,0.0131100918,-0.0001883403,-0.0131480068,-0.0447018049,-0.0357712959,-0.0362696739,0.0085631177,-0.0072984667,-0.029248416,0.0033786465,-0.0229950618,0.0012646177,-0.0039869288,-0.0009190181,-0.0039922223,-0.0375278958,-0.0229847491,-0.0390438304,-0.0525688681,0.0456251659]},"14":{"Abstract":"Visual analytics systems continue to integrate new technologies and leverage modern environments for exploration and collaboration, making tools and techniques available to a wide audience through web browsers. Many of these systems have been developed with rich interactions, offering users the opportunity to examine details and explore hypotheses that have not been directly encoded by a designer. Understanding is enhanced when users can replay and revisit the steps in the sensemaking process, and in collaborative settings, it is especially important to be able to review not only the current state but also what decisions were made along the way. Unfortunately, many web-based systems lack the ability to capture such reasoning, and the path to a result is transient, forgotten when a user moves to a new view. This paper explores the requirements to augment existing client-side web applications with support for capturing, reviewing, sharing, and reusing steps in the reasoning process. Furthermore, it considers situations where decisions are made with streaming data, and the insights gained from revisiting those choices when more data is available. It presents a proof of concept, the Shareable Interactive Manipulation Provenance framework (SIMProv.js), that addresses these requirements in a modern, client-side JavaScript library, and describes how it can be integrated with existing frameworks.","Authors":"A. Camisetty; C. Chandurkar; M. Sun; D. Koop","DOI":"10.1109\/TVCG.2018.2865039","Keywords":"Collaboration;provenance;streaming data;history;web","Title":"Enhancing Web-based Analytics Applications through Provenance","Keywords_Processed":"collaboration;provenance;web;history;stream datum","Keyword_Vector":[0.0148145049,0.0135804171,-0.012734109,-0.0097983143,0.0014399815,-0.0088534847,-0.0142368983,-0.0009167094,-0.0424221289,-0.0042953315,0.0196537215,0.024971166,0.000690057,0.011981477,0.022083912,0.0317542486,0.0305762494,0.03013422,0.0834479365,-0.0079495236,-0.0489501138,0.0388677052,0.0568976628,0.0322322759,-0.0408674369,-0.0135110327,-0.0061977348,-0.01658918,0.0439513887,-0.061468176,0.0061770824,-0.0473637193,0.0275845002,0.0207558225,-0.0086676134,-0.0040804097,-0.010778515,0.0023431118,-0.0133872291,-0.0253737227,-0.0139866584,0.019540704,0.0119938223,0.0037017327,-0.0066384946,0.0065609475,0.0140580848,0.033458647,-0.0057980965,0.0014172994,-0.015163855,0.0158854656],"Abstract_Vector":[0.1755256787,-0.0587214327,-0.041407383,-0.012383774,-0.0569184196,0.0853761335,0.0061516574,0.0298957413,0.0523218851,0.007939809,-0.0787255428,0.0013887656,-0.041329269,-0.0174502688,0.0196500535,0.0643669371,0.0217034227,0.0573040912,0.0682285525,0.0227612123,-0.0187997182,-0.072492867,0.0116135954,0.0198367589,0.0236062586,-0.0686855608,-0.0425598065,0.0132108883,-0.0373538636,-0.0169498353,-0.0113843755,0.0228601902,0.0221278074,-0.0455843496,0.0138882869,0.0020722763,-0.0041727038,-0.0235874654,-0.0292954022,-0.0412357342,-0.0104294658,0.0302006801,0.0496317068,0.0205324972,-0.0036067439,0.0056130715,-0.0555428067,-0.0226033011,-0.0223413996,-0.022854172,0.033113363,-0.0518489201,0.0206785585,-0.0201578765,0.0379598817,0.0157252452,-0.0021718355,-0.0405299364,-0.0253892241,-0.0324973884,-0.0041269898,-0.0304040533,-0.009714464,0.0296488382,-0.0020888732,-0.0523663795,0.0409519537,-0.0474167621,-0.0680703942,0.0088173349,0.0073373421,0.0073777932,-0.0134534273,0.055521103,-0.0576826862,-0.0006021565,0.0257748917,-0.0207480815,0.0284706352,-0.0385305226,0.0031113563,0.0061201803,-0.0023413223,0.0647875639,0.0062824883,0.0361099997,-0.002500132,-0.0387857827,-0.0120868817,0.0160422966,-0.0447238591,0.0023665836,0.013418157,0.0014260884,-0.0009378698,0.0116206013,-0.0138971658,-0.0134042983,-0.0142160466,0.0000294994,0.0148815737,0.0767049308,-0.020061496,0.0104334472,0.0252758756,-0.0079032656,0.0427104043,0.0220531252,0.0110964358,-0.0184270077,0.0119923008,0.0233849583,0.0205263989,-0.0210016507,0.0068691919,0.0331281034]},"140":{"Abstract":"Information visualization has traditionally limited itself to 2D representations, primarily due to the prevalence of 2D displays and report formats. However, there has been a recent surge in popularity of consumer grade 3D displays and immersive head-mounted displays (HMDs). The ubiquity of such displays enables the possibility of immersive, stereoscopic visualization environments. While techniques that utilize such immersive environments have been explored extensively for spatial and scientific visualizations, contrastingly very little has been explored for information visualization. In this paper, we present our considerations of layout, rendering, and interaction methods for visualizing graphs in an immersive environment. We conducted a user study to evaluate our techniques compared to traditional 2D graph visualization. The results show that participants answered significantly faster with a fewer number of interactions using our techniques, especially for more difficult tasks. While the overall correctness rates are not significantly different, we found that participants gave significantly more correct answers using our techniques for larger graphs.","Authors":"O. Kwon; C. Muelder; K. Lee; K. Ma","DOI":"10.1109\/TVCG.2016.2520921","Keywords":"Graph visualization;virtual reality;immersive environments;head-mounted display;Graph visualization;virtual reality;immersive environments;head-mounted display","Title":"A Study of Layout, Rendering, and Interaction Methods for Immersive Graph Visualization","Keywords_Processed":"graph visualization;virtual reality;head mount display;immersive environment","Keyword_Vector":[0.1851651213,0.0020078034,0.260409181,-0.0199337795,-0.0551038724,-0.0440379251,0.1742521081,-0.0415076151,0.0066099392,-0.0292979841,-0.0386817928,0.0294745251,0.076323227,-0.005389482,-0.0126999352,0.0034610241,0.0038614892,-0.0647753719,0.0258030141,-0.0440026392,0.0751822578,0.0083767255,0.0172603413,-0.0195556462,-0.050438637,-0.029996865,-0.0434518296,-0.0389836513,-0.0023436325,0.004631633,-0.0012934431,-0.0370192227,0.0517617467,-0.0477823453,-0.0033920439,-0.0122566439,0.0075332123,0.0249649467,-0.0493806543,0.0145658208,-0.0393914754,-0.0057557337,0.0748545209,0.0370215598,0.0154792991,0.0562657607,-0.0163332589,-0.0759181188,-0.0285608138,0.0129720514,0.0679456946,-0.0588777327],"Abstract_Vector":[0.2824197266,-0.0331918031,0.2257454704,-0.0182605123,0.2033436826,0.120803518,0.088305932,-0.1153615553,-0.044177915,-0.0168490285,-0.071559277,-0.0253787334,-0.0045356687,-0.0275151227,-0.1080013646,0.06682379,0.1317345745,-0.0436520054,0.0205622896,0.0696778074,-0.0004797355,-0.0744661336,-0.0072958262,-0.0822303077,-0.0376208747,-0.0562708175,0.0640365853,-0.0881724834,-0.0949949836,0.028635106,0.1054564471,0.0179023246,-0.0122088241,-0.0013967913,0.027750707,-0.0384802821,0.0498925139,0.0717677728,-0.0858660668,0.0355324639,-0.0006194454,-0.0669722562,-0.0012940555,0.0082051964,0.0468612069,-0.0320854099,0.0039094346,0.0066797605,-0.0315801857,0.0373027299,-0.0054338391,-0.009688664,-0.0373375445,0.0286995587,-0.018454743,-0.0834598767,-0.0334096067,0.0221408077,0.0247408671,-0.003520134,0.0652571982,0.0230589854,0.0067048327,-0.038756749,0.054770016,-0.0054751897,0.0324701727,-0.0068013023,-0.0187719361,0.0018216429,0.0105163506,-0.0280183903,-0.0667907114,0.0090978083,-0.0000471224,-0.0048447137,0.0123877302,-0.0343484558,0.012801387,0.0078515738,-0.0283016224,0.0236607191,-0.0145372636,0.0458516679,0.0223572915,-0.0158934328,-0.0420281482,0.0428633716,-0.0337285674,-0.0336948576,-0.0116080371,0.0515436874,-0.0075019106,0.013680665,-0.0238620969,0.0323004036,0.0098854566,0.0024165057,0.0242853655,-0.0025103371,0.0081698053,-0.0293020226,0.0110176786,0.0296095937,0.0406577947,0.0206235314,-0.0012913902,0.0316154481,-0.0106099066,0.0160282188,0.0085458829,-0.0367483075,-0.0052285852,0.0064547293,0.0153631967,0.0109884792]},"141":{"Abstract":"Conventional dot plots use a constant dot size and are typically applied to show the frequency distribution of small data sets. Unfortunately, they are not designed for a high dynamic range of frequencies. We address this problem by introducing nonlinear dot plots. Adopting the idea of nonlinear scaling from logarithmic bar charts, our plots allow for dots of varying size so that columns with a large number of samples are reduced in height. For the construction of these diagrams, we introduce an efficient two-way sweep algorithm that leads to a dense and symmetrical layout. We compensate aliasing artifacts at high dot densities by a specifically designed low-pass filtering method. Examples of nonlinear dot plots are compared to conventional dot plots as well as linear and logarithmic histograms. Finally, we include feedback from an expert review.","Authors":"N. Rodrigues; D. Weiskopf","DOI":"10.1109\/TVCG.2017.2744018","Keywords":"Nonlinear dot plot;statistical graphics;sweep algorithm;layout","Title":"Nonlinear Dot Plots","Keywords_Processed":"sweep algorithm;statistical graphic;nonlinear dot plot;layout","Keyword_Vector":[0.1121265508,-0.0326466994,0.0125954039,-0.0511563501,0.0306534796,-0.0196549522,-0.024262748,0.0047616553,-0.0119752819,-0.0459040302,-0.0093267021,0.0069639378,-0.032359445,-0.0190195196,-0.0112431113,-0.0012654923,-0.0799371231,0.0223841085,0.0012755881,-0.0108429312,-0.0145899036,-0.021064373,-0.0039419585,0.00387796,-0.010111088,0.0165004012,0.0295504942,-0.0007091141,-0.0454207016,0.0823061063,0.0480598488,-0.0336760744,0.0084064029,0.0409268281,-0.0183121459,0.060294696,-0.0762858927,-0.0086363441,0.0676114265,-0.0786479038,-0.0440031199,0.0370071627,-0.0134892048,-0.0480786377,0.0610133101,-0.0149333048,0.1087441495,-0.0207288617,-0.0910492162,0.0079026941,0.0252112865,0.0636059973],"Abstract_Vector":[0.2558033157,-0.1447751715,-0.0026293597,-0.0455744396,-0.0457536921,-0.0425512308,-0.0115742872,-0.0173207714,-0.0282598362,-0.0390490763,0.0089313805,0.0316392021,0.0285881527,0.0048842791,-0.0110211825,-0.0434677958,-0.0421036021,-0.0330696397,-0.0277609264,-0.0363357263,-0.022746274,-0.0038845571,-0.0710114697,-0.0500551825,-0.0506498965,0.0170008426,0.0104718312,-0.0566745177,-0.0121387227,-0.0242067223,-0.0005974157,0.0182080458,0.0251373592,-0.0245434581,-0.0267270615,0.011626695,0.0505893744,0.0236836366,0.0590477423,0.0079465732,0.0165051296,-0.0423590308,0.0257347979,0.0014552685,-0.0479657274,0.0359528081,0.0193373977,-0.0503024958,0.0025805205,0.0002206574,0.0207494777,-0.0055589951,0.001924396,-0.0401803957,-0.0217053888,-0.0391806335,-0.0069191697,-0.0120490133,0.0267881584,-0.0454794668,0.0089725097,-0.0005736689,-0.0348823629,-0.0351382806,0.049155343,-0.0016501426,0.002523721,-0.0061834993,-0.0120301954,0.0057979255,0.0138277871,-0.0265808478,-0.0336873813,-0.0217839088,-0.001181301,-0.0140998959,0.0296738385,-0.0071670727,-0.0155569795,0.0101026583,0.0030657735,-0.0095006444,0.0107861328,-0.0251225034,0.0120074415,-0.0074189939,0.0407441779,0.0113277226,-0.0102946809,-0.0182832377,0.0035074023,-0.0358824977,0.0158517239,-0.0231045972,0.0225509115,-0.0317933272,-0.0567750925,-0.0014288381,0.0014662975,-0.0161310143,-0.0255530373,0.0054005096,0.0168038751,0.0013885384,-0.0496934857,-0.0041665171,-0.0182939438,0.0138939254,-0.0284203413,-0.0014176585,-0.0100863588,-0.0241955974,-0.0013541102,0.0015606427,-0.0016343828,0.0291600769]},"142":{"Abstract":"In Computer-Aided Design (CAD), Non-Uniform Rational B-Splines (NURBS) are a common model representation for export, simulation and visualization. In this paper, we present a direct rendering method for trimmed NURBS models based on their parametric description. Our approach builds on a novel trimming method and a three-pass pipeline which both allow for a sub-pixel precise visualization. The rendering pipeline bypasses tessellation limitations of current hardware using a feedback mechanism. In contrast to existing work, our trimming method scales well with a large number of trim curves and estimates the trimmed surface's footprint in screen-space which allows for an anti-aliasing with minimal performance overhead. Fragments with trimmed edges are routed into a designated off-screen buffer for subsequent blending with background faces. The evaluation of the presented algorithms shows that our rendering system can handle CAD models with ten thousands of trimmed NURBS surfaces. The suggested two-level data structure used for trimming outperforms state-of-the-art methods while being more precise and memory efficient. Our curve coverage estimation used for anti-aliasing provides an efficient trade-off between quality and performance compared to multisampling or screen-space anti-aliasing approaches.","Authors":"A. Schollmeyer; B. Froehlich","DOI":"10.1109\/TVCG.2018.2814987","Keywords":"Trimming;NURBS;anti-aliasing;adaptive tessellation","Title":"Efficient and Anti-Aliased Trimming for Rendering Large NURBS Models","Keywords_Processed":"trim;anti aliasing;adaptive tessellation;NURBS","Keyword_Vector":[0.2008160469,-0.0508442039,-0.0363368428,0.279515182,0.1709285665,0.0942056628,-0.0333343248,0.0620063386,-0.0492039048,-0.0011274972,0.0138111128,-0.0729179595,0.0925977659,0.1113754627,-0.0813847189,-0.0028780991,0.0018998412,0.0663842579,-0.0186395304,-0.0990503017,-0.0450955343,-0.0579949394,-0.0148512532,-0.0032866151,-0.0139792166,-0.0126971018,-0.0187391514,-0.0706857241,-0.1060430335,0.0791526552,-0.0199699551,-0.0709978156,0.0519420704,0.0119253463,0.0710988243,-0.0778193737,0.09372003,0.1135857125,0.0459518773,-0.0779942841,-0.0014517138,0.0963307535,0.035340191,0.0634712161,0.1123324117,-0.0159382458,-0.0979811296,-0.0139128961,0.0427812434,-0.032163192,-0.1089256902,-0.0384174959],"Abstract_Vector":[0.2403358278,-0.0296265622,-0.040021849,0.0018476015,-0.0217881982,-0.0822146482,0.0381106487,0.0448292925,0.0070944656,-0.0598783603,0.0271908213,-0.0320929294,0.0043166665,-0.0108561499,0.0361126472,-0.0509369726,-0.0168460815,-0.0191333969,0.0039065059,0.0370536627,-0.0271953376,0.0287793398,0.0004628346,0.021733805,-0.0463191181,0.0086715984,0.0083860166,-0.051753004,-0.0304238584,-0.0577467806,0.0576894773,0.005173077,-0.0298137509,0.0132159611,-0.0018327538,-0.0417781516,0.020326265,-0.0197882653,0.0095207548,-0.0282273329,-0.0260315052,0.0702418417,0.0143204876,0.0313963881,-0.0184020687,0.0491144201,0.0059732388,0.0245820657,-0.0192734479,0.0479275207,-0.0071593596,0.0015466961,-0.0281297318,-0.0036288707,-0.0359752635,-0.0004258911,-0.0332838723,-0.0029564189,0.0208566622,-0.024872352,0.0297753467,0.0033375805,0.0129269401,0.0230515973,-0.0017788542,-0.0297670398,-0.0057770878,0.0510085893,-0.0257348384,0.0421791154,-0.0399607175,-0.0207532918,-0.0219494663,-0.0026541472,0.0070639939,-0.010061866,-0.0256384959,-0.0063075092,-0.0417861784,-0.0198593538,0.0137792263,-0.0563057791,0.0370242722,-0.0824007836,0.0157330081,-0.0352363066,0.0490313594,0.0100375821,0.0071668608,0.0701111124,0.0428930522,0.0004233688,0.0230469456,0.0561713535,-0.0444390331,-0.0087606358,0.0577653522,0.0132690496,0.0084609596,-0.0090761317,-0.0057193448,-0.01461936,0.0078184984,0.0383346551,-0.0327025403,-0.0315336741,-0.0324621558,0.0104320357,-0.0049255615,0.0221842716,0.0151847723,0.02467353,-0.0062073167,-0.0272014289,0.0094210221,0.016733394]},"143":{"Abstract":"Dimensionality reduction is commonly applied to multidimensional data to reduce the complexity of their analysis. In visual analysis systems, projections embed multidimensional data into 2D or 3D spaces for graphical representation. To facilitate a robust and accurate analysis, essential characteristics of the multidimensional data shall be preserved when projecting. Orthographic star coordinates is a state-of-the-art linear projection method that avoids distortion of multidimensional clusters by restricting interactive exploration to orthographic projections. However, existing numerical methods for computing orthographic star coordinates have a number of limitations when putting them into practice. We overcome these limitations by proposing the novel concept of shape-preserving star coordinates where shape preservation is assured using a superset of orthographic projections. Our scheme is explicit, exact, simple, fast, parameter-free, and stable. To maintain a valid shape-preserving star-coordinates configuration during user interaction with one of the star-coordinates axes, we derive an algorithm that only requires us to modify the configuration of one additional compensatory axis. Different design goals can be targeted by using different strategies for selecting the compensatory axis. We propose and discuss four strategies including a strategy that approximates orthographic star coordinates very well and a data-driven strategy. We further present shape-preserving morphing strategies between two shape-preserving configurations, which can be adapted for the generation of data tours. We apply our concept to multiple data analysis scenarios to document its applicability and validate its desired properties.","Authors":"V. Molchanov; L. Linsen","DOI":"10.1109\/TVCG.2018.2865118","Keywords":"Star coordinates;multidimensional data projection;multivariate data visualization","Title":"Shape-preserving Star Coordinates","Keywords_Processed":"star coordinate;multidimensional datum projection;multivariate datum visualization","Keyword_Vector":[0.0785447812,-0.0016170939,0.05236132,-0.0109387685,-0.0002583066,-0.0035041265,-0.0028394271,-0.0496593342,-0.0399596469,-0.0217384326,0.030179732,-0.0118491686,-0.1113556805,-0.0190957558,-0.1096356645,-0.0003565315,-0.0311160215,0.0510800279,-0.0002710595,0.0602548184,0.0212701908,-0.048625684,0.0456356463,-0.0018682983,0.0958033576,-0.0444925198,0.0750247056,-0.0727014115,0.1023499874,0.1076651984,0.0383314341,-0.0554515919,0.0267501128,-0.0224828999,0.0059197354,-0.0516756624,-0.0539545544,0.0160646255,-0.0086384617,-0.0154713863,0.0698452788,0.0529528485,-0.0451539564,-0.0013994379,-0.0065116936,-0.0602416014,0.0031054898,0.0152974459,-0.0267270079,-0.0102279487,0.013555181,-0.0012480571],"Abstract_Vector":[0.1659130924,0.0423068944,0.0374090614,-0.0064529673,0.0143349702,-0.1177470477,-0.0339787731,-0.0214284526,-0.0624444131,-0.0855848411,0.0694952783,0.1128109839,-0.0561511227,-0.0554330184,0.0848480885,-0.0482766958,0.0634606454,0.0219265338,0.0421546693,0.0115402623,-0.0293608343,-0.0001841692,0.0269669337,0.0109417567,-0.0546008116,0.0108821693,-0.0354738259,-0.0037670387,-0.0380403585,0.0270171089,-0.0504011712,-0.0057484442,-0.0107717409,0.0021296776,0.0014703604,0.0008309645,-0.0255154282,0.0030027539,0.0124179496,-0.0210821421,0.0107021739,-0.019117445,-0.0456602177,0.0452520195,-0.0099476114,-0.0065396192,0.0361781575,-0.0088696065,-0.0071070701,0.0159137845,0.0060446467,0.0167867727,-0.0233295083,0.004226807,0.009601867,0.0447618875,-0.0020952281,0.0230736274,-0.0110998204,-0.0454710679,-0.0122763531,0.0099659067,-0.0011053998,0.0059136053,0.0193515314,-0.0039137776,-0.0107138342,-0.0121235795,-0.0301453908,0.0395186776,0.0317850957,-0.0112826653,0.012164181,0.0234334178,-0.0162897681,0.0227658443,-0.0072093489,0.0331093536,0.0098813839,0.0104580226,-0.010231868,0.0060640153,0.0223520459,0.0230832316,-0.0054210537,-0.0465131429,0.003709883,-0.0112176159,-0.0118861598,0.0254553121,0.0018545207,0.0002566918,0.0077936113,-0.0125173192,-0.0033613533,-0.0112672268,0.0138215092,-0.0221479003,-0.0110015889,-0.0024450164,-0.0038491196,-0.0158685917,0.0019759009,0.0001822647,-0.0104090221,-0.0116410523,-0.0033712523,0.0036042427,0.0212515599,-0.0349598076,-0.017970143,0.0004310234,0.0295643343,0.0002548713,-0.011780981,-0.0151559069]},"144":{"Abstract":"Meteorologists process and analyze weather forecasts using visualization in order to examine the behaviors of and relationships among weather features. In this design study conducted with meteorologists in decision support roles, we identified and attempted to address two significant common challenges in weather visualization: the employment of inconsistent and often ineffective visual encoding practices across a wide range of visualizations, and a lack of support for directly visualizing how different weather features relate across an ensemble of possible forecast outcomes. In this work, we present a characterization of the problems and data associated with meteorological forecasting, we propose a set of informed default encoding choices that integrate existing meteorological conventions with effective visualization practice, and we extend a set of techniques as an initial step toward directly visualizing the interactions of multiple features over an ensemble forecast. We discuss the integration of these contributions into a functional prototype tool, and also reflect on the many practical challenges that arise when working with weather data.","Authors":"P. S. Quinan; M. Meyer","DOI":"10.1109\/TVCG.2015.2467754","Keywords":"Design study;weather;geographic\/geospatial visualization;ensemble data;Design study;weather;geographic\/geospatial visualization;ensemble data","Title":"Visually Comparing Weather Features in Forecasts","Keywords_Processed":"design study;weather;geographic geospatial visualization;ensemble datum","Keyword_Vector":[0.2252145402,-0.0889243047,-0.0259513924,-0.0237159906,0.002684332,0.003015442,-0.0642832627,-0.0457199019,-0.022407669,-0.0571346247,0.0425491262,-0.0181108563,-0.0175332637,0.1789536186,-0.0698766432,0.0878490776,-0.0679933151,0.0077212842,0.0605046277,-0.1078936112,0.0668953702,-0.0419161989,-0.1409968185,-0.1009268966,0.0060907823,0.0268625073,0.0407342893,0.0363835142,0.0709309126,-0.0379890371,-0.0215960648,0.1313181253,0.1100126549,0.1314528947,-0.0418879613,0.0946974907,-0.0120349601,0.1023538533,-0.0180233253,-0.0272950777,-0.0453837346,0.0588406536,-0.0316998685,0.0143890542,-0.0103770352,0.004085238,-0.0500502179,0.0351776576,-0.0198586845,0.0452509027,-0.002903164,-0.0697689431],"Abstract_Vector":[0.2484541869,-0.1277683995,-0.0352174564,-0.022250742,-0.0405977038,-0.0733736188,0.0447492616,0.0284012823,0.0285815513,0.0240241315,-0.0021286183,0.010948812,-0.030266706,0.0412532184,-0.0331065339,-0.0139972701,-0.0199690788,-0.0098457442,-0.0163156337,0.0268877178,-0.0085794331,-0.033333162,-0.087022583,0.0182845441,0.0073426854,-0.041762204,-0.0075121795,-0.0756638549,0.023638622,-0.0376158998,-0.0235295673,0.006396743,0.0099429736,-0.023442936,-0.0014700301,-0.0111730164,-0.0176706763,0.0175818634,-0.0028524664,0.0275492065,-0.0032381244,0.0393225002,0.0120432106,0.0399175147,-0.0075484287,0.034634805,-0.0315134081,0.0028020927,0.0160720607,0.0260952347,-0.0075090768,-0.0231956712,-0.0077827773,-0.0410856704,-0.0256471327,-0.0120549694,0.0107958768,0.0266882897,0.0068306038,-0.0213963443,0.0022511008,-0.0629169029,-0.0211211441,0.0318893909,0.0500690978,0.0277952338,0.0495897754,-0.0334345222,-0.0156767726,0.0352130447,0.0056435804,0.0117713332,-0.0011312748,-0.0348484245,0.0033336038,-0.0196563883,0.0042876945,-0.0104002508,0.0249789596,-0.0329862974,-0.0544014513,-0.0112050164,-0.0367797354,-0.0163523447,-0.0107407058,-0.0452886795,-0.0162028299,-0.0084074992,-0.0041515013,-0.0043129034,0.0160815772,-0.0476327631,0.0117148618,0.0401584731,0.0048695475,-0.0361047356,-0.0002462367,-0.0152892305,0.0133810025,0.0365002331,0.0261843768,0.033166441,0.0165947015,-0.0315187425,0.0152269874,-0.0126022531,0.0044657778,0.031476348,-0.0118401001,0.0368747091,-0.0055300361,-0.022363023,-0.0079654497,-0.0129364879,-0.040514222,-0.0139492996]},"145":{"Abstract":"We present a Cerebral Aneurysm Vortex Classification (CAVOCLA) that allows to classify blood flow in cerebral aneurysms. Medical studies assume a strong relation between the progression and rupture of aneurysms and flow patterns. To understand how flow patterns impact the vessel morphology, they are manually classified according to predefined classes. However, manual classifications are time-consuming and exhibit a high inter-observer variability. In contrast, our approach is more objective and faster than manual methods. The classification of integral lines, representing steady or unsteady blood flow, is based on a mapping of the aneurysm surface to a hemisphere by calculating polar-based coordinates. The lines are clustered and for each cluster a representative is calculated. Then, the polar-based coordinates are transformed to the representative as basis for the classification. Classes are based on the flow complexity. The classification results are presented by a detail-on-demand approach using a visual transition from the representative over an enclosing surface to the associated lines. Based on seven representative datasets, we conduct an informal interview with five domain experts to evaluate the system. They confirmed that CAVOCLA allows for a robust classification of intra-aneurysmal flow patterns. The detail-on-demand visualization enables an efficient exploration and interpretation of flow patterns.","Authors":"M. Meuschke; S. Oeltze-Jafra; O. Beuing; B. Preim; K. Lawonn","DOI":"10.1109\/TVCG.2018.2834923","Keywords":"Medical visualizations;aneurysms;blood flow;parametrization;classification","Title":"Classification of Blood Flow Patterns in Cerebral Aneurysms","Keywords_Processed":"medical visualization;aneurysm;classification;blood flow;parametrization","Keyword_Vector":[0.2151641238,-0.0645373203,0.2004870637,-0.1069081258,-0.0771059985,-0.1064170823,-0.0023812635,0.0270444983,-0.0172105352,0.2680092378,-0.0927406767,-0.1059220913,0.0776383307,0.0547147352,0.0318787309,-0.0131326044,-0.1113893257,0.0158297556,0.0187491076,-0.0176349756,0.0051335584,0.0118562596,-0.0573458856,0.0034469419,-0.0451153485,0.0687507015,0.0072102949,0.0737073455,0.0043608321,0.0129562019,0.0142357916,-0.0662301966,-0.0570041183,-0.0372611324,-0.1180545083,-0.0631884305,0.0629793088,-0.0630718231,-0.0531173939,-0.0361401483,0.0880847173,0.036084965,-0.0374541583,-0.0091927111,0.0256582593,0.0383925341,0.0152697456,-0.0250583014,-0.0005471718,0.0042631294,-0.0233821498,-0.0261639871],"Abstract_Vector":[0.1882386034,-0.0596330294,0.0109298494,0.0029192095,-0.0011474853,0.0213888532,-0.0004372518,-0.0185501549,0.0620621571,0.0264506315,-0.0122819412,0.0401356749,0.016798917,0.0276846925,-0.0333388274,-0.0292888544,0.0091400369,-0.054884259,0.0169144054,-0.0697371392,0.0042952517,-0.0439529187,0.0225874373,-0.0046612404,-0.0019233193,-0.0439912647,0.0104788529,0.0174835783,-0.0225003342,-0.0160632837,0.0053458159,-0.020719076,0.0052397132,-0.042561797,0.0035328276,0.0164642548,0.0168641485,-0.0520817529,0.0097220077,0.0365390855,0.0362869715,-0.0059826308,0.0503409753,0.0247148074,-0.0202945184,-0.0480067138,0.0058979023,0.0247875197,-0.0079642035,0.0219926526,-0.0501502034,-0.04158538,0.006361501,-0.0295755413,0.0093491974,0.0659112155,-0.0048682286,0.0441743427,-0.0002887682,-0.0579826335,0.0416524301,-0.0492147311,0.0744589856,-0.0681169985,0.0277228771,0.0242523129,0.03865828,-0.0139846005,-0.0422660459,0.0411402733,0.0374354736,0.0570764788,0.0190764376,-0.0205889505,0.0335984841,0.0265433994,0.0309836544,-0.0110297165,0.0092009561,-0.0218329223,-0.0133177521,0.0552642002,0.0102105227,-0.0053560025,0.0352935364,-0.010103583,-0.0238967023,0.0629628805,-0.0150637257,-0.0028892014,0.0136141931,-0.0094119421,-0.0320188785,-0.0138715359,-0.0373099748,0.0238832699,-0.0075699767,-0.0168677574,-0.0371376314,0.0538805903,-0.0011406582,-0.0513682814,-0.0066667304,-0.0547640141,-0.0374160161,-0.0220374149,0.079493954,0.0246895441,0.0498219288,0.0357782338,-0.0571256998,0.0893528538,-0.0090584717,0.0033869036,0.0693637715,0.0017757193]},"146":{"Abstract":"We extend the quadratic program (QP)-based task-space character control approach-initially intended for individual character animation-to multiple characters interacting among each other or with mobile\/articulated elements of the environment. The interactions between the characters can be either physical interactions, such as contacts that can be established or broken at will between them and for which the forces are subjected to Newton's third law, or behavioral interactions, such as collision avoidance and cooperation that naturally emerge to achieve collaborative tasks from high-level specifications. We take a systematic approach integrating all the equations of motions of the characters, objects, and articulated environment parts in a single QP formulation in order to embrace and solve the most general instance of the problem, where independent individual character controllers would fail to account for the inherent coupling of their respective motions through those physical and behavioral interactions. Various types of motions\/behaviors are controlled with only the one single formulation that we propose, and some examples of the original motions the framework allows are presented in the accompanying video.","Authors":"J. Vaillant; K. Bouyarmane; A. Kheddar","DOI":"10.1109\/TVCG.2016.2542067","Keywords":"I.3 Computer graphics;I.3.7 three-dimensional graphics and realism;I.3.7.a animation;I.6 simulation;modeling;and visualization;I.6.8 types of simulation;I.6.8.a animation","Title":"Multi-Character Physical and Behavioral Interactions Controller","Keywords_Processed":";and visualization;simulation;three dimensional graphic and realism;type of simulation;Computer graphic;animation;model","Keyword_Vector":[0.1181172123,-0.1180945744,-0.0990695541,0.0612997669,-0.0710741563,-0.1179740677,0.0556365652,0.033176698,0.0499233078,-0.0285276884,0.0065313849,-0.0056603948,-0.0346411878,-0.0797471226,0.0266539643,-0.000822749,-0.035664042,0.0224456145,-0.0041881542,0.0238663171,0.0088628531,0.0072631041,0.012435661,0.0070885328,-0.0009128699,0.0094084314,0.0404351479,-0.0076349379,0.0178121565,-0.0161916346,0.0292519996,0.0063995101,0.007712812,0.0107876945,-0.0048718987,-0.0160388792,0.0035737039,-0.0106164052,0.0085767744,-0.0098036116,-0.0025627336,-0.0145957947,0.0228968525,-0.0153906634,-0.0091020149,-0.0275839568,-0.0078406237,-0.0062168396,0.008932467,0.0056261666,0.0255582578,0.0055012731],"Abstract_Vector":[0.172001275,-0.0650282634,-0.002025453,0.0648027996,0.0015251479,0.0438247118,0.0049592709,0.0056593354,-0.0513173317,-0.0039416518,-0.0273618399,0.0805059434,0.0094770836,-0.0134750118,-0.0040876869,0.094638949,0.1294412879,0.0864168315,0.0583336821,-0.0133024025,0.0949974113,0.0605025533,0.0632897404,-0.0025681489,0.0872053594,0.0681047135,-0.0327519732,-0.0089794036,-0.0004981675,-0.0187726116,-0.0413395032,-0.0124955538,-0.065129814,0.0672751838,-0.0837596013,-0.0818423233,-0.0412008939,-0.1048143305,0.0714725414,0.0099171905,-0.0153196491,0.0037889722,0.0551308914,-0.0094430356,0.005624228,0.1105278377,-0.0203662022,-0.0215807473,-0.0443700665,-0.0361475039,0.0218343112,-0.0056485082,-0.0118886354,0.0118296781,-0.0154167198,0.0165411895,0.0301596524,0.047251287,0.0200727992,0.0020872567,0.0261363196,-0.0022143767,0.0235073549,-0.0346022668,-0.0037930293,0.0460923393,-0.0439876974,0.0261579256,-0.0086662918,0.0548986348,-0.0093582904,-0.0494204517,-0.0374736648,-0.0250232939,0.0180668145,-0.0045354742,-0.0363837867,-0.0077446813,-0.0294247093,-0.0082803614,0.0512233689,0.0206046222,-0.0246330733,0.0173701384,0.053289775,-0.0214608089,0.0453747993,0.0304032458,0.0330078473,-0.0105553395,0.0301589099,-0.0083278025,0.0544205637,-0.0122852443,0.024704839,-0.012434158,-0.0002195604,0.033562102,0.0130762065,0.0250195343,0.0114088972,-0.0249515667,0.047721828,-0.0244139663,-0.0047099266,-0.0193053797,-0.0470230728,0.0213155535,-0.0181325121,-0.012092,-0.0062538254,0.0090246303,-0.0107160946,0.0180085588,-0.0025109968,0.0237627882]},"147":{"Abstract":"Datasets commonly include multi-value (set-typed) attributes that describe set memberships over elements, such as genres per movie or courses taken per student. Set-typed attributes describe rich relations across elements, sets, and the set intersections. Increasing the number of sets results in a combinatorial growth of relations and creates scalability challenges. Exploratory tasks (e.g. selection, comparison) have commonly been designed in separation for set-typed attributes, which reduces interface consistency. To improve on scalability and to support rich, contextual exploration of set-typed data, we present AggreSet. AggreSet creates aggregations for each data dimension: sets, set-degrees, set-pair intersections, and other attributes. It visualizes the element count per aggregate using a matrix plot for set-pair intersections, and histograms for set lists, set-degrees and other attributes. Its non-overlapping visual design is scalable to numerous and large sets. AggreSet supports selection, filtering, and comparison as core exploratory tasks. It allows analysis of set relations inluding subsets, disjoint sets and set intersection strength, and also features perceptual set ordering for detecting patterns in set matrices. Its interaction is designed for rich and rapid data exploration. We demonstrate results on a wide range of datasets from different domains with varying characteristics, and report on expert reviews and a case study using student enrollment and degree data with assistant deans at a major public university.","Authors":"M. A. Yal\u00e7in; N. Elmqvist; B. B. Bederson","DOI":"10.1109\/TVCG.2015.2467051","Keywords":"Multi-valued attributes;sets;visualization;set visualization;data exploration;interaction;design;scalability;Multi-valued attributes;sets;visualization;set visualization;data exploration;interaction;design;scalability","Title":"AggreSet: Rich and Scalable Set Exploration using Visualizations of Element Aggregations","Keywords_Processed":"set visualization;scalability;interaction;design;datum exploration;visualization;set;Multi value attribute","Keyword_Vector":[0.1956420096,-0.1032959647,-0.011838042,-0.0145307682,0.00051895,0.1120652715,-0.1041732787,-0.0520013265,-0.0910187732,-0.0072152123,-0.047530815,0.0763837411,0.0020053675,-0.031587764,-0.0605515294,0.0119640756,-0.1970215047,-0.0017196389,-0.0358297016,-0.02437877,-0.0157220963,-0.1454035534,0.0726397161,-0.0916755132,-0.089853231,-0.0311691202,-0.0198620495,0.1170527848,-0.0152793357,0.0030048079,0.0954231129,0.1176619568,0.1189612662,0.1828266977,-0.0485267095,0.1278363101,-0.203585622,0.0095090308,-0.0168413575,-0.1004690768,0.0037238412,0.0736886941,-0.0922684313,0.0344487785,0.0299869127,-0.050415436,0.0110912049,0.0201819611,0.0006636557,-0.0024030542,0.013613575,0.0187736091],"Abstract_Vector":[0.1639869496,-0.0988588193,0.0036880214,0.0064587152,-0.0215805322,0.005939656,-0.0265654438,0.0177204131,0.0626412392,-0.0358565481,-0.0176656669,0.0992791263,0.0080251143,0.0102832098,0.0697530753,-0.0640609084,-0.0901566702,-0.036920884,-0.0082695288,-0.034253514,0.0282609085,-0.0574753669,-0.0168392266,-0.0287423247,-0.0315088059,0.0050005178,-0.025200366,-0.063233058,-0.0082836211,-0.0578748301,0.0681869511,-0.0573731287,0.0183463066,0.0178987717,-0.0597166216,-0.0499491761,0.0503987314,-0.0099882823,0.0070615057,-0.0286761118,0.0072253196,0.0965547778,-0.0483761504,-0.0100758549,-0.0207777347,-0.0185192485,0.0349874531,-0.0064967285,0.0089836537,-0.0104275468,-0.0001424092,-0.0100235183,0.0047618289,-0.0129383865,-0.010106348,-0.0397130412,-0.0200961754,0.0388981013,0.0366833144,-0.0034532617,-0.0349049715,0.0153973255,0.0057609553,0.0291057566,-0.0244653013,-0.0419348998,-0.0209895799,-0.0161628016,-0.0078794367,0.0222155128,0.0451982744,0.025318115,-0.0421527553,-0.0225004726,-0.0098850381,-0.0022317456,0.0020458578,-0.0181984954,-0.0346776017,-0.0237370345,-0.0332000627,0.02149547,-0.0001530151,0.02984759,0.0099062996,0.0039825349,0.0170550076,0.0086363876,-0.0120799904,-0.025200054,-0.0335455794,0.0195250689,0.0117073749,0.0029805894,0.0064034652,-0.0169440938,-0.0267403259,0.0152529064,0.0261609216,0.0367290492,0.003271128,-0.0396285909,0.0461179338,-0.0031859524,-0.0018611465,-0.0424579933,0.029972341,0.0110859454,-0.0245549433,-0.0001299782,0.0428913954,0.0450624223,-0.0107650056,-0.0042669871,-0.0192034525,-0.0017989106]},"148":{"Abstract":"We present the first approach to integrative structural modeling of the biological mesoscale within an interactive visual environment. These complex models can comprise up to millions of molecules with defined atomic structures, locations, and interactions. Their construction has previously been attempted only within a non-visual and non-interactive environment. Our solution unites the modeling and visualization aspect, enabling interactive construction of atomic resolution mesoscale models of large portions of a cell. We present a novel set of GPU algorithms that build the basis for the rapid construction of complex biological structures. These structures consist of multiple membrane-enclosed compartments including both soluble molecules and fibrous structures. The compartments are defined using volume voxelization of triangulated meshes. For membranes, we present an extension of the Wang Tile concept that populates the bilayer with individual lipids. Soluble molecules are populated within compartments distributed according to a Halton sequence. Fibrous structures, such as RNA or actin filaments, are created by self-avoiding random walks. Resulting overlaps of molecules are resolved by a forced-based system. Our approach opens new possibilities to the world of interactive construction of cellular compartments. We demonstrate its effectiveness by showcasing scenes of different scale and complexity that comprise blood plasma, mycoplasma, and HIV.","Authors":"T. Klein; L. Autin; B. Kozl\u00edkov\u00e1; D. S. Goodsell; A. Olson; M. E. Gr\u00f6ller; I. Viola","DOI":"10.1109\/TVCG.2017.2744258","Keywords":"Interactive modeling;population;biological data;interactive visualization","Title":"Instant Construction and Visualization of Crowded Biological Environments","Keywords_Processed":"interactive visualization;population;biological datum;interactive modeling","Keyword_Vector":[0.1594273817,-0.0429584643,0.0439767149,-0.0715733805,0.1248770173,0.0925937525,0.1298523795,0.0017236301,0.1966888135,0.1030199257,0.1581598695,0.182542345,-0.0057180306,0.0486990585,0.0201925613,0.0693064662,0.0492616849,0.0249150563,-0.0819353674,-0.0230196315,0.0134787142,-0.0155464015,0.0861196894,0.0814498005,-0.0747348531,-0.0129763809,-0.0237357577,-0.0035165544,-0.0487147323,0.0560212945,0.0078557146,0.0185523749,-0.0108136554,0.0194208796,0.0080932306,0.0107248022,-0.0178797635,0.030217245,-0.0461704628,-0.0106481513,0.009352328,0.0001558723,0.0344922121,-0.0397213565,0.0003575754,-0.1050126146,0.0263165599,-0.0051918387,-0.0150160951,0.0190874573,-0.0041148865,-0.0099364129],"Abstract_Vector":[0.217806692,-0.0644157347,0.0023756197,0.0138825643,0.0519575252,-0.1082969778,0.0535596585,-0.0482452256,0.1152627283,0.1294795856,-0.0029730608,0.0584902561,-0.0328478101,0.0888680521,-0.0029238981,-0.0475430311,0.0346212384,-0.0241229759,-0.0546358528,-0.0593072161,0.0376842709,-0.0330371713,-0.0399263652,0.050929181,0.0459887695,0.0507052612,0.0453380125,-0.0098222303,-0.0684150969,-0.0256467981,-0.0483980663,-0.0472163189,-0.0086445557,-0.02206354,-0.0438789346,-0.0023446365,-0.0255446864,-0.0218806757,-0.0424675688,0.0602603826,0.0015568461,-0.0003015759,-0.0419843612,-0.0000790962,0.0907453576,-0.0517848431,0.0552956689,-0.0160638683,0.028886017,-0.0176266082,0.0087004646,0.0352033585,0.0018322587,0.0500270616,-0.0067977647,0.0127020065,0.0047467465,-0.079765311,-0.0304161713,0.029273622,0.0139207191,0.0346976867,-0.0047506574,0.0262159992,-0.0226114045,-0.0350155514,-0.0040176768,-0.031007011,0.0643820414,-0.0151696241,-0.0018682269,0.0536999072,-0.0174805855,-0.0397314107,0.011113013,0.042599337,0.0083664574,-0.0001677621,0.0527432928,0.0234483317,0.007400621,-0.0212429391,0.0024018063,0.0038798886,-0.1044504933,0.0377747269,0.0286754265,-0.0077258647,-0.0097085896,-0.0613502006,-0.0140747168,0.0235070374,-0.0600759456,0.0064043154,0.0028918429,0.0065414027,0.0402789581,0.0340140076,0.0121927876,0.0212435256,0.0023180981,0.0102624757,-0.0768797378,0.0104337785,-0.0031720936,0.009348063,0.0029350712,0.0510645551,-0.014307198,-0.0371989098,0.0014648514,-0.0297842629,0.0100983475,-0.0104389601,-0.0111387904,0.0187748299]},"149":{"Abstract":"Five years after the first state-of-the-art report on Commercial Visual Analytics Systems we present a reevaluation of the Big Data Analytics field. We build on the success of the 2012 survey, which was influential even beyond the boundaries of the InfoVis and Visual Analytics (VA) community. While the field has matured significantly since the original survey, we find that innovation and research-driven development are increasingly sacrificed to satisfy a wide range of user groups. We evaluate new product versions on established evaluation criteria, such as available features, performance, and usability, to extend on and assure comparability with the previous survey. We also investigate previously unavailable products to paint a more complete picture of the commercial VA landscape. Furthermore, we introduce novel measures, like suitability for specific user groups and the ability to handle complex data types, and undertake a new case study to highlight innovative features. We explore the achievements in the commercial sector in addressing VA challenges and propose novel developments that should be on systems' roadmaps in the coming years.","Authors":"M. Behrisch; D. Streeb; F. Stoffel; D. Seebacher; B. Matejek; S. H. Weber; S. Mittelst\u00e4dt; H. Pfister; D. Keim","DOI":"10.1109\/TVCG.2018.2859973","Keywords":"System comparison;commercial landscape;visual analytics research;advances;development roadmap","Title":"Commercial Visual Analytics Systems\u2013Advances in the Big Data Analytics Field","Keywords_Processed":"system comparison;advance;commercial landscape;visual analytic research;development roadmap","Keyword_Vector":[0.0422887091,0.0285167161,-0.0284920165,0.0147465415,0.0164408576,-0.0335970462,0.0067101001,0.0156575039,-0.0274040288,-0.0213131638,0.0059727938,-0.0028503776,-0.0007023987,0.0397068304,0.0470431193,0.018615542,-0.0270060286,-0.1076577079,0.0748510675,-0.0524414871,0.0189436391,-0.0778914401,-0.0535832076,0.0659428484,0.028824255,-0.0737878416,0.2088270203,0.0542618299,-0.0648046122,0.0499550714,-0.0318439101,0.0317257594,-0.0144445488,-0.080401854,0.0354737831,0.0810700634,-0.08327053,-0.0650740541,-0.0523482823,-0.0155622511,0.0409007868,-0.0117606838,0.0304663772,0.0378523111,-0.0522081349,-0.0852853633,0.0426091779,-0.0031377688,0.0629280961,-0.0044274076,-0.0264333491,-0.0250288568],"Abstract_Vector":[0.1391165207,0.0208031507,0.0316945848,0.0143599739,-0.0067911087,0.0047563764,-0.0434699122,-0.0040032966,0.0244947584,-0.0052453377,-0.0194410613,0.0236154682,-0.0672267353,0.0237064146,-0.0092842792,0.0061949321,-0.0502006296,-0.0070785025,0.0274903951,-0.0088472054,-0.026746912,0.0281120608,-0.0148353099,0.0349799937,0.0307398043,0.0225611373,0.0276746873,0.0316535785,0.0340980335,-0.002325003,-0.003099228,0.0358173031,-0.0283619818,0.0120912832,-0.0373431155,-0.0018522367,-0.0280098256,-0.0033506718,-0.035511985,0.0191007981,0.0053854582,0.0158140735,-0.0234126066,-0.040811831,-0.0534327219,-0.0226331278,0.017935115,0.0253758189,-0.0124547073,-0.0264621042,-0.0389495175,-0.0169560991,0.0244194198,-0.015503748,-0.0155323702,0.0048742736,-0.0043782006,0.0026843402,-0.0053424999,-0.0061916166,-0.0030135143,-0.0267659365,0.0416367796,-0.0199068531,0.0159395696,0.0037907485,0.0066349981,-0.0148111053,-0.0016933106,-0.0097490819,-0.0181564977,0.0080523632,-0.0071949085,-0.0055231683,0.0147071314,0.0290812016,0.0064297276,0.0315021767,-0.0060872845,-0.0331977812,-0.0030371361,-0.0164347711,0.0003199687,-0.011533013,0.0109397065,0.014337035,-0.0094644992,-0.014357481,0.0059662384,-0.0079907791,0.0160947209,0.0319639802,-0.0107103967,0.0011896173,0.0029279548,-0.0382244485,-0.0301936742,0.0196708813,-0.0092639353,0.0319012091,0.0222023534,0.0174826358,-0.0047879434,0.0100282429,0.0182807071,-0.0212005953,0.0166744185,-0.0148357552,-0.0144094196,-0.0157131657,-0.0133081681,-0.0276389095,0.0175423312,-0.0101544201,0.0074591961,0.0095848041]},"15":{"Abstract":"This work introduces a tool for interactive exploration and visualization using MetaTracts. MetaTracts is a novel method for extraction and visualization of individual fiber bundles and weaving patterns from X-ray computed tomography (XCT) scans of endless carbon fiber reinforced polymers (CFRPs). It is designed specifically to handle XCT scans of low resolutions where the individual fibers are barely visible, which makes extraction of fiber bundles a challenging problem. The proposed workflow is used to analyze unit cells of CFRP materials integrating a recurring weaving pattern. First, a coarse version of integral curves is used to trace sections of the individual fiber bundles in the woven CFRP materials. We call these sections MetaTracts. In the second step, these extracted fiber bundle sections are clustered using a two-step approach: first by orientation, then by proximity. The tool can generate volumetric representations as well as surface models of the extracted fiber bundles to be exported for further analysis. In addition a custom interactive tool for exploration and visual analysis of MetaTracts is designed. We evaluate the proposed workflow on a number of real world datasets and demonstrate that MetaTracts effectively and robustly identifies and extracts fiber bundles.","Authors":"A. Bhattacharya; J. Weissenb\u00f6ck; R. Wenger; A. Amirkhanov; J. Kastner; C. Heinzl","DOI":"10.1109\/TVCG.2016.2582158","Keywords":"MetaTracts;fiber bundle extraction;analysis and visualization;carbon fiber reinforced polymers;X-ray computed tomography;interactive visual exploration and analysis","Title":"Interactive Exploration and Visualization Using MetaTracts extracted from Carbon Fiber Reinforced Composites","Keywords_Processed":"interactive visual exploration and analysis;fiber bundle extraction;ray compute tomography;metatract;carbon fiber reinforce polymer;analysis and visualization","Keyword_Vector":[0.2022391435,-0.1425432033,-0.0537185204,0.0992093051,-0.0798911365,0.1853317955,-0.0523493482,-0.0695935396,-0.0375047176,-0.0209215865,0.0586554489,-0.0524939517,0.0861242104,0.1181239921,-0.0940952334,-0.0146976953,0.0502257687,-0.0053332915,-0.0191530979,-0.0600219189,-0.1109675888,-0.0570730483,0.0189481257,0.0032115556,0.0317822146,-0.0108521727,-0.019523607,0.0431596485,-0.010908821,-0.0554280777,0.0248096221,0.1178682512,-0.101753416,-0.095543517,0.1327965431,-0.0936757536,-0.0467054621,0.0051564364,-0.0681564558,-0.0592428317,-0.0688032799,0.2516131551,0.00472013,-0.0498849146,-0.0488676445,0.0533061553,0.0207571446,-0.0058805926,-0.0894699201,-0.074990655,-0.0159317268,0.0275212961],"Abstract_Vector":[0.1429729415,-0.1005571504,0.036338443,-0.0177561102,-0.0294078493,-0.0304617273,0.0507845157,-0.0079536719,-0.0296413445,0.0583903064,-0.0214577399,0.0347097332,-0.0720269108,0.0185129706,-0.0019686011,-0.0156001152,-0.0106676277,-0.0132731029,-0.0156486104,0.0005294331,0.0259874594,-0.0351827527,0.0023533691,0.0032223164,-0.0095838938,0.0412916806,-0.0158566645,0.0142442163,-0.0535338578,-0.065313464,0.0420737777,0.0259884781,0.0133398379,0.0086716426,-0.0041818296,-0.0137501764,-0.0178877053,-0.0110033305,-0.0103344811,0.0301467672,-0.0140025421,-0.0204030404,-0.011811146,0.0029305051,0.0325190789,-0.0188098666,0.0072385705,-0.0197354667,-0.009804399,0.0167362995,-0.0153990778,0.0172154737,0.0100484783,0.0012183722,0.0032057154,-0.0181667233,0.0152140795,0.0129220814,-0.0045479896,0.0167112476,-0.025500871,0.0150975638,-0.0210057035,-0.0040956043,0.0183398228,0.0127452263,-0.0306796656,0.0037903758,-0.0461661425,0.0190900076,-0.0212335766,-0.041707106,-0.0138757146,0.0288548012,0.000369988,-0.0114589453,-0.0239208622,0.0106102753,-0.0039160284,0.007119132,0.0057195468,-0.0111004262,0.0566491039,-0.0215622859,0.0184555011,0.0336431601,0.0085731184,0.0079261914,-0.0087953214,-0.0244838601,-0.0003826832,0.0204962336,0.003720086,0.0245488632,0.0222551158,0.0247190944,0.0033231782,-0.0002084832,-0.0063007792,-0.0035326271,0.000418915,0.0152765069,-0.0142993833,0.0146466709,0.0031915479,-0.0047991431,0.0336002022,-0.0165895786,0.017505862,0.0075797638,-0.0017604248,-0.0144733052,-0.0112229745,-0.0219937911,-0.0244965132,0.0172711786]},"150":{"Abstract":"We present a framework to design inverse rig-functions-functions that map low level representations of a character's pose such as joint positions or surface geometry to the representation used by animators called the animation rig. Animators design scenes using an animation rig, a framework widely adopted in animation production which allows animators to design character poses and geometry via intuitive parameters and interfaces. Yet most state-of-the-art computer animation techniques control characters through raw, low level representations such as joint angles, joint positions, or vertex coordinates. This difference often stops the adoption of state-of-the-art techniques in animation production. Our framework solves this issue by learning a mapping between the low level representations of the pose and the animation rig. We use nonlinear regression techniques, learning from example animation sequences designed by the animators. When new motions are provided in the skeleton space, the learned mapping is used to estimate the rig controls that reproduce such a motion. We introduce two nonlinear functions for producing such a mapping: Gaussian process regression and feedforward neural networks. The appropriate solution depends on the nature of the rig and the amount of data available for training. We show our framework applied to various examples including articulated biped characters, quadruped characters, facial animation rigs, and deformable characters. With our system, animators have the freedom to apply any motion synthesis algorithm to arbitrary rigging and animation pipelines for immediate editing. This greatly improves the productivity of 3D animation, while retaining the flexibility and creativity of artistic input.","Authors":"D. Holden; J. Saito; T. Komura","DOI":"10.1109\/TVCG.2016.2628036","Keywords":"Animation rig;character animation;regression","Title":"Learning Inverse Rig Mappings by Nonlinear Regression","Keywords_Processed":"character animation;regression;animation rig","Keyword_Vector":[0.0879236193,0.0973362398,-0.0272841878,0.0216637174,-0.0393594423,0.0452732538,0.0142917999,-0.0236048481,0.0156769651,0.0506224553,0.0198916103,-0.0051323839,0.0042679017,0.0247240124,0.0677679777,0.1282843668,-0.0036775333,-0.0544407071,-0.0719265695,-0.0114038049,0.0103526739,0.0226592417,0.0467914831,0.0254525377,-0.0449409966,0.0288459083,0.01245851,-0.0310954772,-0.0265860015,0.0360539124,-0.0591655668,-0.0249996982,-0.0000439839,0.0372376336,0.002689376,-0.0002745381,0.0113296031,-0.013818499,0.0210669799,0.0133948223,-0.0302214175,0.0211275188,0.037530228,0.0063531923,-0.0349098754,-0.0196290319,0.0082144867,0.0056925361,0.0034937879,0.0268986709,0.0262191013,-0.0137962649],"Abstract_Vector":[0.1132173281,0.0743569851,-0.0412526723,-0.008318471,-0.0114189453,0.0597219334,0.0303339663,0.0270714803,0.0529746429,0.0012738093,-0.0220709971,-0.0175883735,-0.058381949,-0.0377787393,-0.0005866317,0.0008454816,-0.0086433106,-0.0193161263,0.0427978996,-0.0662339131,0.0024934558,-0.0268367379,0.0268743199,-0.0448454048,-0.0158413189,-0.0225588288,0.0189822089,0.0686105242,-0.0412610723,0.06557696,0.0620004116,-0.023579702,-0.0344229773,-0.0304375739,0.0175698743,0.0314913095,-0.0196279182,0.0266893031,0.0111333417,0.0215192829,0.0632406333,0.0017583703,-0.0385294343,-0.0173859351,0.0544285687,0.0016620311,0.000166424,0.022431968,-0.0199133796,-0.0214498671,-0.0007482972,-0.0550725656,-0.0157112419,0.0391481739,-0.0366027944,-0.0122856478,-0.0551021948,-0.0239020019,-0.0281070037,-0.0530244381,-0.004106287,-0.0371246528,0.0172883715,-0.0305384223,-0.0237310954,0.066100372,-0.0247738459,0.0292161808,-0.0238510468,-0.0061876821,0.0711686958,0.0357379363,-0.0349223421,-0.0012157641,-0.0140682971,-0.0136157084,-0.0244319137,0.0167812683,-0.0304529906,-0.0068331515,0.0214922,-0.0740006539,0.0012046424,-0.032767801,-0.013589184,-0.0377037287,-0.0132189553,0.0049853209,0.0203712332,-0.0009729672,-0.0146033448,0.006160511,0.0148719734,0.0120554686,-0.005473877,-0.0019067906,0.048598366,-0.0303256675,-0.0075156786,0.042195022,0.0311684195,0.0188358458,0.0263171368,-0.0275780117,-0.0354278259,0.0206975743,-0.0457300779,-0.0205990547,-0.0199084098,-0.0082332664,-0.0183535451,-0.0096755265,-0.0056569824,-0.0059260938,0.0555398138,0.0250558831]},"151":{"Abstract":"People often rank and order data points as a vital part of making decisions. Multi-attribute ranking systems are a common tool used to make these data-driven decisions. Such systems often take the form of a table-based visualization in which users assign weights to the attributes representing the quantifiable importance of each attribute to a decision, which the system then uses to compute a ranking of the data. However, these systems assume that users are able to quantify their conceptual understanding of how important particular attributes are to a decision. This is not always easy or even possible for users to do. Rather, people often have a more holistic understanding of the data. They form opinions that data point A is better than data point B but do not necessarily know which attributes are important. To address these challenges, we present a visual analytic application to help people rank multi-variate data points. We developed a prototype system, Podium, that allows users to drag rows in the table to rank order data points based on their perception of the relative value of the data. Podium then infers a weighting model using Ranking SVM that satisfies the user's data preferences as closely as possible. Whereas past systems help users understand the relationships between data points based on changes to attribute weights, our approach helps users to understand the attributes that might inform their understanding of the data. We present two usage scenarios to describe some of the potential uses of our proposed technique: (1) understanding which attributes contribute to a user's subjective preferences for data, and (2) deconstructing attributes of importance for existing rankings. Our proposed approach makes powerful machine learning techniques more usable to those who may not have expertise in these areas.","Authors":"E. Wall; S. Das; R. Chawla; B. Kalidindi; E. T. Brown; A. Endert","DOI":"10.1109\/TVCG.2017.2745078","Keywords":"Mixed-initiative visual analytics;multi-attribute ranking;user interaction","Title":"Podium: Ranking Data Using Mixed-Initiative Visual Analytics","Keywords_Processed":"mixed initiative visual analytic;user interaction;multi attribute ranking","Keyword_Vector":[0.0462825614,-0.0109021858,-0.0275743755,-0.0190856612,0.0223193399,-0.0538786269,-0.0433020566,-0.0527230885,-0.0613240903,0.0278778399,-0.0385472542,0.0633245389,-0.029972128,0.0765261087,-0.076920794,0.0027121587,-0.1822683588,-0.0174956894,-0.0290366268,0.0546067726,0.0207048507,-0.0580941845,-0.0317594561,0.0379533049,-0.0415469312,-0.1559237158,-0.1071091292,0.0508537907,-0.0324606301,-0.0941343636,0.0913474162,0.0306119014,0.0228684643,0.1090709879,-0.0384130421,0.1058027677,-0.0982668041,0.0407490709,0.0550176245,0.0071662741,-0.0134444923,0.0394114867,-0.0187454182,0.0514740377,-0.0228052004,-0.0252721278,-0.0782897511,0.0096252332,0.0427568871,-0.0096030926,0.0223972292,-0.060202326],"Abstract_Vector":[0.1553528372,-0.0010365007,0.0036814332,0.0588554345,-0.1155530494,0.0660092377,0.0080147235,0.0341680648,-0.0092889652,-0.0809766805,-0.0086985623,0.0054533009,-0.0627160125,0.0180422966,0.0146935985,0.0680755535,0.1312498525,0.1357257451,-0.0113958835,-0.1967335517,0.1326279685,0.0314050688,0.0348370118,0.0347022395,-0.033761633,0.0275369656,0.1146874234,-0.0340443986,0.0372868923,-0.0244093237,-0.0373937841,-0.0368976391,0.014328621,-0.0173276231,-0.0577220022,-0.0660878191,0.0166045373,-0.0332412658,-0.0059856239,-0.0847111665,-0.0611802917,0.0434440855,-0.0121809119,-0.0377351096,-0.0127573834,0.0434573507,-0.0096623281,0.0422122379,0.0293804499,-0.0056998142,-0.0279257245,-0.0042326224,0.0114269269,0.0164981361,-0.0203515962,-0.027258811,0.0025838955,0.0204684426,0.0291560154,0.0452312152,0.0249452198,0.0074276475,-0.0164198125,0.0198994856,-0.0180606587,-0.0028183954,-0.0109487094,0.0048517608,-0.0178087253,0.01441211,-0.0299545275,0.0109643834,-0.0136294634,0.0030983246,-0.0172970501,0.0267362165,0.0308729242,0.0185031771,0.0398252826,0.0172976218,-0.0122580202,-0.0193278917,-0.005030797,0.0018271092,-0.0375584922,-0.013142993,-0.0053238498,0.0392720495,-0.0141682663,-0.021724016,0.0059732134,-0.0176022533,-0.039438921,0.0458369007,0.0413678603,0.0423517605,0.0325087083,0.0198634053,0.0148299598,0.0031651557,-0.0206826012,-0.0052782376,0.0206367711,-0.0205047166,0.0025458997,0.0242270964,0.0149304662,0.0156440267,-0.0396342136,-0.0000881549,0.0264010488,-0.0136067685,-0.0019633981,0.0403963542,-0.0198080199,-0.0449219882]},"152":{"Abstract":"3D symmetric tensor fields appear in many science and engineering fields, and topology-driven analysis is important in many of these application domains, such as solid mechanics and fluid dynamics. Degenerate curves and neutral surfaces are important topological features in 3D symmetric tensor fields. Existing methods to extract degenerate curves and neutral surfaces often miss parts of the curves and surfaces, respectively. Moreover, these methods are computationally expensive due to the lack of knowledge of structures of degenerate curves and neutral surfaces.<;\/p> <;p>In this paper, we provide theoretical analysis on the geometric and topological structures of degenerate curves and neutral surfaces of 3D linear tensor fields. These structures lead to parameterizations for degenerate curves and neutral surfaces that can not only provide more robust extraction of these features but also incur less computational cost.<;\/p> <;p>We demonstrate the benefits of our approach by applying our degenerate curve and neutral surface detection techniques to solid mechanics simulation data sets.","Authors":"L. Roy; P. Kumar; Y. Zhang; E. Zhang","DOI":"10.1109\/TVCG.2018.2864768","Keywords":"Tensor field visualization;3D symmetric tensor fields;tensor field topology;traceless tensors;degenerate curve extraction;neutral surface extraction","Title":"Robust and Fast Extraction of 3D Symmetric Tensor Field Topology","Keywords_Processed":"degenerate curve extraction;traceless tensor;tensor field visualization;3d symmetric tensor field;neutral surface extraction;tensor field topology","Keyword_Vector":[0.177488059,-0.09705671,-0.0071406476,-0.0082523099,-0.0131216231,0.0457267809,-0.0527748247,-0.0268916058,-0.1269399369,0.0171241532,-0.0402845201,0.060207108,0.0421243784,-0.0175253839,-0.0683933451,-0.0297805017,0.0432532091,-0.0005695343,-0.0199839575,-0.0860813337,0.0445746502,0.0884262017,0.0306922749,0.0615052171,-0.0006690212,0.0363646425,0.0753606716,-0.0115347988,0.0347018106,0.0408407973,-0.0700730397,0.1305563544,-0.0715080668,0.1061706186,0.0178162183,0.0039443192,0.0436784571,-0.0703082672,0.0971441926,0.027616826,-0.0357214941,0.030260292,-0.0154308302,-0.0109621097,0.0043224306,-0.0575231216,0.0625024474,0.0270604887,0.001188376,0.040749898,0.041588708,-0.0375348529],"Abstract_Vector":[0.2646893268,-0.1783667891,0.0382467074,-0.0325508744,-0.0346376223,-0.0228039172,0.0466009193,-0.0401772931,0.0006936831,0.0523222155,-0.0090035924,-0.0186077389,-0.0159097722,0.0276482815,-0.0557894569,-0.0134032098,-0.0240996446,-0.0478943975,-0.0239573934,0.011792899,-0.0241624466,-0.0240046474,0.101540651,0.0022716389,-0.0675662263,-0.0435491241,-0.0327483884,0.0180760682,-0.0741667631,-0.1022258437,0.0978713592,-0.0193264817,0.0133158253,0.0383569213,0.0044874645,0.0079981814,-0.036492108,-0.0339933158,0.020931591,0.0404753453,-0.0157644284,0.0429083372,0.0926347016,0.0286964623,-0.0018891009,-0.0231221444,-0.0086024843,0.097881941,-0.0722819796,-0.001382209,-0.022889935,-0.065409495,0.0127769939,0.0227021004,0.0068953386,0.0488420075,0.075543995,0.0802889273,0.0149712287,0.0439227179,0.0745999843,-0.0287173825,0.0435463513,-0.0961290971,0.0247306394,-0.0347562005,0.0771119755,-0.0273951201,-0.0305170758,0.1006518258,0.0086964858,0.0627013568,0.1114766118,0.0104229652,0.0813901879,-0.0160079679,0.0657686921,-0.0595990401,0.0602029429,-0.0145822854,-0.072297505,0.0405480254,0.0452626033,-0.0330170522,-0.0816813148,0.0189763166,-0.027941288,0.0467527304,0.109625548,-0.0617236601,0.01809915,0.0060970989,0.0017199835,-0.1235836737,0.0366232341,0.0485834926,-0.0094024205,0.0220090694,-0.0624547813,-0.0739044933,0.0257659179,0.0192684437,-0.033958729,-0.0441092921,-0.0934135377,0.0052053555,-0.0583401355,-0.0369133542,-0.0266260828,-0.0158853075,-0.0452012312,-0.0151539729,0.0361889831,-0.0510829336,0.0973055353,-0.019966447]},"153":{"Abstract":"Multi-run simulations are widely used to investigate how simulated processes evolve depending on varying initial conditions. Frequently, such simulations model the change of spatial phenomena over time. Isocontours have proven to be effective for the visual representation and analysis of 2D and 3D spatial scalar fields. We propose a novel visualization approach for multi-run simulation data based on isocontours. By introducing a distance function for isocontours, we generate a distance matrix used for a multidimensional scaling projection. Multiple simulation runs are represented by polylines in the projected view displaying change over time. We propose a fast calculation of isocontour differences based on a quasi-Monte Carlo approach. For interactive visual analysis, we support filtering and selection mechanisms on the multi-run plot and on linked views to physical space visualizations. Our approach can be effectively used for the visual representation of ensembles, for pattern and outlier detection, for the investigation of the influence of simulation parameters, and for a detailed analysis of the features detected. The proposed method is applicable to data of any spatial dimensionality and any spatial representation (gridded or unstructured). We validate our approach by performing a user study on synthetic data and applying it to different types of multi-run spatio-temporal simulation data.","Authors":"A. Fofonov; V. Molchanov; L. Linsen","DOI":"10.1109\/TVCG.2015.2498554","Keywords":"Ensemble visualization;multi-run data visualization;spatio-temporal data visualization;isocontours","Title":"Visual Analysis of Multi-Run Spatio-Temporal Simulations Using Isocontour Similarity for Projected Views","Keywords_Processed":"ensemble visualization;isocontour;multi run datum visualization;spatio temporal datum visualization","Keyword_Vector":[0.0318070914,0.01406336,0.0060112303,0.0044787121,0.0002813827,-0.0185910862,-0.0053740818,-0.0083569207,-0.0343038114,-0.0189999446,0.0109590414,0.0206984992,-0.0233411779,0.0133932035,-0.0068425732,-0.0002199213,-0.019387529,-0.0334586759,0.0355662972,0.0126726752,0.0304938713,-0.0301528,0.0020083081,0.0398602842,0.0303393974,0.0311240132,-0.0033043999,0.0045917724,-0.0289418534,0.0092327705,0.0016321493,0.0021471407,-0.0338652166,0.042179399,0.0423107517,0.0329711673,-0.0435555741,-0.0367287876,-0.0041080565,-0.0435079328,0.0618474684,-0.0577881746,-0.0635864719,0.0373781895,0.0482917384,-0.0172259518,0.0838394355,-0.0265722652,-0.0584856387,0.0209938881,0.0145663132,0.0347949734],"Abstract_Vector":[0.1135042798,-0.0033256479,-0.0010841283,-0.0085270512,-0.0344902937,0.0171892464,0.0216458819,0.0109503793,0.0095755427,0.0017636709,-0.0124928818,-0.0169499682,-0.0358165904,-0.0261675872,-0.0260459447,0.0071837601,0.0107109976,0.043883745,-0.0046930846,0.0078183185,-0.0204576706,-0.0065873642,-0.0038847065,0.0073951971,0.014951375,-0.016847145,-0.018608595,0.0154179049,0.0198409144,0.0101535134,0.0037925132,0.0267920249,0.0130118288,0.002089352,0.0072692287,-0.0112418617,0.0010342768,0.0095502681,0.0004618126,-0.0301300269,0.0373631457,0.0069796377,0.0226395184,-0.0021455472,0.0389537537,-0.0274134529,0.0484134638,-0.0113504214,-0.0067034631,-0.003688488,0.0029091332,0.0340034891,0.005671875,0.0282964542,0.0123746158,0.0064171937,0.0405798784,-0.0142891629,0.0073683529,-0.0303400009,-0.0170915901,-0.0164776814,-0.0014988614,-0.0136767013,-0.0246586961,0.0001083019,0.0012208569,0.0068443511,0.0101339398,0.0026363901,0.0141821688,0.0134181846,-0.0334278023,0.0026900673,-0.0065285188,-0.0530834139,-0.0029235997,0.0127516541,0.0099869448,-0.053101104,-0.0188224257,0.0126379425,-0.0207114335,-0.0146591069,-0.0180025918,0.0137545203,0.027748947,-0.0520676407,-0.0181279321,0.0228339963,-0.0413174349,-0.0050122835,-0.0410501048,-0.0140482537,0.0042548115,-0.0237163859,0.0150229373,-0.0060078597,0.0383919923,-0.0186844695,-0.0020178205,0.0059414391,-0.0039831721,0.0279090043,-0.0232102525,0.0039311812,-0.0191525162,0.0125030204,0.0238699541,0.012990317,0.0192888942,-0.0157727038,-0.0035876305,-0.0150533107,0.0447080739,-0.0330500583]},"154":{"Abstract":"360\u00b0 images and video have become extremely popular formats for immersive displays, due in large part to the technical ease of content production. While many experiences use a single camera viewpoint, an increasing number of experiences use multiple camera locations. In such multi-view 360\u00b0 media (MV360M) systems, a visual effect is required when the user transitions from one camera location to another. This effect can take several forms, such as a cut or an image-based warp, and the choice of effect may impact many aspects of the experience, including issues related to enjoyment and scene understanding. To investigate the effect of transition types on immersive MV360M experiences, a repeated-measures experiment was conducted with 31 participants. Wearing a head-mounted display, participants explored four static scenes, for which multiple 360\u00b0 images and a reconstructed 3D model were available. Three transition types were examined: teleport, a linear move through a 3D model of the scene, and an image-based transition using a M\u00f6bius transformation. The metrics investigated included spatial awareness, users' movement profiles, transition preference and the subjective feeling of moving through the space. Results indicate that there was no significant difference between transition types in terms of spatial awareness, while significant differences were found for users' movement profiles, with participants taking 1.6 seconds longer to select their next location following a teleport transition. The model and M\u00f6bius transitions were significantly better in terms of creating the feeling of moving through the space. Preference was also significantly different, with model and teleport transitions being preferred over M\u00f6bius transitions. Our results indicate that trade-offs between transitions will require content creators to think carefully about what aspects they consider to be most important when producing MV360M experiences.","Authors":"A. MacQuarrie; A. Steed","DOI":"10.1109\/TVCG.2018.2793561","Keywords":"H.5.1 [Information interfaces and presentation]: Multimedia Information Systems \u2014 Artificial, augmented and virtual realities","Title":"The Effect of Transition Type in Multi-View 360\u00b0 Media","Keywords_Processed":";information interface and presentation Multimedia Information Systems Artificial augmented and virtual reality","Keyword_Vector":[0.1780274688,-0.0751256172,-0.0047959047,-0.0578602191,0.1564134468,-0.016778178,0.0335582829,0.0553111855,-0.0663840451,0.0610637813,-0.0603138484,0.1814941617,0.0159348465,-0.022965333,0.0273352135,0.0456167162,0.0117284164,-0.0409419958,0.0678214783,-0.0265790929,0.0351819169,-0.0450955777,0.0640239262,-0.0242694364,0.0549777444,0.0315189504,-0.030247919,-0.1165675909,0.0356114734,0.0009371572,0.0457567685,0.076194744,-0.0413464424,-0.1035273439,-0.0201324772,0.0126068213,-0.0026311737,0.0598527776,0.0406729873,0.0592687761,-0.0332020352,-0.0748694879,-0.0287614849,0.0016500215,-0.0367177468,0.0749644279,-0.0143735002,0.0420501245,0.0795808173,-0.0227751422,-0.0413652535,0.0874966647],"Abstract_Vector":[0.2103293988,-0.1145628525,0.0129579583,0.096326667,0.071909517,0.0926881903,-0.0195317027,0.0070259797,0.0320567953,0.0218513325,0.0164077224,0.011914599,-0.0110110087,0.0458606848,0.0143268232,-0.0392478732,0.0295516368,-0.0014179738,0.0570504449,0.0186893679,0.0134321957,-0.0931471226,-0.0273348489,-0.0198932969,-0.0159523185,0.002311597,-0.0077819556,-0.0268766564,-0.0242002157,-0.0066429259,0.0027704246,-0.0204429736,-0.0669735635,0.0150548641,0.0104182292,-0.0272076288,0.0429169686,0.0173214027,-0.0395148852,-0.0462131256,-0.0137303158,0.0067190834,-0.0979426714,0.0285160809,-0.0420826695,0.0012400434,-0.0342306425,-0.0246003652,-0.0306595201,-0.0134221695,-0.0146407384,-0.0245889284,-0.0078985695,0.0073227367,0.0571844955,-0.0191785353,-0.0640897894,-0.0327260264,0.0503033649,-0.0085462113,-0.0105752345,-0.0187120182,0.0140172821,-0.0282469664,-0.008057133,-0.024160319,-0.0138684528,-0.0191927299,-0.0178812589,-0.0196182042,-0.0479741417,-0.006277035,0.0071456133,0.0278242496,-0.0444061431,0.0073880381,-0.0461641569,-0.0092384057,-0.0189697792,-0.008820773,-0.0102815578,0.0338197713,0.0276026544,-0.0066519577,0.01333509,0.031575482,-0.0038075925,-0.0341367212,0.0278197892,0.0388133694,0.0410094861,0.0124782265,0.0254824202,-0.0333606857,0.00362578,-0.0402143032,-0.0091443676,-0.0311843471,0.0297633764,0.0336345816,-0.0094575512,-0.0352301401,0.0129649654,0.0222279518,-0.0231896092,0.0074946962,0.0116269903,-0.0422461978,0.0101951756,-0.0270175923,0.0122332383,0.0043497619,-0.018080764,0.0126317129,-0.0017495579,0.0074731689]},"155":{"Abstract":"Vortices are one of the most-frequently studied phenomena in fluid flows. The center of the rotating motion is called the vortex coreline and its successful detection strongly depends on the choice of the reference frame. The optimal frame moves with the center of the vortex, which incidentally makes the observed fluid flow steady and thus standard vortex coreline extractors such as Sujudi-Haimes become applicable. Recently, an objective optimization framework was proposed that determines a near-steady reference frame for tracer particles. In this paper, we extend this technique to the detection of vortex corelines of inertial particles. An inertial particle is a finite-sized object that is carried by a fluid flow. In contrast to the usual tracer particles, they do not move tangentially with the flow, since they are subject to gravity and exhibit mass-dependent inertia. Their particle state is determined by their position and own velocity, which makes the search for the optimal frame a high-dimensional problem. We demonstrate in this paper that the objective detection of an inertial vortex coreline can be reduced in 2D to a critical point search in 2D. For 3D flows, however, the vortex coreline criterion remains a parallel vectors condition in 6D. To detect the vortex corelines we propose a recursive subdivision approach that is tailored to the underlying structure of the 6D vectors. The resulting algorithm is objective, and we demonstrate the vortex coreline extraction in a number of 2D and 3D vector fields.","Authors":"T. G\u00fcnther; H. Theisel","DOI":"10.1109\/TVCG.2018.2864828","Keywords":"Vortex extraction;inertial particles;objectivity;vortex coreline","Title":"Objective Vortex Corelines of Finite-sized Objects in Fluid Flows","Keywords_Processed":"inertial particle;objectivity;vortex extraction;vortex coreline","Keyword_Vector":[0.06119896,0.0104869698,-0.0034217346,-0.0145212402,0.0246812006,-0.0195722321,-0.0644503512,0.0057241209,-0.0905627606,-0.0680951041,0.0067003897,0.040156387,0.0131469094,-0.0148877503,0.0450534123,0.0634903928,-0.0251163522,-0.00655298,0.1858194254,-0.058859791,0.0199327761,-0.043351697,0.0852994685,0.0613762004,-0.0475858581,0.1284388691,-0.0570873313,-0.0118947134,-0.0046982828,0.019454606,0.0059748893,-0.0599208905,-0.1120673798,0.0038739615,0.0040166986,-0.018852408,-0.0563202817,0.0326177313,0.0240981285,0.0860530776,0.0007175211,-0.0017158767,-0.0741722591,-0.1152213674,0.073322905,0.003540912,-0.0449330611,0.030835343,0.0877569649,-0.0551767523,0.0561011128,0.0243411065],"Abstract_Vector":[0.2054348023,0.0854638021,-0.0481998326,-0.0147486999,-0.0586233199,0.1393908214,0.0550830721,0.0327506578,0.1141068711,-0.014551661,-0.0412491008,-0.0094889277,-0.0803227338,-0.0305734182,0.0008033992,-0.0410532219,0.063680255,0.0967509253,-0.0001671036,0.0692559224,0.0148490508,0.0131791346,0.0251997529,0.0116316197,-0.0231440518,-0.000533496,0.0671698231,-0.0186306921,0.0042137018,0.0420968959,0.0537761836,0.0102946141,-0.0155054847,0.0087277789,0.0523446191,0.0040420957,-0.0425263063,-0.0486365775,0.0516099131,0.052821864,0.1032545648,-0.0242535827,0.0508129199,0.0009002888,-0.0064229051,-0.0519795181,0.0807308104,0.0019687959,0.0215433791,0.0412180819,0.0041735682,-0.0042318822,-0.0201426811,0.007736225,-0.0103390353,0.0070206775,0.0495528317,-0.0086926135,0.0128958884,0.0461330565,0.0017933353,-0.0518181381,-0.0208972215,0.0080938314,0.0063392557,0.0673043691,-0.0123138671,0.0157919951,-0.0302338554,-0.0039497416,0.0117515402,0.0367570521,-0.0188421127,0.0651753307,-0.0123296989,-0.0108232674,-0.0289815328,-0.024138352,-0.0495696663,-0.038817136,0.0162253504,-0.0204436293,-0.0482666484,0.0012127014,-0.0345345906,-0.0126269454,0.0061626769,0.0243055417,-0.060814614,-0.0446426601,0.0572102719,0.058542252,0.0276392952,-0.0271831999,-0.0242936595,-0.0079484872,0.037727167,0.0098302287,-0.0144102381,-0.0208741919,0.0337874137,-0.0136420399,-0.0013246476,-0.0032425449,-0.0267283243,0.0241592084,-0.0202364194,0.0281516551,0.0163351614,-0.0034269919,-0.0337660687,0.0526563037,-0.0110303791,-0.0275261942,0.0236571873,0.0166377587]},"156":{"Abstract":"Visualizing 3D trajectories to extract insights about their similarities and spatial configuration is a critical task in several domains. Air traffic controllers for example deal with large quantities of aircrafts routes to optimize safety in airspace and neuroscientists attempt to understand neuronal pathways in the human brain by visualizing bundles of fibers from DTI images. Extracting insights from masses of 3D trajectories is challenging as the multiple three dimensional lines have complex geometries, may overlap, cross or even merge with each other, making it impossible to follow individual ones in dense areas. As trajectories are inherently spatial and three dimensional, we propose FiberClay: a system to display and interact with 3D trajectories in immersive environments. FiberClay renders a large quantity of trajectories in real time using GP-GPU techniques. FiberClay also introduces a new set of interactive techniques for composing complex queries in 3D space leveraging immersive environment controllers and user position. These techniques enable an analyst to select and compare sets of trajectories with specific geometries and data properties. We conclude by discussing insights found using FiberClay with domain experts in air traffic control and neurology.","Authors":"C. Hurter; N. H. Riche; S. M. Drucker; M. Cordeil; R. Alligier; R. Vuillemot","DOI":"10.1109\/TVCG.2018.2865191","Keywords":"Immersive Analytics;3D Visualization;Dynamic Queries;Bimanual Interaction;Multidimensional Data","Title":"FiberClay: Sculpting Three Dimensional Trajectories to Reveal Structural Insights","Keywords_Processed":"3D visualization;immersive Analytics;bimanual Interaction;dynamic query;Multidimensional Data","Keyword_Vector":[0.1204500512,-0.1352700638,-0.1233077581,0.0559847891,-0.0611537472,-0.138110363,0.0210983718,0.0378974082,0.0831212053,0.0009622808,0.0252011332,-0.0267122669,-0.0691131178,-0.0169128592,0.0224525837,0.0029261675,-0.052108605,0.0496010633,-0.0657113382,-0.0685507695,0.0440012046,0.022455365,0.039561056,-0.020005789,-0.0285205235,0.0063064846,0.0503042132,-0.0076857462,0.0518043947,-0.0251281485,0.0373401472,0.0400368634,0.0136892291,-0.0713289393,0.0417718208,-0.0588745198,0.0869182785,-0.013283195,-0.0636250131,0.0013868602,-0.0878384316,-0.0349393959,0.0625005342,0.0758200986,-0.0277736001,0.027440726,0.0452801255,0.0498612834,0.1767224954,-0.0983082686,0.2174815012,0.1146943976],"Abstract_Vector":[0.1483845204,-0.084414554,0.0224066556,0.0131512458,-0.0277335052,0.031256952,-0.0268018295,-0.0135122814,0.0114057984,0.0043510044,-0.023582468,0.0469474389,-0.0509531448,0.0260623447,-0.0403467957,-0.008634999,-0.028305108,-0.1207439683,-0.0496405937,0.0331340422,0.0721835557,0.0651667743,-0.0188727887,-0.0512419975,0.0362754517,0.1458964992,0.0917305721,0.0816517836,0.0217788569,0.0555393346,-0.0168889403,-0.064875399,0.0494969555,0.0138826937,0.014881769,-0.0511354191,0.047168163,0.0333803079,0.007377868,-0.0814918957,0.0213600391,-0.0010652183,0.0159037597,0.0200677002,-0.0464556191,-0.0537598121,0.0327565275,0.0032279737,-0.0746575131,-0.0052314348,0.0216400333,0.0145864276,0.012198971,0.0206689984,0.1053423354,-0.0197473006,-0.028584963,0.0191392369,0.007281704,-0.0443656887,-0.041105987,-0.0088895491,0.0107627248,-0.0031552764,-0.053530962,-0.0746163015,0.0036390279,-0.0336394627,-0.0129850989,-0.0145665508,-0.003166743,-0.002960887,0.0101461584,-0.0219018579,-0.0399896689,0.0010809929,-0.0076186203,0.0253185757,-0.0248360944,-0.0420263921,-0.018009381,-0.0179510054,0.0439002361,-0.0256386784,0.0211691367,-0.0403435128,0.0382865157,0.0290964748,-0.0186473439,0.0308420734,-0.0140692611,0.0283124494,0.0364068124,0.0219143302,0.0101762978,-0.0336745732,-0.002501295,-0.0047358055,-0.0328950913,0.0148220149,-0.0102934733,-0.0196351291,0.015167706,-0.0148062987,-0.0239490276,0.000847525,-0.0099751534,0.0085798767,0.0205331424,0.0103403523,-0.012613575,-0.0117248584,0.0105729789,-0.0287025924,0.0147655421,0.0133077871]},"157":{"Abstract":"We present a new approach to rendering a geometrically-correct user-perspective view for a magic lens interface, based on leveraging the gradients in the real world scene. Our approach couples a recent gradient-domain image-based rendering method with a novel semi-dense stereo matching algorithm. Our stereo algorithm borrows ideas from PatchMatch, and adapts them to semi-dense stereo. This approach is implemented in a prototype device build from off-the-shelf hardware, with no active depth sensing. Despite the limited depth data, we achieve high-quality rendering for the user-perspective magic lens.","Authors":"D. Bari\u010devi\u0107; T. H\u00f6llerer; P. Sen; M. Turk","DOI":"10.1109\/TVCG.2016.2559483","Keywords":"Augmented reality;magic lens;user-perspective;image based rendering;gradient domain;semi-dense stereo","Title":"User-Perspective AR Magic Lens from Gradient-Based IBR and Semi-Dense Stereo","Keywords_Processed":"user perspective;image base render;magic lens;gradient domain;semi dense stereo;augmented reality","Keyword_Vector":[0.0806446431,0.0141502469,0.0678787423,0.0382381358,0.0291109,-0.0450796008,-0.0185653369,0.0334537472,-0.0622776783,0.0626376759,0.0113614268,0.053400403,-0.0154121455,0.0329331746,0.0941931277,0.1406331835,-0.1691624063,-0.1172397949,0.3004790896,-0.0762635094,0.1360876916,-0.1471157874,0.1180660301,0.0437759749,0.1189397543,0.1843313588,-0.0933835705,-0.0716615951,-0.0140353249,-0.0470263703,0.0760274693,0.0223991952,-0.0929472149,-0.0589558043,-0.0574699902,-0.1218179349,-0.0239560516,-0.0313108572,-0.0093541598,0.1139009558,-0.0116222434,-0.0445050243,-0.0273437237,-0.0724800071,0.0296202891,0.0435507089,0.037680571,0.0786983597,0.0209491957,0.0113785365,-0.0516283154,-0.0144573909],"Abstract_Vector":[0.2236863235,0.0665522606,0.0046494104,-0.0139997095,0.0130995566,0.0494725203,0.0069004288,0.0961284121,0.1003849571,-0.0281970579,-0.0666669177,0.0238509784,-0.041457497,-0.0662949268,0.0572557992,-0.0626979262,-0.0069041733,0.0816491593,-0.0077094643,0.0594980879,0.0515717317,-0.0235900512,-0.07178235,0.0049802724,-0.0111895787,0.0090686555,0.0842161316,-0.027145408,0.0404087965,-0.0355605116,-0.018838312,-0.0422216544,-0.0395487003,0.0241527248,0.0442264646,-0.0060990156,-0.0006971821,0.0315542955,0.0011319249,0.0326844065,0.0325668261,0.0495187094,-0.0023636259,-0.0337118923,-0.0295431704,0.0400964171,-0.0260577266,0.001626218,-0.0149554907,-0.0382124999,-0.0346355813,0.0044702626,-0.0184026904,0.0327503667,0.0117193171,-0.0564822562,-0.0100633985,-0.034321878,-0.0221524914,-0.0257011016,-0.0016426417,0.0190423814,0.0062054814,0.0045230679,-0.013761507,0.0402062419,0.0249461935,-0.0104272943,0.0085646306,0.0119140008,0.0241767152,0.0052152594,-0.0040422502,-0.0218028216,-0.0244917964,0.0064298284,0.0160443133,0.0215865935,0.0351358506,0.0222858765,-0.0051649878,0.0375533162,0.0638333355,-0.0327723378,-0.0087016635,-0.0136484323,-0.0325932852,-0.0377868132,-0.0126995441,0.0436902973,-0.0165647153,0.0367351845,-0.004308479,0.0097782433,0.0009591029,-0.0504571705,0.025415701,0.0305629781,-0.0108839889,0.0525394852,-0.0522849726,-0.0285933809,-0.0340476729,-0.0478989851,0.0544959036,0.0465094259,0.0018980329,0.0091434349,0.0120876484,0.0104797615,0.009658703,-0.0296799916,0.0407691157,-0.0273067189,0.0164362172,-0.0221591174]},"158":{"Abstract":"In the first crowdsourced visualization experiment conducted exclusively on mobile phones, we compare approaches to visualizing ranges over time on small displays. People routinely consume such data via a mobile phone, from temperatures in weather forecasting apps to sleep and blood pressure readings in personal health apps. However, we lack guidance on how to effectively visualize ranges on small displays in the context of different value retrieval and comparison tasks, or with respect to different data characteristics such as periodicity, seasonality, or the cardinality of ranges. Central to our experiment is a comparison between two ways to lay out ranges: a more conventional linear layout strikes a balance between quantitative and chronological scale resolution, while a less conventional radial layout emphasizes the cyclicality of time and may prioritize discrimination between values at its periphery. With results from 87 crowd workers, we found that while participants completed tasks more quickly with linear layouts than with radial ones, there were few differences in terms of error rate between layout conditions. We also found that participants performed similarly with both layouts in tasks that involved comparing superimposed observed and average ranges.","Authors":"M. Brehmer; B. Lee; P. Isenberg; E. K. Choe","DOI":"10.1109\/TVCG.2018.2865234","Keywords":"Evaluation;graphical perception;mobile phones;range visualization;crowdsourcing","Title":"Visualizing Ranges over Time on Mobile Phones: A Task-Based Crowdsourced Evaluation","Keywords_Processed":"range visualization;graphical perception;crowdsource;mobile phone;evaluation","Keyword_Vector":[0.1062740007,0.1137931047,-0.0587372166,0.0293910441,0.0045286144,0.0127036733,0.0052913948,-0.0450566152,-0.0100311618,-0.0174049037,0.032473552,-0.0103806532,-0.019317064,0.0314089501,0.0684593692,0.0463341181,-0.0377831872,-0.1091523497,0.0703137813,-0.120259563,0.1142020901,0.0511152011,-0.1384325252,-0.0568478582,0.034791089,-0.00865071,0.0633017647,-0.0119773549,0.0911232981,0.0029012099,-0.1154457825,0.0203595506,0.0343294493,0.0585477607,-0.0124133902,-0.015579238,0.0387875062,0.0478650035,-0.0171420626,0.0503108562,-0.033192919,0.0769780822,0.0055417576,-0.0429331395,-0.0372835532,0.0524766775,-0.0340730421,-0.0281666294,-0.0634321444,0.0372122768,0.0511019511,-0.0658083022],"Abstract_Vector":[0.1416510109,0.1148396851,-0.0666362457,-0.0143919881,-0.0277283865,0.0694085983,0.0180808306,0.0024393292,-0.0098812849,0.0132631344,-0.0071281227,0.0085000203,-0.0564733915,-0.0666006074,-0.0035651807,0.0143173306,-0.0473205746,0.0142090952,-0.0261958345,-0.0588313814,-0.0114682842,-0.0855770009,0.0032450533,-0.0273296696,0.0133104596,0.0114351172,0.0339176636,0.0151159998,-0.0187296083,0.0057832733,0.0031614012,0.0076698381,0.0205698226,-0.0171888191,-0.0058605436,-0.0105366113,0.0116537385,0.0561227647,0.0343516753,0.0378946652,0.0933510305,0.0868172788,-0.0085855271,0.055000558,-0.0569983047,0.0313318355,-0.0646101901,-0.0166403048,-0.0575847881,-0.065377652,0.0363211586,0.0193437095,0.0150826739,0.0813803763,-0.0004501107,0.017981625,-0.0628108767,0.0248959007,-0.0560286747,0.0094233182,-0.0710530379,-0.0176602232,0.0290179566,-0.0367979807,-0.0163935668,0.0405837437,-0.0033318264,0.0360563741,-0.0334459406,-0.0614149927,0.0961931394,0.0063609226,0.0026077651,-0.0385844248,0.0006065364,-0.0078505053,0.0500443125,0.0520544077,-0.0063358759,0.0420875238,0.0167513065,0.0097752522,-0.0027192749,-0.0291215385,-0.0293012692,0.0737388676,-0.0390936037,0.0028380692,0.0014063565,0.0185901038,-0.0342952127,0.0393531786,0.0228536052,0.0276038486,0.0159542056,0.0099814392,0.0467037003,0.0047045353,-0.0327733126,0.0128183428,0.0240365941,0.0236255179,0.0382212267,-0.0318230894,-0.0654932964,0.0130937123,0.0139077952,-0.0083504039,0.0622278749,-0.0019301833,0.0147356962,-0.0462018794,-0.008630888,-0.0093662982,-0.0236862034,-0.0407308558]},"159":{"Abstract":"Single-cell analysis through mass cytometry has become an increasingly important tool for immunologists to study the immune system in health and disease. Mass cytometry creates a high-dimensional description vector for single cells by time-of-flight measurement. Recently, t-Distributed Stochastic Neighborhood Embedding (t-SNE) has emerged as one of the state-of-the-art techniques for the visualization and exploration of single-cell data. Ever increasing amounts of data lead to the adoption of Hierarchical Stochastic Neighborhood Embedding (HSNE), enabling the hierarchical representation of the data. Here, the hierarchy is explored selectively by the analyst, who can request more and more detail in areas of interest. Such hierarchies are usually explored by visualizing disconnected plots of selections in different levels of the hierarchy. This poses problems for navigation, by imposing a high cognitive load on the analyst. In this work, we present an interactive summary-visualization to tackle this problem. CyteGuide guides the analyst through the exploration of hierarchically represented single-cell data, and provides a complete overview of the current state of the analysis. We conducted a two-phase user study with domain experts that use HSNE for data exploration. We first studied their problems with their current workflow using HSNE and the requirements to ease this workflow in a field study. These requirements have been the basis for our visual design. In the second phase, we verified our proposed solution in a user evaluation.","Authors":"T. H\u00f6llt; N. Pezzotti; V. van Unen; F. Koning; B. P. F. Lelieveldt; A. Vilanova","DOI":"10.1109\/TVCG.2017.2744318","Keywords":"Hierarchical Data;HSNE;Single-Cell Analysis;Visual Guidance","Title":"CyteGuide: Visual Guidance for Hierarchical Single-Cell Analysis","Keywords_Processed":"visual Guidance;HSNE;Hierarchical Data;Single Cell Analysis","Keyword_Vector":[0.1891569932,-0.0156576008,0.2192939904,-0.0376819457,-0.118451669,-0.0920155922,-0.0079611167,0.1993838022,-0.2926068248,0.1572572529,0.2454094657,0.0253008536,-0.0667441451,-0.002037567,-0.089283824,0.0413367907,-0.0615769174,-0.0007509439,-0.1194850378,-0.0155453012,0.0960229736,0.0163847733,-0.0634633629,-0.0123938232,-0.0244057026,0.0697708712,-0.0266883571,0.0483470191,0.0085589989,-0.0136366732,-0.0273862246,-0.0973678376,-0.0482479873,-0.0688077322,-0.0266385181,-0.0347458071,0.0397058235,-0.0237195619,-0.0752823516,-0.0546710987,0.0188354006,0.0237716601,-0.0808164152,-0.0215661555,0.0199869402,0.0486613112,0.0601161922,-0.0347563423,-0.0256126858,0.0578391305,-0.0201949856,-0.054113331],"Abstract_Vector":[0.2076995849,0.0137043149,0.2426312552,-0.1265723513,0.0810267069,0.0569651763,-0.2493907755,0.4377882426,-0.0146403688,0.1116440954,0.1331709142,0.0855940305,-0.0050757074,0.0661333048,-0.0069598125,0.0604762004,-0.0277033303,-0.0126302908,-0.0626748132,0.0408020981,0.0027520481,-0.0204826039,0.0215910988,-0.0753800717,0.0487796299,-0.0568625525,0.0092570978,-0.0397729275,-0.0593381893,-0.0037994493,-0.046084889,0.0172612112,0.0925228035,-0.0347998772,-0.0020066039,-0.029413859,0.0325916086,-0.0068194233,-0.0243210895,0.0234333395,-0.0087488302,0.0270654833,-0.0109832464,-0.0222281297,0.0051463285,0.0049691517,-0.04545022,-0.0315889179,0.037140421,0.0289929263,-0.0079149486,0.0383166067,-0.0133992958,0.0320385331,0.0093407886,0.0304381513,0.0360634057,0.0308045046,0.0102451358,0.0286090079,-0.0022041156,0.0338128692,0.0175230745,-0.0002241539,-0.0625698582,0.0045908035,0.0381487751,-0.0161109261,0.0356630694,-0.0025505796,0.0497105273,0.0148283976,0.0003017653,0.0106779802,0.0078237287,-0.0266638209,-0.0407358265,0.0113063196,-0.0224339162,0.0195305036,0.0158559508,-0.021947774,0.0008216923,0.0023061257,0.0094009487,0.0103597669,-0.0244831573,0.0453030242,0.0034792641,-0.0371410899,-0.0236626764,0.0104499949,-0.016671547,-0.0206661087,-0.030153689,0.032897942,-0.0118409777,0.0382866101,0.0146860258,0.0072332768,0.0148822714,0.027891244,0.0464250264,0.0184482104,0.0068126701,-0.0041174145,-0.024910529,0.0034597087,0.0046377273,0.0070969193,0.0443652692,0.009529346,-0.0029044225,0.0195796905,-0.0063338468,-0.0231389431]},"16":{"Abstract":"We propose a new class of vortex definitions for flows that are induced by rotating mechanical parts, such as stirring devices, helicopters, hydrocyclones, centrifugal pumps, or ventilators. Instead of a Galilean invariance, we enforce a rotation invariance, i.e., the invariance of a vortex under a uniform-speed rotation of the underlying coordinate system around a fixed axis. We provide a general approach to transform a Galilean invariant vortex concept to a rotation invariant one by simply adding a closed form matrix to the Jacobian. In particular, we present rotation invariant versions of the well-known Sujudi-Haimes, Lambda-2, and Q vortex criteria. We apply them to a number of artificial and real rotating flows, showing that for these cases rotation invariant vortices give better results than their Galilean invariant counterparts.","Authors":"T. G\u00fcnther; M. Schulze; H. Theisel","DOI":"10.1109\/TVCG.2015.2467200","Keywords":"Vortex cores;rotation invariance;Galilean invariance;scientific visualization;flow visualization;line fields;Vortex cores;rotation invariance;Galilean invariance;scientific visualization;flow visualization;line fields","Title":"Rotation Invariant Vortices for Flow Visualization","Keywords_Processed":"scientific visualization;galilean invariance;rotation invariance;flow visualization;vortex core;line field","Keyword_Vector":[0.2012869871,-0.1200090734,-0.0119009686,-0.007525803,0.0467788027,0.1324076611,-0.0707005275,0.0346249471,0.0140534706,0.0028052265,0.0051167165,-0.0952041042,-0.0041638833,-0.0099407866,0.0397404504,-0.0837194034,-0.0471560438,-0.0869006411,0.0764675748,0.1130448865,0.0437319514,-0.02162936,0.061823794,-0.0053138534,0.0397137176,0.0025819807,-0.0334143915,0.031825169,-0.0347390786,-0.0021293468,-0.0652166204,-0.0099945271,0.0112696062,0.014100271,-0.0308805826,0.0498506399,0.0647158322,0.019937537,-0.0316799735,-0.02345493,0.0579441938,-0.0163238383,-0.0029115282,0.0298969335,-0.0457340224,0.0350939325,-0.0284142703,-0.0305742932,0.0063882541,-0.0145364325,0.0235027021,0.0238240214],"Abstract_Vector":[0.1968274155,-0.0520279812,-0.022852249,-0.0201070104,0.0180524206,-0.0973293104,-0.039233565,-0.0308617466,0.0479107844,-0.0235272359,0.0421691302,-0.0557206619,-0.0382083484,-0.0325690987,0.0176348967,-0.0023943872,-0.0267042493,0.0029776026,-0.0477993246,-0.0103753077,-0.0011564416,-0.0229285925,0.0259194834,-0.0479282993,-0.0845318284,0.0101204661,0.016850008,0.0857131284,-0.0250485386,0.0002868486,-0.0058504321,0.0260307507,0.0370027593,0.0405682391,-0.0064323339,0.0305880297,-0.0067096782,-0.0338727916,0.0158530598,0.0396079039,0.0104607429,0.0058765174,0.0316907407,-0.0419243003,-0.022616295,-0.0010464335,-0.0326407261,0.0110536795,-0.016851383,-0.0394944879,-0.0111068533,-0.0736738178,-0.0386878694,0.0572371865,-0.004056413,-0.0736195154,0.0523971675,-0.0460786801,-0.0239600416,0.0302646444,-0.0084106195,-0.0170037246,-0.0614661181,0.0643055589,0.0498293782,-0.0452902322,0.0448062179,-0.0408681492,0.056581247,-0.0057689401,0.0094761493,0.0011139886,0.0157813418,0.0379954562,0.0580170843,-0.0142223789,-0.015916564,-0.045337951,-0.0185709934,0.0209356024,-0.0030109968,-0.0268126237,-0.0725687613,-0.0673948364,-0.01117281,-0.0204863584,0.0080304554,0.0348138969,-0.0684949371,-0.0453333319,0.0226437894,0.0129151061,-0.0132635515,-0.0583289333,-0.0149748,-0.0132697402,0.0383679993,0.0229067579,0.0099739836,0.002107392,0.0582519204,0.0338776954,0.0261847546,-0.0066276703,-0.0287891305,0.0405898366,0.0062562763,-0.0339361009,-0.0236683576,0.009053238,-0.0108701308,0.0564074156,-0.00754149,-0.0070681259,0.0014209931,-0.003042068]},"160":{"Abstract":"The original Summed Area Table (SAT) structure is designed for handling 2D rectangular data. Due to the nature of spherical functions, the SAT structure cannot handle cube maps directly. This paper proposes a new SAT structure for cube maps and develops the corresponding lookup algorithm. Our formulation starts by considering a cube map as part of an auxiliary 3D function defined in a 3D rectangular space. We interpret the 2D integration process over the cube map surface as a 3D integration over the auxiliary 3D function. One may suggest that we can create a 3D SAT for this auxiliary function, and then use the 3D SAT to achieve the 3D integration. However, it is not practical to generate or store this special 3D SAT directly. This 3D SAT has some nice properties that allow us to store it in a storage-friendly data structure, namely Summed Area Cube Map (SACM). A SACM can be stored in a standard cube map texture. The lookup algorithm of our SACM structure can be implemented efficiently on current graphics hardware. In addition, the SACM structure inherits the favorable properties of the original SAT structure.","Authors":"Y. Xiao; T. Ho; C. Leung","DOI":"10.1109\/TVCG.2017.2761869","Keywords":"Cube map;summed area table;prefiltering;environment mapping;soft shadow","Title":"Summed Area Tables for Cube Maps","Keywords_Processed":"prefiltere;environment mapping;cube map;sum area table;soft shadow","Keyword_Vector":[0.0491960279,0.0055320677,-0.015596629,-0.0124512542,0.0191476958,0.0109700931,0.0167045344,-0.0193728018,-0.0059848639,-0.0347405692,0.0160030812,0.0320049332,0.0227029032,0.0078050403,0.0031040138,0.019087039,-0.0018963689,0.0176772876,-0.0682809301,-0.0409167082,0.013173194,0.0336063205,0.0064581983,0.0013462923,0.1133936193,-0.0167842223,0.020789025,0.0078802372,-0.0296974134,-0.0429825989,-0.0151176224,-0.0401295495,0.0002733877,0.0269288886,-0.0137445939,-0.0756685609,-0.0700889464,-0.0564165745,0.1039259715,-0.1000864531,0.2237930662,-0.061713586,0.0505775841,0.007757497,-0.0043370306,0.1101321432,-0.0261479776,0.178915875,-0.0424528996,-0.0213840283,0.0688406839,-0.044931898],"Abstract_Vector":[0.1596718662,0.0156480745,0.033806246,0.0186006238,0.009793997,0.0537571659,-0.0294633586,-0.0057902158,-0.0288239553,-0.0073569745,-0.0300695848,-0.0282835765,-0.0737670807,0.0435392855,0.0249161785,-0.0131187537,-0.0109757079,-0.0090970067,0.0105076564,-0.013428603,-0.0031284803,0.0213594301,-0.0273467677,0.0006210877,-0.0183467805,-0.0043182536,-0.0322547333,0.0125271838,0.0379547172,0.0020867356,0.0114099107,0.0819608098,-0.0230005668,0.0276326877,-0.0004807047,-0.0065096377,0.0075739962,-0.0047857975,0.0100460696,0.0383408,0.0088211503,0.0185682967,0.0108511525,-0.0544983841,-0.0408350694,-0.0112285275,0.0383357301,0.0012464969,-0.0126945026,-0.0021413323,0.0146797755,0.0275046249,0.0120029598,-0.0190120841,-0.0059280256,-0.0030870117,-0.0028285173,0.0169179196,-0.0178309563,-0.0119224211,-0.0216748551,-0.0042590167,0.0062781188,0.0462006699,0.0059753139,0.0246100303,0.0041073316,0.0312985564,0.0168298706,0.0130470711,-0.0403418755,0.0447599368,0.0020600255,-0.0018215078,-0.0341514748,-0.0208728653,-0.0060576409,0.0160935946,0.026133719,-0.0208785738,-0.0031330485,0.0346569668,-0.0191574005,-0.0042280834,-0.023879618,0.002903393,0.0059139845,-0.0172164655,-0.0013722911,-0.0456689169,-0.0209282988,-0.0058938603,0.0138293818,0.0114193021,-0.0273833463,-0.0053830007,0.0025519167,-0.002388012,-0.0044894917,-0.0054895423,0.0218263642,0.0145448654,-0.0050346537,-0.0054307931,-0.01335251,-0.0408161959,-0.0256413421,-0.0047913543,0.0151589592,0.0257732569,-0.012380222,-0.0196216258,0.0102815723,-0.0221883746,0.0341005615,-0.0290584797]},"161":{"Abstract":"Map matching is the process of assigning observed geographic positions of vehicles and their trajectories to the actual road links in a road network. In this paper, we present Visual Interactive Map Matching, a visual analytics approach to fine-tune the data preprocessing and matching process. It is based on ST-matching, a state-of-the-art and easy-to-understand map matching algorithm. Parameters of the preprocessing step and algorithm can be optimized with immediate visual feedback. Visualizations show current matching issues and performance metrics on a map and in diagrams. Manual and computer-supported editing of the road network model leads to a refined alignment of trajectories and roads. We demonstrate our approach with large-scale taxi trajectory data. We show that optimizing the matching on a subsample results in considerably improved matching quality, also when later scaled to the full dataset. An optimized matching ensures data faithfulness and prevents misinterpretation when the matched data might be investigated in follow-up analysis.","Authors":"R. Kr\u00fcger; G. Simeonov; F. Beck; T. Ertl","DOI":"10.1109\/TVCG.2018.2816219","Keywords":"Map matching;data cleaning;data transformation and representation;geographic visualization","Title":"Visual Interactive Map Matching","Keywords_Processed":"datum cleaning;map matching;datum transformation and representation;geographic visualization","Keyword_Vector":[0.020995634,-0.0138728138,-0.002100255,-0.0059634439,0.0189508138,0.0121647052,0.0112649142,0.0137723395,-0.0116476951,0.0095618861,0.0083734538,0.0159464656,-0.0078733823,0.0129325666,-0.0043282181,-0.0230041996,0.0034291569,-0.0069065499,0.0033621356,-0.0057854658,0.0034818433,-0.0277391199,-0.0034257485,0.0039472126,0.0260579605,-0.0029367126,0.0370161352,-0.0150335264,0.0015854813,0.0061625305,-0.00971061,-0.0145007125,-0.0081125137,0.0143413438,0.0043511834,-0.0058057845,-0.0055394863,-0.008377073,-0.0214040992,0.005561675,-0.0186394123,0.0080362665,-0.0246129207,0.0005607873,0.0080233323,0.0303877524,-0.0109839266,-0.0174973312,-0.0095561794,0.0127697267,-0.042084119,0.0066508199],"Abstract_Vector":[0.1745898771,-0.0586257004,-0.0039765428,0.0210611964,-0.0254900001,0.0236527277,-0.0322633665,-0.0034807702,0.0418869292,-0.0107906132,-0.0155109926,-0.0090706636,-0.0182922806,-0.0081548599,-0.0103346953,0.0128305671,-0.0502752654,-0.0149537672,0.0334166961,0.0118251798,-0.0022760878,0.0295012411,0.0062818512,-0.0047851701,-0.0068498719,0.0358856252,0.0008206928,0.0574456459,0.0300932734,0.0160813262,-0.0118692014,0.0119294585,-0.01768623,-0.0041148618,0.0353730719,-0.0399012714,0.0289055412,-0.0033739345,0.0270728106,0.0108591498,-0.0152460593,0.0209713696,-0.0518779147,0.0381723542,0.0331563035,-0.0340615151,0.0150519182,0.008483492,-0.0332261992,0.0216623561,-0.0254562763,-0.0122318789,0.005992778,-0.0216593536,0.0470856354,-0.0015757653,-0.0486743388,-0.061182198,0.0156523682,0.0098174352,-0.0053643126,-0.0308803167,0.0531917055,-0.0030484027,0.0047932437,-0.0142434274,-0.0124484234,-0.0216291606,-0.0302880694,-0.0235496553,0.0027135061,0.0301320866,-0.0103156563,-0.0186741478,0.0568556486,-0.0247691597,0.0149334084,-0.0070190966,0.033730078,0.0034161694,0.0283984234,0.034183455,0.0257793184,0.0025307739,0.0333642594,-0.0016686633,0.0122144875,0.0229222631,-0.0465687153,0.03825597,0.0264383525,-0.0215055282,-0.0169667934,0.0228506065,0.0625236873,0.0011174538,0.0362309436,0.0434918518,-0.0279093054,-0.0725270764,-0.044073754,-0.0453948789,0.0137239427,-0.0049964807,-0.0336094216,-0.0184395293,0.0194768178,-0.0481789666,-0.0184607604,-0.0558749733,-0.0178585589,-0.0095671198,-0.0317762533,-0.0119051121,0.0196855407,0.011124301]},"162":{"Abstract":"Translucent objects such as fog, clouds, smoke, glass, ice, and liquids are pervasive in cinematic environments because they frame scenes in depth and create visually-compelling shots. Unfortunately, they are hard to render in real-time and have thus previously been rendered poorly compared to opaque surfaces. This paper introduces the first model for a real-time rasterization algorithm that can simultaneously approximate the following transparency phenomena: wavelength-varying (\u201ccolored\u201d) transmission, translucent colored shadows, caustics, volumetric light and shadowing, partial coverage, diffusion, and refraction. All render efficiently with order-independent draw calls and low bandwidth. We include source code.","Authors":"M. McGuire; M. Mara","DOI":"10.1109\/TVCG.2017.2656082","Keywords":"Transparency;order-independent;refraction;caustic;shadow;particle;volumetric","Title":"Phenomenological Transparency","Keywords_Processed":"transparency;shadow;caustic;refraction;order independent;particle;volumetric","Keyword_Vector":[0.1583151163,-0.1610764715,-0.1551461354,0.2238161857,-0.0538457491,0.0154334834,0.054357318,0.1197255848,-0.0159613365,-0.0228069551,-0.0660361187,-0.0103270169,-0.0362806276,-0.0923225788,0.0683263396,-0.1197533867,-0.0240229414,0.0382724482,-0.0066142824,-0.018572041,0.040171615,-0.0725576372,-0.0210453312,-0.0048975765,-0.1040425032,-0.0197861832,0.0481277105,-0.0534698117,-0.0069342552,-0.0371444579,0.0223942146,-0.0280947519,0.036437445,0.0513303986,-0.0437315121,-0.0722905067,-0.043422183,-0.0414469313,0.0235582281,-0.0383181045,-0.0045470726,-0.0494621769,0.0414729327,-0.0678403022,0.0020165477,-0.0502673745,-0.0272603931,0.0444415724,-0.0182229598,0.0354284785,0.0260268089,0.0400112501],"Abstract_Vector":[0.2770002028,-0.0772724784,-0.0045991247,-0.0153715374,-0.0135954364,-0.0846851119,0.0041049251,-0.0025650037,0.0151547294,-0.0022876965,0.0490932434,-0.0481146616,-0.0158365324,0.0185577295,-0.0662512437,-0.0267278395,-0.0334493915,-0.0157832179,-0.0085439186,-0.0125902742,0.0019215413,0.0216174676,-0.0082415038,0.030131875,-0.0280537831,-0.0151003454,0.0121862039,0.0039268115,0.0094762613,-0.0452329851,-0.023406458,-0.0081302266,-0.0669195899,-0.0249888682,0.0451498561,-0.0437440107,-0.0169037001,-0.0230389373,0.034778093,0.0120398579,0.0237482471,-0.0145546535,-0.0068930676,0.0539197284,0.0348426417,-0.0299294021,0.0298072021,0.0479623582,-0.0338388541,-0.0457645692,0.0133398688,0.0395695186,-0.0274158458,-0.0523711847,0.0445793453,-0.0259542823,-0.0376502594,-0.0127303077,-0.0213282201,-0.0133138791,-0.0019762642,-0.0390146368,0.0001538408,0.0342873147,0.0026966708,0.0162480441,-0.0008770382,0.0000235124,0.013147211,-0.0119302639,0.0128872781,0.0159000388,0.0437461956,-0.0391134386,-0.0079965888,-0.0002282485,-0.0131754144,0.0243274296,-0.0222286917,-0.0122423664,-0.0004328337,-0.0126921318,0.001887209,-0.0258078237,0.0598539632,-0.0526009513,-0.0145506787,0.0134880147,-0.0083658285,0.0351583795,-0.0067760298,-0.0256864348,-0.0296413885,0.0289095466,0.0479738495,0.0557587391,0.0262242144,-0.0207807894,-0.0432865291,-0.0297927284,-0.0056387843,0.0091268678,-0.0259690142,0.0317047684,0.0052493031,-0.0041361473,0.0402420472,-0.0091913549,0.0033434002,-0.0014218041,-0.0357322715,0.0112398089,-0.0036157007,-0.0280930857,-0.0262117784,0.0007294893]},"163":{"Abstract":"We present a voxel-based rendering pipeline for large 3D line sets that employs GPU ray-casting to achieve scalable rendering including transparency and global illumination effects. Even for opaque lines we demonstrate superior rendering performance compared to GPU rasterization of lines, and when transparency is used we can interactively render amounts of lines that are infeasible to be rendered via rasterization. We propose a direction-preserving encoding of lines into a regular voxel grid, along with the quantization of directions using face-to-face connectivity in this grid. On the regular grid structure, parallel GPU ray-casting is used to determine visible fragments in correct visibility order. To enable interactive rendering of global illumination effects like low-frequency shadows and ambient occlusions, illumination simulation is performed during ray-casting on a level-of-detail (LoD) line representation that considers the number of lines and their lengths per voxel. In this way we can render effects which are very difficult to render via GPU rasterization. A detailed performance and quality evaluation compares our approach to rasterization-based rendering of lines.","Authors":"M. Kanzler; M. Rautenhaus; R. Westermann","DOI":"10.1109\/TVCG.2018.2834372","Keywords":"Ray-casting;large 3D line sets;transparency;global illumination","Title":"A Voxel-Based Rendering Pipeline for Large 3D Line Sets","Keywords_Processed":"transparency;ray cast;global illumination;large 3d line set","Keyword_Vector":[0.0855482605,-0.0785580605,0.0021750178,-0.1496888026,0.1743812886,0.0358329262,0.3555638457,-0.0969398344,-0.135527868,0.0054879029,-0.0613156302,0.1280996906,0.0711417936,0.0377601729,0.0016181858,-0.0280244532,0.0135352391,-0.0086077645,-0.0114679591,-0.0269167612,0.1071711906,0.0707686313,0.0625198061,0.0351394098,-0.100232559,-0.0054442825,0.0828569145,-0.0987308585,-0.1169844206,0.0435597123,0.0423395948,-0.0727060922,0.0275033923,0.0141209055,0.0365270155,-0.0169988692,0.0130915877,0.0129983509,-0.0540344203,0.042350847,0.0079750764,-0.0080575424,0.0826790628,-0.0767088339,-0.0080919042,0.0376837626,0.0527561939,-0.1061677831,-0.0417149185,-0.1073973111,0.0371629885,-0.0051494664],"Abstract_Vector":[0.1936784769,-0.0483801431,-0.0412170548,0.258644539,0.1542451643,0.1235369486,-0.0008273743,-0.0098545677,-0.0355598899,0.0814662743,0.0278537642,0.0480602248,-0.0594561762,-0.1138555209,0.2143453022,0.1205544739,-0.1136670278,0.0775095802,-0.1465181059,-0.0858866862,-0.1013927092,0.1085924223,-0.0098660457,0.051946665,-0.08922785,-0.066732775,-0.0201926839,-0.0297894998,0.071665156,-0.0499942777,0.012529703,-0.0425912512,0.0020284022,-0.0245438734,0.086070034,-0.0131187257,-0.0081114039,0.0772236153,-0.0540992457,0.0101606384,-0.0004477039,-0.0492544072,0.0313976507,0.032785374,-0.0133162067,0.05354906,0.0341368623,-0.0056718592,-0.1068558733,0.0352009426,-0.0037760702,0.0658311642,0.0099081191,-0.0410678937,0.0209527772,-0.0118076032,0.0583794029,0.0248548021,-0.0258802802,0.0009923565,0.0140310789,-0.0594402515,0.0078250304,-0.0153685186,-0.0181920865,0.0067947207,-0.0339714753,0.0367461149,0.0236249705,-0.0467508595,-0.0240956854,0.0137017147,0.0506819655,0.0249067018,-0.0045823627,-0.0284148135,-0.0117512967,0.0685388068,0.0366909051,0.0181606637,0.0477940765,0.0016026275,-0.0019486387,0.0094939197,-0.0109481592,-0.0272188746,0.0570459027,-0.0372253539,-0.0163000104,-0.033700832,-0.0891493637,-0.0214165481,-0.0195730041,-0.0670920663,0.0255780307,-0.0081155109,-0.00195829,-0.0130818372,-0.003648796,0.0016344117,-0.0199780671,0.0457610522,-0.0350713865,-0.0223037629,0.0221613148,0.036286241,-0.0082839049,-0.0123353433,0.0057498638,0.0174880918,-0.0095201465,-0.0287489454,-0.0142362285,-0.0138089695,-0.0134807948,-0.0360347907]},"164":{"Abstract":"The normals of feature points, i.e., the intersection points of multiple smooth surfaces, are ambiguous and undefined. This paper presents a unified definition for point cloud normals of feature and non-feature points, which allows feature points to possess multiple normals. This definition facilitates several succeeding operations, such as feature points extraction and point cloud filtering. We also develop a feature preserving normal estimation method which outputs multiple normals per feature point. The core of the method is a pair consistency voting scheme. All neighbor point pairs vote for the local tangent plane. Each vote takes the fitting residuals of the pair of points and their preliminary normal consistency into consideration. Thus the pairs from the same subspace and relatively far off features dominate the voting. An adaptive strategy is designed to overcome sampling anisotropy. In addition, we introduce an error measure compatible with traditional normal estimators, and present the first benchmark for normal estimation, composed of 152 synthesized data with various features and sampling densities, and 288 real scans with different noise levels. Comprehensive and quantitative experiments show that our method generates faithful feature preserving normals and outperforms previous cutting edge normal estimation methods, including the latest deep learning based method.","Authors":"J. Zhang; J. Cao; X. Liu; H. Chen; B. Li; L. Liu","DOI":"10.1109\/TVCG.2018.2827998","Keywords":"Normal estimation;point clouds;sharp feature;subspace;sampling anisotropy","Title":"Multi-Normal Estimation via Pair Consistency Voting","Keywords_Processed":"sharp feature;normal estimation;sample anisotropy;point cloud;subspace","Keyword_Vector":[0.0199806084,0.0035976023,0.0137377537,0.0020739478,-0.0034960426,-0.0062873798,0.0005034354,0.0061569551,-0.0122497699,0.0048691707,-0.0192622375,0.0111164832,0.0173600235,0.0083742516,0.0182836793,-0.0021971781,0.0112303948,-0.0071986389,-0.0036048126,0.0061287787,-0.005420245,-0.0119905219,0.0035533237,0.0009670126,-0.0079068256,0.0043244477,0.0237318278,0.0092477071,-0.0104048348,-0.0087230031,0.0336965696,-0.0070435995,-0.0044265231,0.0158430115,-0.0011574756,0.0069150463,0.0342985229,0.0421418701,0.003536599,-0.0114558769,0.0085326009,0.0248893195,-0.0533385723,-0.0321104972,-0.0240580636,0.0182694669,0.033528076,0.0156477511,-0.0446357147,0.0134162908,-0.0055079371,0.0450419074],"Abstract_Vector":[0.1002817196,0.0254049853,0.0127614925,0.0457668509,-0.0051369272,0.0186852861,-0.0190950524,0.012616891,0.0099630847,-0.0047152371,-0.0124640293,0.0190209853,-0.0062589664,-0.010356375,-0.0146113578,0.0062624125,0.0019932896,0.0308324256,-0.0006860865,-0.021549865,-0.0101116149,0.0068269663,0.0013592302,0.0245196687,-0.0275675575,0.0201540824,0.0134614819,0.0269005086,-0.0053179878,0.0270408031,0.0302344102,0.0078204823,-0.0330667653,-0.0082131578,0.006368708,0.0099818086,0.0013905961,0.018410061,-0.0182658581,0.0038617537,-0.0006118755,-0.0119818627,0.0101346907,-0.0178215625,-0.0302221911,-0.000916265,0.0110122886,-0.0042169038,0.0120770953,0.0198487513,0.002351312,-0.0130244259,0.0268188611,0.0080097901,0.0152772902,0.0134201546,0.0110840433,0.0256014904,0.0142511094,0.0040051285,0.02387307,-0.0015717312,-0.0034661712,-0.0253643845,0.0083761535,-0.0035052103,0.0263290083,0.0075505801,0.0217649535,0.0206406592,-0.0332305376,0.0080340681,-0.0156725503,-0.0302665487,0.0133814146,-0.026984026,0.0057106078,0.0271428341,-0.0278978222,-0.0066320738,-0.0071301389,-0.0065389465,0.0200141112,0.0083556435,-0.018138646,0.0273593293,-0.0051765175,0.0192305608,-0.0254813603,0.0011657689,-0.012404314,0.0117769762,0.0038529714,0.0054774079,-0.0021578902,-0.0061372981,0.0203404589,0.0169518686,-0.0017251616,-0.0100234019,-0.0289169246,-0.0007861309,-0.0090315564,-0.0279379767,0.0029448459,-0.0123309935,-0.02082048,0.0075923175,0.0067153924,-0.0144945151,0.0010936914,-0.0265366551,0.0098427713,0.0127521913,0.0235823835,0.0017017646]},"165":{"Abstract":"In this article, we investigate methods for suggesting the interactivity of online visualizations embedded with text. We first assess the need for such methods by conducting three initial experiments on Amazon's Mechanical Turk. We then present a design space for Suggested Interactivity (i. e., visual cues used as perceived affordances-SI), based on a survey of 382 HTML5 and visualization websites. Finally, we assess the effectiveness of three SI cues we designed for suggesting the interactivity of bar charts embedded with text. Our results show that only one cue (SI3) was successful in inciting participants to interact with the visualizations, and we hypothesize this is because this particular cue provided feedforward.","Authors":"J. Boy; L. Eveillard; F. Detienne; J. Fekete","DOI":"10.1109\/TVCG.2015.2467201","Keywords":"Suggested interactivity;perceived affordances;information visualization for the people;online visualization;Suggested interactivity;perceived affordances;information visualization for the people;online visualization","Title":"Suggested Interactivity: Seeking Perceived Affordances for Information Visualization","Keywords_Processed":"information visualization for the people;perceive affordance;online visualization;suggest interactivity","Keyword_Vector":[0.034895676,0.0123159411,-0.0157905187,-0.0213368017,0.0159154268,-0.0420199146,-0.0525708142,-0.0064838659,-0.0984777134,-0.0315703459,0.0318612919,0.0801915764,0.012563888,0.0285066793,0.0745079924,0.072285392,0.0427029672,0.0718078699,0.1722439146,-0.0042921515,-0.0892235297,0.0488848286,0.1064475044,0.0600529849,-0.0930235938,-0.0296670163,-0.0199887876,0.0151644197,0.083392641,-0.0910301097,-0.0091939591,-0.0907276667,0.0816837265,0.0637909696,-0.0465616401,0.0169018693,-0.0935952449,0.0002195117,-0.004757637,-0.0942080758,-0.0240468788,0.0690116936,0.0010937975,-0.0095072,-0.010627416,-0.0333145263,0.0944318723,0.0365240741,-0.0635525292,0.0554130353,0.0058001688,0.0329904005],"Abstract_Vector":[0.1659988763,-0.0706621425,0.0027798215,0.0172442915,-0.0714969105,0.0725985722,-0.0327738862,0.0080002949,0.0210187034,0.0092248337,-0.0325808738,0.0302081593,-0.0103315453,0.0075314154,-0.0079480103,0.0712527054,-0.0035823873,-0.0129645632,0.0562745169,0.0050907048,-0.0063369803,-0.0474116385,0.0001890699,0.0215275169,-0.0212661522,-0.0231686151,-0.0320407572,-0.0208391557,0.0159101298,0.0176900158,0.0391934513,0.0169599786,0.0151406709,0.0019978518,0.0601955703,-0.0438925555,0.009654322,-0.0184295882,-0.0192815878,0.0000462139,0.0520249951,0.082533834,0.0261271671,0.0097790264,0.0049269771,-0.0208889624,0.0216759035,0.0099340349,-0.0185810905,-0.0357860439,-0.001156116,-0.0342352735,0.0464845857,0.005167795,0.0358950988,-0.0003002096,0.0443829252,0.0158196887,-0.0436894337,0.0069646379,0.0208808001,0.0120251214,0.0094225116,0.0210432035,-0.0196328041,-0.038396896,-0.0041882406,-0.0391945854,0.0048447359,0.0629229867,0.0114122576,0.0194216819,0.0412639186,0.0446536823,0.0306728967,-0.0063998463,0.0011210814,-0.0385468718,0.0274677998,-0.0085774529,-0.0352604341,-0.0028047667,0.0244534995,-0.0139885562,0.0111849403,-0.0543783626,0.0050693371,-0.0032193413,-0.0258378449,-0.0151054394,-0.0064239466,-0.0517937252,-0.0148905385,0.0031771152,0.0287862205,-0.0303332105,-0.0111336839,0.0362187839,0.0043453137,0.0028548546,-0.0045723123,0.024657066,0.0126308618,-0.0069493772,-0.0052994094,-0.0194705915,-0.0061435652,-0.0059399685,-0.016466053,-0.0131372904,0.0018315742,0.0173583009,0.0255524539,-0.0367538639,-0.0001921524,-0.0068822012]},"166":{"Abstract":"The analysis of protein-ligand interactions is a time-intensive task. Researchers have to analyze multiple physico-chemical properties of the protein at once and combine them to derive conclusions about the protein-ligand interplay. Typically, several charts are inspected, and 3D animations can be played side-by-side to obtain a deeper understanding of the data. With the advances in simulation techniques, larger and larger datasets are available, with up to hundreds of thousands of steps. Unfortunately, such large trajectories are very difficult to investigate with traditional approaches. Therefore, the need for special tools that facilitate inspection of these large trajectories becomes substantial. In this paper, we present a novel system for visual exploration of very large trajectories in an interactive and user-friendly way. Several visualization motifs are automatically derived from the data to give the user the information about interactions between protein and ligand. Our system offers specialized widgets to ease and accelerate data inspection and navigation to interesting parts of the simulation. The system is suitable also for simulations where multiple ligands are involved. We have tested the usefulness of our tool on a set of datasets obtained from protein engineers, and we describe the expert feedback.","Authors":"D. Duran; P. Hermosilla; T. Ropinski; B. Kozl\u00edkov\u00e1; \u00c1. Vinacua; P. V\u00e1zquez","DOI":"10.1109\/TVCG.2018.2864851","Keywords":"Molecular visualization;simulation inspection;long trajectories","Title":"Visualization of Large Molecular Trajectories","Keywords_Processed":"molecular visualization;simulation inspection;long trajectory","Keyword_Vector":[0.1086209229,-0.0288148254,0.0120868791,-0.0307082128,0.0313831347,0.0056696239,0.0124725766,0.0243617102,-0.016679658,-0.0571824628,0.0016502025,0.0228859748,-0.0210890023,0.0326368574,-0.0043554041,0.0213655944,0.0118758878,0.039788572,0.044583374,-0.0184078781,-0.0020528915,0.0305493634,-0.0942735346,-0.0424496725,0.0388385721,0.0215941923,0.0292355736,-0.0087920452,0.0383058678,0.0052387043,0.0276864673,0.0431249822,0.0098743656,0.0706178615,0.0051238587,0.0642942327,0.0537220763,-0.024150887,-0.0357966549,0.0094981121,0.0067090543,0.0481209258,-0.0103667963,-0.0307215338,0.0245956803,-0.0371021861,0.0442231976,0.0458027638,0.0402015526,0.038963216,-0.0065499602,-0.0657537928],"Abstract_Vector":[0.18323214,-0.1323501521,0.0020580328,0.0151668855,0.0148917862,-0.0142383875,-0.0450456251,-0.0292744123,-0.0031825212,0.0052657737,0.0191170955,-0.0592033113,-0.0228972693,-0.0002839836,-0.0070100316,0.0252120605,-0.0458454941,-0.0188699764,0.0506256229,0.0161579611,0.0318562343,-0.0581451821,-0.0031285547,-0.0579746477,0.004186942,-0.0127185527,-0.0115693677,-0.0258737985,0.0035197121,-0.021069259,-0.0054929489,-0.0468979495,-0.0326152168,0.0528067933,-0.049634784,-0.0588045295,-0.0041840896,-0.000932507,-0.0522949707,-0.0200197666,-0.0184117294,-0.0096488038,0.0292873426,-0.0448245801,0.0234239848,-0.0695723343,0.029325465,0.058952876,-0.0105404682,-0.0396616807,-0.0532873972,-0.0421475343,0.0424257973,-0.0127768765,-0.0243464519,-0.0270817636,0.011554094,-0.0166670989,-0.0133769621,0.0088813503,0.0888088549,-0.0047291634,-0.0375876488,0.0725139011,0.0037013528,-0.0025109878,0.0346314187,0.0675469014,-0.0548247494,-0.0417960574,0.0360955135,0.0060881083,0.0267481742,0.039963769,-0.0063091789,0.0297611352,0.0185199715,-0.0288794273,-0.0468691554,0.0609653264,-0.04894495,-0.081841326,0.0576979027,0.0241487207,0.0165429629,0.0801665789,-0.0059840744,-0.0516563499,-0.0007149993,0.0304805419,0.0080965576,0.0344717039,-0.0221809251,-0.0378599959,0.0128200292,0.0258659463,0.0614622038,0.0270997296,-0.0026171507,0.0308141955,-0.0245689238,-0.0507742691,-0.0293189731,-0.0293836045,0.0108113801,-0.0262673292,0.0034366772,0.0088297093,0.0169689277,-0.0193564367,0.0252937317,-0.0309257343,-0.0322549114,-0.0494047482,0.0099193518,0.0119883716]},"167":{"Abstract":"Visualization designers regularly use color to encode quantitative or categorical data. However, visualizations \u201cin the wild\u201d often violate perceptual color design principles and may only be available as bitmap images. In this work, we contribute a method to semi-automatically extract color encodings from a bitmap visualization image. Given an image and a legend location, we classify the legend as describing either a discrete or continuous color encoding, identify the colors used, and extract legend text using OCR methods. We then combine this information to recover the specific color mapping. Users can also correct interpretation errors using an annotation interface. We evaluate our techniques using a corpus of images extracted from scientific papers and demonstrate accurate automatic inference of color mappings across a variety of chart types. In addition, we present two applications of our method: automatic recoloring to improve perceptual effectiveness, and interactive overlays to enable improved reading of static visualizations.","Authors":"J. Poco; A. Mayhua; J. Heer","DOI":"10.1109\/TVCG.2017.2744320","Keywords":"Visualization;color;chart understanding;information extraction;redesign;computer vision","Title":"Extracting and Retargeting Color Mappings from Bitmap Images of Visualizations","Keywords_Processed":"computer vision;chart understanding;color;visualization;redesign;information extraction","Keyword_Vector":[0.0543300342,-0.0076536489,0.0379979243,0.0432580201,-0.0250482008,0.0468252982,-0.0005677296,-0.0459269114,0.0059164567,0.0509662788,0.0254443213,0.0038478902,-0.0303948173,0.0181725695,0.0730976195,0.1196667175,0.0097801659,-0.0013023269,-0.0723365673,0.0096934568,0.0051654782,0.0044182198,0.0157889397,0.0714810249,-0.0225456668,0.023012923,-0.0044031317,-0.0482086145,-0.0404965045,-0.0007402527,-0.0392263835,-0.0140686269,-0.0114778245,0.0145253254,0.0164343889,0.0292076363,0.0175714587,0.0329764089,-0.0188158073,-0.015146248,0.0002800585,-0.0250809104,-0.0285788722,0.0045685136,-0.0033979925,-0.0809165302,-0.0032785752,0.0505694459,0.0103106086,-0.0094687971,-0.0086533758,-0.0026715015],"Abstract_Vector":[0.2316329508,-0.001303687,-0.0722028891,-0.007758909,-0.0095977108,0.01990374,0.0934891092,0.0302139992,0.0030115173,0.0662773572,0.034677762,-0.0741520803,0.0057933429,0.0018683794,0.0363722047,-0.0176743056,0.0292086085,0.0327971429,0.0198723486,0.0194317336,-0.0330186693,-0.0031741381,0.0125073706,0.0601977348,-0.0225766014,-0.0288183607,-0.0016707499,0.0159036433,-0.0149726725,0.0332006688,-0.0051424989,0.0172905416,-0.0191857121,-0.0029061291,-0.0115007533,-0.0257376391,0.0470526245,-0.0138542009,0.009204458,-0.0178503519,0.0269478693,0.0323248489,0.0483712384,0.0815099802,0.0868946226,0.031594465,0.0384840101,0.0726991313,0.0401179822,-0.0158392621,0.0167825014,0.0506341587,-0.0296679234,-0.0125505949,0.0175536223,0.0071289013,-0.021966064,-0.0464736904,-0.0295469914,0.0050329825,0.0205150939,0.0032009595,-0.009724662,0.0042885996,0.0079704012,-0.0184265444,-0.0009123686,0.0038206931,-0.0184997763,0.0034153593,0.0252916551,-0.0021122353,0.0110982927,0.0039316252,-0.0353711986,-0.0162225278,-0.0091762508,0.04051285,-0.0090501718,-0.0207998016,0.0009284035,0.0054746709,0.0165460528,-0.0378750587,0.0066476662,-0.0169325952,0.0158503977,0.0024743265,-0.0003941938,-0.0064693219,0.0020107918,-0.0038151818,-0.0283399205,0.0415243534,-0.0346740268,-0.0380767404,-0.015441884,-0.0190313787,-0.021237549,0.0424114642,0.0180900159,0.0099081447,-0.0043789566,-0.0045179693,-0.0344973622,-0.0344402345,-0.0308056917,0.0259268824,-0.0216025679,0.0259439941,-0.0099292146,0.0171105547,-0.003092925,-0.0149653773,0.0179484589,-0.0175588173]},"168":{"Abstract":"Scientific visualization developed successful methods for scalar and vector fields. For tensor fields, however, effective, interactive visualizations are still missing despite progress over the last decades. We present a general approach for the generation of separating surfaces in symmetric, second-order, three-dimensional tensor fields. These surfaces are defined as fiber surfaces of the invariant space, i.e. as pre-images of surfaces in the range of a complete set of invariants. This approach leads to a generalization of the fiber surface algorithm by Klacansky et al. [16] to three dimensions in the range. This is due to the fact that the invariant space is three-dimensional for symmetric second-order tensors over a spatial domain. We present an algorithm for surface construction for simplicial grids in the domain and simplicial surfaces in the invariant space. We demonstrate our approach by applying it to stress fields from component design in mechanical engineering.","Authors":"F. Raith; C. Blecha; T. Nagel; F. Parisio; O. Kolditz; F. G\u00fcnther; M. Stommel; G. Scheuermann","DOI":"10.1109\/TVCG.2018.2864846","Keywords":"visualization;tensor field;invariants;fiber surface;interaction","Title":"Tensor Field Visualization using Fiber Surfaces of Invariant Space","Keywords_Processed":"invariant;interaction;fiber surface;visualization;tensor field","Keyword_Vector":[0.123643725,-0.077102588,0.0016524739,0.0776261372,-0.1015780775,-0.1603292069,0.0030295236,-0.0054194142,0.0632386782,-0.093294986,-0.0448359704,0.1102331275,0.059142148,-0.0758771742,-0.0185193356,-0.0603784157,0.0690897001,-0.0814664399,-0.0092974311,-0.0303540272,-0.0175025164,0.0085302289,-0.0026675617,0.0113966935,-0.0287172277,0.0540624757,-0.0355125702,-0.0651374844,0.0173324354,0.0235833807,-0.0872097627,-0.0586535438,-0.0329667009,0.0237030976,0.0459378416,0.0154649602,-0.0725993147,0.0793016315,0.0265866333,0.0295298948,0.0714782575,-0.0374681028,-0.1078285671,0.0852965237,0.0647745343,-0.033825511,0.0129995566,-0.0056853832,-0.0280082077,0.0033011002,-0.0003568289,-0.0194007405],"Abstract_Vector":[0.2605807678,-0.0397234645,0.2498505045,-0.0783633139,0.0896784631,0.0819736547,0.2223615669,-0.1532753667,-0.021268053,-0.1100085637,0.0012482672,-0.0483620515,-0.1097454189,0.0350442872,-0.0746169686,0.1514423191,-0.1135665729,-0.0274326823,0.0126915034,0.1198991287,0.0327017229,0.0074118912,0.0357147391,-0.0293123703,-0.1081026584,-0.0335067275,0.0983455958,-0.0498880433,-0.0830074555,0.0080348995,-0.0103105703,0.084324101,0.0657802217,0.0457394985,-0.0357347985,0.04834958,-0.000680548,-0.0495850105,-0.0403153165,0.0174237092,0.0321598681,0.0399021941,-0.0028966217,-0.0674127254,0.0395080563,0.009803885,-0.06416906,0.0354639273,0.0125571029,0.0283643451,-0.0011022173,0.052178967,-0.0485600108,-0.0434878147,0.029861741,-0.0424183903,-0.0099910484,-0.0031932978,-0.0208546437,0.0081167991,0.014001581,-0.0751747438,0.0676141834,0.0065211866,-0.0147059504,-0.0075810394,0.0105594962,0.0062881127,-0.0232889302,-0.0023740179,-0.0065041925,-0.0110290685,-0.0307393923,-0.0200279066,-0.0684339253,0.0013833344,-0.0066832554,0.0070900548,0.0221475803,-0.0201992388,0.0221887309,-0.0320238654,-0.0012157195,0.0037540811,0.0100957423,-0.014085906,-0.0103511621,0.014245769,-0.014754505,-0.0462087462,-0.0282690935,0.0205187179,0.0125721754,-0.0121092458,-0.0201827107,0.0260385236,-0.0014582444,-0.0042074942,0.0058397875,-0.0319753833,-0.0161762296,-0.0192163884,-0.0257724595,-0.0264937588,-0.0135914454,-0.0058694361,-0.0304797617,0.0107533074,0.034010892,0.0262701086,0.0007964526,-0.0127376134,0.0271201328,0.0211159079,-0.0257797769,-0.0041167362]},"169":{"Abstract":"Effective communication using visualization relies in part on the use of viable encoding strategies. For example, a viewer's ability to rapidly and accurately discern between two or more categorical variables in a chart or figure is contingent upon the distinctiveness of the encodings applied to each variable. Research in perception suggests that color is a more salient visual feature when compared to shape and although that finding is supported by visualization studies, characteristics of shape also yield meaningful differences in distinctiveness. We propose that open or closed shapes (that is, whether shapes are composed of line segments that are bounded across a region of space or not) represent a salient characteristic that influences perceptual processing. Three experiments were performed to test the reliability of the open\/closed category; the first two from the perspective of attentional allocation, and the third experiment in the context of multi-class scatterplot displays. In the first, a flanker paradigm was used to test whether perceptual load and open\/closed feature category would modulate the effect of the flanker on target processing. Results showed an influence of both variables. The second experiment used a Same\/Different reaction time task to replicate and extend those findings. Results from both show that responses are faster and more accurate when closed rather than open shapes are processed as targets, and there is more processing interference when two competing shapes come from the same rather than different open or closed feature categories. The third experiment employed three commonly used visual analytic tasks - perception of average value, numerosity, and linear relationships with both single and dual displays of open and closed symbols. Our findings show that for numerosity and trend judgments, in particular, that different symbols from the same open or closed feature category cause more perceptual interference when they are presented together in a plot than symbols from different categories. Moreover, the extent of the interference appears to depend upon whether the participant is focused on processing open or closed symbols.","Authors":"D. Burlinson; K. Subramanian; P. Goolkasian","DOI":"10.1109\/TVCG.2017.2745086","Keywords":"scatterplot;visualization design;perceptual category;open shape;closed shape","Title":"Open vs. Closed Shapes: New Perceptual Categories?","Keywords_Processed":"close shape;visualization design;perceptual category;open shape;scatterplot","Keyword_Vector":[0.1228059698,-0.161960705,-0.1402741285,0.0855968086,-0.0938874435,-0.1720774721,0.1648205476,0.0061576186,0.0669647476,-0.046990469,0.0135758018,0.0213627885,-0.0156149695,-0.107603965,0.0611672178,0.0617992447,0.0038265561,0.0065431023,-0.030314488,-0.0830778893,0.0104737999,0.066480181,0.0275609996,-0.0274621382,0.159195595,-0.0410089884,0.0105797518,0.0569059977,-0.1018465516,-0.1059429477,-0.0272977233,-0.0499179086,0.0733161921,0.0171973303,-0.047284588,-0.0253706025,-0.0069824927,-0.0436711868,0.1176277813,-0.0176941882,0.0326361773,0.040920866,-0.0275110253,-0.0904847584,0.0422269547,-0.0127357192,-0.0278445707,0.0011130176,0.0148426959,0.028430285,0.0388779827,0.042875128],"Abstract_Vector":[0.1725806454,-0.1311072617,0.0272288053,0.0614035076,0.035531713,0.0318791288,0.0295426728,-0.0211197407,-0.1016153075,0.0999830963,0.0701076627,-0.1277129274,-0.0981099678,0.073542969,0.1521213683,-0.0465921627,-0.0302506734,0.0483546789,0.0218694368,-0.0439798121,-0.0332604384,0.0263412366,-0.011716406,-0.0194102358,-0.0072192426,0.0132606621,0.0102852409,-0.0010955302,0.0108298456,-0.0052441433,0.0038441213,0.0048833799,-0.0229924487,-0.015031492,0.007430643,0.0181787985,-0.0130829599,0.0066806436,-0.0102970831,0.0225824638,-0.0285842043,0.0411028768,-0.0159356818,0.0146566951,0.0184160729,-0.021446985,0.0230924895,-0.0350539662,-0.0339158607,-0.003489243,0.0199790776,0.0379989678,-0.0051279664,-0.0019713733,0.0184538675,0.003736729,0.0332372882,-0.0068628441,0.0006871598,0.0418767064,0.0000555149,0.0160127499,-0.0135361394,-0.0228178337,0.0433513877,-0.0064837361,-0.0125605257,0.006606253,-0.02782585,0.0171749771,0.0189446633,0.027555662,0.0346914599,-0.0258613252,0.016290401,0.0113189596,-0.0095818732,0.0193200936,-0.0163149448,0.0106189762,0.0365638854,0.0111316937,-0.0072363688,-0.0312578584,0.0255676421,0.0076572567,0.0039056114,0.0172649431,0.0206495265,-0.0012961289,0.0036801411,-0.0223171929,0.0029119668,-0.036918319,0.0287662719,-0.0263314723,0.0311938192,-0.0141332593,-0.0190141374,-0.0047831052,-0.01011351,0.0277630443,-0.0255263318,0.0181091147,0.0038174803,0.0075915618,-0.0039178262,0.024038082,0.0037329743,0.0108708874,0.0195685189,0.0172767535,0.0040324058,-0.0078840933,-0.0086097177,-0.0088901153]},"17":{"Abstract":"Temporal data visualization is used to analyze dependent variables that vary over time, with time being an independent variable. Visualizing temporal data is inherently difficult, due to the many aspects that need to be communicated to the users (e.g., time and variable changes). This is an important topic in visualization, and a wide range of visualization techniques dealing with different tasks have already been designed. In this paper we propose popup-plots, a novel concept where the common interaction of 3D rotation is used to navigate through the data. This allows the users to view the data from different perspectives without having to learn and adapt to new interaction concepts. Popup-plots are therefore a novel method for visualizing and interacting with dependent variables over time. We extend 2D plots with the temporal information by bending the space according to the time. The bending is calculated based on a spherical coordinates approach, which is continuously influenced by the viewing direction towards the plot. Hence, the plot can be viewed from various angles with seamless transitions in between, offering the possibility to analyze different aspects of the represented data. As the current viewing direction is inherently depicted by the shape of the data, the users are able to deduce which part of the data is currently viewed. The temporal information is encoded into the visualization itself, resembling annual rings of a tree. We demonstrate our method by applying it to data from two different domains, comprising measurements at spatial positions over time, and we also evaluated the usability of our solution.","Authors":"J. Schmidt; D. Fleischmann; B. Preim; N. Br\u00e4ndle; G. Mistelbauer","DOI":"10.1109\/TVCG.2018.2841385","Keywords":"Temporal data;time-dependent visualization;3D plots;ellipsoidal coordinate system","Title":"Popup-Plots: Warping Temporal Data Visualization","Keywords_Processed":"temporal datum;3d plot;ellipsoidal coordinate system;time dependent visualization","Keyword_Vector":[0.0340731439,0.0123350501,-0.0145772217,-0.011119341,0.0305807441,-0.0279787115,-0.0519441402,0.0130839654,-0.1094931353,-0.0214633318,0.0463257934,0.0823676243,-0.0002413311,0.0216203073,0.0740329276,0.1050266897,0.0092169785,0.0163446042,0.281031227,-0.0310461954,-0.0643119512,0.0027132137,0.1953114537,0.0842194752,-0.0310896154,0.0466092302,-0.0745372253,-0.0515742846,0.0763357555,-0.1322465894,0.0239993004,-0.0936014396,0.0288107403,0.0192373064,-0.0444219402,-0.0611111071,-0.0232410428,-0.0142664062,-0.0120341138,-0.0089846555,-0.0164639145,0.0377646339,0.0356536554,0.0014960523,0.0021095963,-0.0195221755,0.025614101,0.0847762504,-0.0037484198,0.0356071353,-0.0495430441,0.0119340346],"Abstract_Vector":[0.1078754468,-0.0050425785,-0.0281741882,-0.0194196941,-0.0192143908,0.0589348839,0.0005971113,0.053256105,0.1084209132,-0.0169687096,-0.0423366642,-0.0072548008,-0.0273743537,-0.035075405,0.0271473532,-0.0493794556,0.0010104749,0.0724023128,0.0274758728,0.0964714708,0.0133504254,-0.007606859,-0.0355065735,-0.0108978368,-0.0083372424,-0.0057619225,0.02133034,-0.015137254,0.006934297,-0.0238892822,0.0307580896,-0.0391497538,-0.0019524868,0.0285227141,0.0330210317,0.0058086814,-0.0074938103,0.0131908923,0.0164708283,-0.0116803803,0.0021956187,0.0067672877,-0.0112285205,0.0079263022,0.0065065638,0.0030075835,-0.0217239413,-0.0190248611,0.0245612326,0.0008868504,-0.0159477215,-0.0201869095,-0.0019143986,-0.0414493511,0.0374096034,-0.0061139115,0.0070697467,-0.0123927513,-0.0125562369,0.0049510307,0.0222645072,0.006380965,0.0240618504,-0.0130800026,-0.0207600003,0.0196521121,0.0229654043,-0.028529603,-0.0087515906,0.0315106191,0.0110199986,-0.0244844307,0.012455149,-0.0307876612,-0.0178233725,-0.0205793527,0.0153196868,-0.0175097521,0.0376386235,-0.0063213807,0.0033574413,0.0473927223,0.0216550412,-0.0523192972,-0.0142508733,-0.0265137404,-0.0301918663,-0.0029588676,0.0278708586,-0.0090276745,-0.019079482,-0.0023009419,-0.0065713082,0.0231170653,0.0310289913,0.0105767782,0.0112823269,0.003475413,-0.0146467587,0.0175909321,-0.0284270677,-0.0192937744,-0.0164005258,0.0026018196,0.0262453542,-0.00441757,-0.0063600623,-0.0039416469,0.0023953825,0.003799302,-0.0121763833,-0.0273352484,0.0197890038,0.0041682363,0.0072448929,-0.0294425923]},"170":{"Abstract":"There are many ways to visualize event sequences as timelines. In a storytelling context where the intent is to convey multiple narrative points, a richer set of timeline designs may be more appropriate than the narrow range that has been used for exploratory data analysis by the research community. Informed by a survey of 263 timelines, we present a design space for storytelling with timelines that balances expressiveness and effectiveness, identifying 14 design choices characterized by three dimensions: representation, scale, and layout. Twenty combinations of these choices are viable timeline designs that can be matched to different narrative points, while smooth animated transitions between narrative points allow for the presentation of a cohesive story, an important aspect of both interactive storytelling and data videos. We further validate this design space by realizing the full set of viable timeline designs and transitions in a proof-of-concept sandbox implementation that we used to produce seven example timeline stories. Ultimately, this work is intended to inform and inspire the design of future tools for storytelling with timelines.","Authors":"M. Brehmer; B. Lee; B. Bach; N. H. Riche; T. Munzner","DOI":"10.1109\/TVCG.2016.2614803","Keywords":"Timelines;storytelling;narrative visualization;design space;animated transitions","Title":"Timelines Revisited: A Design Space and Considerations for Expressive Storytelling","Keywords_Processed":"animate transition;timeline;design space;narrative visualization;storytelle","Keyword_Vector":[0.1707825267,-0.2316196772,-0.1767215264,0.0678947524,-0.0491470784,-0.1299933181,0.3296235849,-0.0478499792,0.0188939253,-0.0753685428,0.0468389187,0.0107922454,0.0270114981,0.04468915,-0.0081570405,0.0374566061,0.0224422357,0.066012159,-0.0219932499,-0.1040854107,-0.01109561,0.033875281,0.0482036251,-0.0642497525,0.1445199226,-0.0265734805,-0.0073611309,0.0225443731,-0.0916247259,-0.0627362879,-0.0326382456,-0.0641759024,0.0459325008,-0.0240639557,-0.0666756728,-0.0372506325,-0.029199224,-0.0007215129,0.0629745576,-0.0630835902,0.0230814587,-0.0292346716,0.0201168448,-0.0340015836,0.0171614788,0.0289963645,-0.044231259,-0.0163599329,0.0016870114,-0.0186219694,-0.0008492175,0.008295819],"Abstract_Vector":[0.1437370967,-0.0673444836,-0.0075159625,0.0652894725,0.0151659623,0.0367970029,0.0294710905,0.0009961674,-0.0567598221,0.0457865587,0.0384424295,-0.0442512049,-0.057247686,0.0216798421,0.074819541,-0.02512806,-0.0249003146,0.0217228883,0.0098577668,-0.0095627646,-0.0075630164,0.0057940153,0.0068319228,0.0219173563,-0.0017644104,-0.0054123981,0.0067675173,0.0016361395,0.0251532574,0.0172752659,-0.0072012373,0.0213277741,-0.0319042948,-0.0129514587,-0.0145555235,0.0063502845,-0.0165951529,-0.0354040932,0.0162042992,0.010652722,0.0338038497,0.0254106034,0.0090485139,0.0137436723,-0.0022208835,0.0138426197,-0.0078003505,0.0393332107,0.0080485802,0.0307732254,0.0164228705,-0.0064394378,0.0041580257,-0.0185000811,0.027564454,0.0174450481,0.0090464962,0.0192087183,0.0342465659,-0.0255023107,-0.0143390193,0.0010922504,0.0171618995,-0.0160382975,-0.002701478,-0.0134880982,0.0125610668,0.0121013769,-0.0101784773,0.0210696236,0.0213513019,0.0053955368,0.0090776763,-0.0322564977,-0.0161075356,-0.0007829406,0.0057579834,0.0234111992,0.0048608919,-0.0013285251,-0.0109250528,0.0156383705,-0.0031062989,0.010760857,-0.0007351657,-0.016459694,0.005703033,0.0031040311,-0.0080454247,-0.0495556167,0.0204826003,-0.02169161,0.0291320007,-0.0050058453,-0.0271290309,0.0053864825,-0.009340895,0.0252800543,-0.0134455988,0.0108017515,0.0040657824,-0.0083713629,0.0117349403,0.0159909155,-0.0037079629,0.0327575049,0.014395157,0.0081586085,0.0298345871,0.017108156,0.0114378698,-0.0012934027,-0.0748719332,-0.0264072496,0.0254680401,0.0156645221]},"171":{"Abstract":"Transfer function (TF) design is a central topic in direct volume rendering. The TF fundamentally translates data values into optical properties to reveal relevant features present in the volumetric data. We propose a semi-automatic TF design scheme which consists of two steps: First, we present a clustering process within 1D\/2D TF domain based on the proximities of the respective volumetric features in the spatial domain. The presented approach provides an interactive tool that aids users in exploring clusters and identifying features of interest (FOI). Second, our method automatically generates a TF by iteratively refining the optical properties for the selected features using a novel feature visibility measurement. The proposed visibility measurement leverages the similarities of features to enhance their visibilities in DVR images. Compared to the conventional visibility measurement, the proposed feature visibility is able to efficiently sense opacity changes and precisely evaluate the impact of selected features on resulting visualizations. Our experiments validate the effectiveness of the proposed approach by demonstrating the advantages of integrating feature similarity into the visibility computations. We examine a number of datasets to establish the utility of our approach for semi-automatic TF design.","Authors":"B. Ma; A. Entezari","DOI":"10.1109\/TVCG.2017.2776935","Keywords":"Volume visualization;transfer function (TF);direct volume rendering (DVR);classification;feature visibility;multidimensional TFs;feature similarity;isovalue clustering;isosurfaces","Title":"Volumetric Feature-Based Classification and Visibility Analysis for Transfer Function Design","Keywords_Processed":"direct volume render dvr;feature visibility;classification;isosurface;isovalue clustering;multidimensional tf;feature similarity;transfer function tf;volume visualization","Keyword_Vector":[0.20196239,-0.2114686887,-0.1311147868,0.1478013621,-0.1429161622,-0.0002210703,0.0039426021,-0.0614870482,-0.0138924706,0.1185429806,0.0001312213,0.1330780369,0.0008235847,-0.0669791519,0.0400239559,0.1703000457,-0.0138277985,-0.0629345654,-0.1451072641,0.0516430623,0.0077075134,-0.0891166714,0.091021893,-0.0030166619,-0.0769774189,-0.0202161778,-0.0015578289,-0.0213054811,-0.0510498708,-0.0336264374,0.0067888447,0.1321486558,0.1057869349,0.1246581394,-0.025935155,0.0593548601,-0.0205485022,0.0493686068,0.0261797354,-0.0489853121,0.0100617184,-0.0009888786,-0.0173602682,0.0385891899,-0.0186878702,-0.1424132628,-0.0713668918,0.0008988183,0.0960774267,-0.0125267163,0.031489501,0.013777442],"Abstract_Vector":[0.2860281985,-0.1892071592,0.0332882189,-0.0191927091,-0.0579636067,-0.033169629,0.0202908995,0.0113452496,-0.0422666211,0.0363829776,-0.0021958803,-0.002335193,-0.0141422936,-0.0241451189,-0.0448141996,0.0101299091,-0.0142855025,-0.0588569184,0.0041449258,0.0102624235,0.0260779952,-0.0059312001,-0.0291168096,0.0002513849,-0.0473912503,-0.0179378539,0.004877887,-0.0324666503,-0.0164164583,-0.0371427186,0.0338301227,0.009368052,-0.1025887004,-0.0324013601,-0.007521956,-0.0032646735,-0.0506380999,-0.0080739878,0.0183775468,-0.0586633144,0.0418867487,-0.0040598339,-0.0207073723,0.0002723608,-0.0024416826,0.0178600464,0.0217133017,0.0257867274,-0.0507679542,-0.0055197412,0.0051897836,-0.0147443452,-0.0046006321,0.025678677,-0.0161008766,-0.0294580942,0.0744155642,-0.000395651,0.0110257414,0.0332260844,-0.0010935888,-0.0147907952,-0.0402159799,0.0123794569,-0.0033331699,-0.0229541442,-0.0273519997,-0.0089089307,-0.0030079484,-0.008019917,0.0370199131,0.0227024975,0.0298283242,-0.0421475261,-0.0408692362,-0.0160137618,-0.0181180103,0.0044861864,0.0256167181,0.0001895354,0.0052448994,-0.0183119264,-0.0831665912,-0.0286281709,-0.0560419463,-0.0118807237,-0.0099522419,-0.0388716892,0.0352005936,0.0810216726,-0.0430169765,-0.0333505653,0.0065810518,-0.0222401382,-0.0021519034,-0.0260543621,0.0068711077,-0.0390944071,0.0405455706,-0.0497780146,0.0065443701,0.0032921402,0.0005052964,-0.0088984346,0.0232439264,0.0045333324,-0.00668529,0.0313746544,-0.020205813,0.0013314438,-0.0214327983,0.1030120786,-0.0089831615,0.0101211479,0.026313603,0.0419682705]},"172":{"Abstract":"The inverse diffusion curve problem focuses on automatic creation of diffusion curve images that resemble user provided color fields. This problem is challenging since the 1D curves have a nonlinear and global impact on resulting color fields via a partial differential equation (PDE). We introduce a new approach complementary to previous methods by optimizing curve geometry. In particular, we propose a novel iterative algorithm based on the theory of shape derivatives. The resulting diffusion curves are clean and well-shaped, and the final image closely approximates the input. Our method provides a user-controlled parameter to regularize curve complexity, and generalizes to handle input color fields represented in a variety of formats.","Authors":"S. Zhao; F. Durand; C. Zheng","DOI":"10.1109\/TVCG.2017.2721400","Keywords":"Vector graphics;diffusion curves;inverse problem;shape optimization;Fr\u00e9chet derivative","Title":"Inverse Diffusion Curves Using Shape Optimization","Keywords_Processed":"diffusion curve;vector graphic;fr chet derivative;shape optimization;inverse problem","Keyword_Vector":[0.1700258138,0.0334781949,0.1028912994,0.0482217954,0.0052054719,-0.0603329747,-0.0352905011,-0.1606496198,-0.0222844642,-0.1159952949,0.0031041033,0.0161621401,-0.2394020428,0.08391658,0.0894931982,-0.0675008546,0.1162174645,-0.0169541009,0.0267838866,-0.0560203471,0.0170173396,0.0819231736,-0.0455444195,-0.1055075635,0.0316712369,0.0226179197,-0.0450982861,-0.006215809,0.0283764555,0.0613210805,0.0389483743,0.0807036726,-0.033125323,0.0931509565,-0.0744406109,0.0904800851,0.0521576295,-0.0124501882,-0.0211175893,0.0275383879,-0.0000581904,0.1206234551,0.058204701,-0.0731966921,-0.0278142391,-0.0123116288,0.0296261564,-0.0189361132,0.0387196652,0.0185790943,0.0032241075,-0.0285950927],"Abstract_Vector":[0.2248259131,0.1930039135,-0.0045260514,-0.0067737702,-0.0266787621,0.0030017704,0.0997369227,0.033269161,0.0265913654,0.0361340756,0.0396163021,-0.0138754432,-0.0252649749,-0.059292797,-0.0382102326,-0.0361280185,-0.0941134013,0.0380496534,0.0171317332,-0.0553814346,0.0140537154,-0.1024204962,0.0049361061,0.0038906977,0.1561389247,-0.0026796508,-0.0256121413,0.0779657695,0.0101468766,-0.0167361141,0.0086460305,0.0206710371,-0.078371189,0.0742633637,0.0871244264,0.0109958875,0.0531285111,0.0105423925,-0.0464844107,-0.0176660656,0.0094086963,-0.0529325222,-0.0225915542,-0.0130577308,-0.0448892968,0.0244906746,-0.0205892276,-0.0370516067,-0.0453841023,-0.0473793307,0.0723533526,-0.0208883512,-0.0019739153,0.0651518964,0.004890701,0.0104110257,-0.0207992767,-0.0356738805,0.0560202131,0.0174159061,-0.0516590778,-0.0553759033,-0.0136699213,-0.0228046289,-0.0350371516,-0.0007238267,-0.0212657384,0.050803986,-0.016169938,-0.0022179958,0.0326256184,0.0272809477,0.0277614039,-0.0093275097,-0.0227939388,0.0172108339,0.0066963449,-0.0150060263,-0.0279065921,-0.0182535019,-0.0159200475,-0.0716074743,0.0418222682,-0.0020270182,0.0085863841,-0.0096086547,0.0089285596,0.0086743374,-0.0139992233,-0.0021230004,-0.0145945898,0.0182817606,-0.0122355902,-0.0078156221,0.0012459501,-0.0057421704,0.0257225121,-0.0131026873,0.0168003094,0.0133980583,0.0416506718,0.0268849062,0.0226834388,-0.0251664043,0.0022830055,0.0253135802,0.0004415238,-0.0077781753,-0.0287940125,-0.021102424,0.0250053291,-0.0152025554,0.0211476831,0.004160315,0.0203496965,0.0196863174]},"173":{"Abstract":"There exists a gap between visualization design guidelines and their application in visualization tools. While empirical studies can provide design guidance, we lack a formal framework for representing design knowledge, integrating results across studies, and applying this knowledge in automated design tools that promote effective encodings and facilitate visual exploration. We propose modeling visualization design knowledge as a collection of constraints, in conjunction with a method to learn weights for soft constraints from experimental data. Using constraints, we can take theoretical design knowledge and express it in a concrete, extensible, and testable form: the resulting models can recommend visualization designs and can easily be augmented with additional constraints or updated weights. We implement our approach in Draco, a constraint-based system based on Answer Set Programming (ASP). We demonstrate how to construct increasingly sophisticated automated visualization design systems, including systems based on weights learned directly from the results of graphical perception experiments.","Authors":"D. Moritz; C. Wang; G. L. Nelson; H. Lin; A. M. Smith; B. Howe; J. Heer","DOI":"10.1109\/TVCG.2018.2865240","Keywords":"Automated Visualization Design;Perceptual Effectiveness;Constraints;Knowledge Bases;Answer Set Programming","Title":"Formalizing Visualization Design Knowledge as Constraints: Actionable and Extensible Models in Draco","Keywords_Processed":"Perceptual Effectiveness;answer Set programming;Knowledge basis;constraint;Automated Visualization Design","Keyword_Vector":[0.0342770293,0.0007352375,0.0003452275,0.0155312415,-0.0089972824,-0.0059612627,0.0083563768,0.0098859343,-0.0451638867,-0.0275607754,0.0244688176,0.0036841724,-0.0120559113,-0.0123369269,-0.0136759265,-0.0010859646,0.0057889813,0.0042122829,0.0135335113,-0.0058012739,-0.0040028713,0.0017723666,-0.001776392,0.064912903,-0.0018564082,-0.0618134612,0.0439374709,-0.0154276505,-0.0589277715,0.0292297109,0.0417662388,0.0289776759,0.0157392856,0.0198917901,0.0213653179,-0.041867493,0.0465111196,-0.0439829106,0.0535762538,-0.0329372704,0.0789496376,-0.020797853,0.0064744098,0.0438926492,0.0827102723,-0.0641194739,0.0074701582,-0.0490453838,-0.0375546889,-0.0129632668,0.0277039305,-0.0386106476],"Abstract_Vector":[0.2841535164,-0.0763261374,-0.0490950703,-0.0591728583,-0.0238251321,0.008507685,0.026157372,0.0099721003,-0.0689945059,0.0605281751,0.024050883,-0.0055504074,0.0159718066,-0.0152263989,-0.0036207608,-0.008899954,-0.0042885964,0.0115847816,0.0273520933,0.0674248621,-0.0499541514,-0.0050958978,-0.0440693355,0.0139275057,-0.0030628333,0.0159938847,-0.0428112432,-0.0210807517,-0.0101089161,-0.0463043515,0.0501902678,0.0058091724,-0.0141233669,0.0556511724,-0.044071292,-0.0585928041,0.0243840772,0.0169653714,0.0001103539,-0.0121487467,-0.0691751994,0.0469118407,-0.0489101016,-0.0720500636,0.0220036904,0.0063507455,0.0159324644,0.0188377607,-0.0096472508,0.0315838283,0.0155623918,-0.0082659621,-0.003554436,0.078677128,-0.0384015102,0.0213445857,-0.0252159075,-0.0165020756,0.0235324491,-0.0335219959,0.0530694432,0.0135046622,-0.0074242932,-0.0112962698,-0.0506403069,0.0703000178,0.0185348105,0.000625215,0.0521865551,0.0115784825,-0.0028257549,-0.0150964915,-0.0404347744,0.0315287727,-0.0056913501,0.0398388688,0.0372122912,0.0134384269,-0.0016437048,0.010124319,0.032970533,0.0349193823,-0.0699307649,-0.0010169814,0.0231540347,0.014331937,0.0327887524,-0.0671622159,-0.0372165222,-0.000095863,-0.0075639634,0.0105390327,-0.0078717457,-0.0254798122,-0.0225137089,0.0184551404,0.0947552119,0.0111106427,0.0152667204,-0.0434443899,0.0298959733,0.0110436715,-0.0353870524,-0.0037981393,0.008653137,0.022964203,-0.0341276762,-0.0103907656,0.1036183608,-0.0014327964,-0.0343264773,-0.0343172849,-0.0111952123,-0.0445951297,0.0315990506,-0.0328282016]},"174":{"Abstract":"Predicting specularities in images, given the camera pose and scene geometry from SLAM, forms a challenging and open problem. It is nonetheless essential in several applications such as retexturing. A recent geometric model called JOLIMAS partially answers this problem, under the assumptions that the specularities are elliptical and the scene is planar. JOLIMAS models a moving specularity as the image of a fixed 3D quadric. We propose dual JOLIMAS, a new model which raises the planarity assumption. It uses the fact that specularities remain elliptical on convex surfaces and that every surface can be divided in convex parts. The geometry of dual JOLIMAS then uses a 3D quadric per convex surface part and light source, and predicts the specularities by a means of virtual cameras, allowing it to cope with surface's unflatness. We assessed the efficiency and precision of dual JOLIMAS on multiple synthetic and real videos with various objects and lighting conditions. We give results of a retexturing application. Further results are presented as supplementary video material.","Authors":"A. Morgand; M. Tamaazousti; A. Bartoli","DOI":"10.1109\/TVCG.2017.2734538","Keywords":"Specularity Prediction;Augmented Reality;Retexturing;Quadric;Multiple Light Sources","Title":"A Multiple-View Geometric Model of Specularities on Non-Planar Shapes with Application to Dynamic Retexturing","Keywords_Processed":"Multiple Light source;augmented reality;retexture;Quadric;Specularity Prediction","Keyword_Vector":[0.0599469552,-0.0005999295,0.0094102779,0.0007868576,-0.0059614959,-0.0028641401,-0.0476372347,-0.0265087921,-0.0813255044,0.0009520465,0.042224184,0.0765111412,-0.0146491674,0.041938203,0.1006240154,0.1665743094,0.0592411745,0.0581148942,0.1246963203,-0.0062230138,-0.0882971256,0.0612304368,0.1445298375,0.1153186294,-0.1286957437,-0.0127650673,-0.0149556931,-0.0215108162,0.0493981631,-0.0922728968,-0.0277773595,-0.1050224177,0.0653869276,0.0763962908,-0.0362859626,0.0260277923,-0.0856271841,0.002234991,0.0075774029,-0.0903424177,-0.0260451729,0.0615609849,-0.0045974296,-0.0194195599,-0.0101221588,-0.0244022861,0.0832526568,0.0560136081,-0.0432126002,0.0263279043,-0.0239501818,0.0619857732],"Abstract_Vector":[0.1729103967,-0.0401579311,-0.0097276822,0.00173771,-0.0756470639,0.0942968113,0.0022906078,0.0092778007,0.0611369403,0.0187328671,-0.0537408268,0.0676603953,-0.0442931179,0.0136265718,0.0001148736,0.0555557679,0.0295620363,-0.0169396373,0.048371572,-0.0810916884,0.0084406306,-0.0493606218,0.0395734514,0.0139456732,0.0087073766,-0.0396598113,0.0012431701,-0.0325259764,0.0076363386,0.0255679515,0.0652362547,0.0040590871,-0.0290780205,-0.0220867648,0.0528767754,-0.064934248,-0.0181542198,-0.0480394953,-0.0460358836,0.0321114099,0.0099738404,0.0768539435,0.0153617428,0.0120761258,0.0079400109,0.0296534407,0.0270491564,0.0444767916,-0.0304055958,-0.0387921306,-0.0099147254,-0.0444170967,-0.0126694765,0.0057917403,0.025546899,-0.0186142765,0.0870423957,0.0555170337,-0.016935423,-0.0175122132,0.062455935,0.0116422666,0.0165660099,-0.0104859144,-0.0198623686,-0.0020514648,0.0404110248,-0.0595082273,-0.0452786082,0.0620337429,0.003715775,0.0226856863,0.0424954494,0.0139788494,0.0509790693,-0.0211825086,0.0308336583,-0.0239678279,0.0220076436,-0.0225693231,-0.0381336373,0.0291752209,0.0356811129,0.0062588817,0.0117769415,0.004747502,-0.0097950088,0.0306781448,0.0315842014,-0.0155547704,0.0337198196,0.0050662285,0.0175852671,-0.0294072835,0.0270951762,-0.0018422185,0.0137281086,-0.002128457,-0.031166985,-0.070352797,-0.032159094,-0.0047447802,0.0185605995,-0.0140850218,-0.0074933778,-0.0001044663,-0.0069865509,-0.0012160925,-0.0327131351,-0.0295397498,-0.0236593178,0.0030104106,0.039050251,-0.0420974114,0.0183614384,-0.0137114601]},"175":{"Abstract":"In this paper we discuss the creation of visual mementos as a new application area for visualization. We define visual mementos as visualizations of personally relevant data for the purpose of reminiscing, and sharing of life experiences. Today more people collect digital information about their life than ever before. The shift from physical to digital archives poses new challenges and opportunities for self-reflection and self-representation. Drawing on research on autobiographical memory and on the role of artifacts in reminiscing, we identified design challenges for visual mementos: mapping data to evoke familiarity, expressing subjectivity, and obscuring sensitive details for sharing. Visual mementos can make use of the known strengths of visualization in revealing patterns to show the familiar instead of the unexpected, and extend representational mappings beyond the objective to include the more subjective. To understand whether people's subjective views on their past can be reflected in a visual representation, we developed, deployed and studied a technology probe that exemplifies our concept of visual mementos. Our results show how reminiscing has been supported and reveal promising new directions for self-reflection and sharing through visual mementos of personal experiences.","Authors":"A. Thudt; D. Baur; S. Huron; S. Carpendale","DOI":"10.1109\/TVCG.2015.2467831","Keywords":"Visual Memento;Memories;Personal Visualization;Movement Data;World Wide Web;Visual Memento;Memories;Personal Visualization;Movement Data;World Wide Web","Title":"Visual Mementos: Reflecting Memories with Personal Data","Keywords_Processed":"memory;visual memento;personal visualization;Movement Data;World wide web","Keyword_Vector":[0.0506711883,0.0219952876,-0.0140458007,-0.0178247619,0.0205825068,-0.0487957073,-0.0528508843,-0.0284477306,-0.116432242,-0.0381486136,0.0540123873,0.0768203684,-0.0087538804,0.0479653504,0.0903872284,0.0772078241,0.0741889035,0.0794789941,0.2374188269,-0.0005031733,-0.1018614651,0.1098143576,0.0859807326,0.0963598287,-0.1008592779,-0.0335596087,-0.0378790624,-0.0229075495,0.1260994046,-0.1497990666,-0.0494716221,-0.1438383801,0.0610988683,0.0226425757,-0.0723137118,0.0009874321,-0.0169507991,-0.0149013356,-0.0784425867,-0.0623236077,-0.018861736,0.0731809697,0.0088910286,0.0588239661,0.007560137,-0.0456991762,0.0382251024,0.0713480436,0.0344849664,0.0392533168,-0.0128522313,-0.0077506754],"Abstract_Vector":[0.1254932834,-0.042266795,-0.012987835,-0.0119461118,-0.058652687,0.0744833208,0.0180426413,0.0355747385,0.0345593694,0.0135380786,-0.0359426081,0.024978888,-0.0187013175,-0.0086088146,0.0014338907,0.0507662329,0.0208962857,0.0300718864,0.0331557493,-0.0088614772,-0.0029890077,-0.0572106086,0.0260059343,-0.0100719693,-0.0075316439,0.0157824245,0.0196215387,0.0023151401,0.0149582155,0.051559278,0.0425593602,0.0265018592,-0.0449587799,-0.0310612337,0.0270902422,-0.007116399,-0.0395538812,-0.0206211504,-0.0180510147,-0.0204394467,0.0600751629,0.0411214682,0.018827263,-0.023130621,0.0643489012,0.0113663683,0.0138239321,0.0199960363,0.0203510704,-0.0176309997,-0.0006094948,-0.0141649787,-0.0011054181,-0.017784073,-0.0110404509,0.0024729184,0.0551726326,0.01882307,-0.0209185943,-0.0575095839,-0.0111930071,0.0159797902,-0.0178499583,0.0021786515,-0.0268513888,-0.017161702,-0.0211308783,-0.0395326179,-0.0246445119,0.0262599153,-0.0184634173,0.0385861595,0.0273758371,0.06001912,-0.0182246195,-0.0833139532,0.0308881091,0.0308065602,-0.0175248752,-0.0601242434,0.0001724442,0.002169128,-0.0617535896,0.062879259,0.0174760341,0.0103545407,-0.0238212647,-0.0088774482,-0.0747982985,-0.0223757345,-0.0039522062,-0.0081305112,-0.0089202815,0.047677938,-0.0057666622,-0.0519846321,0.0094010579,-0.0214365597,0.0069380437,-0.0350760168,0.0265442846,-0.0088707361,0.0010507054,0.0030079972,0.0088702858,0.0117215634,-0.0102541329,0.0381611829,-0.0249161855,-0.0412755081,-0.0213831243,0.0373970367,0.0093100384,-0.0221705866,0.0498938722,0.0637431388]},"176":{"Abstract":"We propose an efficient framework to realistically simulate foam effects in which 3D water particles from a base water solver are first projected onto 2D screen space in order to reduce computational complexity of finding foam particles. Because foam effects are often created primarily in fast and complicated water flows, we analyze acceleration and curvature values to identify the areas exhibiting such flow patterns. Identified foam particles are emitted in 3D simulation space, and each foam particle is advected by its classified type based on its velocity, thereby capturing the essential characteristics of foam wave motions (e.g., floating waves or scattering bubbles). In addition, we provide an intuitive and flexible mechanism (e.g., user sketch or image) to customize parameters and control the appearance of foam effects while minimizing the occurrence of popping artifacts. Experiments convincingly demonstrate that the proposed approach is efficient and easy to use while delivering high-quality results.","Authors":"J. Kim; J. Lee; S. Cha; C. Kim","DOI":"10.1109\/TVCG.2016.2609429","Keywords":"Foam effects;projective space;foam wave patterns","Title":"Efficient Representation of Detailed Foam Waves by Incorporating Projective Space","Keywords_Processed":"foam effect;projective space;foam wave pattern","Keyword_Vector":[0.0677978539,-0.0268080352,0.0153971072,-0.0157067932,0.0038664246,-0.0182178544,-0.0028221351,-0.0058592784,0.0021017721,-0.0220323352,-0.0076271382,0.0183418626,-0.0010696607,-0.0200282501,-0.005440139,0.0122097707,0.0026600937,0.0112436193,-0.0026063577,-0.0265366757,-0.0198571297,0.0157157131,-0.0126623418,0.0021243557,0.0258866971,-0.0074800633,0.0139506572,0.0390562301,-0.0334723117,0.00542448,-0.017298761,0.0148182969,-0.0020159272,0.019111835,-0.0079355596,-0.0121059236,-0.0077751521,0.0458154078,0.0280473107,0.0647857389,0.0247511034,0.004324652,0.0019412103,0.0388435546,0.0414411601,-0.0205445581,0.0153466911,0.0205668763,0.0019082159,0.0340483932,-0.0146813723,0.0319986362],"Abstract_Vector":[0.1652633476,-0.038586488,0.0084140664,0.0196960241,0.030014834,0.0037684556,-0.0027554323,-0.0132102216,-0.0160034825,0.027336896,-0.0484678061,0.0318953591,0.0071318577,0.02129558,0.0133126614,-0.0146627405,-0.0032196109,0.0208295349,0.0087029576,0.0014470934,0.0228087643,0.0148781991,0.0188218222,-0.0047417108,0.0496543243,0.0417829422,-0.03853149,-0.0080622392,0.0006441164,0.0015388177,-0.001754079,0.013012179,-0.0571712526,0.0031341467,-0.0526767066,-0.0030659863,-0.0018026357,-0.0141578486,0.0661918193,0.0019822561,0.010591717,0.0255768755,-0.0040282838,0.0181775045,0.0218190024,0.0499094645,-0.0179731733,-0.0253757471,0.0052723952,-0.0375067089,-0.0448450149,-0.0025109756,-0.0319578056,0.019275568,-0.0209326016,-0.0027034725,0.0428106112,-0.0241368207,-0.0125978529,-0.0041205797,0.0171978971,0.0509807078,-0.0071471761,0.000273526,-0.0097080899,-0.0101359548,0.020128102,-0.0088461702,0.0226338366,0.0113014528,-0.002287854,0.0105026466,-0.0230491516,0.021147457,0.0261745216,-0.0475228795,-0.0426258604,0.020192326,0.0388227806,-0.0220169217,0.009071191,-0.0095566832,-0.0189125641,-0.0374004804,0.0093795643,0.0381618119,0.0207954564,0.0037770222,0.0329451168,-0.0022286557,0.0006260413,0.0088916636,-0.0027948772,0.0005921223,0.011643159,-0.0167139762,-0.0068781221,-0.0369580836,0.0137736519,-0.0108748529,0.0112059734,0.0016283016,-0.017117687,0.0308364303,0.0064521624,-0.0340824299,-0.027218097,0.0159897646,-0.0016444454,-0.0047998406,-0.0301948361,-0.0057551979,-0.0100507146,-0.024821155,-0.019132968,-0.0228652077]},"177":{"Abstract":"The digital humanities have experienced tremendous growth within the last decade, mostly in the context of developing computational tools that support what is called distant reading - collecting and analyzing huge amounts of textual data for synoptic evaluation. On the other end of the spectrum is a practice at the heart of the traditional humanities, close reading - the careful, in-depth analysis of a single text in order to extract, engage, and even generate as much productive meaning as possible. The true value of computation to close reading is still very much an open question. During a two-year design study, we explored this question with several poetry scholars, focusing on an investigation of sound and linguistic devices in poetry. The contributions of our design study include a problem characterization and data abstraction of the use of sound in poetry as well as Poemage, a visualization tool for interactively exploring the sonic topology of a poem. The design of Poemage is grounded in the evaluation of a series of technology probes we deployed to our poetry collaborators, and we validate the final design with several case studies that illustrate the disruptive impact technology can have on poetry scholarship. Finally, we also contribute a reflection on the challenges we faced conducting visualization research in literary studies.","Authors":"N. McCurdy; J. Lein; K. Coles; M. Meyer","DOI":"10.1109\/TVCG.2015.2467811","Keywords":"Visualization in the humanities;design studies;text and document data;graph\/network data;Visualization in the humanities;design studies;text and document data;graph\/network data","Title":"Poemage: Visualizing the Sonic Topology of a Poem","Keywords_Processed":"design study;text and document datum;graph network datum;visualization in the humanity","Keyword_Vector":[0.1815732632,-0.0445569551,0.0351807766,-0.1007467136,0.0304290072,0.0100495305,-0.079597399,0.0033976815,0.0360617541,-0.0094799695,0.0391397548,0.0675407786,-0.0220760383,0.0403043647,-0.0628553097,0.0415716724,-0.1054833908,-0.0107000415,-0.0111072544,0.0343025221,0.0310535384,0.002560874,0.0127761036,-0.1392946028,-0.0541374604,0.0586340233,0.012788123,0.0395307547,0.027015218,-0.0813524035,0.1415127284,0.1455020858,0.0736810551,0.0600540569,0.0032377587,0.069333041,-0.0887722152,0.0064375187,0.0749313945,0.0034698382,-0.0080417103,0.0272592441,0.0132529572,0.075826238,0.0151956588,-0.0768048562,-0.0643204836,0.0175569845,0.0269895127,0.0099882777,0.0630013457,0.0383171372],"Abstract_Vector":[0.2357968689,-0.0012032723,-0.006922247,0.1164261501,-0.0286609461,0.0863988655,-0.0224303136,0.0149327457,0.0555114264,-0.0185427228,-0.059548681,0.0583195398,-0.0415497961,-0.0337463887,0.0447663388,0.1798415223,0.078809565,0.0662621392,-0.0154555908,-0.1567980425,0.0040535998,0.0481487823,0.0049342296,0.1065488155,-0.060911871,-0.0283142173,0.0446482443,0.0884422663,-0.0020640356,0.0215208094,-0.0252773845,-0.0385309437,-0.0006051869,-0.0372508862,-0.0158688288,0.0012734283,0.0049796532,0.0191961764,-0.0277284174,-0.0046332177,-0.030662811,-0.0025599015,0.0191747975,0.0094957528,-0.0639579747,0.0616405107,0.0471819328,0.0660219927,0.0329687773,0.0098566528,0.0092191123,0.0770543611,0.078189013,0.0167260078,-0.0059336549,-0.0181850405,0.0935828625,-0.0144254939,0.0343672307,0.0080029249,0.0117826737,-0.0651043347,-0.0025654412,-0.002121008,-0.0389192419,0.0354334715,0.0164737205,0.0573059932,0.0158104034,-0.011455154,-0.1186529275,-0.0457210571,0.0187665947,0.0051987581,-0.0866945218,0.1146485018,0.0304662143,0.0709694014,-0.0146592606,-0.0549104601,-0.0155023956,-0.033980777,0.0766255176,-0.0241356487,-0.0218151638,0.0509077656,0.0298208704,0.0374536859,0.0119941913,-0.0012788333,-0.059932813,-0.0038476894,-0.0266885647,-0.0041261761,-0.0704081469,0.0292205542,0.0492892455,0.0219632418,-0.0095242007,0.015856691,0.0360776813,0.0545945824,0.0580172354,-0.0658603,0.1149534702,-0.0085781058,-0.0450153972,-0.0376862718,-0.0574152352,-0.0172507382,-0.0032909236,-0.0228882437,-0.0162266989,0.0626233763,-0.0198900942,-0.0211949327]},"178":{"Abstract":"This paper presents a declarative grammar for conveniently and effectively specifying advanced volume visualizations. Existing methods for creating volume visualizations either lack the flexibility to specify sophisticated visualizations or are difficult to use for those unfamiliar with volume rendering implementation and parameterization. Our design provides the ability to quickly create expressive visualizations without knowledge of the volume rendering implementation. It attempts to capture aspects of those difficult but powerful methods while remaining flexible and easy to use. As a proof of concept, our current implementation of the grammar allows users to combine multiple data variables in various ways and define transfer functions for diverse input data. The grammar also has the ability to describe advanced shading effects and create animations. We demonstrate the power and flexibility of our approach using multiple practical volume visualizations.","Authors":"M. Shih; C. Rozhon; K. Ma","DOI":"10.1109\/TVCG.2018.2864841","Keywords":"Volume visualization;direct volume rendering;declarative specification;multivariate\/multimodal volume data;animation","Title":"A Declarative Grammar of Flexible Volume Visualization Pipelines","Keywords_Processed":"multivariate multimodal volume datum;direct volume render;animation;declarative specification;volume visualization","Keyword_Vector":[0.0275239366,-0.0087991038,0.004859576,-0.0032405201,-0.0077454887,-0.0099768582,-0.0290198435,0.0136732797,-0.081576783,-0.0109560034,0.0331022334,0.0393456715,-0.0100548731,-0.0122167888,0.0205741304,0.0462673591,0.0591774003,0.0349225215,0.0789755271,-0.0169592769,-0.0481053219,0.0495864055,0.078740434,0.0601133827,-0.0561080563,-0.0288321869,0.0154118289,-0.0007220571,0.0239167838,-0.0653403676,0.0005284566,-0.0111440582,0.0364540674,0.04151809,-0.0081418059,-0.0524773901,-0.0070011474,-0.0173716185,0.0170099738,0.0231784718,-0.0046598579,0.0081752918,0.0131020258,0.0194916319,0.0120189976,-0.0302445477,0.0129154748,0.0439174354,-0.0033810578,0.020385672,0.0150467518,-0.006398602],"Abstract_Vector":[0.1875930713,-0.0140545128,-0.009657794,-0.0080096966,-0.0179364124,-0.0306120818,0.0254144213,-0.0095909053,0.0509389456,0.0536217855,-0.0138744049,0.0367811227,-0.0415465801,0.029480334,-0.0329547353,-0.014248102,0.0264741202,-0.0101755265,-0.016933508,-0.0805981941,-0.0376334501,-0.036245789,-0.0040757855,-0.00602275,-0.0393719108,0.03430803,0.0144620213,-0.0281013813,-0.019443962,-0.0016443173,0.0567887323,0.0089136899,-0.0573416328,-0.0460766414,-0.0144719566,-0.0103869107,-0.0308477468,0.0263093111,-0.0001715121,0.0662922636,0.0294956213,0.0053404913,0.0690194446,-0.0286753029,0.0040746384,0.009292125,0.0822010252,0.0544680918,0.0196847048,0.0282472471,-0.0333163761,-0.0219140359,-0.0103055171,-0.0156496925,0.0123370943,0.0090497576,0.0068214627,-0.0223405434,0.0263057134,-0.0294518796,-0.0137741868,-0.0087169023,0.013524717,-0.006882228,-0.0211296253,0.0401107596,-0.0202410207,0.0471122974,-0.01705088,-0.0090484019,0.0018703234,0.0699330204,0.0030944737,0.0179678869,0.0001573781,0.0117755871,-0.0069268214,-0.0198145546,-0.0095416485,-0.0364938178,-0.0176963759,0.0017477374,-0.0500302169,0.028198755,0.0134380705,0.0235661352,-0.0235266551,0.0514371817,-0.0441289724,0.0257832685,-0.0170229513,-0.0000609058,-0.0010073591,0.0329828028,-0.0307914395,0.0015085204,0.0182666839,-0.0202756459,-0.0057780169,0.0304432703,0.0292988511,0.065328486,-0.0058975645,-0.0015470074,-0.0218037446,-0.0221859238,-0.0175165101,0.0181931587,-0.04952498,0.0031654943,-0.0484264468,-0.0402240954,0.016604171,-0.0226136767,0.0363523161,0.0011618688]},"179":{"Abstract":"We present VisTiles, a conceptual framework that uses a set of mobile devices to distribute and coordinate visualization views for the exploration of multivariate data. In contrast to desktop-based interfaces for information visualization, mobile devices offer the potential to provide a dynamic and user-defined interface supporting co-located collaborative data exploration with different individual workflows. As part of our framework, we contribute concepts that enable users to interact with coordinated & multiple views (CMV) that are distributed across several mobile devices. The major components of the framework are: (i) dynamic and flexible layouts for CMV focusing on the distribution of views and (ii) an interaction concept for smart adaptations and combinations of visualizations utilizing explicit side-by-side arrangements of devices. As a result, users can benefit from the possibility to combine devices and organize them in meaningful spatial layouts. Furthermore, we present a web-based prototype implementation as a specific instance of our concepts. This implementation provides a practical application case enabling users to explore a multivariate data collection. We also illustrate the design process including feedback from a preliminary user study, which informed the design of both the concepts and the final prototype.","Authors":"R. Langner; T. Horak; R. Dachselt","DOI":"10.1109\/TVCG.2017.2744019","Keywords":"Mobile devices;coordinated & multiple views;multi-display environment;cross-device interaction","Title":"VisTiles: Coordinating and Combining Co-located Mobile Devices for Visual Data Exploration","Keywords_Processed":"cross device interaction;coordinate multiple view;multi display environment;mobile device","Keyword_Vector":[0.0515269613,0.0561242256,0.0168206458,0.1199013944,0.1324498301,-0.0581753881,0.0004751735,-0.1318227186,0.0027235048,0.0232846471,0.0699607683,-0.0562658402,0.00713815,-0.0655021232,-0.1162894672,-0.0732680762,0.0019197968,0.0815640723,-0.0054550186,0.0373881639,-0.0041255282,-0.0928586021,-0.0366622895,0.1389785457,-0.011326285,0.1528276788,0.1383841401,-0.0365804548,-0.066116441,-0.1781275377,-0.0401707661,0.0293245944,0.0967018263,-0.0726010261,-0.0340932091,0.1463396146,0.055347592,0.0607460228,-0.0087785393,0.0431618776,0.0694693197,0.0383249704,0.0356264993,0.0088183647,0.054887095,0.1265494412,0.0200081835,-0.0120980441,0.0592021619,0.0228697684,0.0768815396,0.0039362267],"Abstract_Vector":[0.1907937014,0.0890338357,0.0083660133,0.0745984683,-0.0443640051,-0.0153305337,0.0174694858,0.0341647997,-0.0115867691,-0.0349058203,-0.0560070398,0.0011134545,-0.0465016136,0.1174688545,-0.008951675,-0.0343146513,-0.01761179,-0.0492398568,-0.0484744211,-0.0205524832,-0.0140120597,0.0096991494,-0.0573295787,-0.0315900175,-0.0073562425,-0.0189985926,0.0014413039,0.0125184807,0.0398811284,-0.0029531431,0.0496085688,0.0633614726,-0.0185995157,0.0579749827,0.0174991213,-0.0286204267,0.0360794893,0.0728261989,0.0429858552,0.0229062006,-0.0157538138,-0.0526164431,0.0320853762,0.0280915284,-0.0380260957,-0.0436422761,0.0619035177,0.0300422059,0.0219134926,-0.0476030877,-0.0424852947,-0.0208665206,-0.023435075,0.0388229241,0.0405665974,0.0220369389,0.0195361431,-0.0297931996,-0.0260566381,-0.0040591157,-0.0106717077,0.0072210461,-0.0062527396,0.0153845595,-0.0146371702,-0.0037982168,-0.0358272018,-0.0069803597,0.0012174354,0.0083783519,-0.050259224,-0.0552491327,0.0039173521,0.0471834057,-0.0152401717,-0.0233651706,-0.0165782526,-0.0219562252,-0.025882538,-0.0192820193,0.024481825,0.0244470024,0.0234172867,-0.0235528524,-0.0145877808,0.0381871989,0.0089774304,-0.0290833871,-0.0029481474,-0.0008357248,-0.0381048486,-0.0752288032,-0.041495984,0.020993949,-0.0277313211,0.0106467051,-0.0089101908,-0.0122687801,-0.0001811495,-0.0167455997,-0.019554032,-0.063956387,0.0183848809,-0.0308855045,-0.0123030226,-0.0225490162,-0.0461176386,-0.0101259341,-0.0211106337,0.0027319139,0.0059055295,0.0194671377,-0.0386759111,0.0269090978,0.0788960552,0.0160419607]},"18":{"Abstract":"Balancing accuracy gains with other objectives such as interpretability is a key challenge when building decision trees. However, this process is difficult to automate because it involves know-how about the domain as well as the purpose of the model. This paper presents TreePOD, a new approach for sensitivity-aware model selection along trade-offs. TreePOD is based on exploring a large set of candidate trees generated by sampling the parameters of tree construction algorithms. Based on this set, visualizations of quantitative and qualitative tree aspects provide a comprehensive overview of possible tree characteristics. Along trade-offs between two objectives, TreePOD provides efficient selection guidance by focusing on Pareto-optimal tree candidates. TreePOD also conveys the sensitivities of tree characteristics on variations of selected parameters by extending the tree generation process with a full-factorial sampling. We demonstrate how TreePOD supports a variety of tasks involved in decision tree selection and describe its integration in a holistic workflow for building and selecting decision trees. For evaluation, we illustrate a case study for predicting critical power grid states, and we report qualitative feedback from domain experts in the energy sector. This feedback suggests that TreePOD enables users with and without statistical background a confident and efficient identification of suitable decision trees.","Authors":"T. M\u00fchlbacher; L. Linhardt; T. M\u00f6ller; H. Piringer","DOI":"10.1109\/TVCG.2017.2745158","Keywords":"Model selection;classification trees;visual parameter search;sensitivity analysis;Pareto optimality","Title":"TreePOD: Sensitivity-Aware Selection of Pareto-Optimal Decision Trees","Keywords_Processed":"Model selection;classification tree;sensitivity analysis;visual paramet search;pareto optimality","Keyword_Vector":[0.0626014347,0.0385292467,0.0780455748,0.0819736401,-0.0009590946,-0.0096145859,0.021632715,-0.1758296565,-0.0126238149,0.028962359,-0.0166099167,-0.0645074125,-0.2751039331,-0.0436364097,-0.195563456,0.0177927376,0.1287966891,0.0591096694,0.0888871406,0.0615930244,0.163466752,-0.1216040482,0.1098897141,-0.0590884919,0.0742506023,-0.1543950148,0.0888956174,-0.0768894224,0.1274170067,0.1321459756,0.0858857252,-0.101018726,-0.085001175,-0.0206581856,-0.0467931404,-0.0448316811,-0.0918473664,0.0253105587,0.0494449166,-0.0904588234,0.1250670192,0.0937917931,-0.0072332583,0.0413542448,-0.0267234737,-0.0247936155,-0.0125933369,-0.0118890336,0.0037121994,0.0159511251,0.0366087759,0.0293681197],"Abstract_Vector":[0.2070912845,0.0410610733,0.0427402651,-0.0129210734,0.0266033656,-0.1395022214,-0.0281311451,-0.0578843641,-0.1450905752,-0.1085816358,0.130126648,0.2596174386,-0.1020378364,-0.0150146239,0.0821319661,-0.0682967917,0.127264225,0.0578636134,0.0930053883,0.0364188621,0.0211751848,0.0349154985,0.0098364675,-0.020940786,-0.0554274428,-0.0367888407,-0.0479832783,0.0380427464,0.01659401,0.0018156337,-0.025904882,0.0440349743,0.0062909664,-0.0115946738,0.0459712849,0.0860614487,0.025246751,-0.0044681653,0.0092825152,-0.0483309959,0.0488137391,-0.0062292903,-0.0072160341,-0.0026640019,0.0316798972,-0.0219788988,0.0315902128,0.0437582335,-0.013631129,0.0092415074,-0.0061636612,0.0413570446,0.0007335468,0.0078695757,-0.0011025599,-0.0173657528,0.01884805,-0.0090353071,0.0062926801,-0.0231389236,0.0097572316,0.0028465696,-0.0515265468,0.0107619656,-0.0080531365,-0.0309645036,0.0089378629,0.0020743224,-0.0431822427,0.0322512107,0.0713618264,-0.0428067903,0.0144814646,0.0214562967,0.0193636623,0.0186198414,-0.0367503006,0.0067060709,-0.0042250968,-0.0106547346,-0.0010806776,0.0289358706,-0.021866481,-0.0011234648,-0.0188027418,-0.0357642375,-0.005758224,0.0441519015,0.0053585938,0.0526158134,0.0238335007,-0.0326098521,0.0470690994,0.0178416476,0.0365236453,0.0075318955,-0.0232941254,-0.0212345162,0.0022801346,0.012533623,-0.0159922099,-0.0001198291,-0.0321947859,-0.0300112341,-0.0166906762,-0.0097870193,-0.0013743094,0.0400426944,-0.0052758044,-0.0115328944,-0.0181877411,0.0460527472,-0.0207084206,-0.0328365429,0.0031620958,-0.0146460063]},"180":{"Abstract":"The motion of a thin viscous film of fluid on a curved surface exhibits many intricate visual phenomena, which are challenging to simulate using existing techniques. A possible alternative is to use a reduced model, involving only the temporal evolution of the mass density of the film on the surface. However, in this model, the motion is governed by a fourth-order nonlinear PDE, which involves geometric quantities such as the curvature of the underlying surface, and is therefore difficult to discretize. Inspired by a recent variational formulation for this problem on smooth surfaces, we present a corresponding model for triangle meshes. We provide a discretization for the curvature and advection operators which leads to an efficient and stable numerical scheme, requires a single sparse linear solve per time step, and exactly preserves the total volume of the fluid. We validate our method by qualitatively comparing to known results from the literature, and demonstrate various intricate effects achievable by our method, such as droplet formation, evaporation, droplets interaction and viscous fingering. Finally, we extend our method to incorporate non-linear van der Waals forcing terms which stabilize the motion of the film and allow additional effects such as pearling.","Authors":"O. Vantzos; O. Azencot; M. Wardeztky; M. Rumpf; M. Ben-Chen","DOI":"10.1109\/TVCG.2016.2605083","Keywords":"Computer graphics;three-dimensional graphics and realism;animation","Title":"Functional Thin Films on Surfaces","Keywords_Processed":"computer graphic;three dimensional graphic and realism;animation","Keyword_Vector":[0.2611008359,-0.1908704012,-0.0036549858,-0.1748117742,0.1428792701,-0.0168780205,0.4841689503,-0.0820767721,-0.0586463646,-0.1038751213,-0.0155281868,0.1349892459,0.0533435343,0.0189361441,-0.0318900187,0.0337455487,-0.0205465102,0.0024781555,-0.0069249549,0.074555751,0.058068621,0.0757726797,0.0174063747,-0.088268243,-0.0943983688,0.0027730486,0.0475808654,-0.0569231933,0.0236227271,0.0014368854,0.0485905994,0.0828634583,-0.0191837241,-0.0134701946,0.0511105105,0.0153009605,0.0116323064,0.029500134,-0.1271766855,0.0230083896,0.0096554876,-0.013594205,0.0073117334,-0.0529661288,0.109513607,-0.0089457568,0.1206284448,0.0575848413,-0.0296019999,0.1697151358,-0.0106132435,-0.0384971183],"Abstract_Vector":[0.2097807513,-0.1224389585,0.0014474267,0.2309920486,0.1872350353,0.1571607652,-0.0522498208,-0.0337820541,-0.0377371339,-0.0083302011,0.0323299754,0.0129407699,0.0082722664,0.0055708491,-0.0770155992,-0.0389455162,0.0256454689,0.01799389,-0.0271089341,-0.0068449107,0.0094681748,-0.0388712317,-0.0322740684,-0.0337107325,-0.0413934767,-0.0237472769,0.0327653761,0.0146832469,0.0446098078,-0.0269962563,-0.0142438828,0.0665089392,-0.0134802437,0.0311069163,0.014915338,0.0108937251,-0.0110548718,0.0184381125,0.0175090505,-0.0115740831,0.0167121921,0.022395815,-0.0000072369,0.0212540613,0.0290098305,0.0121457245,0.0050946319,-0.0637357438,-0.035105215,0.0376103849,0.029484474,0.0560613902,-0.0255345269,-0.0402765812,0.0028874772,-0.004129392,0.0274925674,-0.0435446183,0.0207445832,0.0002889899,-0.005366967,-0.0010091638,0.0529471087,0.0234537078,-0.0535339615,-0.0285030543,0.0336536279,-0.0320222279,-0.0324121217,-0.0215467343,-0.0074021383,0.0033643441,0.0584642125,0.007155207,0.0010169975,0.0370444157,0.0297398445,-0.0292082927,-0.0028562054,-0.0075630659,0.0050712945,-0.025447339,-0.0170738062,0.0224383495,-0.0092419524,-0.0178769328,-0.0045516633,-0.0216305417,-0.0110529779,-0.0195249805,-0.0201135693,-0.0413090099,0.0333041751,-0.0164780172,0.0451108031,-0.0330181345,-0.0198204431,0.0051165609,-0.0185325305,-0.0025539559,-0.029161257,0.0164770763,-0.0207614811,0.0192938953,-0.0290692034,0.0105324036,-0.0025350249,0.0429142248,-0.0104850195,0.0090238039,0.0006517169,-0.0177339655,0.0214667249,0.0031277693,-0.0273549456,0.0251929883]},"181":{"Abstract":"Many techniques facilitate real-time collision detection against complex models. These typically work by pre-computing information about the spatial distribution of geometry into a form that can be quickly queried. When models deform though, expensive pre-computations are impractical. We present radial fields: a variant of distance fields parameterised in cylindrical space, rather than Cartesian space. This 2D parameterisation significantly reduces the memory and computation requirements of the field, while introducing minimal overhead in collision detection tests. The interior of the mesh is defined implicitly for the entire domain. Importantly, it maps well to the hardware rasteriser of the GPU. Radial fields are much more application-specific than traditional distance fields. For these applications - such as collision detection with articulated characters-however, the benefits are substantial.","Authors":"S. Friston; A. Steed","DOI":"10.1109\/TVCG.2018.2859924","Keywords":"Distance fields;collision detection;deformable meshes;articulated characters","Title":"Real-Time Collision Detection for Deformable Characters with Radial Fields","Keywords_Processed":"distance field;articulate character;deformable mesh;collision detection","Keyword_Vector":[0.0290457538,0.031339484,-0.0202682486,0.0086438516,0.0011615575,0.0190504793,0.0192583834,0.0235362498,-0.0197190233,0.0203692202,-0.0585834313,-0.0135547127,-0.0457190916,0.0280791584,-0.0123607791,-0.0214849795,0.0385074196,-0.0636940546,-0.0412957135,-0.0033100472,-0.0110424136,0.0016450083,0.0259583772,0.0410600845,0.058090277,0.0545899688,0.0093442787,0.088939147,0.0479508701,-0.0119956864,0.0395934696,-0.0411598754,0.034576743,-0.0055171846,0.0172850091,0.0263124212,-0.045986432,0.0225231596,-0.0327300043,-0.0257071841,-0.0231294217,-0.0252817017,0.0298083436,-0.0113896535,0.022739198,-0.0097371644,-0.0017923098,0.0028218548,-0.0315882031,0.0187757662,-0.0241836394,-0.0051003633],"Abstract_Vector":[0.1607778363,0.0798963314,-0.0315869266,0.0063789571,-0.0765201943,0.0362593699,0.0002889972,-0.0291962164,-0.0218182343,0.0029302307,0.0170176803,0.0063304761,-0.0427079796,-0.000517885,0.0085514818,0.0040664384,0.0358960953,0.0501606752,-0.0575347983,-0.075943935,0.0608487338,-0.0339980843,0.023785565,0.0396492738,0.013358189,0.0003906651,0.0456900625,-0.0207994896,-0.0359059949,0.0057919126,-0.0631299108,-0.0306054606,-0.0233401793,-0.0077737353,-0.0183249338,-0.0138282065,0.018086757,-0.0115781777,0.0059747094,-0.0161776572,0.0141707036,0.0145489081,-0.0350926026,0.0248709798,0.0078993047,0.0303479014,-0.0334001015,-0.0000784702,-0.0012819063,-0.0128666798,0.0121592144,0.0265097024,-0.0050842151,0.0274637677,-0.0102694793,0.0022471091,-0.0495596391,-0.0045769854,0.0010242664,-0.0107177708,0.0413027069,0.0500469289,-0.0095102216,0.0401155769,0.0169918523,-0.0044542663,-0.0184601776,-0.0060893215,-0.0063456478,-0.0002507388,0.019933505,0.0388789114,-0.0410227393,-0.00061975,0.0165010221,-0.0014409445,-0.0067686665,0.048538885,0.0152132856,0.0337063485,0.0354807206,-0.0055344196,-0.0075496816,-0.0212214548,-0.0179246926,0.0101126827,0.0035066391,0.0406469564,-0.0112324562,-0.0086749719,-0.0013185873,0.0442445022,-0.0081301209,-0.0048090105,0.0250671056,0.0070150162,-0.0111424787,0.0027385478,-0.0076920843,-0.000735541,-0.0381013218,-0.0040202584,0.0061777724,-0.0140774369,-0.0128368046,0.0055300883,-0.0078620876,0.0129205403,-0.0354073655,0.0444701282,0.009900505,-0.0416625548,-0.0207772945,0.0176839225,0.0158351789,-0.0020672933]},"182":{"Abstract":"Much research has been done regarding how to visualize and interact with observations and attributes of high-dimensional data for exploratory data analysis. From the analyst's perceptual and cognitive perspective, current visualization approaches typically treat the observations of the high-dimensional dataset very differently from the attributes. Often, the attributes are treated as inputs (e.g., sliders), and observations as outputs (e.g., projection plots), thus emphasizing investigation of the observations. However, there are many cases in which analysts wish to investigate both the observations and the attributes of the dataset, suggesting a symmetry between how analysts think about attributes and observations. To address this, we define SIRIUS (Symmetric Interactive Representations In a Unified System), a symmetric, dual projection technique to support exploratory data analysis of high-dimensional data. We provide an example implementation of SIRIUS and demonstrate how this symmetry affords additional insights.","Authors":"M. Dowling; J. Wenskovitch; J. T. Fry; S. Leman; L. House; C. North","DOI":"10.1109\/TVCG.2018.2865047","Keywords":"Dimension reduction;semantic interaction;exploratory data analysis;observation projection;attribute projection","Title":"SIRIUS: Dual, Symmetric, Interactive Dimension Reductions","Keywords_Processed":"attribute projection;exploratory datum analysis;dimension reduction;observation projection;semantic interaction","Keyword_Vector":[0.044152405,-0.0583771127,-0.0655570431,0.0373607035,-0.0131987187,-0.0916239294,0.0123009207,0.0258543363,-0.0004040596,-0.0083472436,0.0196178386,0.0321934467,-0.0163215772,-0.0381757636,0.0643118138,0.0735528569,-0.0442247306,-0.0465609263,0.1300843459,0.0077806853,0.0281895798,-0.0553634158,0.1258299022,0.0296006931,0.0558345819,0.0790570499,-0.0577299772,-0.043774116,-0.0197131575,-0.0489031129,0.0482335979,0.0081525514,0.0048012158,-0.0357335076,-0.0063388713,-0.0698805103,-0.0152918238,-0.0053250211,-0.0251959244,-0.0118919856,-0.0081137836,-0.0122437786,0.0368888677,-0.0466043046,-0.0258693869,-0.0055033616,-0.0245150925,0.0425623632,0.0328997186,-0.0219489263,0.0259828219,-0.0018145762],"Abstract_Vector":[0.2565248436,-0.128385517,0.0470740339,-0.0047116575,-0.045055643,0.1005191964,0.000433383,0.038162019,0.0282545094,-0.0314393334,-0.0578363623,0.0603281945,0.1100476051,-0.0092620514,0.0024763185,-0.0458282418,0.0300908797,0.0195225174,0.0460755463,0.0548605971,-0.0189166553,-0.0462752361,-0.0555124601,0.0088996772,-0.0385355014,-0.0060639487,-0.0014070806,0.0516378066,0.0058936926,0.0407044112,-0.0211912487,0.0033890599,0.0413322849,-0.0018505648,-0.0253427227,0.0103864477,0.0193404493,0.0040092645,0.0000938716,0.0386264777,0.0322994829,-0.0349973471,-0.0655743547,0.0576187795,-0.0038111745,0.0807196871,0.0014317881,-0.0247234496,0.0851482249,0.002507023,-0.011082257,-0.0189201809,-0.0152958273,-0.0411232109,-0.0012193061,-0.0096269596,-0.0150252463,0.0361842482,-0.0195457733,-0.0081994895,-0.0191757361,0.0903132758,0.0149880905,-0.0192690711,-0.0077921156,-0.0022563469,0.0079943189,-0.0000228102,0.0369828811,-0.0046038524,-0.0070514017,-0.0085265253,0.0148212992,-0.040773056,-0.053224434,-0.0582358315,0.0101473427,-0.0373931078,-0.0145035316,-0.0662135204,-0.0096494366,-0.001351059,0.0022581225,-0.0157856969,0.0140852941,0.0030222166,-0.0051226927,-0.0329114434,0.0029578494,-0.0216275887,0.0123682149,-0.0475103476,0.0068966127,0.0004228987,0.0082811268,0.0471378016,-0.040512222,0.0174836582,-0.0020160396,-0.0134956803,-0.0629525881,0.0337561713,-0.0209852351,0.0095890182,-0.0073821547,0.0223459283,0.0162206445,-0.0052885763,0.0309295245,-0.0090271437,0.0540142159,0.0211705968,-0.013366001,0.0390983024,0.0066895767,0.0003107442]},"183":{"Abstract":"In Augmented Reality (AR), search performance for outdoor tasks is an important metric for evaluating the success of a large number of AR applications. Users must be able to find content quickly, labels and indicators must not be invasive but still clearly noticeable, and the user interface should maximize search performance in a variety of conditions. To address these issues, we have set up a series of experiments to test the influence of virtual characteristics such as color, size, and leader lines on the performance of search tasks and noticeability in both real and simulated environments. We evaluate two primary areas, including 1) the effects of peripheral field of view (FOV) limitations and labeling techniques on target acquisition during outdoor mobile search, and 2) the influence of local characteristics such as color, size, and motion on text labels over dynamic backgrounds. The first experiment showed that limited FOV will severely limit search performance, but that appropriate placement of labels and leaders within the periphery can alleviate this problem without interfering with walking or decreasing user comfort. In the second experiment, we found that different types of motion are more noticeable in optical versus video see-through displays, but that blue coloration is most noticeable in both. Results can aid in designing more effective view management techniques, especially for wider field of view displays.","Authors":"E. Kruijff; J. Orlosky; N. Kishishita; C. Trepkowski; K. Kiyokawa","DOI":"10.1109\/TVCG.2018.2854737","Keywords":"Augmented reality;head mounted display;perception;peripheral vision;visualization","Title":"The Influence of Label Design on Search Performance and Noticeability in Wide Field of View Augmented Reality Displays","Keywords_Processed":"peripheral vision;augmented reality;perception;visualization;head mount display","Keyword_Vector":[0.1016596596,0.0424035697,0.0770137376,0.127085991,0.0631555776,-0.0636958456,-0.0261427399,-0.0721923134,-0.1108332212,-0.0475294228,0.0461399495,0.0399803854,-0.0967355308,0.0164462937,0.070988077,-0.1207982578,0.0255117836,-0.0292981467,-0.0085811614,0.0093987848,-0.0193553363,-0.0651414933,0.0060301267,0.0970243418,0.029158205,0.044282393,0.0541787246,0.025102154,-0.0950661631,-0.0811257868,0.02301108,0.0690725607,0.0455655021,-0.0080371214,-0.0319712964,0.0597136399,0.1505781202,0.0299248191,0.0001368586,0.0372653148,0.1181218121,0.0371797563,-0.0005486014,-0.0087764591,0.0250097913,0.0824444117,-0.0359749128,0.0360645078,-0.0139655461,0.0138803564,0.0605943116,0.0326535989],"Abstract_Vector":[0.2185261092,0.107726405,-0.0047543778,0.1293056568,-0.0727071181,0.007849783,-0.0106799252,0.0603651639,-0.0297588412,-0.1009902698,-0.0483326387,-0.0145002924,-0.0712436078,0.1181656886,-0.0847577197,0.0570882317,0.0744609436,0.0219848694,0.0294755316,-0.0785886558,-0.0622909536,-0.0057643248,-0.0157196345,-0.0628229708,-0.0503304543,0.0090813965,0.057309349,0.0233927958,0.0281642684,0.0062559432,0.0585097069,0.0479539687,-0.0022312281,-0.0018476305,0.0323862793,-0.0233680713,0.0067539483,0.0106457912,0.01678833,0.0324411643,0.0339798832,-0.0261178399,0.0335550015,-0.0279339077,0.0325908627,0.048570695,0.0578326818,0.0224171121,-0.01869337,-0.0156802849,-0.0017735433,-0.0082781802,-0.0560102448,-0.0144877683,0.0303811608,-0.007705893,-0.022525997,-0.0342283001,0.0470136651,0.0322753143,0.0644447927,0.0222944669,-0.0089401545,0.0420318981,0.0018305931,0.0121129899,-0.0213008853,-0.0467154012,0.0018653968,0.0160801628,-0.0440652017,-0.0032480278,-0.0253242836,-0.0134614695,0.0176022967,0.0121078796,0.0028688303,-0.0169865604,-0.0141831613,0.0387524596,0.0249090774,-0.0001634026,0.00227508,0.0471705167,-0.0068625977,-0.0239325587,0.0160913592,-0.0033076487,-0.0099034737,-0.0108891017,-0.0162560389,-0.0065169365,0.0002362956,-0.0092867969,-0.010384451,0.0159067462,-0.0043733579,-0.0356096777,0.0379846839,-0.0059635977,0.0058232095,-0.0327044403,-0.0003370003,-0.0308965433,-0.0288888893,0.0104647274,0.0199370526,0.0562800404,0.0146797364,0.0356331734,0.0220032176,-0.0026748618,0.0264070249,0.0251223945,-0.0223181519,-0.0048419466]},"184":{"Abstract":"Discovering and analyzing biclusters, i.e., two sets of related entities with close relationships, is a critical task in many real-world applications, such as exploring entity co-occurrences in intelligence analysis, and studying gene expression in bio-informatics. While the output of biclustering techniques can offer some initial low-level insights, visual approaches are required on top of that due to the algorithmic output complexity. This paper proposes a visualization technique, called BiDots, that allows analysts to interactively explore biclusters over multiple domains. BiDots overcomes several limitations of existing bicluster visualizations by encoding biclusters in a more compact and cluster-driven manner. A set of handy interactions is incorporated to support flexible analysis of biclustering results. More importantly, BiDots addresses the cases of weighted biclusters, which has been underexploited in the literature. The design of BiDots is grounded by a set of analytical tasks derived from previous work. We demonstrate its usefulness and effectiveness for exploring computed biclusters with an investigative document analysis task, in which suspicious people and activities are identified from a text corpus.","Authors":"J. Zhao; M. Sun; F. Chen; P. Chiu","DOI":"10.1109\/TVCG.2017.2744458","Keywords":"Biclustering;coordinated relationship analysis;visual analytics","Title":"BiDots: Visual Exploration of Weighted Biclusters","Keywords_Processed":"biclustering;visual analytic;coordinated relationship analysis","Keyword_Vector":[0.017134124,0.015704389,-0.0102138882,-0.0097758161,0.0027249555,-0.0131857948,-0.0236778819,-0.0027848682,-0.0532180957,-0.0122181219,0.0268819882,0.0386188835,0.0021434461,0.014890639,0.0368848884,0.0416119796,0.0380924613,0.0396355353,0.1205803393,-0.0020852857,-0.0682301884,0.0508432998,0.067618434,0.0363735356,-0.055133128,-0.0307499818,-0.0015222029,-0.0105819686,0.0609417412,-0.0928993202,-0.0270554876,-0.0778310878,0.0431618417,0.0206135889,-0.0339765551,-0.0132680909,-0.0089475095,0.0037601673,-0.0358283678,-0.0348753184,-0.0172746256,0.0356266276,0.0039466689,0.0321675049,-0.0035168354,-0.0294632786,0.0112703109,0.0289059126,-0.0101876352,0.0292021489,-0.0243172248,-0.0093179181],"Abstract_Vector":[0.1336889856,-0.0218194717,-0.0130640375,0.0042570631,-0.0404661179,0.0321924705,-0.0166906306,0.0170074609,0.0207070758,-0.0135857566,-0.0124112293,0.0377018916,-0.0198687791,0.0126276054,0.0193681949,0.031941397,-0.0115435238,0.0254203111,0.0298369334,-0.0042859134,0.001829029,-0.0193088404,-0.0059356854,0.014146453,0.0019361006,-0.0029465624,0.0171099957,0.006275344,0.0131052523,0.0154116694,-0.0059748085,0.0028057261,-0.0248002149,-0.0059170594,0.0239360009,-0.0473624109,-0.007522,-0.0087402044,-0.0199865967,-0.0150619666,0.0351285851,0.0364069442,0.0137770679,-0.0114631886,-0.0009123018,0.0279966362,0.016235131,-0.0151664116,0.0211312522,-0.0430284989,-0.0014796637,-0.00481203,0.0035611052,0.0029777781,0.0194943438,-0.0069707659,0.0166318228,0.0235136383,-0.0260670373,-0.0342988123,0.0257749951,0.0221897426,-0.0021876922,0.0148672881,-0.0361033957,-0.0061975917,0.0151343191,-0.015681141,0.0021629747,0.0339316152,-0.0230001807,0.0175440969,0.0039770114,0.0200921082,-0.0000686482,-0.0368199242,0.0351419208,0.0383089018,-0.0007901054,-0.0121284768,-0.0140240935,-0.0276129555,0.008309866,0.0299436216,0.0183695722,-0.0213152436,-0.0069656118,0.0138430663,-0.0263475397,-0.0186087166,0.0289144,-0.0112372187,0.0015285964,0.0259931408,-0.0019677545,-0.0293178897,0.0142426354,-0.0298988382,0.0015993046,-0.0210662248,0.0297715654,-0.0210905033,0.0289618017,0.0048225093,-0.0100141281,0.0050498917,0.0082758852,0.0378992382,-0.0143643628,-0.0207699056,-0.0112443021,0.00674078,0.0238642253,0.0018058983,0.0007869454,0.0208138339]},"185":{"Abstract":"Cloth is made of yarns that are stitched together forming semi-regular patterns. Due to the complexity of stitches and patterns, the macroscopic behavior of cloth is dictated by the contact interactions between yarns, not by the mechanical properties of yarns alone. The computation of cloth mechanics at the yarn level appears as a computationally complex and costly process at first sight, due to the need to resolve many fine-scale contact interactions. We propose instead an efficient representation of cloth at the yarn level that treats yarn-yarn contacts as persistent, but with the possibility to slide, thereby avoiding expensive contact handling altogether. We introduce a compact representation of yarn geometry and kinematics, capturing the essential deformation modes of yarn crossings, loops, stitches, and stacks, with a minimum cost. Based on this representation, we design force models that reproduce the characteristic macroscopic behavior of yarn-based fabrics. Our approach is suited for both woven and knitted fabrics. We demonstrate the efficiency of our method on simulations with millions of degrees of freedom (hundreds of thousands of yarn loops), almost one order of magnitude faster than previous techniques. We also compare the different macroscopic behavior under woven and knitted patterns with the same yarn density.","Authors":"G. Cirio; J. Lopez-Moreno; M. A. Otaduy","DOI":"10.1109\/TVCG.2016.2592908","Keywords":"Yarns;knitted cloth;woven cloth;physically based simulation","Title":"Yarn-Level Cloth Simulation with Sliding Persistent Contacts","Keywords_Processed":"yarn;physically base simulation;weave cloth;knit cloth","Keyword_Vector":[0.0977112555,-0.1176005799,-0.110134591,0.0721197571,-0.0871344834,-0.0745123847,0.0516508326,-0.0167095478,0.0130594203,-0.0313213978,0.0350857438,-0.0131076073,-0.0096700117,-0.0436601755,-0.0382414863,0.0074426416,-0.0329272218,-0.0249092699,-0.0457499739,0.0126297609,-0.0182396235,0.0042303515,0.0866528304,0.0467976248,0.0997488779,-0.0221105311,0.0649684544,-0.0208839198,0.0969728178,-0.0087083983,0.0429980385,0.0302428209,-0.0341574522,-0.0812407571,0.0330398635,-0.1232143478,-0.0487172576,0.0158040694,-0.0366832068,-0.0471023084,0.0026366404,0.2078594647,-0.0118501316,-0.0606049369,-0.0290467113,-0.0233430047,-0.0439596232,0.0008427995,0.0149882923,-0.0759461462,0.1254044496,0.0013550756],"Abstract_Vector":[0.1890470041,-0.1402312082,-0.0001254838,-0.0197682505,-0.0318289475,-0.0301172881,0.0123673072,-0.028842692,-0.0755739779,0.0108285424,0.0767297645,0.0023118681,-0.1031753783,-0.0070248906,0.027206831,0.0019063762,0.0554969003,-0.0474808114,0.0204127943,0.0046020707,-0.0495200245,0.0188382102,0.0239259264,0.0550130559,0.0126144605,-0.04021452,0.0325708576,0.0479864595,-0.0265043638,-0.0318912826,0.0325401035,-0.0236223255,-0.0045239896,0.0483253994,0.0724450392,0.0278963802,-0.038138722,-0.0085504823,-0.0004366976,-0.0065972083,0.0051513155,-0.0157752239,0.0641342042,-0.038326483,0.0087376322,0.0235738069,0.0019316961,-0.0007171114,-0.0081926209,-0.0211312266,-0.0220230682,-0.0756770712,0.0096535273,0.0466421876,0.0124378428,-0.0257366438,-0.0208205888,0.0594095399,-0.0698407872,0.0542987809,0.0144089258,-0.062359491,0.0315139762,-0.0548099372,-0.0434214062,-0.0202690334,0.0005223524,0.0076286529,-0.028426219,-0.0354664704,-0.0365982432,-0.000351601,-0.0286704955,0.0138098721,-0.0167392341,0.0329417607,-0.0255716829,0.0054921446,-0.0466828049,-0.0084679369,-0.0314231583,0.061840725,-0.0331484925,-0.0048570875,-0.0802199147,-0.0681423723,0.0004034068,-0.0138539822,0.0657473649,0.078750954,0.0303996236,-0.0308263683,0.0100650182,0.0430859835,0.0020217733,0.0236285595,-0.0377000919,0.046985252,-0.0179168903,-0.0077984875,-0.007769482,-0.0165868326,0.0672370199,0.0009243164,-0.0687673886,0.0500094621,-0.0323303458,0.0365885517,-0.0558539236,-0.0301652581,0.0463423866,0.0426612901,0.0406519851,0.0037899524,-0.0059427126,-0.015425957]},"186":{"Abstract":"We present a novel algorithm to generate virtual acoustic effects in captured 3D models of real-world scenes for multimodal augmented reality. We leverage recent advances in 3D scene reconstruction in order to automatically compute acoustic material properties. Our technique consists of a two-step procedure that first applies a convolutional neural network (CNN) to estimate the acoustic material properties, including frequency-dependent absorption coefficients, that are used for interactive sound propagation. In the second step, an iterative optimization algorithm is used to adjust the materials determined by the CNN until a virtual acoustic simulation converges to measured acoustic impulse responses. We have applied our algorithm to many reconstructed real-world indoor scenes and evaluated its fidelity for augmented reality applications.","Authors":"C. Schissler; C. Loftin; D. Manocha","DOI":"10.1109\/TVCG.2017.2666150","Keywords":"Sound propagation;material optimization;recognition","Title":"Acoustic Classification and Optimization for Multi-Modal Rendering of Real-World Scenes","Keywords_Processed":"material optimization;sound propagation;recognition","Keyword_Vector":[0.1402998516,-0.0468631997,-0.0584267881,-0.0850415845,0.0189066785,-0.0676898045,-0.157602998,-0.0344200475,-0.2287996385,0.1655328827,-0.0736846346,0.2420564407,0.1255470687,-0.0483062865,0.0846364733,-0.023369231,0.1851670716,0.0209866371,0.2096523438,-0.0191004677,-0.1500967113,0.1313225211,0.2009809109,0.0836769482,-0.0246957902,-0.122686903,0.1282515622,-0.104518891,0.1388893712,-0.1159724807,0.0236075453,-0.0387376502,-0.0241273951,0.0591786136,-0.0050439578,0.0675534943,0.0864858712,0.1341375454,0.0518158909,-0.0371083782,-0.06927414,0.0501786662,0.0017932694,-0.0231336197,-0.1190243482,0.0634960639,-0.0396374483,0.0883178062,-0.0298119429,0.0157912289,-0.0051252235,0.1395249183],"Abstract_Vector":[0.1403652071,-0.0574386301,0.0013955423,0.004426158,-0.0422850782,0.0493843515,-0.006427753,0.0104855282,0.0518151929,0.015096102,-0.0510564607,0.0240165678,0.003392354,0.0074166142,0.0025761217,0.0445849018,0.0099796045,0.0145472098,0.0377909074,0.0111206079,-0.0010486962,-0.0460125694,0.0165057285,-0.0101644895,-0.0285728158,-0.0005171864,-0.0040220969,0.0066141477,-0.0019330364,0.0008153564,-0.0382247575,0.004147044,-0.0361485765,0.0131742646,0.0282507692,-0.035561385,-0.0074704518,-0.0338746247,-0.0435134311,-0.0318809757,0.0168170332,0.0163759073,0.0109750168,-0.0356828453,0.0044249922,-0.0326660391,0.0152673762,0.0421509322,0.0050490635,-0.0057459636,-0.0121695927,-0.0432771563,0.0230440567,-0.00462221,0.0529437335,0.0253052777,0.0140730826,0.0282242207,-0.0076303644,-0.0254129187,0.0026342986,-0.0446888694,0.0196865785,-0.0302874343,-0.0061892637,-0.0069907723,0.0149464051,-0.0098346724,-0.0445772797,-0.0015741847,-0.0190018027,0.0267297384,0.0391186868,-0.0185233021,0.0210112003,-0.0396896334,0.013060001,0.0191693009,0.0103616912,0.0165930033,-0.0076490927,0.0263132641,0.0111772311,-0.0117386247,-0.0197257684,0.0239763997,-0.006626142,-0.0395719704,0.017215128,-0.0059020189,0.0621379458,-0.0273670226,-0.0034607149,-0.0372978111,-0.022078319,0.0332193632,-0.016019557,-0.0626829137,0.017416133,0.0045815776,-0.0089336826,-0.0611623263,0.0170706408,-0.0467994024,0.0076003976,0.0078402378,-0.0177601077,0.0409057183,0.0109544175,0.0148563136,0.0431754435,0.0097608576,-0.0156840651,-0.0614469694,-0.003566484,0.0138662323]},"187":{"Abstract":"The reduced gravity experienced in lunar or Martian surfaces can be simulated on the earth using a cable-driven system, where the cable lifts a person to reduce his or her weight. This paper presents a novel cable-driven system designed for the purpose. It is integrated with a head-mounted display and a motion capture system. Focusing on jump motion within the system, this paper proposes to scale the jump and reports the experiments made for quantifying the extent to which a jump can be scaled without the discrepancy between physical and virtual jumps being noticed by the user. With the tolerable range of scaling computed from these experiments, an application named retargeted jump is developed, where a user can jump up onto virtual objects while physically jumping in the real-world flat floor. The core techniques presented in this paper can be extended to develop extreme-sport simulators such as parasailing and skydiving.","Authors":"M. Kim; S. Cho; T. Q. Tran; S. Kim; O. Kwon; J. Han","DOI":"10.1109\/TVCG.2017.2657139","Keywords":"Virtual reality;reduced gravity;scaled jump;detection thresholds;visual gain","Title":"Scaled Jump in Gravity-Reduced Virtual Environments","Keywords_Processed":"virtual reality;reduce gravity;scale jump;detection threshold;visual gain","Keyword_Vector":[0.0068236107,0.0049358232,0.0042524791,0.0097906437,0.0116681181,-0.0005549396,-0.0010271795,-0.0129868514,-0.0078142679,-0.0042623058,0.0071675475,-0.0059352038,-0.0206822009,-0.0109405726,0.0000010944,-0.0167924541,-0.0020633379,-0.0084579348,-0.0005621787,-0.0069006362,0.0035772876,0.0013309004,-0.0056170967,-0.0089765053,0.0142477604,0.0025349187,0.0138165532,-0.0039560544,-0.0138897096,-0.0110414373,-0.0262363302,-0.0186821847,-0.0041971304,0.0074125194,-0.0023319228,-0.0156614725,0.0004564988,0.0304533493,-0.0190583025,-0.0121883089,0.0118967926,0.0164726224,-0.0188909173,-0.007813158,0.0352094396,-0.033307645,-0.0195528418,0.0374253021,-0.0021631567,-0.009400993,-0.0042218779,0.0038333384],"Abstract_Vector":[0.1967397063,0.2024245957,0.0206898421,0.2000617705,-0.1312087604,-0.0828680173,0.0029745393,0.0160707419,-0.017797538,-0.0444279221,-0.0243039209,-0.0215450353,-0.0317681408,0.0580795305,-0.0073063492,0.0299588459,-0.0444457827,-0.0305474746,-0.0040422477,0.0426016589,-0.0727924277,-0.0334473072,0.0242921507,-0.058064749,0.0116780376,0.003052,-0.0380175318,-0.0225936816,-0.0131920899,-0.045880778,-0.0071762926,-0.0164571686,0.0260083075,0.0097034904,0.0044826853,-0.0185001799,0.030784545,0.0528217289,0.0566622176,0.0471626181,-0.0339823867,-0.024262048,0.0234278499,0.0283849245,0.06153933,0.0128384099,-0.0037899998,-0.0169387726,-0.0230394497,-0.043141637,-0.044107969,-0.000725194,0.0272268891,-0.0060260222,-0.03389777,-0.0177808436,0.0200555571,0.0027199909,-0.0139023686,-0.0270934875,-0.01570731,0.0277256158,-0.0541119781,-0.0402034948,0.0142659568,-0.0622395942,-0.0266766906,-0.0326974212,0.0335747293,-0.0084338532,0.0389216034,0.0183236858,0.0095167658,-0.0113817769,0.0012892752,0.003852875,0.0174122083,0.0222320018,-0.0592147595,0.0252892789,0.0563577992,0.0247373824,0.0193610026,0.0200538871,0.0355444815,0.0060557897,-0.0622615455,0.0207711599,-0.0169121389,0.0270441893,0.023455862,-0.0106105673,0.0551229403,-0.005877111,-0.0170518525,-0.0461483407,-0.0358623657,0.0303274526,0.0408314756,-0.0203416449,-0.0416723128,-0.0101438259,0.0252712111,-0.0027441948,-0.0319086741,0.0076792777,-0.0096865591,0.0036484612,-0.0084660679,0.01644522,-0.0078954044,-0.0106942817,-0.0092604741,-0.0264001221,-0.0169488806,0.0078565802]},"188":{"Abstract":"We present Aggregate G-Buffer Anti-Aliasing (AGAA), a new technique for efficient anti-aliased deferred rendering of complex geometry using modern graphics hardware. In geometrically complex situations where many surfaces intersect a pixel, current rendering systems shade each contributing surface at least once per pixel. As the sample density and geometric complexity increase, the shading cost becomes prohibitive for real-time rendering. Under deferred shading, so does the required framebuffer memory. Our goal is to make high per-pixel sampling rates practical for real-time applications by substantially reducing shading costs and per-pixel storage compared to traditional deferred shading. AGAA uses the rasterization pipeline to generate a compact, pre-filtered geometric representation inside each pixel. We shade this representation at a fixed rate, independent of geometric complexity. By decoupling shading rate from geometric sampling rate, the algorithm reduces the storage and bandwidth costs of a geometry buffer, and allows scaling to high visibility sampling rates for anti-aliasing. AGAA with two aggregates per-pixel generates results comparable to 32$\\times$  MSAA, but requires 54 percent less memory and is up to 2.6$\\times$  faster ( $-30$  percent memory and 1.7 $\\times$  faster for 8 $\\times$  MSAA).","Authors":"C. Crassin; M. McGuire; K. Fatahalian; A. Lefohn","DOI":"10.1109\/TVCG.2016.2586073","Keywords":"Anti-aliasing;graphics pipelines;pre-filtering;shading","Title":"Aggregate G-Buffer Anti-Aliasing -Extended Version-","Keywords_Processed":"anti aliase;pre filter;shade;graphic pipeline","Keyword_Vector":[0.0320130564,0.0333269141,0.0149866994,0.0172746433,0.0101231404,-0.0367518223,-0.0252570769,-0.0244582644,-0.0022178962,-0.0578981485,0.0147478523,0.0143718176,0.0877143273,0.011171189,0.0542873674,-0.001490038,0.0331789225,0.0312782904,-0.0299439103,0.0342138803,0.0627071125,-0.0640009151,0.0398453234,-0.0178624592,-0.0043891161,0.0088025546,0.0210781271,0.0833394665,0.0485582774,0.0140498669,0.0127205453,-0.0663395832,-0.0154985074,0.0016319938,0.0011365053,0.0342829328,0.0088217006,-0.0154987332,0.0195624071,0.0445851138,-0.0170434869,-0.0054292932,-0.0184676863,-0.0125478687,0.0052999007,-0.0236867653,0.0392765495,0.0128193516,0.0049232521,-0.0213725645,-0.0177492204,-0.0177808156],"Abstract_Vector":[0.139889503,0.012226304,0.0308232191,-0.024726498,-0.0225821957,0.0367071097,-0.0002271094,0.0603260308,0.0109549744,-0.0049555947,0.03710689,0.0147935512,-0.0382731068,0.0117324654,-0.0236090799,0.016893575,0.0139947632,0.067385784,-0.0179382994,-0.0879512763,0.0632771451,-0.0226626215,0.0091046227,0.0362222958,0.0356539014,0.0462244985,0.0119081694,-0.0124154442,0.0224361205,0.0146383345,-0.0021769526,0.0373515798,-0.0141175065,0.041281281,0.0050490128,-0.0073942044,-0.0256964686,0.0357954199,-0.011689939,-0.0183436105,0.0501522361,0.0194140525,0.0189773831,0.0067297971,-0.0204623494,-0.0407376134,-0.0145120975,-0.0218124605,-0.020215112,-0.0214057128,0.0319809833,-0.000163204,0.0115938397,0.030684826,-0.021357622,-0.0121186262,0.0267771248,-0.0237179661,0.0134758598,0.0154180914,0.0043973591,-0.0132831667,0.0081191961,-0.0161940623,-0.0325275775,-0.0532182009,-0.0156200279,-0.0474147809,0.0125998211,0.0452504913,-0.0054065052,0.0135021264,-0.0052154228,0.0291852311,-0.0053898215,0.0125650268,-0.0071542404,-0.0164762598,-0.0071041042,-0.0300428303,0.001306165,0.0008787404,-0.0130822415,0.0183172025,0.0064973182,-0.0450197596,0.0098593127,-0.0094795241,0.0088595779,0.0352251242,0.0360393934,0.0021311,-0.0112146486,-0.0138948216,-0.0129110167,-0.0179580492,0.0232043453,-0.0330624911,-0.0049743586,-0.0191852636,0.0276381308,0.0011486591,-0.0190564069,0.0259991959,-0.0415439272,-0.0070663401,-0.0128121489,-0.0272320608,0.0113218727,0.0524160133,0.0212216647,-0.0574344707,-0.0249284519,0.0129800526,0.0047844716,-0.013718933]},"189":{"Abstract":"A 3D modeling system with all-inclusive functionality is too demanding for a casual 3D modeler to learn. There has been a shift towards more approachable systems, with easy-to-learn, intuitive interfaces. However, most modeling systems still employ mouse and keyboard interfaces, despite the ubiquity of tablet devices and the benefits of multi-touch interfaces. We introduce an alternative 3D modeling and fabrication paradigm using developable surfaces, inspired by traditional papercrafting, and we implement it as a complete system designed for a multi-touch tablet, allowing a user to fabricate 3D scenes. We demonstrate the modeling and fabrication process of assembling complex 3D scenes from a collection of simpler models, in turn shaped through operations applied to virtual paper. Our fabrication method facilitates the assembly of the scene with real paper by automatically converting scenes into a series of cutouts with appropriately added fiducial markers and supporting structures. Our system assists users in creating occluded supporting structures to help maintain the spatial and rigid properties of a scene without compromising its aesthetic qualities. We demonstrate several 3D scenes modeled and fabricated in our system, and evaluate the faithfulness of our fabrications relative to their virtual counterparts and 3D-printed fabrications.","Authors":"P. Paczkowski; J. Dorsey; H. Rushmeier; M. H. Kim","DOI":"10.1109\/TVCG.2018.2820068","Keywords":"Multi-touch interface;3D modeling;fabrication;papercraft","Title":"PaperCraft3D: Paper-Based 3D Modeling and Scene Fabrication","Keywords_Processed":"Multi touch interface;3d modeling;papercraft;fabrication","Keyword_Vector":[0.0460135785,-0.0115888038,-0.0002392493,0.0516387957,-0.0032653779,-0.0201718355,0.0145879184,-0.0153277464,-0.0512106867,0.0129836127,0.0588329574,0.0041104219,-0.0218263447,0.1080150867,-0.0382517175,-0.046530264,0.0302894576,0.0085353539,-0.0240137385,-0.0190299382,-0.0076305572,-0.0404623359,-0.0529566285,0.0143698249,0.0089306244,0.0524118349,0.0010187206,-0.0027635219,-0.0034444414,0.0023193002,-0.0167457825,-0.0378653649,-0.0093502778,0.0125658645,-0.0198661393,0.0436817656,0.0313294301,-0.0011491828,-0.0460866674,0.0092854663,-0.0396936105,-0.010907872,0.0292866139,0.0139541232,0.0482795526,-0.0109295014,0.0184129135,0.0791620629,-0.0098615666,-0.0726688371,-0.0532394428,0.0474420265],"Abstract_Vector":[0.1360814032,0.007642988,0.0308096547,0.0139728139,-0.0408736092,0.017129136,-0.0410123927,0.0534201217,-0.0122590465,0.0018301847,0.0125661532,-0.0006482898,-0.0261358033,0.0071113874,0.0051119157,0.0137180282,-0.0392808551,0.0004246151,-0.0177451699,-0.0081442438,0.0028145823,-0.0349179897,0.0214076197,-0.0200476443,0.0160357044,0.0123571726,-0.0018708407,0.074894013,-0.0192311594,-0.0161771956,0.0322094197,0.0036540795,-0.0260627599,-0.008540002,-0.0176705828,-0.014958892,-0.0007058879,0.0435232818,0.0131105908,0.0405668297,0.0377551191,0.0352797677,0.0115869343,-0.0234660833,-0.021247273,-0.0160244029,-0.0044248993,0.053264056,-0.0144303844,-0.0198306098,0.0153904509,-0.002527287,0.0022831826,-0.005105085,-0.0084242947,0.0237404184,-0.0044054972,-0.0056767065,-0.0317775928,0.0410330852,0.0221542651,0.011695203,0.022288969,-0.0312604582,-0.0096295327,0.0533974484,0.040148488,-0.0120963312,0.0317617157,0.0744550631,-0.032285655,-0.0067727575,0.0625426611,0.0083820208,0.0674645348,0.0284135766,-0.0717384567,-0.0408293453,0.0105445642,0.0508187021,0.0140109301,-0.0581056754,0.0304672174,-0.0537673716,0.0343111437,-0.0502474793,-0.0327771199,-0.0366381552,-0.0151793462,0.028671161,-0.0317502646,-0.0098469262,-0.0404317421,0.0298896185,-0.0100809001,-0.0518108346,-0.0147907341,0.0623567115,-0.0128228034,-0.0174842961,-0.040400185,0.0036050268,0.0121890392,-0.0070616888,0.0286595767,0.0000367448,0.0093471478,-0.0202688265,-0.0006473859,0.0071608787,-0.0390172266,0.0625723862,-0.0201180767,-0.0060626162,-0.0368237905,0.0450791704]},"19":{"Abstract":"Multivariate, tabular data is one of the most common data structures used in many different domains. Over time, tables can undergo changes in both structure and content, which results in multiple versions of the same table. A challenging task when working with such derived tables is to understand what exactly has changed between versions in terms of additions\/deletions, reorder, merge\/split, and content changes. For textual data, a variety of commonplace \u201cdiff\u201d tools exist that support the task of investigating changes between revisions of a text. Although there are some comparison tools which assist users in inspecting differences between multiple table instances, the resulting visualizations are often difficult to interpret or do not scale to large tables with thousands of rows and columns. To address these challenges, we developed TACO, an interactive comparison tool that visualizes the differences between multiple tables at various levels of detail. With TACO we show (1) the aggregated differences between multiple table versions over time, (2) the aggregated changes between two selected table versions, and (3) detailed changes between the selected tables. To demonstrate the effectiveness of our approach, we show its application by means of two usage scenarios.","Authors":"C. Niederer; H. Stitz; R. Hourieh; F. Grassinger; W. Aigner; M. Streit","DOI":"10.1109\/TVCG.2017.2745298","Keywords":"Table comparison;matrix;difference visualization","Title":"TACO: Visualizing Changes in Tables Over Time","Keywords_Processed":"matrix;difference visualization;table comparison","Keyword_Vector":[0.1372018909,-0.1723368824,-0.1609185917,0.1687979542,-0.1130220721,-0.0403936692,0.0537353749,0.0113209833,0.0530096884,-0.0128224956,0.0296154322,-0.0141341936,-0.0032549012,-0.0906225502,0.0468102034,-0.0302898676,-0.0342270166,0.0022650745,-0.0116976486,-0.0138931994,0.0010532596,-0.0052083549,0.0455731169,-0.0081551004,-0.0234421308,-0.018932744,-0.0151372075,-0.001072049,0.013002103,-0.0084331033,0.0285197154,-0.001174837,0.0283367725,-0.0242811634,0.020330395,-0.0109189779,0.0167499804,-0.0194946127,-0.0653174436,-0.0516611406,-0.0047538472,-0.0268648123,0.019019987,-0.0187519262,0.0022525481,-0.0040838709,-0.0147365802,-0.0166012367,0.0619487913,-0.0144605028,0.0680088874,0.0179950129],"Abstract_Vector":[0.1899481499,-0.1673592837,0.0020203428,0.0054927712,-0.0621787596,0.0238552727,0.0407776215,0.0275511291,-0.1009968138,0.0371635553,-0.0091224208,-0.0256097392,-0.0334696313,-0.0290064493,-0.0155611437,0.0055182186,-0.005168698,-0.0198971874,0.010079019,0.0723859893,0.0018634174,0.0243903923,-0.0049928641,0.0397271778,0.0283658492,-0.0135113083,-0.019038932,0.0209521047,0.0248843051,0.0074084929,0.0511429812,0.0043507956,0.0221294805,0.011787058,-0.0039674358,0.0027803566,0.0156708824,-0.0133724897,0.0247289345,0.0176665583,-0.0002203353,0.0013563884,-0.010949126,0.0334220805,0.0212103083,0.02294763,0.0031220139,0.0046468842,0.020808896,0.0050571216,0.0131343388,-0.0084147583,0.0065471455,0.0057628877,0.0102868525,-0.01827818,-0.0112504779,0.0259817592,0.0025664399,-0.0146501666,0.0195367844,-0.0190112177,0.0142238198,0.0252092708,-0.0245570613,-0.0071723237,-0.011410471,-0.0228698622,0.0053819927,-0.006277167,0.0149336363,0.0216514894,0.0360338905,-0.018992128,-0.021132275,-0.0063177947,0.0069841105,0.0109342488,-0.0231974713,-0.0111109902,-0.026859461,0.0040702878,0.0058807254,-0.0256528548,0.0007134472,-0.0127260586,0.0058439117,0.0164618162,0.0315057145,-0.025811177,-0.0245681066,-0.0260068563,0.0340119191,-0.0054302018,0.0217236245,0.0316765106,0.015926996,0.0077097442,-0.0223382576,-0.0094817294,-0.0112036524,-0.0199340815,-0.0401740522,-0.0263540558,0.0091288609,-0.0036319318,0.0166651375,-0.0106440347,-0.005873161,0.0190869035,0.005970951,0.0115611882,-0.0034531109,0.0138916004,0.0396816811,0.0122411608]},"190":{"Abstract":"We propose a novel approach to simulating the formation and evolution of stains on cloths in motion. We accurately capture the diffusion of a pigmented solution over a complex knitted or woven fabric through homogenization of its inhomogeneous and\/or anisotropic properties into bulk anisotropic diffusion tensors. Secondary effects such as absorption, adsorption and evaporation are also accounted for through physically-based modeling. Finally, the influence of the cloth motion on the shape and evolution of the stain is captured by evaluating the inertial (e.g., centrifugal and Coriolis) forces experienced by the solution. The governing equations of motion are integrated in time directly on a deforming triangle mesh discretizing the inelastic cloth for efficiency and robustness. The deformation of the cloth can be precomputed or integrated through simplified two-way coupling, by using off-the-shell cloth simulations. Finally, numerical experiments demonstrate the plausibility of our results in practical applications by reproducing the usual shape and behavior of stains on various fabrics.","Authors":"X. Wang; S. Liu; Y. Tong","DOI":"10.1109\/TVCG.2017.2789203","Keywords":"Cloth animation;inhomogeneous and anisotropic cloth material;stain formation;deforming surfaces","Title":"Stain Formation on Deforming Inelastic Cloth","Keywords_Processed":"deforming surface;inhomogeneous and anisotropic cloth material;stain formation;cloth animation","Keyword_Vector":[0.0925430352,-0.0585249134,-0.0161555756,-0.0517759145,0.0021065348,-0.032461911,-0.0483395661,-0.0076477403,-0.0104163117,0.0500307106,-0.0229415994,0.0580964764,0.0187730146,-0.0505224577,0.0117830926,0.004402219,0.0464861773,-0.0450404928,-0.0085124704,0.060547798,-0.0595354871,-0.0696348318,-0.0517322701,-0.0393294941,0.0457432321,-0.0033370776,0.019110607,-0.0274445784,0.0061655558,0.0153659435,-0.00264085,0.0157640581,0.0005693503,-0.0208271717,-0.0349139724,-0.0043721441,0.003053519,0.0356660953,0.0137082122,-0.0016906405,-0.0277490567,-0.0089432022,0.0436312138,0.035101567,-0.0153440807,0.0041617491,0.0035994088,0.0124378824,0.0448280054,0.0115741808,0.0137654169,-0.0002426704],"Abstract_Vector":[0.2264410686,-0.1308813841,0.0085242293,0.0190335594,-0.0246145348,-0.0081721743,0.0159619559,0.0070986705,-0.059460001,0.0351564947,-0.013860622,-0.0030592566,-0.0338729714,-0.0029065245,-0.0948435985,0.0029070327,-0.0223696149,-0.0380683432,-0.0088695615,0.0315793756,0.0210675576,0.0038955035,-0.0185976197,-0.0109823044,0.0044486319,-0.0353916408,-0.0128263571,-0.0211340368,-0.0178949606,0.0035342157,0.0122473897,-0.0158429891,-0.0599025811,0.0147113884,-0.00691757,0.0081111151,-0.0359551744,0.0477970036,0.0205694389,-0.0440301655,-0.0218901325,0.0134666692,0.0411018486,-0.014218935,-0.0215865229,0.0083937665,0.0532998168,0.0310045524,0.003815237,-0.0195338688,-0.0023491076,0.0214581534,0.013026784,0.0516413525,-0.021822982,0.0204524912,0.0110028531,-0.0121386888,0.0363813627,0.0100933062,0.0344311211,-0.0048023137,-0.0070776353,-0.0254322966,0.0141903176,0.0159967151,0.0117709923,0.024916656,0.0361279395,-0.0012205204,0.0024319517,0.06061762,0.0640004234,0.0119966204,0.0113595544,-0.0081269038,-0.0092011525,0.0417711758,0.0129306407,-0.0137065465,-0.0485834011,0.0155518005,0.0135590035,-0.0248893166,0.0196659774,0.0218095886,-0.0516315749,0.0376057873,-0.0137970413,0.0195712928,-0.0098854216,-0.0141319926,-0.0133546161,0.0445375603,0.0428267877,-0.0354742066,0.0089427438,0.0391327734,-0.0343212512,-0.011042418,-0.0111490548,0.0148177726,-0.0074036785,-0.0489555146,0.0142864668,0.0053047603,0.0416819237,0.0148774307,0.0465677296,0.0107943911,-0.0426109323,0.0460206435,-0.0102698466,0.0004049008,0.0274375287,0.0417526795]},"191":{"Abstract":"Sharing data for public usage requires sanitization to prevent sensitive information from leaking. Previous studies have presented methods for creating privacy preserving visualizations. However, few of them provide sufficient feedback to users on how much utility is reduced (or preserved) during such a process. To address this, we design a visual interface along with a data manipulation pipeline that allows users to gauge utility loss while interactively and iteratively handling privacy issues in their data. Widely known and discussed types of privacy models, i.e., syntactic anonymity and differential privacy, are integrated and compared under different use case scenarios. Case study results on a variety of examples demonstrate the effectiveness of our approach.","Authors":"X. Wang; J. Chou; W. Chen; H. Guan; W. Chen; T. Lao; K. Ma","DOI":"10.1109\/TVCG.2017.2745139","Keywords":"Privacy preserving visualization;utility aware anonymization;syntactic anonymity;differential privacy","Title":"A Utility-Aware Visual Approach for Anonymizing Multi-Attribute Tabular Data","Keywords_Processed":"privacy preserve visualization;differential privacy;syntactic anonymity;utility aware anonymization","Keyword_Vector":[0.0449858181,0.0154781688,0.0249095099,0.0124426014,0.0036210744,-0.0210801786,-0.0219829527,-0.0357740347,-0.0289348973,-0.057664674,0.0091692469,0.01830252,-0.0539099642,0.0016941994,0.0457127422,0.0288595431,0.0070028006,-0.0458111371,0.0471743237,-0.0313009227,0.0867799245,0.0200342116,-0.0943832717,0.0423902594,-0.0238011224,-0.0000242821,0.0079700133,0.0311512868,0.0134395388,-0.0067904042,-0.0700277861,-0.0114029946,0.0107552397,-0.0190550793,-0.0313591031,-0.0127132117,-0.0184423828,-0.028925922,-0.0521495666,0.0297962363,-0.0599527197,0.0118364734,-0.0512962367,0.0292593376,0.0073285661,0.0308651163,-0.0129573106,0.0185835762,0.0162144458,0.002000311,0.0136886148,-0.0112318066],"Abstract_Vector":[0.0936284407,0.0221945812,0.0408534173,-0.0080350628,-0.0000537514,0.0075803668,0.0015532424,0.0115521237,-0.0225594232,-0.0123693705,-0.0017487601,0.0209960981,-0.0205419675,-0.0112198678,-0.0202944877,0.0101557624,-0.032487569,0.0156228447,-0.0088413488,-0.0297287636,0.0032202479,-0.0093681691,-0.0098029465,-0.0058521642,0.0085770844,0.0209241,0.019660928,-0.0001784167,0.0218678946,-0.0059837513,-0.0055279946,0.013365443,-0.0362675384,-0.0037552826,-0.0005851847,0.01422514,0.0075388991,0.0089693947,0.0022493918,-0.024106529,0.0093564429,0.0037749439,0.0218760361,0.0051590553,-0.0250120952,-0.0051799811,0.0158126385,-0.0082442769,0.0082885547,0.0362832802,-0.0254010311,-0.015811754,0.0426321295,0.0260468371,-0.0121852131,0.0218320593,-0.0029446263,-0.0263166113,-0.0113557965,0.0295018137,-0.0110703154,0.018865714,-0.00567184,0.0150100307,-0.0216637465,0.0119416288,-0.003880591,-0.0368241022,0.0136695605,0.0111529577,0.0143150133,0.0070024432,-0.0081481954,0.0252506868,-0.0000275806,0.0081523112,-0.0356571636,0.0194357176,-0.0128400359,-0.0077712154,-0.0163344614,0.0234691404,0.0031162698,0.0056933433,0.0003793088,-0.0081808598,-0.0060105992,-0.0077280004,0.0103579845,0.0089086226,0.0421466305,-0.0134565232,-0.0024376867,-0.0088867093,0.0152401438,-0.0179314597,-0.0140233602,-0.0014369746,0.0252783406,-0.0139298208,0.0351337029,0.020994686,-0.0152580704,0.0161628387,0.0087998129,-0.0092898376,0.031384633,-0.025759698,0.0029586263,-0.0105375393,0.0264712497,-0.0118598187,0.0019633193,0.0139725775,-0.0013461412,0.0254179324]},"192":{"Abstract":"Underexposed video enhancement aims at revealing hidden details that are barely noticeable in LDR video frames with noise. Previous work typically relies on a single heuristic tone mapping curve to expand the dynamic range, which inevitably leads to uneven exposure and visual artifacts. In this paper, we present a novel approach for underexposed video enhancement using an efficient perception-driven progressive fusion. For an input underexposed video, we first remap each video frame using a series of tentative tone mapping curves to generate an multi-exposure image sequence that contains different exposed versions of the original video frame. Guided by some visual perception quality measures encoding the desirable exposed appearance, we locate all the best exposed regions from multi-exposure image sequences and then integrate them into a well-exposed video in a temporally consistent manner. Finally, we further perform an effective texture-preserving spatio-temporal filtering on this well-exposed video to obtain a high-quality noise-free result. Experimental results have shown that the enhanced video exhibits uniform exposure, brings out noticeable details, preserves temporal coherence, and avoids visual artifacts. Besides, we demonstrate applications of our approach to a set of problems including video dehazing, video denoising and HDR video reconstruction.","Authors":"Q. Zhang; Y. Nie; L. Zhang; C. Xiao","DOI":"10.1109\/TVCG.2015.2461157","Keywords":"Underexposed video enhancement;visual perception;Fusion;filtering;Underexposed video enhancement;visual perception;fusion;filtering","Title":"Underexposed Video Enhancement via Perception-Driven Progressive Fusion","Keywords_Processed":"filter;visual perception;underexposed video enhancement;fusion","Keyword_Vector":[0.2300517022,-0.2073801069,-0.1954603051,0.0454984957,-0.1265147087,-0.2480740247,-0.0399151133,-0.0152529879,-0.0412295267,0.0516365986,-0.0627064578,0.071407765,-0.0265012838,0.0575826918,-0.1515047394,0.0091492195,-0.0687936661,0.1031098187,-0.0785235421,-0.1132146868,0.1010374,0.1217033935,0.0650590709,0.0013154403,-0.1228387822,0.0792262478,-0.0008027025,0.0761829958,-0.0366127719,-0.0143104596,-0.070640455,-0.0454413586,-0.105401679,-0.0557101894,0.0375274532,-0.1294417799,0.0576527917,0.0832619736,-0.0963983575,0.0948251875,0.0777904415,-0.0453122578,0.0537883184,0.1238542568,-0.0145290275,0.0300931861,0.0730057213,-0.0356755045,-0.0326875039,-0.0339440528,-0.0616715923,-0.0144082395],"Abstract_Vector":[0.1809145895,-0.1207007975,-0.0140870296,-0.0221190528,-0.0668939867,-0.0210663722,0.0471109673,0.0240638946,-0.0598903978,0.0259752122,0.0154806547,-0.0045743763,-0.0219664608,-0.0100435119,-0.0287353715,0.0254887393,-0.0010139493,-0.0000188901,0.0013950854,0.0354529512,-0.0093862014,-0.0176416436,-0.001655377,0.059187008,-0.0185015693,0.0032745641,-0.0324764029,0.0080730508,-0.0304501177,-0.0319599427,0.0500662097,0.0090275215,-0.0047085597,0.0112029327,-0.0230971791,-0.0062571996,-0.0278906199,-0.0382348154,0.0067449238,0.0084037377,0.0113631754,0.0224297851,-0.0194682682,0.025371694,0.0058147173,0.0209052985,-0.0030244428,-0.0037641563,0.0092645357,0.0109279456,0.0056544952,0.0087015455,0.0136512658,-0.0026459965,0.0465131942,-0.0027934112,-0.0024418139,-0.0084589423,-0.0193827247,-0.0208749226,0.0125823724,0.0000895858,-0.0469998664,0.015119237,-0.0286032261,0.004129367,-0.0120783866,0.0270340394,-0.0123559862,0.0355858023,0.0026403341,0.0196372355,0.017469714,0.0177693728,-0.0097747071,0.0490708281,-0.0046281397,0.0171794917,-0.0025433582,0.0172027316,-0.0074779715,0.0198255132,0.0200772164,-0.0024544541,-0.0014542057,-0.0363815423,-0.0084655168,-0.0005990883,-0.0118808813,-0.032638598,0.0159436216,0.0221819633,-0.0216676486,-0.0108096733,0.0047569203,-0.0019909029,-0.0127047723,0.0001121656,-0.0334450967,0.0001784579,0.0238806222,0.0048072952,-0.0061049576,0.0173472258,0.0170608763,0.0051413662,0.007396463,-0.0042655719,-0.0127467231,-0.000394046,0.0184372692,0.0285883403,0.0005696216,0.0084130628,-0.0224016998,-0.0191457804]},"193":{"Abstract":"Appropriate choice of colors significantly aids viewers in understanding the structures in multiclass scatterplots and becomes more important with a growing number of data points and groups. An appropriate color mapping is also an important parameter for the creation of an aesthetically pleasing scatterplot. Currently, users of visualization software routinely rely on color mappings that have been pre-defined by the software. A default color mapping, however, cannot ensure an optimal perceptual separability between groups, and sometimes may even lead to a misinterpretation of the data. In this paper, we present an effective approach for color assignment based on a set of given colors that is designed to optimize the perception of scatterplots. Our approach takes into account the spatial relationships, density, degree of overlap between point clusters, and also the background color. For this purpose, we use a genetic algorithm that is able to efficiently find good color assignments. We implemented an interactive color assignment system with three extensions of the basic method that incorporates top K suggestions, user-defined color subsets, and classes of interest for the optimization. To demonstrate the effectiveness of our assignment technique, we conducted a numerical study and a controlled user study to compare our approach with default color assignments; our findings were verified by two expert studies. The results show that our approach is able to support users in distinguishing cluster numbers faster and more precisely than default assignment methods.","Authors":"Y. Wang; X. Chen; T. Ge; C. Bao; M. Sedlmair; C. Fu; O. Deussen; B. Chen","DOI":"10.1109\/TVCG.2018.2864912","Keywords":"Color perception;visual design;scatterplots","Title":"Optimizing Color Assignment for Perception of Class Separability in Multiclass Scatterplots","Keywords_Processed":"color perception;visual design;scatterplot","Keyword_Vector":[0.1444252153,-0.0566798423,-0.0441696153,0.1271179955,-0.0130284992,0.184031629,-0.0575559721,0.0667244725,-0.0850480829,-0.0403612312,-0.0639228543,0.0454516797,0.0304486757,-0.0244572695,0.0229630989,-0.1020022269,-0.051913176,-0.0255431534,0.0049178075,-0.0193519663,0.0746249145,-0.1591694801,0.0843709887,0.007880439,-0.0478289631,0.0600869124,0.0018342787,0.0695981055,-0.0510338043,0.0383707819,-0.0120059394,-0.134712502,-0.05315782,0.1113321619,0.0283470528,0.0527087566,-0.0483248104,-0.0918163811,0.052093281,0.0251160712,-0.0044884984,-0.0459129827,-0.0197365264,-0.0251349506,0.0441093638,0.0252297984,0.1016597783,-0.0210176405,-0.18342528,0.1187492679,0.0841158219,0.1498510616],"Abstract_Vector":[0.2416892638,0.0045573228,-0.0277786927,-0.0252307883,0.0611767401,-0.0734040228,-0.1373647632,-0.051534547,0.0849274866,-0.1487276151,0.0089977679,-0.0194861904,-0.0458699075,0.0544798367,0.1125331692,-0.0521784844,-0.1126502796,-0.000619224,-0.0865775327,0.0307465174,0.0636214696,-0.0328132224,0.0025784317,-0.0010821402,-0.0181254127,-0.035360866,0.0640412867,0.0375925216,0.0165237553,0.1393325154,0.12543656,0.1323435168,0.0392269436,0.1354946932,-0.0661708509,0.0798375112,-0.0771463263,-0.1011125453,-0.0370312468,-0.0189954842,-0.1396562951,-0.109964021,0.0844801298,0.0927298548,0.0503968582,0.0392909808,-0.0284998864,0.0655323191,-0.0176137759,-0.0435439695,0.0156368807,-0.0052364288,0.0143703331,0.0329397584,0.0243281574,-0.0235331636,0.0158376727,-0.0083492142,-0.1362440494,0.005297859,-0.0183181333,0.0015024541,0.0080206475,0.036051821,-0.037857306,0.0376355385,-0.0361935173,0.0628125072,-0.0476757584,-0.0334058406,-0.0089430604,0.0311509676,-0.032627214,-0.0260978776,-0.0540720918,0.0088150968,-0.0294426359,0.0294299308,0.0311543023,-0.024955432,-0.03993176,0.0142017853,0.0479863204,0.0010598586,-0.0325223899,0.0356600386,0.029980112,-0.0452653288,-0.018914052,0.0296707959,-0.0197923429,-0.0090401596,0.0164277678,0.0231349698,-0.0004067227,-0.0230096799,0.0062997256,-0.0265069942,0.0303644581,-0.0126423305,-0.0013817855,0.0098866846,0.0082926423,0.0070892682,0.0053671423,-0.0114479086,-0.0512914576,-0.0118547421,0.0786464419,-0.0222460117,0.0385928412,0.0109837258,0.0282255549,-0.0021280063,-0.0066819913,-0.0394282463]},"194":{"Abstract":"Interactive ranking techniques have substantially promoted analysts' ability in making judicious and informed decisions effectively based on multiple criteria. However, the existing techniques cannot satisfactorily support the analysis tasks involved in ranking large-scale spatial alternatives, such as selecting optimal locations for chain stores, where the complex spatial contexts involved are essential to the decision-making process. Limitations observed in the prior attempts of integrating rankings with spatial contexts motivate us to develop a context-integrated visual ranking technique. Based on a set of generic design requirements we summarized by collaborating with domain experts, we propose SRVis, a novel spatial ranking visualization technique that supports efficient spatial multi-criteria decision-making processes by addressing three major challenges in the aforementioned context integration, namely, a) the presentation of spatial rankings and contexts, b) the scalability of rankings' visual representations, and c) the analysis of context-integrated spatial rankings. Specifically, we encode massive rankings and their cause with scalable matrix-based visualizations and stacked bar charts based on a novel two-phase optimization framework that minimizes the information loss, and the flexible spatial filtering and intuitive comparative analysis are adopted to enable the in-depth evaluation of the rankings and assist users in selecting the best spatial alternative. The effectiveness of the proposed technique has been evaluated and demonstrated with an empirical study of optimization methods, two case studies, and expert interviews.","Authors":"D. Weng; R. Chen; Z. Deng; F. Wu; J. Chen; Y. Wu","DOI":"10.1109\/TVCG.2018.2865126","Keywords":"Spatial ranking;visualization","Title":"SRVis: Towards Better Spatial Integration in Ranking Visualization","Keywords_Processed":"visualization;spatial ranking","Keyword_Vector":[0.0700515945,-0.0369482459,0.0226375527,0.0123246718,-0.0201107376,0.0217439499,-0.0018479956,-0.0381595253,-0.0293483368,0.0520361938,-0.0090288692,0.0280808844,-0.011541118,0.0549764095,0.0465878746,0.1192266297,-0.0515222373,-0.0251273315,-0.0834382875,-0.0071948025,0.0549791583,-0.0473098156,0.0774154016,-0.0058013913,-0.0827196835,0.0209646521,-0.0172171002,0.0095796042,-0.0504839145,0.0010068009,0.0198350836,0.0514617274,0.0526996764,0.1090189738,-0.0106987664,0.1388027623,-0.0934627479,0.0195326535,0.0217756073,-0.0254186798,-0.0033837383,0.0462420484,-0.0423513344,0.0129506536,0.0084784384,-0.0486043209,0.0158151784,0.0136944893,0.0289185522,-0.0524746582,-0.0011875351,-0.0206415623],"Abstract_Vector":[0.2129545337,-0.0898722837,0.043094564,0.0176597113,0.0319797013,-0.0098663456,0.0034009802,-0.0214674967,0.0505644288,0.0482207963,-0.0467502377,0.0799321372,-0.0271453679,0.0351284318,-0.0505720244,-0.1152273089,0.0066719101,-0.0395371169,-0.0533486097,-0.0646449826,0.0155474082,-0.037451045,-0.0310768165,-0.0066422055,-0.0309830441,0.0565080493,-0.0144095915,-0.0152579508,-0.0321675675,-0.0153655288,0.0591069464,-0.0242799452,0.0031077849,-0.0528133769,-0.0176546923,-0.022407152,0.0500103284,-0.0137573676,-0.0057036924,0.0267285152,0.019396619,0.0439983503,0.0158806889,0.0425972343,0.0452379604,0.0007804217,0.0103026691,-0.0379322577,0.0025550325,-0.0289120928,-0.0637654975,0.0330374215,-0.0405057255,-0.0159476965,-0.0021951744,0.0265339524,-0.0055260472,-0.0304554565,0.0354278183,0.0564369145,-0.0097699559,0.0217904526,0.0176855238,-0.0258233659,0.0152359494,-0.0621308427,0.0129624078,0.0110028821,0.0383849738,0.010684119,0.0272030324,0.0553540213,-0.048737591,-0.0225950652,-0.0256289047,-0.0157503346,0.0504788041,-0.0335701152,-0.0303067842,0.052763475,-0.0366261099,-0.0779410621,-0.023376958,0.0242825187,-0.0426038926,0.0253157201,-0.0130587255,0.0371874057,-0.0464223034,-0.0376029567,0.0166784093,0.0199697116,-0.0480903408,-0.0566545751,-0.04931103,0.0007251434,0.0407140779,-0.0241822694,-0.0441080532,-0.0000942588,-0.0227906154,0.0076164979,-0.0316025339,0.0230673039,0.0739522235,-0.0419285299,-0.057608815,0.0244793752,-0.0367114324,-0.0483150121,0.0115769479,0.0009069166,-0.0286410674,0.0325378707,0.0132655698,0.0150808307]},"195":{"Abstract":"Virtual colonoscopy (VC) is a non-invasive screening tool for colorectal polyps which employs volume visualization of a colon model reconstructed from a CT scan of the patient's abdomen. We present an immersive analytics system for VC which enhances and improves the traditional desktop VC through the use of VR technologies. Our system, using a head-mounted display (HMD), includes all of the standard VC features, such as the volume rendered endoluminal fly-through, measurement tool, bookmark modes, electronic biopsy, and slice views. The use of VR immersion, stereo, and wider field of view and field of regard has a positive effect on polyp search and analysis tasks in our immersive VC system, a volumetric-based immersive analytics application. Navigation includes enhanced automatic speed and direction controls, based on the user's head orientation, in conjunction with physical navigation for exploration of local proximity. In order to accommodate the resolution and frame rate requirements for HMDs, new rendering techniques have been developed, including mesh-assisted volume raycasting and a novel lighting paradigm. Feedback and further suggestions from expert radiologists show the promise of our system for immersive analysis for VC and encourage new avenues for exploring the use of VR in visualization systems for medical diagnosis.","Authors":"S. Mirhosseini; I. Gutenko; S. Ojal; J. Marino; A. Kaufman","DOI":"10.1109\/TVCG.2019.2898763","Keywords":"Immersive Environments;Immersive Analytics;Interaction Design;Volume Rendering;Biomedical Visualization;Colon Cancer Screening;Medical Diagnosis","Title":"Immersive Virtual Colonoscopy","Keywords_Processed":"volume Rendering;Biomedical visualization;immersive Analytics;immersive environment;Colon Cancer Screening;medical Diagnosis;interaction Design","Keyword_Vector":[0.140116701,-0.1694843979,-0.1848254104,0.1074161905,-0.0710840328,-0.2025438049,0.1451979909,-0.0202594538,0.048337042,-0.0284720561,0.0148764899,0.0468623506,-0.0391310304,-0.0641333499,0.0293597878,0.0225951447,-0.1006183708,0.0136686213,-0.0543326191,0.0134251269,0.0075761344,0.061110297,-0.0379784274,0.046774044,0.1566399677,-0.1501239276,-0.0779937212,0.0307532482,-0.0936266083,-0.1656683452,0.0418396522,-0.114302405,-0.0282330098,0.0064285354,-0.0266710999,0.0038224395,-0.0715497428,-0.053259708,0.1320084876,-0.0032418299,-0.0039456136,-0.0157045872,0.0009430508,-0.0361065052,-0.0355519985,-0.0090577363,-0.0852861523,-0.0194711728,0.0410798035,0.0183574616,0.0151532655,-0.0143959914],"Abstract_Vector":[0.1718767138,-0.1056857751,-0.0012079026,0.0246141289,-0.0947518223,0.0254206479,0.0505067343,0.0281876591,-0.0491394827,0.0026334608,-0.0116033416,-0.0056011286,-0.0919318245,-0.001674521,0.0197084149,0.050752359,0.0840863501,0.0574105278,-0.0070604151,-0.068172364,0.0752025585,-0.0078046208,0.041102743,0.0281017776,-0.0141418967,-0.0060750129,0.0396546122,-0.0130199807,0.0067208472,-0.030143821,-0.0121818399,0.0112638354,-0.0022986091,0.0102521195,-0.0315366584,-0.0315560559,0.0034720773,-0.0726615773,-0.0019914481,0.0025751469,-0.0317411598,0.0143254403,-0.057504842,0.0027647805,0.0187517386,0.0437446499,-0.0074385328,0.0399063266,-0.0227653143,0.0015475547,0.0013282616,-0.0127005567,-0.0093889664,0.0125450605,-0.0002342501,-0.0258971309,0.0310903816,0.0046026022,0.0197166255,0.0231740314,0.0079147135,0.0114276304,-0.0470084166,0.0101421408,0.0099107615,0.0008624274,0.0137270505,-0.0074347523,-0.0018435733,-0.0028196432,-0.0220281452,-0.007672868,-0.0025401172,0.0053922709,-0.0291620658,-0.0099846111,0.0167889909,-0.0073181599,0.0093037987,-0.0273089358,0.0081540343,0.0035786783,0.0102990537,0.0192328166,-0.0234000283,-0.0100012326,0.0145060863,0.0014035344,0.0184874947,-0.0146678516,-0.0065143549,-0.0017964643,-0.0027534452,-0.0317629944,-0.0059601249,0.0493535381,-0.0252896269,0.0098417555,-0.007944924,0.0056313281,0.002750805,0.0010828858,0.0197535056,0.0059148137,0.0064946034,-0.0297704667,0.0142350029,-0.0205726694,0.0112048352,0.0449991412,0.0001529749,0.0072031712,0.0042589427,-0.0107342011,-0.0039758837,0.0149331503]},"196":{"Abstract":"Rendering in virtual reality (VR) requires substantial computational power to generate 90 frames per second at high resolution with good-quality antialiasing. The video data sent to a VR headset requires high bandwidth, achievable only on dedicated links. In this paper we explain how rendering requirements and transmission bandwidth can be reduced using a conceptually simple technique that integrates well with existing rendering pipelines. Every even-numbered frame is rendered at a lower resolution, and every odd-numbered frame is kept at high resolution but is modified in order to compensate for the previous loss of high spatial frequencies. When the frames are seen at a high frame rate, they are fused and perceived as high-resolution and high-frame-rate animation. The technique relies on the limited ability of the visual system to perceive high spatio-temporal frequencies. Despite its conceptual simplicity, correct execution of the technique requires a number of non-trivial steps: display photometric temporal response must be modeled, flicker and motion artifacts must be avoided, and the generated signal must not exceed the dynamic range of the display. Our experiments, performed on a high-frame-rate LCD monitor and OLED-based VR headsets, explore the parameter space of the proposed technique and demonstrate that its perceived quality is indistinguishable from full-resolution rendering. The technique is an attractive alternative to reprojection and resolution reduction of all frames.","Authors":"G. Denes; K. Maruszczyk; G. Ash; R. K. Mantiuk","DOI":"10.1109\/TVCG.2019.2898741","Keywords":"Temporal multiplexing;rendering;graphics;perception;virtual reality","Title":"Temporal Resolution Multiplexing: Exploiting the limitations of spatio-temporal vision for more efficient VR rendering","Keywords_Processed":"virtual reality;temporal multiplexing;graphic;perception;render","Keyword_Vector":[0.1262006875,0.1634720782,-0.0977827289,-0.008695274,-0.0090888734,0.0044725717,0.0407107738,0.013035821,-0.0151861829,0.0010448163,-0.0016606799,-0.0064207202,-0.0192197861,0.0031013539,0.0084767973,0.0097626103,-0.0276970394,-0.048088711,0.0399909418,-0.0029327421,0.0122276342,-0.0295530795,-0.0608579826,0.0530632368,-0.0089410959,-0.0366398532,0.056181266,-0.0353106702,-0.0534214563,0.0212732409,0.0038243812,-0.0060951923,-0.0309595976,0.0006287588,0.0407946628,0.0190866239,0.0391713368,-0.0715558541,0.0028432765,-0.0150424118,0.0888691934,-0.0031449745,0.0403644255,0.0358823674,-0.0032686433,0.0012478007,0.0375026498,0.0064493076,-0.0318408222,-0.0151599466,0.0161574067,0.0121371578],"Abstract_Vector":[0.2089224483,0.0953769743,-0.1611753232,-0.0685266366,0.0425486026,0.0262703499,0.018672951,-0.0049517745,0.0381125826,0.0505056316,0.0337767804,-0.0117010658,-0.0379795427,-0.0901030284,0.00497514,0.0491306202,-0.04993774,-0.0305631958,0.0303639206,-0.022693697,-0.037482935,-0.0228980319,0.0220532456,-0.0866582979,-0.0444911742,0.0120773587,0.0259518984,0.0761146443,-0.0100603656,0.034679299,0.0157644585,0.0224350796,-0.030876709,0.041771081,0.0209429241,-0.0481163699,-0.0464310666,0.0253612393,0.0217784572,0.0017377451,-0.091490055,0.0232093634,-0.0630415562,-0.0932757766,0.0905949482,0.0210429075,-0.0648628338,0.012640118,-0.0311274835,0.0165584219,0.0123490402,-0.1026784036,0.021984013,0.0613997194,-0.071606127,0.0193601967,-0.0487871903,-0.0589771476,0.0232380282,-0.0411571685,-0.027197674,-0.052250897,-0.0056090978,-0.0579370933,-0.0078299538,0.0232937329,0.0241968262,-0.0152322467,-0.0408747408,0.0394131992,0.0312209282,0.0090198721,-0.0490920665,-0.0655804661,-0.0101671608,0.022890548,0.0339530774,-0.0343531092,-0.0190011104,-0.0627125866,-0.004858533,-0.0814391958,0.0348404042,0.0157119746,-0.0290577541,-0.0115565847,-0.0098817453,0.0447518253,0.069495659,0.0243264924,0.0039132141,0.0192188992,0.0411325765,-0.0067391747,0.0001943144,0.0127235678,-0.0253377102,-0.0340172849,0.0204112876,0.0565612643,0.0084622223,0.0071989212,-0.0401845012,0.0204776703,0.0304525603,0.0279562568,0.0424376637,-0.010665366,-0.0257446593,0.0307818587,-0.0210856263,0.0232253744,0.0218377214,0.027546062,-0.0003786185,0.0125494811]},"197":{"Abstract":"This paper proposes a real-time method for 3D eye performance reconstruction using a single RGBD sensor. Combined with facial surface tracking, our method generates more pleasing facial performance with vivid eye motions. In our method, a novel scheme is proposed to estimate eyeball motions by minimizing the differences between a rendered eyeball and the recorded image. Our method considers and handles different appearances of human irises, lighting variations and highlights on images via the proposed eyeball model and the  $L0$-based optimization. Robustness and real-time optimization are achieved through the novel 3D Taylor expansion-based linearization. Furthermore, we propose an online bidirectional regression method to handle occlusions and other tracking failures on either of the two eyes from the information of the opposite eye. Experiments demonstrate that our technique achieves robust and accurate eye performance reconstruction for different iris appearances, with various head\/face\/eye motions, and under different lighting conditions.","Authors":"Q. Wen; F. Xu; J. Yong","DOI":"10.1109\/TVCG.2016.2641442","Keywords":"Eye reconstruction;facial animation;gaze tracking;multilinear model;RGBD camera","Title":"Real-Time 3D Eye Performance Reconstruction for RGBD Cameras","Keywords_Processed":"multilinear model;facial animation;gaze tracking;eye reconstruction;rgbd camera","Keyword_Vector":[0.1850275111,-0.1218746056,-0.0827397128,0.1066297296,-0.0835664053,0.0449756611,0.0210157937,-0.0020419525,-0.0101330337,-0.04844429,0.0292934957,-0.0075020014,-0.0093133821,0.1425748896,-0.0513587163,-0.0294855329,-0.0054212128,0.0646145441,0.0233262089,-0.0302674808,-0.0565025849,-0.0580551718,-0.0138314072,-0.0125225575,-0.0258679161,-0.0226538948,-0.0004906973,-0.0462360181,-0.0200621493,0.0191445563,0.0258622715,0.0111853586,0.0106157452,0.0045002123,0.0074049621,0.0467346494,-0.0017100564,-0.0027563508,-0.0213548095,-0.0349741125,-0.0516515892,-0.0469964489,-0.0291928704,0.008066694,0.002957605,0.0396422843,0.0509465841,-0.0144828489,0.0255582056,-0.0381989739,-0.0914333683,0.0109662503],"Abstract_Vector":[0.2085504228,0.0434317106,-0.0307416584,-0.1027473967,0.0204553942,0.0532372353,-0.048119934,-0.0447365192,-0.0041612086,0.0020221053,0.0135136392,0.030907432,-0.0089157546,0.0078215575,0.0243238049,0.0007859699,-0.006961921,-0.0140737787,0.0338748178,-0.0391730194,-0.021129419,-0.0593180497,-0.0289883862,0.0000564563,-0.036600735,-0.0122002782,0.0154561048,0.0985593411,0.0646991157,-0.0239000013,0.0153764412,-0.0303472715,-0.0443998291,0.0343218101,-0.0363734408,-0.0157535602,0.034768549,-0.0215290047,-0.0418442769,-0.0812608052,-0.0345187339,0.0629582464,0.0421751136,0.0156877926,0.0500566728,-0.0345966968,-0.0153576712,-0.0372453759,-0.0001811029,-0.0286043439,0.0082381703,0.0493092044,-0.0320817661,-0.0422786296,-0.0702725997,0.0365976119,-0.0318867699,0.0192536235,0.0302805056,-0.0065535975,0.0080251071,-0.0257166285,0.0106031612,-0.0191549085,0.0422044973,0.0038217749,0.0029202399,-0.032203392,0.0520436203,0.0276567917,-0.0136612818,-0.0665267266,-0.0092863538,-0.0110938472,0.0245523803,0.0132257526,0.0525769908,-0.0144540268,-0.0622927791,-0.0034125814,0.0540936512,0.0305618319,-0.0012429434,0.0270248118,0.0193699625,-0.0091587575,-0.0320183943,0.0251513601,-0.0041003387,0.0138383244,-0.0451754312,-0.0697456108,0.0348511024,-0.0686517178,0.0680201416,0.0763066474,0.0045030947,0.0159536269,-0.0097377177,0.0853367857,-0.0083925441,-0.0550030345,-0.0389916761,0.0521255375,-0.0069361474,-0.0101772067,-0.0465311186,0.0515973351,0.0210336856,-0.0324302551,-0.0377207255,0.0128838681,-0.0351105985,-0.0022789058,-0.0454757764,0.0685161221]},"198":{"Abstract":"In this work, we present a study that traces the technical and cognitive processes in two visual analytics applications to a common theoretic model of soft knowledge that may be added into a visual analytics process for constructing a decision-tree model. Both case studies involved the development of classification models based on the \u201cbag of features\u201d approach. Both compared a visual analytics approach using parallel coordinates with a machine-learning approach using information theory. Both found that the visual analytics approach had some advantages over the machine learning approach, especially when sparse datasets were used as the ground truth. We examine various possible factors that may have contributed to such advantages, and collect empirical evidence for supporting the observation and reasoning of these factors. We propose an information-theoretic model as a common theoretic basis to explain the phenomena exhibited in these two case studies. Together we provide interconnected empirical and theoretical evidence to support the usefulness of visual analytics.","Authors":"G. K. L. Tam; V. Kothari; M. Chen","DOI":"10.1109\/TVCG.2016.2598829","Keywords":"Visual analytics;classification;decision tree;model;facial expression;visualization image;information theory","Title":"An Analysis of Machine- and Human-Analytics in Classification","Keywords_Processed":"decision tree;classification;visualization image;information theory;model;visual analytic;facial expression","Keyword_Vector":[0.1281584624,-0.0705802963,-0.0251624275,-0.1165918775,0.0074254248,-0.088780095,-0.1299698072,-0.0375940188,-0.141098513,0.1250212245,-0.147041274,0.1198661408,0.001414377,0.0453717866,-0.1586873359,-0.0027208181,-0.0855887706,0.0658204377,-0.1096443799,-0.0792373983,0.1707943988,0.1193584152,0.0585470447,-0.0310759684,-0.0736415379,0.0802562075,0.0526281516,0.0420761722,-0.0266121057,-0.0522560525,-0.0830500536,-0.0279758726,-0.0807939103,-0.0747884775,0.0370908346,-0.0913658991,0.1309055399,-0.1055715936,-0.0422891573,-0.0305353583,0.0322269842,-0.0091797466,0.0074725608,-0.0373850857,0.003438788,-0.0324052107,0.0423599727,-0.0091126157,0.054113013,0.0064509744,-0.037024246,-0.0411474246],"Abstract_Vector":[0.1171980732,-0.048814719,0.0136193954,-0.0053945335,-0.0430209044,0.0107585147,0.0284373981,0.0266694499,-0.0252369954,-0.0059293571,-0.0146127067,0.0166064349,0.0200945001,-0.0128624099,0.0011603777,-0.0125883324,-0.020079462,0.0148961882,-0.000598595,-0.0262220148,0.0133198034,0.0088613687,-0.0075396924,-0.0028429506,-0.0130686899,0.0149471479,-0.0059282875,0.009374788,0.0043098117,-0.0083854942,-0.0179854804,-0.0022541498,-0.0097100215,-0.0122167659,-0.0189016901,0.00914582,-0.025615525,0.0009352356,0.0026417498,0.0224037939,-0.0107100321,-0.0243499799,-0.0068096482,-0.013347454,0.0036742361,0.0081455126,0.0110468087,-0.0144896237,0.0195277336,0.0048354703,-0.0001885054,0.0188013915,0.0020036061,0.0065470959,-0.0067094175,0.0064579773,-0.0205091616,0.0036029268,0.0055391827,-0.0120695144,0.0150276949,-0.00187648,-0.0287762665,0.0149374906,0.0169831002,-0.0046517251,-0.011576819,0.018317525,-0.0271789536,0.0034190875,-0.0115227815,0.0056760109,0.0175737124,-0.0138920922,0.0129649304,-0.0059335038,0.0014157498,0.0296923144,0.0048634982,0.0081170275,-0.0059182162,-0.0039492987,0.0086010541,0.0070697113,-0.0211469232,0.0110961451,-0.004902804,-0.005955769,0.0216688395,-0.030249407,0.0002493965,0.0046211054,0.0102921263,-0.018238775,0.0283580831,-0.0161914576,0.006256946,-0.009858486,0.0053835363,0.0169641056,0.0066773904,0.0004528453,0.0096647735,0.0011747106,0.008636926,0.0109945326,-0.0017244642,-0.0027169383,0.0110793836,0.0174540954,0.0060853429,-0.0328867647,0.0147982502,-0.0093292725,0.0022828187,-0.0062184799]},"199":{"Abstract":"Most mountain ranges are formed by the compression and folding of colliding tectonic plates. Subduction of one plate causes large-scale asymmetry while their layered composition (or stratigraphy) explains the multi-scale folded strata observed on real terrains. We introduce a novel interactive modeling technique to generate visually plausible, large scale terrains that capture these phenomena. Our method draws on both geological knowledge for consistency and on sculpting systems for user interaction. The user is provided hands-on control on the shape and motion of tectonic plates, represented using a new geologically-inspired model for the Earth crust. The model captures their volume preserving and complex folding behaviors under collision, causing mountains to grow. It generates a volumetric uplift map representing the growth rate of subsurface layers. Erosion and uplift movement are jointly simulated to generate the terrain. The stratigraphy allows us to render folded strata on eroded cliffs. We validated the usability of our sculpting interface through a user study, and compare the visual consistency of the earth crust model with geological simulation results and real terrains.","Authors":"G. Cordonnier; M. Cani; B. Benes; J. Braun; E. Galin","DOI":"10.1109\/TVCG.2017.2689022","Keywords":"Terrains;mountains;interactive design;geology","Title":"Sculpting Mountains: Interactive Terrain Modeling Based on Subsurface Geology","Keywords_Processed":"geology;interactive design;mountain;terrain","Keyword_Vector":[0.031069212,-0.0025336117,-0.0032633238,0.0093600408,0.0106989645,-0.0009949723,0.0043115137,-0.0152493477,-0.0162660362,0.0006510696,0.0063529083,0.002899414,-0.0120154547,-0.0104995124,0.0225272169,0.0136073356,-0.0470664175,-0.0365237375,0.0347994638,-0.0164560375,0.0289548052,-0.0045347563,-0.0450542236,0.0120081955,-0.0022106791,-0.0149407173,0.0506513854,0.022533217,0.0375076654,-0.0225072548,-0.037008404,0.0147011204,-0.0207903022,0.0054332609,-0.0109860765,-0.0267972627,-0.0284099441,0.0071416804,0.014525774,0.0227226939,-0.0510848517,-0.0186458041,0.0109507429,0.0158321601,0.0215974194,0.0223568008,-0.0228205954,-0.0249141225,-0.0205548328,0.0436781685,0.0102066194,-0.0081577154],"Abstract_Vector":[0.1480394397,-0.0109298655,0.0084159943,0.0481691859,-0.0113648936,0.0191679841,-0.0454217661,0.010388474,0.021214861,-0.0289406105,-0.0346613855,0.0231437535,-0.0719655424,-0.0400248795,0.0033467982,0.0032574809,-0.0431626647,-0.0087543304,-0.024871328,0.0021111419,-0.0111387543,0.009081294,-0.0090382205,0.0300477243,0.023631789,0.0045930023,0.0165126401,0.0054533675,0.0286637632,-0.0380573925,0.0392736807,0.0104074465,-0.0536677827,-0.0377762355,0.0216914012,0.0099068087,-0.0385414024,0.0521674822,-0.0379997432,0.0150492056,-0.0067365232,0.007257705,0.035451843,-0.0253510349,0.0141753526,0.0299603722,0.0350574277,-0.0070677173,-0.0001451024,-0.0020568105,-0.0094693712,-0.006972975,-0.0259289016,-0.00107355,0.0261508866,-0.0404313735,-0.0024378712,-0.0144544518,0.0076717881,0.0142670553,0.0458884049,-0.0038503236,0.0448473862,0.0272582628,-0.0383521064,-0.0138588043,-0.0100835635,-0.0478852539,0.0215488646,-0.0311011503,-0.012877568,0.0075213366,0.0161752081,-0.027671973,0.0113906711,0.0602962844,0.0417665725,0.0559476945,-0.0388691368,-0.0064435451,-0.007329009,0.0108026338,-0.0322116663,0.0208143159,0.000965655,-0.0041415105,-0.0509874795,-0.0140543911,0.0064126052,-0.0029339172,-0.0142466493,0.027259941,0.005044428,-0.0014284073,-0.029274169,0.0054975445,0.0142680892,-0.0102742337,0.019561959,-0.0025550086,-0.0027532998,-0.0175436352,0.010929675,-0.0003619484,0.0239093846,0.0082079909,0.0034296768,-0.0049639459,-0.0286863868,0.0150896601,0.0124022675,0.0247031294,0.0028614153,-0.0160359363,-0.0016945892,-0.0027788336]},"2":{"Abstract":"Flow fields are usually visualized relative to a global observer, i.e., a single frame of reference. However, often no global frame can depict all flow features equally well. Likewise, objective criteria for detecting features such as vortices often use either a global reference frame, or compute a separate frame for each point in space and time. We propose the first general framework that enables choosing a smooth trade-off between these two extremes. Using global optimization to minimize specific differential geometric properties, we compute a time-dependent observer velocity field that describes the motion of a continuous field of observers adapted to the input flow. This requires developing the novel notion of an observed time derivative. While individual observers are restricted to rigid motions, overall we compute an approximate Killing field, corresponding to almost-rigid motion. This enables continuous transitions between different observers. Instead of focusing only on flow features, we furthermore develop a novel general notion of visualizing how all observers jointly perceive the input field. This in fact requires introducing the concept of an observation time, with respect to which a visualization is computed. We develop the corresponding notions of observed stream, path, streak, and time lines. For efficiency, these characteristic curves can be computed using standard approaches, by first transforming the input field accordingly. Finally, we prove that the input flow perceived by the observer field is objective. This makes derived flow features, such as vortices, objective as well.","Authors":"M. Hadwiger; M. Mlejnek; T. Theu\u00dfl; P. Rautek","DOI":"10.1109\/TVCG.2018.2864839","Keywords":"Flow visualization;observer frames of reference;Killing vector fields;infinitesimal isometries;Lie derivatives;objectivity","Title":"Time-Dependent Flow seen through Approximate Observer Killing Fields","Keywords_Processed":"lie derivative;objectivity;observer frame of reference;flow visualization;kill vector field;infinitesimal isometry","Keyword_Vector":[0.11267235,-0.1383882543,-0.1157370273,0.1047392493,-0.1055518202,-0.0820227355,0.0649763633,-0.022649553,0.0063505531,-0.0388345054,0.0432890744,-0.0543919777,0.0405056165,0.1554218781,-0.0494665346,0.0166866409,0.0698417475,0.0158853769,0.0220283487,-0.1226539505,-0.0296687201,-0.0261923123,-0.0056078182,-0.0139608718,0.0569193349,-0.0210600899,-0.021168414,0.0117168536,-0.0569762547,-0.018125555,-0.0363651871,0.0080167372,0.0500736354,-0.0139796614,-0.0637274448,-0.0308777082,0.0248682137,-0.0028777749,0.0402969348,0.0123740817,-0.0077323397,0.0236704718,-0.0618389268,-0.0544304699,0.064693786,-0.0017844746,0.0101039937,0.031575378,-0.0150029028,0.0335174216,0.0813631836,0.0448755018],"Abstract_Vector":[0.2370184914,-0.1499287593,0.0191771598,0.0053563711,-0.0458040786,0.0297946187,-0.0119558802,0.0048649949,-0.0565035642,0.0423144233,-0.024888551,0.0042305462,-0.0605863329,0.0085881871,0.0094324831,0.0785685527,0.0022814132,0.0051909182,-0.0067857451,0.0040679806,-0.0346178961,-0.0083696194,-0.0065647311,0.0332325245,0.0151740355,-0.0409822534,-0.019199767,0.0158470065,0.0318208511,-0.0151297588,-0.0005206763,0.0275472998,-0.005417692,-0.0166259681,0.023485192,-0.0368342882,0.0180092724,-0.0443810535,-0.0362759224,-0.048697251,-0.0182559874,0.0144479488,-0.0072628561,-0.0170431452,-0.0034988569,-0.0007268,-0.0301468918,-0.0094201124,-0.0205258522,-0.0527381473,-0.0142190225,-0.0092035708,0.0273214565,0.0646105398,0.0086693596,-0.0147337193,0.0468787033,-0.0009313988,-0.0439918928,0.0266873995,0.0284621549,0.0250929936,-0.0074879235,-0.0480586055,-0.0200570335,0.0009841697,-0.0250593087,-0.0130439987,0.0185950059,-0.0095953047,-0.0105674419,0.0071520447,0.0100651162,0.0600382337,0.0230338962,0.015181216,-0.0395551541,0.013399663,-0.0130069771,-0.0079149529,0.000782486,-0.0019794519,-0.0258757347,0.0082520735,0.0113484239,-0.0405219732,-0.0204539122,-0.0135352061,0.0223490829,-0.0185443134,-0.0014207169,0.0252088598,-0.0226346269,0.010340889,-0.0011692844,0.0497296731,-0.0246722,-0.0115273856,0.0063039115,-0.0089007203,-0.0077686084,0.0280703444,0.0575277945,0.0093389992,0.0040518313,-0.0371248054,-0.0218942276,0.0448661744,0.0095053707,0.0032230468,-0.0303882179,0.0043731629,0.0264384077,-0.0416646799,-0.0149468849,0.0003941142]},"20":{"Abstract":"We present Visualization-by-Sketching, a direct-manipulation user interface for designing new data visualizations. The goals are twofold: First, make the process of creating real, animated, data-driven visualizations of complex information more accessible to artists, graphic designers, and other visual experts with traditional, non-technical training. Second, support and enhance the role of human creativity in visualization design, enabling visual experimentation and workflows similar to what is possible with traditional artistic media. The approach is to conceive of visualization design as a combination of processes that are already closely linked with visual creativity: sketching, digital painting, image editing, and reacting to exemplars. Rather than studying and tweaking low-level algorithms and their parameters, designers create new visualizations by painting directly on top of a digital data canvas, sketching data glyphs, and arranging and blending together multiple layers of animated 2D graphics. This requires new algorithms and techniques to interpret painterly user input relative to data \u201cunder\u201d the canvas, balance artistic freedom with the need to produce accurate data visualizations, and interactively explore large (e.g., terabyte-sized) multivariate datasets. Results demonstrate a variety of multivariate data visualization techniques can be rapidly recreated using the interface. More importantly, results and feedback from artists support the potential for interfaces in this style to attract new, creative users to the challenging task of designing more effective data visualizations and to help these users stay \u201cin the creative zone\u201d as they work.","Authors":"D. Schroeder; D. F. Keefe","DOI":"10.1109\/TVCG.2015.2467153","Keywords":"Visualization design;multivariate;art;sketch;color map;glyph;Visualization design;multivariate;art;sketch;color map;glyph","Title":"Visualization-by-Sketching: An Artist's Interface for Creating Multivariate Time-Varying Data Visualizations","Keywords_Processed":"color map;multivariate;visualization design;glyph;sketch;art","Keyword_Vector":[0.1786232076,-0.1362005593,-0.08705114,0.0607266665,-0.0592304717,0.0289746712,-0.0439479915,-0.0347587596,-0.0434974574,-0.0103541098,-0.014024485,0.0420515393,0.0121838417,-0.057226549,-0.0335128131,-0.0128952543,-0.1393079613,-0.0066763477,-0.0278088567,-0.0369616693,-0.0048341243,-0.0657882192,0.0848906515,-0.0425780824,-0.0725008703,-0.015826167,-0.0254607131,0.0894765303,0.0072013126,-0.0077492784,0.0871880823,0.1292210289,0.0472135889,0.1083021812,-0.0124730171,0.0843301958,-0.1574444082,-0.003379678,0.0189972312,-0.0787891704,-0.0058771238,0.153422213,-0.0799237885,-0.0106439444,0.0175266631,-0.0313459001,0.045283666,0.0108725396,-0.0358038044,-0.0278315268,-0.0030164593,0.0114803359],"Abstract_Vector":[0.1845646168,-0.0425467679,0.0219762225,-0.0206349816,0.023592202,-0.0447359841,0.0122751345,-0.0334212457,0.1032177323,0.0801614573,-0.0390067384,0.063984395,-0.0486791712,0.0866241815,0.0146151085,-0.0563654508,-0.0317114983,-0.045570491,-0.0139608714,-0.0689408476,0.0351877062,-0.0752795902,0.0022791608,0.0384413858,-0.0055672486,-0.002752216,-0.0084743166,0.0421735481,-0.0266378001,0.0100999629,0.00308175,-0.0448807302,-0.0212477632,0.0119224512,-0.0164788236,-0.00010019,-0.0549590444,-0.095146153,-0.0102245544,0.0348686111,0.0160899237,0.008941151,0.0226367684,0.0062518548,0.0419064904,0.0247838926,0.0045187613,0.0368536956,0.0581454623,0.0057297861,0.02576273,0.041280145,-0.0026133038,-0.0521778888,-0.0004844767,-0.0066578143,0.0096174856,0.0050394487,0.0012660783,-0.0263726665,0.0244818137,-0.0005325959,0.0021906315,0.0050679669,-0.0464793078,-0.0174743129,-0.0251560954,0.0106847082,-0.0417858972,0.0519284906,0.0118865851,-0.0385901712,-0.035954883,0.0000592335,-0.0116233462,-0.0309176135,0.0116239773,-0.0084237531,0.0113841461,-0.0154832492,-0.0097064854,0.0140651241,0.0064067486,0.0033179349,-0.0157828762,-0.0121887467,0.0113737759,-0.0143751417,-0.0178596625,0.0180501412,-0.022629082,-0.0419443157,0.0653081646,-0.0188019977,0.0231697354,0.0461590082,0.0122117634,0.0289923869,-0.0094439432,0.0237412581,-0.0770531116,0.0030125532,0.0734414747,-0.0109582742,0.0554780878,0.0457098897,0.0072699238,0.0456787271,-0.0418681491,-0.0001238675,0.0451848817,-0.0173541958,-0.0161462032,0.0403330118,0.038162274,-0.0442667424]},"200":{"Abstract":"Visualization researchers and practitioners engaged in generating or evaluating designs are faced with the difficult problem of transforming the questions asked and actions taken by target users from domain-specific language and context into more abstract forms. Existing abstract task classifications aim to provide support for this endeavour by providing a carefully delineated suite of actions. Our experience is that this bottom-up approach is part of the challenge: low-level actions are difficult to interpret without a higher-level context of analysis goals and the analysis process. To bridge this gap, we propose a framework based on analysis reports derived from open-coding 20 design study papers published at IEEE InfoVis 2009-2015, to build on the previous work of abstractions that collectively encompass a broad variety of domains. The framework is organized in two axes illustrated by nine analysis goals. It helps situate the analysis goals by placing each goal under axes of specificity (Explore, Describe, Explain, Confirm) and number of data populations (Single, Multiple). The single-population types are Discover Observation, Describe Observation, Identify Main Cause, and Collect Evidence. The multiple-population types are Compare Entities, Explain Differences, and Evaluate Hypothesis. Each analysis goal is scoped by an input and an output and is characterized by analysis steps reported in the design study papers. We provide examples of how we and others have used the framework in a top-down approach to abstracting domain problems: visualization designers or researchers first identify the analysis goals of each unit of analysis in an analysis stream, and then encode the individual steps using existing task classifications with the context of the goal, the level of specificity, and the number of populations involved in the analysis.","Authors":"H. Lam; M. Tory; T. Munzner","DOI":"10.1109\/TVCG.2017.2744319","Keywords":"Framework;Data Analysis;Analysis Goals;Design Studies;Open Coding;Task Classifications","Title":"Bridging from Goals to Tasks with Design Study Analysis Reports","Keywords_Processed":"task classification;Design Studies;analysis goal;framework;open coding;Data Analysis","Keyword_Vector":[0.0886615088,0.019071049,-0.0019245126,-0.000444482,0.1220103398,-0.0114998354,-0.0473332138,0.1377133433,0.0230872649,0.0275197236,0.070544954,0.1323381537,-0.0432933309,0.0462273685,0.0663286221,0.0304679246,0.0911763757,0.1200793386,0.1628875089,-0.0511632296,-0.093750144,0.0534522037,0.1574412248,0.0730582096,-0.1037034114,-0.0702694917,-0.0278997482,-0.0013291464,0.0743519726,-0.1241230647,-0.0432002786,-0.100789219,0.044622505,-0.0002119562,-0.0246783474,-0.0293809447,-0.0242946302,0.0300894128,-0.0434993366,-0.0689289667,-0.0058962216,0.0983187417,0.0263093105,0.0325873109,0.0011670781,-0.0175859429,-0.0118553171,0.0040176478,0.0016274679,0.0563144425,-0.0217558681,-0.0569374952],"Abstract_Vector":[0.1127742503,-0.0610123221,-0.0069704409,-0.0063276375,-0.047539088,0.0167711397,0.0222437469,0.0171066329,0.007221925,0.0138427972,-0.0185853491,0.0145438067,-0.0101586345,-0.0051539238,0.0038255577,0.0191981501,0.004170164,-0.0031903123,0.0145673381,0.0209080641,0.0119500911,-0.0298289996,-0.0050558077,0.0175635252,-0.008228743,0.0067285853,0.0077769816,-0.0081941104,-0.0119008523,-0.0055563286,0.0075369533,0.0194567013,-0.0073473951,0.0164205101,0.0229152653,-0.0008409045,-0.0198946962,-0.0059939071,-0.0131248769,-0.0110097542,0.0268256526,0.028643582,0.0159043444,-0.0101967713,0.0088889151,0.002838198,0.0036044286,-0.0020691738,0.016579196,0.0005756107,-0.0022017392,-0.000673394,0.0190469016,-0.0072759263,0.0091431132,-0.0105154496,0.0005879702,-0.0114950567,-0.0297037082,-0.0341207521,-0.0003239893,0.0172094483,0.0070945412,-0.0032500152,-0.0257399117,-0.033949793,0.0069163073,-0.0054227662,-0.0139122659,0.0055441703,-0.0017695045,0.0080981761,0.0105051148,-0.0077410037,0.0235687434,-0.0050197768,0.0223535502,0.0067679082,0.010466338,0.0042696827,0.0115227177,-0.0085966084,0.0088110095,-0.0014032681,0.0139122162,0.0077033381,0.0078928996,0.0097263831,-0.0131630638,-0.0168813361,0.0015962184,-0.0135828353,0.0090204135,-0.0004518976,-0.005799243,-0.0045423863,0.0302037159,0.0081454052,-0.0001288564,-0.0208960665,0.0013956105,0.017796245,0.012423335,0.0260676821,0.0040166909,-0.0216309861,-0.0080839978,-0.005064226,-0.0060959108,-0.0288766885,0.0074404131,0.0039346424,0.0094919696,-0.008546044,0.0104282568,-0.0216628935]},"201":{"Abstract":"Large image deformations pose a challenging problem for the visualization and statistical analysis of 3D image ensembles which have a multitude of applications in biology and medicine. Simple linear interpolation in the tangent space of the ensemble introduces artifactual anatomical structures that hamper the application of targeted visual shape analysis techniques. In this work we make use of the theory of stationary velocity fields to facilitate interactive non-linear image interpolation and plausible extrapolation for high quality rendering of large deformations and devise an efficient image warping method on the GPU. This does not only improve quality of existing visualization techniques, but opens up a field of novel interactive methods for shape ensemble analysis. Taking advantage of the efficient non-linear 3D image warping, we showcase four visualizations: 1) browsing on-the-fly computed group mean shapes to learn about shape differences between specific classes, 2) interactive reformation to investigate complex morphologies in a single view, 3) likelihood volumes to gain a concise overview of variability and 4) streamline visualization to show variation in detail, specifically uncovering its component tangential to a reference surface. Evaluation on a real world dataset shows that the presented method outperforms the state-of-the-art in terms of visual quality while retaining interactive frame rates. A case study with a domain expert was performed in which the novel analysis and visualization methods are applied on standard model structures, namely skull and mandible of different rodents, to investigate and compare influence of phylogeny, diet and geography on shape. The visualizations enable for instance to distinguish (population-)normal and pathological morphology, assist in uncovering correlation to extrinsic factors and potentially support assessment of model quality.","Authors":"M. Hermann; A. C. Schunke; T. Schultz; R. Klein","DOI":"10.1109\/TVCG.2015.2467198","Keywords":"Statistical deformation model;stationary velocity fields;image warping,;interactive visual analysiimage warping,;Statistical deformation model;stationary velocity fields;image warping;interactive visual analysis","Title":"Accurate Interactive Visualization of Large Deformations and Variability in Biomedical Image Ensembles","Keywords_Processed":"interactive visual analysis;image warp;stationary velocity field;interactive visual analysiimage warping;statistical deformation model","Keyword_Vector":[0.2462706624,-0.1128834843,0.0491762547,-0.1921662688,0.0418644294,-0.0199533541,-0.0747597563,0.0082222989,0.0456785824,-0.0161200773,0.0082243635,-0.1306732126,-0.0301583756,-0.0235920396,0.0075046159,0.0148386817,-0.0251316965,0.011225253,-0.0068093919,0.0339574566,-0.0527067488,-0.0332978226,-0.0422953366,-0.0156606759,0.0124060993,0.010159548,0.0089235435,0.0091581026,0.0130867351,0.0026239689,0.0081490818,0.0094423493,-0.0066895104,-0.0011100224,-0.024094184,-0.0025861322,0.0133526267,-0.0037289053,-0.0010034512,0.0090569325,-0.0182608332,-0.0050106237,0.0365114461,0.013522352,-0.0043973209,0.0003883719,0.0381209561,0.017949943,-0.0209212444,0.0294391559,-0.0283247766,0.0239629769],"Abstract_Vector":[0.1404724789,-0.0718407888,0.006189203,-0.0005140672,-0.0461375204,0.0197017618,0.0353553651,0.0352305668,-0.0117953287,-0.0200978175,-0.0263657647,0.0625659327,0.0772889507,-0.0338289237,0.0138201327,-0.0308010501,-0.0558853584,0.0105473487,-0.0201024988,-0.042119859,-0.0097573831,0.0088013296,0.0039551931,-0.017523342,-0.0376746094,0.0173366208,-0.0010873903,-0.0133113186,-0.0006803271,0.0207798553,-0.0338131385,0.0006998705,0.02694251,-0.0374633313,0.0047164302,0.0018744099,-0.0153403289,0.0012005705,-0.0029370581,0.0241514171,-0.0270666308,-0.011849781,0.0234701374,-0.0288760091,0.010582049,-0.0048764359,0.0136020026,-0.0081538889,0.0234516868,0.0246265408,0.0114816346,0.0063737477,0.0061679875,-0.0114460815,-0.0379463378,-0.0076824009,-0.0045628448,0.0110381044,0.0069008529,-0.0152561434,-0.0178210241,-0.0196699024,-0.0308761124,0.0074196971,0.026818506,0.0091282529,0.0120896289,0.0099644151,0.0155837636,-0.0164531467,-0.0167259008,-0.008213592,0.0010992651,-0.0020081236,0.0094148834,-0.0015305785,-0.0112254902,0.0147354216,0.0017470293,0.0019488963,0.0013528472,-0.0031047418,-0.0032391931,0.0096808787,-0.0209765563,0.0006280084,0.0253556122,-0.0552845298,0.0161807908,0.0004869443,0.0079345389,0.0021476423,0.0068688543,0.0026209658,-0.0178510241,-0.0036816812,0.0056126396,0.0204776646,0.017933144,-0.0039250111,0.0117794943,-0.0285365124,-0.0126427836,-0.0215979816,-0.0017277708,0.0039163913,-0.0018369823,0.0121566632,-0.0124736106,-0.0142351715,-0.0164157826,-0.0149261369,0.0057059733,-0.0288393074,0.0442928628,0.0319763878]},"202":{"Abstract":"In this work we present a volume exploration method designed to be used by novice users and visitors to science centers and museums. The volumetric digitalization of artifacts in museums is of rapidly increasing interest as enhanced user experience through interactive data visualization can be achieved. This is, however, a challenging task since the vast majority of visitors are not familiar with the concepts commonly used in data exploration, such as mapping of visual properties from values in the data domain using transfer functions. Interacting in the data domain is an effective way to filter away undesired information but it is difficult to predict where the values lie in the spatial domain. In this work we make extensive use of dynamic previews instantly generated as the user explores the data domain. The previews allow the user to predict what effect changes in the data domain will have on the rendered image without being aware that visual parameters are set in the data domain. Each preview represents a subrange of the data domain where overview and details are given on demand through zooming and panning. The method has been designed with touch interfaces as the target platform for interaction. We provide a qualitative evaluation performed with visitors to a science center to show the utility of the approach.","Authors":"D. J\u00f6nsson; M. Falk; A. Ynnerman","DOI":"10.1109\/TVCG.2015.2467294","Keywords":"Transfer function;scalar fields;volume rendering;rendering;touch interaction;visualization;user interfaces;Transfer function;scalar fields;volume rendering;touch interaction;visualization;user interfaces","Title":"Intuitive Exploration of Volumetric Data Using Dynamic Galleries","Keywords_Processed":"scalar field;touch interaction;transfer function;volume render;user interface;visualization;render","Keyword_Vector":[0.0611092651,-0.0432007084,-0.0155368645,-0.0213168204,0.0045244409,-0.0064212169,-0.0359369948,-0.0505347075,-0.0539133134,0.0186091358,-0.0216942034,0.1366754861,0.0086476346,-0.0524258215,0.0453580604,-0.0039135405,0.1132182161,-0.0196290996,-0.0064817943,0.0557921721,0.017670237,-0.0012916978,-0.1284619459,0.0069002316,0.0434679776,0.0266926103,-0.0286726564,0.019055513,0.0141824653,0.0370328276,0.0031162378,0.0279337468,0.0469242986,-0.1174984166,-0.0448169349,-0.0413394796,-0.0493364807,-0.0829267797,-0.0012133385,0.039077016,-0.0323556759,0.0736644181,-0.0037938858,0.1392587124,-0.0677422457,0.0244986049,-0.040995726,0.0730655797,0.0595647333,0.0285654564,-0.032983255,0.1017792901],"Abstract_Vector":[0.1148687983,-0.025350969,0.0297939286,-0.0044481786,0.0003284333,0.0053043043,-0.0187974311,-0.0096150907,0.0167437947,0.0228935816,-0.0126651817,-0.0083109676,-0.0116981144,0.0255288022,-0.0090118428,-0.0013885478,-0.0019486218,-0.0047101462,0.021731026,-0.0414351226,0.0154318714,-0.0055022913,0.0155908677,-0.0201907853,0.014222006,0.0366479227,-0.0157707258,0.0529354776,-0.0340901906,-0.0255215186,0.0092568501,-0.0410913185,-0.022906383,0.0182623715,0.0211291757,0.0575529214,0.0154422646,0.0122599593,0.0343360653,0.0071862608,-0.0482372525,0.042645832,-0.0099151736,0.024588681,0.0198091919,-0.0198469826,0.0003924271,0.0113929506,0.0147159585,0.003208638,0.0260665803,0.0486719658,0.0326275804,-0.024845015,0.0132111871,-0.0053234301,0.0583741936,-0.0016310156,-0.0193434153,-0.0395396467,0.0558622362,0.0193999872,0.0595994006,-0.0043402716,-0.0055082762,-0.0058979132,0.0229371746,-0.0182555578,-0.0022675613,-0.0372043216,-0.0127899888,-0.0270137021,-0.0392570201,0.012074913,0.0196077558,-0.0206346904,0.0072070572,0.0060204935,-0.0221607894,0.0268726001,0.0080340044,-0.0291407004,-0.0034982704,0.0303894902,0.0009023572,-0.0076258356,0.0001419577,0.0180230434,0.0007402973,-0.0166569944,0.0167687277,0.024791085,-0.0000103906,-0.0237840088,0.0059253026,-0.0108658217,0.0127146873,-0.0027343977,-0.0150451425,-0.0102754511,0.0110628859,-0.006032863,-0.0176855849,-0.0144051374,-0.0097825006,-0.0328802014,0.0026979106,0.0008242095,-0.0032469024,0.0114246723,0.0361029729,0.0014919301,0.0045139971,0.020330071,-0.0282680449,-0.0064076972]},"203":{"Abstract":"Clustering is a core building block for data analysis, aiming to extract otherwise hidden structures and relations from raw datasets, such as particular groups that can be effectively related, compared, and interpreted. A plethora of visual-interactive cluster analysis techniques has been proposed to date, however, arriving at useful clusterings often requires several rounds of user interactions to fine-tune the data preprocessing and algorithms. We present a multi-stage Visual Analytics (VA) approach for iterative cluster refinement together with an implementation (SOMFlow) that uses Self-Organizing Maps (SOM) to analyze time series data. It supports exploration by offering the analyst a visual platform to analyze intermediate results, adapt the underlying computations, iteratively partition the data, and to reflect previous analytical activities. The history of previous decisions is explicitly visualized within a flow graph, allowing to compare earlier cluster refinements and to explore relations. We further leverage quality and interestingness measures to guide the analyst in the discovery of useful patterns, relations, and data partitions. We conducted two pair analytics experiments together with a subject matter expert in speech intonation research to demonstrate that the approach is effective for interactive data analysis, supporting enhanced understanding of clustering results as well as the interactive process itself.","Authors":"D. Sacha; M. Kraus; J. Bernard; M. Behrisch; T. Schreck; Y. Asano; D. A. Keim","DOI":"10.1109\/TVCG.2017.2744805","Keywords":"Visual Analytics;Interaction;Visual Cluster Analysis;Quality Metrics;Guidance;Self-Organizing Maps;Time Series","Title":"SOMFlow: Guided Exploratory Cluster Analysis with Self-Organizing Maps and Analytic Provenance","Keywords_Processed":"visual Cluster Analysis;Self Organizing map;Quality metric;Time Series;interaction;guidance;visual analytic","Keyword_Vector":[0.0653823044,0.0237219341,-0.005697218,0.0087020873,0.0144753842,-0.005997309,-0.0392825217,-0.016896994,-0.1006042688,-0.0493153887,0.0450777359,0.0549594387,0.0232960021,0.0040363001,0.0897424749,0.0590604505,0.0296993302,0.0124514462,0.1221983934,-0.0451732131,0.0154716477,-0.0179011455,0.0805686669,0.0617027001,-0.0723654751,-0.0237936783,0.085616626,0.0578922496,0.1138287293,-0.1066924952,-0.0307588501,-0.0466337055,0.0275925882,0.033441268,0.0013821654,-0.0642442435,0.0051091731,-0.0036829169,0.014955364,0.0340194466,-0.0647971451,0.014248701,-0.0478545165,-0.0254805361,-0.0101867899,0.0212253104,-0.0162813019,0.0650755872,-0.0007655808,-0.0186666268,0.0589066701,-0.0241292722],"Abstract_Vector":[0.1627669353,0.091586979,0.0221577798,0.0460189324,-0.1307167134,0.0641405022,-0.0476324978,-0.0579452093,0.0888914804,0.0420716837,0.0242543347,-0.0137158018,-0.0225019573,-0.0113559571,0.0145228841,-0.012111534,-0.0159933049,-0.0101115182,-0.0136263893,0.03771558,0.0169810644,-0.0133589428,0.0059594328,-0.0128584044,-0.0182226939,-0.0121268951,-0.0018557067,0.0177470039,-0.0108806904,0.0293531812,0.0386167225,0.0188521734,-0.023471565,-0.000450331,0.0331291205,0.0063657737,-0.0160495579,-0.0290729944,0.0185607272,0.0406980696,0.0172595096,-0.0347949003,0.0068859324,-0.0001943329,0.000590015,-0.0066610876,-0.0049763465,0.0173596664,-0.0357093563,0.0157426349,0.0005270243,0.0046272552,0.0133761682,0.0080128109,0.0112397602,-0.0108460265,-0.0088872697,-0.0024482887,0.0007257271,-0.0205386772,-0.0060779348,-0.0205567646,-0.0054956082,0.0246462956,0.0005579138,-0.0088142987,0.0255922902,0.0105541494,0.0316990964,-0.0072873372,0.0293288165,-0.0210974139,-0.020071694,0.0136962408,-0.0044467513,-0.0032893008,-0.0318105264,-0.0466361943,-0.0050687422,-0.0238596026,0.0034843782,0.005619452,-0.0090194287,-0.0156881653,0.0078637932,0.0025349062,0.0174775107,-0.0046700761,-0.020428475,-0.0574566608,0.0183914088,0.0125350668,0.0129042235,-0.0163075165,-0.0084641325,0.0119129615,0.0128995924,-0.0031757968,-0.0161946231,0.0034398628,0.0122912826,-0.0022629208,0.000819573,0.0007302235,-0.0090224548,0.0067469972,-0.0112008321,0.0025100812,-0.0025712762,0.0032584932,0.0100386668,0.0300525707,-0.0199946935,-0.0047025253,-0.011930195,0.0198191412]},"204":{"Abstract":"Stochastically solving the rendering integral (particularly visibility) is the de-facto standard for physically-based light transport but it is computationally expensive, especially when displaying heterogeneous volumetric data. In this work, we present efficient techniques to speed-up the rendering process via a novel visibility-estimation method in concert with an unbiased importance sampling (involving environmental lighting and visibility inside the volume), filtering, and update techniques for both static and animated scenes. Our major contributions include a progressive estimate of partial occlusions based on a fast sweeping-plane algorithm. These occlusions are stored in an octahedral representation, which can be conveniently transformed into a quadtree-based hierarchy suited for a joint importance sampling. Further, we propose sweep-space filtering, which suppresses the occurrence of fireflies and investigate different update schemes for animated scenes. Our technique is unbiased, requires little precomputation, is highly parallelizable, and is applicable to a various volume data sets, dynamic transfer functions, animated volumes and changing environmental lighting.","Authors":"P. von Radziewsky; T. Kroes; M. Eisemann; E. Eisemann","DOI":"10.1109\/TVCG.2016.2606498","Keywords":"Visibility;raytracing;volume rendering;stochastic rendering;importance sampling","Title":"Efficient Stochastic Rendering of Static and Animated Volumes Using Visibility Sweeps","Keywords_Processed":"importance sample;volume render;visibility;raytrace;stochastic rendering","Keyword_Vector":[0.0502617486,-0.0139986354,-0.0120790181,-0.0462780243,-0.0035971393,-0.0314796739,-0.0494044352,-0.0208598633,-0.0711257221,0.039406169,-0.0694271421,0.061560183,-0.0013138535,0.0245290464,-0.0692263073,-0.0049283465,-0.0165579798,0.0157120325,-0.0441272936,-0.0239825812,0.1017581573,0.0553489536,-0.0494529381,0.0123558956,-0.0812736224,0.0921354492,-0.0620709268,0.0881274652,-0.0488985325,0.0090575169,-0.0506616121,-0.0769399044,-0.0156525726,-0.1185162085,-0.0267212743,-0.0848209055,0.0367214285,-0.1826431295,-0.0152195183,-0.0241491692,0.0886549309,0.0290953903,-0.0827733965,0.0570424989,-0.0796224905,0.0341457949,0.023385095,0.0485378834,0.020773403,0.0379667816,0.0010238878,-0.0369570635],"Abstract_Vector":[0.168648526,0.0184570301,-0.0470040411,-0.043993571,-0.0337249967,0.1070203857,0.0159287699,-0.0004490631,0.0322627442,-0.0023411874,-0.0743385756,0.0135101601,-0.0443736455,0.0137255635,-0.0109390545,-0.0174440894,0.0127252016,0.0542708735,-0.0431182152,-0.0369452692,-0.0183213973,-0.0403422486,0.0374974099,0.0353538225,0.0178960368,-0.007502541,0.0195674339,0.006466159,-0.0038636864,0.0233665598,-0.0339847527,-0.0149859591,-0.0208979167,-0.0510037335,-0.0066602402,0.0708836301,0.0271025938,-0.003786157,0.0582829729,0.0754866282,0.1882166304,0.1270721299,0.0135353673,0.1057350109,-0.0112753247,-0.0044285704,0.024753295,0.0400678271,-0.0333359656,-0.0347200596,0.0019612643,0.0078189351,-0.0336450509,0.0978113731,0.0008098587,-0.0108698585,-0.0182520762,0.0013227343,-0.0592552101,-0.055436565,-0.0242920198,-0.0412898812,0.01730815,-0.0273572326,-0.0176537964,0.0375190179,-0.0057243441,0.0719833318,-0.0356633054,-0.0629186741,0.0319409704,0.0354085791,-0.0336957288,0.061539978,-0.0138797532,-0.0441767982,0.0277651485,0.1234268486,-0.0380579618,0.0215257715,0.0184892841,-0.0089165766,0.0036699216,-0.0522471993,-0.0174679701,0.0053943186,0.0136645444,-0.0626001937,-0.042601178,0.0239007584,0.020781314,0.0130301098,0.0099822384,0.0255376639,-0.0309689745,-0.0555204939,0.096754749,-0.0152574822,0.032817678,0.033270551,-0.0028562669,-0.0180977298,0.0010467439,-0.0228029675,-0.0372669095,0.0424391155,0.0046655445,0.0664938432,0.0419299162,-0.0234862085,0.0445422536,0.062594388,-0.0207696468,0.0133221756,0.0239001981,0.0378457494]},"205":{"Abstract":"This paper deals with the texture mapping of a triangular mesh model given a set of calibrated images. Different from the traditional approach of applying projective texture mapping with model parameterizations, we develop an image-space texture optimization scheme that aims to reduce visible seams or misalignment at texture or depth boundaries. Our novel scheme starts with an efficient local (and parallel) texture adjustment scheme at these boundaries, followed by a global correction step to rectify potential texture distortions caused by the local movement. Our phased optimization scheme achieves 50~100 times speed up on GPU (or 6\u00d7 on CPU) compared to previous state-of-the-art methods. Experiments on a variety of models showed that we achieve this significant speedup without sacrificing texture quality. Our approach significantly improves resilience to modeling and calibration errors, thereby allowing fast and fully automatic creation of textured models using commodity depth sensors by untrained users.","Authors":"W. Li; H. Gong; R. Yang","DOI":"10.1109\/TVCG.2018.2831220","Keywords":"Texture mapping;parameterization;texture optimization","Title":"Fast Texture Mapping Adjustment via Local\/Global Optimization","Keywords_Processed":"texture optimization;texture mapping;parameterization","Keyword_Vector":[0.0230712365,-0.0163749764,-0.0048674333,-0.0154765221,0.0209697041,-0.0041300375,0.0168047021,-0.000967885,-0.0132063346,0.0140722969,-0.016445092,-0.0045666386,-0.0002442882,-0.0202946557,0.0105691778,-0.0105830782,0.0229673362,-0.0096391367,0.0080400918,0.0054731361,-0.0110530434,-0.011318981,0.0112567698,-0.0168732487,0.0112187615,-0.0126432558,0.0270913037,-0.0261207924,0.0042343266,-0.0348850935,0.0191739083,-0.0411146212,-0.0037460829,0.0913375508,0.0941717169,-0.0103501412,-0.0273370649,-0.0418268986,-0.0437387324,0.051865567,-0.0073298079,-0.0065964291,-0.0428857655,0.0520353734,0.0141681776,0.0296052804,-0.0371300201,0.0071894753,0.0448093178,-0.0180715741,0.0083047455,0.00024051],"Abstract_Vector":[0.2224110205,0.0290081409,0.0059384393,0.068520681,0.0140723795,0.0008848773,0.050251281,0.0130970324,0.0082047486,0.0845413646,-0.0348743632,0.0411484069,-0.0123769945,-0.0038865099,0.046152444,0.0301312102,0.0481358179,0.0664201168,0.0028785586,-0.053394709,0.0447448752,0.0386168242,0.004160133,0.0136095343,-0.0133506755,0.019667101,-0.00689425,0.0092070043,-0.0205680136,0.0729315188,-0.0083713311,-0.0237232333,-0.020191737,-0.0065118326,0.0037615658,-0.0437824411,-0.0185578698,0.009235721,-0.0199680488,-0.0178931407,0.0549605575,0.0350388473,0.0288425848,0.0088767809,0.009697159,0.0573154196,-0.0199330841,0.0058716128,0.0384746853,-0.0241197223,0.0721137376,-0.0135124341,0.0178995003,-0.0139196586,0.0110752884,0.0186441173,0.029380216,0.016496194,0.0111341186,-0.030609705,-0.0213586953,-0.0243288119,-0.0031418462,0.0066610375,-0.0020500413,-0.0000413278,-0.0142971617,0.0305372818,0.0377908578,0.0496059107,0.0025485754,-0.0175905864,-0.000263417,-0.0214058656,-0.0117830429,-0.0225569398,-0.0355504581,0.0175900457,-0.0326476212,0.0476871856,0.0095110745,0.0221835558,-0.008218817,-0.0295553165,-0.0287498213,0.0393823856,-0.0365402421,0.0052898264,-0.0139043541,0.0154143024,-0.0078007062,0.0129477047,0.0338251158,-0.0016401114,-0.0174027706,0.0327700622,-0.0424230363,0.0212974486,0.0071219722,-0.0066102703,0.0208891633,0.0227857374,-0.0197209518,-0.052215001,0.0251902694,-0.0529715963,0.0251086526,-0.0198641608,0.0312246697,-0.0303729791,0.0478464172,0.0068002614,0.0052386887,0.0095660923,0.0197652557,-0.0090410427]},"206":{"Abstract":"Weathering effects are ubiquitous phenomena in cities. Buildings age and deteriorate over time as they interact with the environment. Pollution accumulating on facades is a particularly visible consequence of this. Even though relevant work has been done to produce impressive images of virtual urban environments including weathering effects, so far, no technique using a global approach has been proposed to deal with weathering effects. Here, we propose a technique based on a fast physically-inspired approach, that focuses on modeling the changes in appearance due to pollution soiling on an urban scale. We consider pollution effects to depend on three main factors: wind, rain and sun exposure, and we take into account three intervening steps: deposition, reaction and washing. Using a low-cost pre-computation, we evaluate the pollution distribution throughout the city. Based on this and the use of screen-space operators, our method results in an efficient approach able to generate realistic images of urban scenes by combining the intervening factors at interactive rates. In addition, the pre-computation demands a reduced amount of memory to store the resulting pollution map and, as it is independent from scene complexity, it can suit large and complex models by adapting the map resolution.","Authors":"I. Mu\u00f1oz-Pandiella; C. Bosch; N. M\u00e9rillou; G. Patow; S. M\u00e9rillou; X. Pueyo","DOI":"10.1109\/TVCG.2018.2794526","Keywords":"Weathering;appearance;interactive rendering;urban scenes;screen-space techniques;shading and texture;picture\/image generation","Title":"Urban Weathering: Interactive Rendering of Polluted Cities","Keywords_Processed":"appearance;picture image generation;weather;urban scene;interactive render;screen space technique;shade and texture","Keyword_Vector":[0.1427486995,-0.0818761184,-0.0467616903,0.0084004931,-0.0170937183,0.0431531661,-0.0664114696,-0.0194598198,-0.0203353034,0.1111461974,-0.0050773925,0.1622122112,0.0754120662,0.0385617559,-0.0239261982,-0.0486145934,0.1162928096,-0.0582239069,0.0003800897,0.0371369697,-0.0506933668,-0.0519358955,-0.04604103,-0.0403401112,0.0439989956,-0.0212569197,0.0425924258,-0.0577991092,-0.0017075622,0.0145744482,0.0229117114,0.033446495,-0.0347603676,-0.0155819428,-0.0008606798,0.0108555714,0.0388360318,0.0602072203,0.0288006726,0.0252444193,-0.0083349215,0.0123628794,-0.0297472156,0.0311656509,-0.0332380603,0.0001572654,-0.0232362629,0.0084263527,0.0070884162,0.0411936992,-0.0125595038,0.1048443575],"Abstract_Vector":[0.2398743319,-0.0369295963,0.0001651293,0.0118784358,0.017610713,-0.0245644029,-0.0205833737,-0.0096959934,0.036417237,0.0141514666,-0.0037566627,-0.0103368491,-0.0487985222,-0.0286656076,-0.0390664388,0.0418045564,-0.0515833017,-0.0477005341,0.0672838933,-0.0113186952,-0.0225830422,-0.0691276429,0.0084741877,-0.0221343609,0.0261244932,0.051603902,-0.0775527214,-0.0130014452,0.0477188862,-0.016408737,-0.0323087336,-0.0036653092,-0.0532126676,-0.0186301516,-0.001952864,-0.0017579381,0.051090323,0.0189358297,-0.0787449028,-0.0089096914,-0.0460502888,-0.0002983047,-0.063982789,-0.0080430583,-0.0170034655,0.0244842539,0.012780232,0.0313816529,-0.0385302551,-0.0729015757,0.033857225,-0.0001946,-0.0277393319,-0.0302796371,0.0270975938,0.0510934257,0.000738367,0.0241216003,0.0863423547,0.0093547905,-0.0148686726,-0.0744575581,0.0271425746,-0.0018328743,-0.0195324982,0.0367329174,-0.0284531682,-0.0174197127,-0.0242461867,-0.0273197879,0.0119508618,0.0398268593,0.0553476891,0.0351671482,-0.0146544347,0.0233996024,-0.0574575556,0.0150104769,-0.0105687965,0.010318503,0.0247848167,0.0112593096,0.0399737038,-0.0175311195,0.0335376698,-0.036751346,0.0351670107,0.0548753711,0.0143412581,-0.0213606145,-0.0307379994,0.0163770838,-0.0414788058,0.0062308394,0.0041129036,-0.042131201,0.0514696358,0.0148049971,0.0064809673,-0.0219803652,0.0052365131,-0.0341207337,-0.0063855078,-0.0130335231,0.0467428296,-0.0133851569,0.0039515759,-0.0610358051,-0.0255004217,-0.008932006,-0.020125326,-0.0086935268,0.0217935961,0.0062349965,-0.0143749683,-0.0242609877]},"207":{"Abstract":"Animated representations of outcomes drawn from distributions (hypothetical outcome plots, or HOPs) are used in the media and other public venues to communicate uncertainty. HOPs greatly improve multivariate probability estimation over conventional static uncertainty visualizations and leverage the ability of the visual system to quickly, accurately, and automatically process the summary statistical properties of ensembles. However, it is unclear how well HOPs support applied tasks resembling real world judgments posed in uncertainty communication. We identify and motivate an appropriate task to investigate realistic judgments of uncertainty in the public domain through a qualitative analysis of uncertainty visualizations in the news. We contribute two crowdsourced experiments comparing the effectiveness of HOPs, error bars, and line ensembles for supporting perceptual decision-making from visualized uncertainty. Participants infer which of two possible underlying trends is more likely to have produced a sample of time series data by referencing uncertainty visualizations which depict the two trends with variability due to sampling error. By modeling each participant's accuracy as a function of the level of evidence presented over many repeated judgments, we find that observers are able to correctly infer the underlying trend in samples conveying a lower level of evidence when using HOPs rather than static aggregate uncertainty visualizations as a decision aid. Modeling approaches like ours contribute theoretically grounded and richly descriptive accounts of user perceptions to visualization evaluation.","Authors":"A. Kale; F. Nguyen; M. Kay; J. Hullman","DOI":"10.1109\/TVCG.2018.2864909","Keywords":"uncertainty visualization;hypothetical outcome plots;psychometric functions","Title":"Hypothetical Outcome Plots Help Untrained Observers Judge Trends in Ambiguous Data","Keywords_Processed":"psychometric function;uncertainty visualization;hypothetical outcome plot","Keyword_Vector":[0.2294353542,-0.0770391342,0.0079475429,0.2882083837,0.2035924893,0.2637089778,-0.0643667841,0.1856609265,-0.0636618426,-0.0268723478,-0.1012699148,0.0093031315,0.0181704453,-0.0665570676,0.0715056735,-0.0171917433,-0.0180015922,-0.0051786613,-0.0500004091,-0.0571613545,-0.03806579,-0.0429885032,0.0981749042,-0.039063245,-0.002993464,-0.0725022987,-0.1222722825,-0.0040074662,0.0048094852,0.019719516,-0.0820507632,0.0137872785,-0.021416819,-0.1019529129,0.0391168519,0.1111353652,0.0611722578,0.0138147471,-0.0829797796,-0.0675581068,0.1124563908,0.0698859327,-0.0487715965,0.0822316287,-0.0036423422,0.0546399163,-0.1185162013,-0.0870899425,0.0494316768,-0.0037621653,0.0256757614,-0.063763019],"Abstract_Vector":[0.2047977334,-0.0146573683,-0.0194541757,0.0219043609,-0.0139038814,0.0592897597,-0.0012415719,0.0078646485,0.0624772641,-0.0272576522,-0.0192872447,-0.0311098266,-0.0183092293,-0.0122008926,-0.0324769024,0.03451055,0.0595589805,-0.0284911153,0.0478580224,-0.0537818596,0.0264343134,0.0223532497,0.0575829394,-0.0572872755,0.0273990768,0.0570199109,0.0572173206,0.0178437979,-0.0250925154,0.0711913999,0.0366532558,0.0098550162,-0.0438201796,-0.0116235338,-0.0168033396,0.0581057701,0.0065081107,0.0281933922,0.0175847293,0.0826759014,0.0377552312,0.0097217263,0.018110268,-0.0167861241,0.0454010377,-0.0151440271,0.1322256676,0.0616915408,0.0179877232,0.0040139392,-0.0376719393,0.0302574026,0.0307364152,0.0199759653,0.0211094362,0.0089767118,0.0199538523,0.0122595849,0.0381954194,-0.0628031791,-0.0146666729,-0.0358523648,-0.0100857731,-0.0019662726,-0.0372689456,0.0098930463,-0.0260101868,0.0266080145,0.0452085161,-0.0526135654,-0.0380885342,-0.0205537451,-0.0344531215,-0.0112394759,-0.0387036302,0.0897059222,-0.0090699323,-0.0299036657,-0.0618108226,-0.0522013684,0.0327991451,-0.0056374601,0.0533439277,-0.0553827364,0.0137472462,0.057501463,0.0308098183,0.0250627817,0.0105963491,0.0172290953,-0.0339103159,0.0056381556,-0.0080713217,0.0153494261,-0.0587566175,0.0051447232,0.0668176024,-0.020574846,-0.0269655678,0.0301330924,0.0109832518,0.012988399,0.0269959579,-0.0345059795,0.0420332185,0.0059677129,-0.0114035278,-0.0195724691,-0.0063582696,0.0273883047,0.0029028037,-0.0291353621,-0.01015271,0.0649878652,0.0364567448,0.0202789813]},"208":{"Abstract":"In single-phase flow visualization, research focuses on the analysis of vector field properties. In two-phase flow, in contrast, analysis of the phase components is typically of major interest. So far, visualization research of two-phase flow concentrated on proper interface reconstruction and the analysis thereof. In this paper, we present a novel visualization technique that enables the investigation of complex two-phase flow phenomena with respect to the physics of breakup and coalescence of inclusions. On the one hand, we adapt dimensionless quantities for a localized analysis of phase instability and breakup, and provide detailed inspection of breakup dynamics with emphasis on oscillation and its interplay with rotational motion. On the other hand, we present a parametric tightly linked space-time visualization approach for an effective interactive representation of the overall dynamics. We demonstrate the utility of our approach using several two-phase CFD datasets.","Authors":"G. K. Karch; F. Beck; M. Ertl; C. Meister; K. Schulte; B. Weigand; T. Ertl; F. Sadlo","DOI":"10.1109\/TVCG.2017.2692781","Keywords":"Flow visualization;two-phase flow;feature deformation;space-time analysis;feature tracking","Title":"Visual Analysis of Inclusion Dynamics in Two-Phase Flow","Keywords_Processed":"space time analysis;feature tracking;flow visualization;feature deformation;two phase flow","Keyword_Vector":[0.0443770744,0.0165141653,-0.0106409329,-0.003656593,0.0317922497,-0.0292746467,-0.0417056709,0.0226952045,-0.1078383289,-0.038936475,0.0412014941,0.0891552724,-0.0040916408,0.0229369824,0.077038497,0.0909846972,0.0622469087,0.0481354156,0.2115823546,-0.0251681034,-0.1104568107,0.0591394018,0.1517672927,0.0746581993,-0.0641023146,-0.0523275391,-0.0425886761,-0.0247208681,0.1109525617,-0.1317677397,-0.0491785166,-0.1033802328,0.0673826151,0.0235064839,-0.0118090959,0.0734160167,0.0712725651,-0.0017061308,-0.0465747988,-0.0967080814,0.0323248552,0.0595746745,0.0460609886,0.0827193799,-0.0285342817,-0.007999508,-0.0677314677,0.0097320254,-0.0445151201,0.0486551264,0.012120789,0.0081773662],"Abstract_Vector":[0.1280456273,-0.0375275467,0.0179455699,-0.0043168018,-0.0274591187,0.0565379449,0.0031031043,0.003308297,0.0427514727,0.0315895424,-0.0306068071,0.0254479568,-0.0089900651,0.0163357793,-0.0116012547,0.0125243896,-0.0058810048,0.0277766325,0.0363733765,-0.0173292968,-0.030232847,-0.0531856393,0.0155034818,0.0384574491,-0.0082084126,0.0057037633,0.0030813235,0.0391340599,0.0425815079,0.017996444,-0.002585096,-0.0013197853,-0.060422813,-0.0024390979,0.0181677224,0.0038243302,-0.0233867743,-0.0807470725,-0.0078571507,-0.0153975958,0.0282892209,0.0670336385,0.0121205664,-0.0358744335,0.0168326573,0.0087442109,0.0012641687,0.0176486675,0.012029374,-0.0159850018,0.022314824,-0.0354659747,-0.0044847047,-0.0327596919,-0.0399108103,0.0004228658,0.0182670904,0.0248066179,-0.0154878726,-0.0548435487,0.0277133478,-0.0364255777,0.0070888949,-0.0236871483,0.0235282601,-0.0306424969,-0.040994803,-0.029672804,-0.0735523522,0.0372751714,-0.011113019,-0.0130161273,0.0413442703,0.0787912914,0.0227213208,-0.0734642807,0.0547258138,0.0353704451,0.0094906539,-0.0350541267,-0.0183392291,-0.0003279002,-0.0483343689,0.0531815977,-0.0120028487,-0.0081552463,-0.0039076906,-0.021136717,-0.041606955,-0.0540080941,0.0091029513,-0.0154583452,0.0446341848,0.0223034174,-0.0015303397,-0.0027827944,0.0367051042,0.0160392688,0.0209855446,-0.0121066697,-0.026238532,-0.0116181829,0.0119640378,0.0154212885,0.0427271627,0.0621160958,0.0593696302,0.0430024481,0.0350226828,-0.0520627772,0.0509494611,0.0351422964,0.0040590882,-0.0035321044,0.0094530517,0.0133082006]},"209":{"Abstract":"Results of planetary mapping are often shared openly for use in scientific research and mission planning. In its raw format, however, the data is not accessible to non-experts due to the difficulty in grasping the context and the intricate acquisition process. We present work on tailoring and integration of multiple data processing and visualization methods to interactively contextualize geospatial surface data of celestial bodies for use in science communication. As our approach handles dynamic data sources, streamed from online repositories, we are significantly shortening the time between discovery and dissemination of data and results. We describe the image acquisition pipeline, the pre-processing steps to derive a 2.5D terrain, and a chunked level-of-detail, out-of-core rendering approach to enable interactive exploration of global maps and high-resolution digital terrain models. The results are demonstrated for three different celestial bodies. The first case addresses high-resolution map data on the surface of Mars. A second case is showing dynamic processes, such as concurrent weather conditions on Earth that require temporal datasets. As a final example we use data from the New Horizons spacecraft which acquired images during a single flyby of Pluto. We visualize the acquisition process as well as the resulting surface data. Our work has been implemented in the OpenSpace software [8], which enables interactive presentations in a range of environments such as immersive dome theaters, interactive touch tables, and virtual reality headsets.","Authors":"K. Bladin; E. Axelsson; E. Broberg; C. Emmart; P. Ljung; A. Bock; A. Ynnerman","DOI":"10.1109\/TVCG.2017.2743958","Keywords":"Astronomical visualization;globe rendering;public dissemination;science communication;space mission visualization","Title":"Globe Browsing: Contextualized Spatio-Temporal Planetary Surface Visualization","Keywords_Processed":"astronomical visualization;globe render;space mission visualization;public dissemination;science communication","Keyword_Vector":[0.2873775378,-0.1441471746,0.0678699736,-0.2566200108,0.0449726281,-0.0553780626,-0.1194892986,-0.0080707701,-0.0185396125,0.0519512484,0.0155159718,-0.1882567397,0.0297559757,-0.0325340683,0.0578867222,-0.0507271752,0.0369560325,-0.0042289199,-0.0033371729,-0.0227169833,0.0067081405,0.0520399792,0.0612510501,-0.0006066194,-0.0035864737,-0.0457488226,-0.0033063702,-0.029081961,0.0043305914,0.0268910941,0.0279431643,0.0058163658,0.0095195792,-0.0526768823,-0.0289514655,0.0919360207,-0.0319652964,0.0370290541,0.0003335814,0.014285074,-0.0869733935,-0.029586801,0.0700785437,-0.0226588045,0.0048935738,0.0593147975,0.0016029108,0.055911931,-0.0116965673,-0.0192486231,-0.0160042648,-0.0188094333],"Abstract_Vector":[0.1530128386,-0.0948796025,0.015605446,0.0287456244,-0.0007431587,0.0549973801,0.0385673075,0.0311757946,-0.032604426,0.0195076976,-0.0264949395,0.014174898,0.047496424,-0.0211596747,-0.0214574897,-0.0003435189,0.0092172671,0.0017982977,-0.0021436313,0.0355192476,-0.0095508373,-0.0633004406,-0.0324825489,0.0478652924,-0.0011870702,-0.0447530236,-0.0448169178,0.0291726716,0.031514693,0.0376796501,0.0077735464,-0.0083425386,-0.0111831587,0.0128644621,-0.0212166012,0.0076605434,0.0428658198,-0.0469108128,0.0057622702,-0.0244095095,-0.0018660235,0.0666489848,-0.0128714142,0.0337600042,0.0130185416,0.0179275162,0.0385533174,-0.0038603471,-0.0095765411,-0.0035946605,-0.0085133457,0.0151538744,0.0025253923,-0.0094638742,-0.0108860714,-0.012510646,-0.0170272539,0.0171382731,-0.0228713333,-0.0503783758,0.0295275354,0.021580789,0.0416326691,-0.0237308466,-0.0069355737,-0.0201786077,-0.0152033961,-0.0077930161,-0.0003057956,-0.0061028091,-0.0202842507,0.0031783453,-0.0271862896,-0.0182297164,-0.013157891,-0.0004818912,0.0022613687,-0.0041770135,0.0140223978,0.0058766679,-0.0168336528,-0.0205133295,-0.0007890727,0.0037683153,0.0277212023,0.0225014425,-0.0107900019,0.0050340894,-0.0020748687,-0.0067734191,-0.024886976,0.0046852201,0.0000610501,-0.0116273862,0.0124865846,0.0011411653,0.030654731,-0.0075147191,-0.0100901027,-0.0157399063,0.005496247,0.0145625731,-0.0005380906,0.0260323785,-0.0311529474,0.0048589932,-0.0002638969,-0.0160698992,0.0077868969,-0.0075950278,-0.0024674985,0.0127999091,-0.0242291286,-0.0093547088,0.0245847114,-0.0035323549]},"21":{"Abstract":"We propose a computer generated integral photography (CGIP) method that employs a lens based rendering (LBR) algorithm for super-multiview displays to achieve higher frame rates and better image quality without pixel resampling or view interpolation. The algorithm can utilize both fixed and programmable graphics pipelines to accelerate CGIP rendering and inter-perspective antialiasing. Two hardware prototypes were fabricated with two high-resolution liquid crystal displays and micro-lens arrays (MLA). Qualitative and quantitative experiments were performed to evaluate the feasibility of the proposed algorithm. To the best of our knowledge, the proposed LBR method outperforms state-of-the-art CGIP algorithms relative to rendering speed and image quality with our super-multiview hardware configurations. A demonstration experiment was also conducted to reveal the interactivity of a super-multiview display utilizing the proposed algorithm.","Authors":"G. Chen; C. Ma; Z. Fan; X. Cui; H. Liao","DOI":"10.1109\/TVCG.2017.2756634","Keywords":"Lens based rendering;integral photography;super-multiview display;GPU","Title":"Real-Time Lens Based Rendering Algorithm for Super-Multiview Integral Photography without Image Resampling","Keywords_Processed":"gpu;integral photography;super multiview display;Lens base render","Keyword_Vector":[0.1013411486,-0.1604475528,-0.1771751756,0.1371905786,-0.1219195318,-0.191440014,0.0946187014,0.0348743483,0.0658994292,-0.0113762768,0.0158312272,0.0131750629,-0.0283485377,-0.0945876351,0.0458558801,-0.0098561206,-0.02734642,0.0108342091,-0.0038294704,0.010103181,0.0316590722,0.0321315595,0.0252596843,0.0070453875,-0.0147409832,0.0176068357,0.0168996138,-0.0082922373,0.0354429312,-0.0054732947,0.0247452225,0.011416367,-0.0028994014,0.0289955746,-0.0085998898,0.0005615496,0.0003973543,-0.0251954581,0.0114583652,-0.0069720069,-0.0008761767,-0.0231473476,-0.0031524031,-0.0029262057,0.0182955201,-0.012456109,-0.0043134698,0.0047435543,0.0082768351,0.0145704099,-0.012203735,-0.0017451263],"Abstract_Vector":[0.1782126483,-0.0598842481,0.0086200993,0.0209869125,-0.0792616786,-0.0035594942,0.0327748448,0.0216762638,-0.0409893928,-0.028611742,-0.0051547737,-0.0179665879,-0.0556652048,0.0257555385,-0.022578583,-0.0227185236,0.0854348426,0.0319063348,-0.0603658851,-0.080115824,0.0598278404,0.0141236175,0.031441943,0.0148208991,-0.0587668209,0.0233958882,0.0413910825,0.0611641481,-0.0539778019,-0.0560074227,-0.0117075525,-0.0674478149,-0.0126756212,0.0537613059,-0.0460776372,0.0089978368,-0.0390024723,0.013467041,0.0252957932,-0.0484106175,-0.1210734341,0.0211465078,0.0464838958,-0.0420529628,-0.0093874525,0.0097134164,0.0022679845,0.0999690148,-0.0211348011,-0.0230135105,-0.0237018223,-0.0642628275,0.0028973607,0.0364612123,0.0153440654,0.0547561485,0.0181282618,0.0240797589,0.0824882702,0.072350881,0.1030008036,0.0052737537,0.0666337708,-0.0076748855,-0.045783101,0.0313176972,0.0327142517,0.1412972794,0.018176235,-0.0235259516,-0.0235835358,0.1389222116,0.043619436,0.0131124381,0.0576992846,-0.0047371707,0.0428729366,-0.0464932494,0.0545223628,-0.0126273905,-0.0593075164,-0.0164622189,0.0033000167,0.0233393132,0.0680088469,0.0181608802,-0.0019724876,0.0413406138,-0.0846810501,0.0617106498,-0.0165065828,-0.0990554718,0.014634471,0.0473388097,-0.0823927752,-0.0148924443,0.0109312436,0.0392180456,0.0268917757,0.0372269392,0.037243153,0.0322313021,-0.0362513858,0.0317366433,-0.0414029607,-0.0439306853,-0.0657091355,-0.028013725,0.0548556207,-0.0073819935,0.0205437769,-0.0991786073,-0.0006999348,-0.0148272451,0.012658626,0.0254984836]},"210":{"Abstract":"Non-linear dimensionality reduction (NDR) methods such as LLE and t-SNE are popular with visualization researchers and experienced data analysts, but present serious problems of interpretation. In this paper, we present DimReader, a technique that recovers readable axes from such techniques. DimReader is based on analyzing infinitesimal perturbations of the dataset with respect to variables of interest. The perturbations define exactly how we want to change each point in the original dataset and we measure the effect that these changes have on the projection. The recovered axes are in direct analogy with the axis lines (grid lines) of traditional scatterplots. We also present methods for discovering perturbations on the input data that change the projection the most. The calculation of the perturbations is efficient and easily integrated into programs written in modern programming languages. We present results of DimReader on a variety of NDR methods and datasets both synthetic and real-life, and show how it can be used to compare different NDR methods. Finally, we discuss limitations of our proposal and situations where further research is needed.","Authors":"R. Faust; D. Glickenstein; C. Scheidegger","DOI":"10.1109\/TVCG.2018.2865194","Keywords":"Non-linear dimensionality reduction;auto-differentiation","Title":"DimReader: Axis lines that explain non-linear projections","Keywords_Processed":"non linear dimensionality reduction;auto differentiation","Keyword_Vector":[0.1909642864,-0.2379205553,-0.2282634672,0.2136591324,-0.1653495703,-0.0816116312,0.0502827727,-0.0133416682,0.0326210867,0.0243354952,0.0054453352,0.0825987586,0.01741498,-0.1830794229,0.0721329939,-0.0172380338,0.0038888925,-0.0344715437,-0.0083961684,0.0232996964,-0.0237480107,-0.0183446415,0.0366475368,-0.005366632,-0.0075667456,0.0127850624,-0.0197091708,-0.0010859473,0.0044776877,0.0098491784,-0.0048923284,-0.0306979244,0.05734733,-0.0192137644,0.0067657161,-0.0265900763,-0.0527950231,-0.0340649221,-0.0483343089,-0.0927322161,-0.0009113824,-0.0249051145,-0.033244472,-0.0553940553,0.0726553086,0.0012586146,0.0070599238,-0.0143532922,-0.0022816969,-0.0302254388,0.0900691002,-0.0588734075],"Abstract_Vector":[0.2048627543,-0.1296566174,0.0148106923,-0.0182312784,-0.0436071857,-0.0166976169,0.0254988817,-0.0013814981,-0.0363075289,0.0498240158,-0.002896282,-0.009037689,-0.0869779409,0.0100756977,-0.0100682073,-0.0009940353,0.0050083681,-0.0022774957,-0.0142888144,0.0318444737,0.0212004386,-0.0199672095,-0.0058005875,0.0272166013,-0.0198048753,0.0273123007,0.0062415433,0.0436110075,-0.0025527107,-0.0338966093,0.0163124605,0.0275350208,0.0061259323,0.0159078989,-0.069658273,0.0130231281,-0.0112013971,-0.0395968232,-0.0469586368,0.0444982581,-0.0062007781,-0.0020232682,-0.0549464915,0.0298083041,0.0312759201,0.0201811269,-0.0017668656,-0.0173062769,-0.0285295732,0.00107378,-0.0288709122,-0.0202640041,0.034708944,0.0268778879,0.0054259122,-0.0078486651,0.0116474612,0.0197684964,0.0089044835,0.0240230746,-0.0183902801,0.0025351068,-0.0440637442,-0.0083693778,0.0282134536,0.0181102197,0.0196729948,-0.0165234209,-0.0281585794,-0.0129941986,-0.0128766539,-0.0309802791,-0.0109048426,0.0007124112,0.008406122,0.0295249128,-0.0062505044,0.0134314675,-0.0017864134,-0.0088637287,-0.0112684295,-0.0272486514,0.0273787595,0.0110743121,-0.0230545627,0.0336190073,0.0091449409,-0.0086536076,-0.0139080391,-0.0025189369,0.013411335,0.0104534885,-0.0042710372,-0.0132731743,0.0190218859,0.0302659872,-0.0304025095,0.0112978267,0.0080596853,-0.02112669,-0.0145228766,0.0499981103,0.018999097,-0.001034249,-0.0133423228,-0.0404274071,0.0292580487,0.0057761551,-0.0219871895,0.0006996103,-0.0068168658,-0.0144513227,-0.0088963243,-0.0003450903,-0.0108964965,0.017646938]},"211":{"Abstract":"Recurrent neural networks, and in particular long short-term memory (LSTM) networks, are a remarkably effective tool for sequence modeling that learn a dense black-box hidden representation of their sequential input. Researchers interested in better understanding these models have studied the changes in hidden state representations over time and noticed some interpretable patterns but also significant noise. In this work, we present LSTMVis, a visual analysis tool for recurrent neural networks with a focus on understanding these hidden state dynamics. The tool allows users to select a hypothesis input range to focus on local state changes, to match these states changes to similar patterns in a large data set, and to align these results with structural annotations from their domain. We show several use cases of the tool for analyzing specific hidden state properties on dataset containing nesting, phrase structure, and chord progressions, and demonstrate how the tool can be used to isolate patterns for further statistical analysis. We characterize the domain, the different stakeholders, and their goals and tasks. Long-term usage data after putting the tool online revealed great interest in the machine learning community.","Authors":"H. Strobelt; S. Gehrmann; H. Pfister; A. M. Rush","DOI":"10.1109\/TVCG.2017.2744158","Keywords":"Visualization;Machine Learning;Recurrent Neural Networks;LSTM","Title":"LSTMVis: A Tool for Visual Analysis of Hidden State Dynamics in Recurrent Neural Networks","Keywords_Processed":"visualization;lstm;Recurrent Neural network;Machine Learning","Keyword_Vector":[0.08542663,-0.0163999605,-0.0213020688,0.1221240322,0.0524622326,-0.0177432861,0.0190590973,-0.1057749957,-0.0192744635,0.0115260264,0.0740919718,-0.0602307668,0.0129654806,0.1202689467,-0.1093680153,0.0087358944,0.01482804,0.0177436282,0.0477640781,-0.0778751806,0.0306315509,-0.1183125424,-0.0931501995,0.033933687,0.0059634168,0.0529636806,0.075351279,-0.0094955143,0.0198031478,-0.1009747205,-0.1387612582,0.0379831648,0.1426115838,-0.0435488367,0.019202965,0.0217941796,0.0088470231,0.0668612688,0.0076300995,0.0714546275,-0.0424045127,-0.0413959636,0.0016630658,-0.0378649708,0.0498731958,0.1262956885,-0.0055849483,-0.0231334201,-0.090603109,0.0185402657,0.0767426915,-0.0184440443],"Abstract_Vector":[0.260713041,0.0786545337,-0.0044499858,0.173355108,-0.0151390584,-0.0994054909,0.0453692744,0.1175658043,-0.0507236389,-0.1170983136,-0.0870135849,-0.1095583899,-0.0066983918,0.2190367483,-0.0283343753,0.0890638056,0.0123496158,0.0161135118,0.059279853,0.020318705,-0.0890825411,0.0354532495,-0.0005940332,-0.1265188297,0.0224949817,0.0526829831,-0.0421570567,0.0485869752,0.0249721962,-0.0596483705,0.0736268119,-0.1052482405,-0.0439361451,0.0902143274,0.0064338166,0.0004569072,-0.034358896,0.0083511685,-0.00309101,0.0818992537,-0.0052668529,0.0291646772,-0.0603880568,0.0532211944,0.0277994626,0.0436865168,0.0217391624,-0.0577368818,-0.0259226537,-0.0204477333,0.0218570493,0.0559013729,0.0486894364,0.0363675342,-0.0394445445,0.0058642387,0.0515415719,-0.0564488146,0.0216360849,-0.0505523664,-0.0186117099,-0.0151517006,0.044059125,0.0315079668,-0.0442929847,-0.0938503598,-0.0191046074,-0.0315120248,-0.0416961353,-0.054694315,0.0154364094,0.0074933005,0.0227831631,-0.033061278,-0.0085568554,0.0343749946,0.0288071267,-0.0343353186,-0.0087127073,0.0069345664,-0.0146630001,0.0068013945,-0.0308823868,0.0506961832,-0.0364843769,0.0430190254,0.0384713499,0.0040432652,-0.0018867582,-0.0231265884,-0.0019724838,-0.0481838344,0.0292919955,-0.0220926815,-0.0015812469,-0.0221608215,-0.0317822035,0.0531887269,-0.0187546269,0.0226310884,-0.0133774277,0.0176816715,-0.0321705963,-0.0369404026,0.0254106753,0.0094544143,-0.0356687296,0.0733619843,0.0306403871,0.0513679093,-0.0023981372,0.0211219713,-0.0183850782,0.0168757404,-0.0193227309,0.0401871715]},"212":{"Abstract":"We propose a new semantic-level crowd evaluation metric in this paper. Crowd simulation has been an active and important area for several decades. However, only recently has there been an increased focus on evaluating the fidelity of the results with respect to real-world situations. The focus to date has been on analyzing the properties of low-level features such as pedestrian trajectories, or global features such as crowd densities. We propose the first approach based on finding semantic information represented by latent Path Patterns in both real and simulated data in order to analyze and compare them. Unsupervised clustering by non-parametric Bayesian inference is used to learn the patterns, which themselves provide a rich visualization of the crowd behavior. To this end, we present a new Stochastic Variational Dual Hierarchical Dirichlet Process (SV-DHDP) model. The fidelity of the patterns is computed with respect to a reference, thus allowing the outputs of different algorithms to be compared with each other and\/or with real data accordingly. Detailed evaluations and comparisons with existing metrics show that our method is a good alternative for comparing crowd data at a different level and also works with more types of data, holds fewer assumptions and is more robust to noise.","Authors":"H. Wang; J. Ond\u0159ej; C. O\u2019Sullivan","DOI":"10.1109\/TVCG.2016.2642963","Keywords":"Crowd simulation;crowd comparison;data-driven;clustering;hierarchical Dirichlet process;stochastic optimization","Title":"Trending Paths: A New Semantic-Level Metric for Comparing Simulated and Real Crowd Data","Keywords_Processed":"crowd simulation;clustering;datum drive;crowd comparison;hierarchical Dirichlet process;stochastic optimization","Keyword_Vector":[0.0836379828,-0.0248432351,-0.0342592178,0.0409364783,-0.027051168,0.0553141733,-0.023479468,-0.0331293848,-0.0058830851,-0.0429558711,0.0382635013,-0.0000887453,-0.0124748861,-0.047896861,0.0191280886,-0.017254717,-0.0760657607,-0.0315359141,0.0755721492,0.0272975649,0.073054544,0.0359485242,-0.0133402444,0.0431881964,0.0712348005,-0.0459437559,0.1123875157,0.0355302341,-0.0282270053,0.0585032372,-0.0670672952,-0.0114385968,0.0799643412,-0.0638994196,0.0053846575,0.1111369799,0.0302837264,-0.0792750747,-0.0147877624,0.0277684787,-0.09366582,0.074636502,-0.0542352268,0.0875664679,0.1351567334,0.1733691544,-0.0853720898,0.0054397649,-0.1655344264,-0.0140252188,-0.0950685495,-0.0807902985],"Abstract_Vector":[0.2561375363,-0.0647252758,-0.0103515898,-0.0562663177,0.0207775408,-0.162757716,0.0235547533,-0.0078371095,0.0239142455,-0.0723277231,0.0835502271,-0.054673388,-0.0160184419,0.0119300653,-0.0073567631,-0.0384112621,-0.0319011185,0.0897357618,-0.0536810831,-0.0080959522,0.0259237985,-0.070398218,-0.0041063863,0.013751063,0.0326401514,0.0117681175,0.029365303,-0.0053737914,0.0334100632,-0.0083186708,0.0064643971,0.0352904059,-0.0106795009,0.0237769365,-0.0415495349,-0.0002112355,-0.0036286315,0.079463651,-0.0683459112,0.0003662626,0.0221434872,0.0335546486,0.010005792,-0.0257071316,0.046904667,0.0521098029,-0.0438901924,-0.0154840954,-0.0202422072,-0.0214033311,0.0195631987,0.0281551253,0.0305153765,-0.0196368884,-0.040545238,-0.0715349762,0.0109611773,-0.0149340553,-0.0008788268,-0.0144028881,0.0230062657,0.063128932,0.0080106975,-0.0214705102,0.1183684635,0.0048692259,-0.0386600193,0.0279159971,0.0175960836,0.0107168358,0.0285747319,0.0252391583,0.0666985521,0.0464176036,0.0458699141,-0.0394929985,-0.0834202087,0.0610855464,-0.0370040329,-0.0624452388,0.0183279243,0.0095102462,0.0454604443,-0.0181249701,-0.0150810981,-0.0140446211,0.0010788423,0.0296804409,0.0170734372,0.0400636721,0.0483909423,0.002377399,0.0223810015,-0.0198606586,-0.019800186,-0.0070201879,0.0327372201,-0.0319092983,-0.0295842552,-0.0109915919,-0.0483603121,-0.008210286,0.0237824324,0.0349819648,-0.0143990881,0.0283324935,-0.029082987,-0.0460062908,0.0501272464,-0.0199083858,0.0395612943,0.0182872887,0.0004544748,-0.0492316234,0.0148079091,0.0069053921]},"213":{"Abstract":"In this paper, we present a novel pairwise-force smoothed particle hydrodynamics (PF-SPH) model to enable simulation of various interactions at interfaces in real time. Realistic capture of interactions at interfaces is a challenging problem for SPH-based simulations, especially for scenarios involving multiple interactions at different interfaces. Our PF-SPH model can readily handle multiple types of interactions simultaneously in a single simulation; its basis is to use a larger support radius than that used in standard SPH. We adopt a novel anisotropic filtering term to further improve the performance of interaction forces. The proposed model is stable; furthermore, it avoids the particle clustering problem which commonly occurs at the free surface. We show how our model can be used to capture various interactions. We also consider the close connection between droplets and bubbles, and show how to animate bubbles rising in liquid as well as bubbles in air. Our method is versatile, physically plausible and easy-to-implement. Examples are provided to demonstrate the capabilities and effectiveness of our approach.","Authors":"T. Yang; R. R. Martin; M. C. Lin; J. Chang; S. Hu","DOI":"10.1109\/TVCG.2017.2706289","Keywords":"Smoothed particle hydrodynamics (SPH);pairwise force;surface tension;bubble animation;fluid simulation","Title":"Pairwise Force SPH Model for Real-Time Multi-Interaction Applications","Keywords_Processed":"fluid simulation;pairwise force;smoothed particle hydrodynamic SPH;bubble animation;surface tension","Keyword_Vector":[0.0389451816,-0.0006306202,-0.0055227375,0.018996977,0.0189774171,0.0151720536,-0.0120690072,0.0013456208,0.0207769239,0.0042266888,0.0343013724,0.0177677118,0.0213240069,0.0677579394,0.0078483991,0.0088151096,0.0210440593,-0.0455524185,-0.0014320275,0.0644551766,0.0554093736,0.0586342276,-0.0415844079,-0.034612504,-0.0136308342,0.0339365174,0.0464447534,-0.0339580529,0.0168507909,-0.0376750046,0.0396753215,0.0226865347,-0.0048597037,-0.0415636997,0.0398439041,-0.0285853157,-0.0426963452,-0.0257848926,0.0637027581,-0.026619382,0.1242700591,-0.0465704043,0.0542926715,0.0108954863,0.031371397,0.024097605,-0.0524606246,0.1137449251,-0.0471185832,0.0184278219,0.0189755264,0.0132346478],"Abstract_Vector":[0.1133261472,-0.0012593144,0.023238297,-0.0103487368,-0.02857873,0.0070235599,0.011464047,-0.0085120654,0.0562096653,0.0441751559,-0.0240629692,0.0250635767,-0.0099795322,0.0355201418,-0.0276781701,-0.0160124445,-0.0007254327,-0.0065622005,0.0030600949,-0.0635867024,-0.0235171847,-0.0169115954,0.0628967998,-0.0092641669,-0.0340194064,0.0060931318,-0.0364565875,0.015068129,-0.0166501141,-0.0306091037,0.0611026113,-0.0074217796,-0.0396466203,0.0075142173,0.0437553426,-0.0160164525,-0.0053374073,-0.012950037,0.0368373098,0.0218027773,0.0012332968,0.0115124267,0.0641147361,0.0528581219,-0.0310257419,-0.0344397886,-0.0187176313,0.0697061855,-0.0086586713,0.0518463576,-0.0273332584,-0.0305622884,-0.0130451883,0.0118729167,0.0482754051,0.0425609331,0.0385648044,0.0450194251,-0.0029399629,-0.0033010091,0.0305119816,-0.0288540626,0.0431000471,-0.0469870348,0.0052697265,0.0207165483,0.06344322,-0.0046855458,-0.0240275576,0.0814642969,-0.0300780729,0.030948651,0.0901719785,-0.0176900733,0.0559235547,-0.0219077814,0.0320434411,0.0030344066,0.0396760574,-0.0406135655,-0.0282772345,0.059833545,0.0409191736,-0.0371344257,-0.1331520008,0.0271873149,-0.0513969307,0.0133144641,0.088927649,-0.0260959417,0.0531484419,0.0340076707,0.0447844895,-0.1221572309,0.0240628329,0.0465622907,-0.0184444239,-0.0028049945,-0.0526573272,-0.0719796808,-0.0194945604,-0.0317161472,-0.0396450034,-0.028504893,-0.050482981,-0.0352000183,-0.0163118996,0.0232213339,-0.0151283834,-0.0238714879,-0.023846708,0.0305235818,0.0307401677,-0.0192903084,0.0726573734,-0.0085367767]},"214":{"Abstract":"We introduce an interactive user-driven method to reconstruct high-relief 3D geometry from a single photo. Particularly, we consider two novel but challenging reconstruction issues: i) common non-rigid objects whose shapes are organic rather than polyhedral\/symmetric, and ii) double-sided structures, where front and back sides of some curvy object parts are revealed simultaneously on image. To address these issues, we develop a three-stage computational pipeline. First, we construct a 2.5D model from the input image by user-driven segmentation, automatic layering, and region completion, handling three common types of occlusion. Second, users can interactively mark-up slope and curvature cues on the image to guide our constrained optimization model to inflate and lift up the image layers. We provide real-time preview of the inflated geometry to allow interactive editing. Third, we stitch and optimize the inflated layers to produce a high-relief 3D model. Compared to previous work, we can generate high-relief geometry with large viewing angles, handle complex organic objects with multiple occluded regions and varying shape profiles, and reconstruct objects with double-sided structures. Lastly, we demonstrate the applicability of our method on a wide variety of input images with human, animals, flowers, etc.","Authors":"C. Yeh; S. Huang; P. K. Jayaraman; C. Fu; T. Lee","DOI":"10.1109\/TVCG.2016.2574705","Keywords":"Reconstruction;high-relief;lenticular posters;single image;folded;double-sided;object modeling;depth cues;completion;inflation","Title":"Interactive High-Relief Reconstruction for Organic and Double-Sided Objects from a Photo","Keywords_Processed":"inflation;lenticular poster;depth cue;object modeling;reconstruction;completion;single image;double sided;high relief;fold","Keyword_Vector":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"Abstract_Vector":[0.1574128755,-0.0798350893,-0.0096651695,0.023950998,-0.0840147905,0.0600688028,-0.0020875575,0.0191475712,-0.0538982233,-0.0239938294,-0.0831885185,0.0657392501,0.0519628711,-0.0191056723,0.0656117642,0.0518858156,-0.0726079067,0.0210151366,-0.0774722305,-0.0311933898,-0.0022864958,-0.0623841266,0.0021665402,-0.0630630168,-0.0371216954,0.0216073668,-0.0173942528,-0.0053297496,-0.0717671019,-0.0392085061,-0.0402492583,0.0100792628,0.0318294205,0.0554299874,0.0403438717,-0.007169939,-0.0291545736,0.0155084314,-0.0524422132,0.0710847771,0.0248670496,0.0024139128,0.0453002737,-0.152028246,0.0173560934,-0.0087402881,-0.0214797987,-0.0032069949,-0.0320895924,0.0593567965,0.006328865,0.0453723783,0.0764330051,0.0090127163,0.0583256769,0.0243288289,-0.0754913894,-0.057988612,-0.0372992678,-0.0399032502,-0.0120499722,-0.0425123089,0.0184807489,-0.0619988517,-0.070603328,0.0285146478,-0.0097865411,-0.0253500404,-0.034074729,0.0166446427,0.0041313753,-0.0909257154,0.001242852,0.0280871424,0.0523844964,0.0270778039,-0.0271727278,0.0398550824,0.0275538811,0.0191559351,0.0376181011,-0.0071754401,-0.0104587783,-0.0354121491,0.0162471335,0.047316842,0.011000701,0.0268115349,-0.0289148189,-0.0148889552,-0.021653489,0.0719458602,-0.0303499775,0.0345428809,0.020951023,0.0351971545,0.0477925393,-0.0059336897,-0.0464503378,0.0222636461,0.0468411188,0.0273765785,-0.0061629917,0.0126194919,-0.0283706445,-0.0401921247,-0.0088321545,-0.0328497431,0.0264016051,-0.0077347172,-0.0271152727,-0.0162162288,-0.0184169419,-0.0157427647,0.0112321264,0.0115696049]},"215":{"Abstract":"Data grouping is among the most frequently used operations in data visualization. It is the process through which relevant information is gathered, simplified, and expressed in summary form. Many popular visualization tools support automatic grouping of data (e.g., dividing up a numerical variable into bins). Although grouping plays a pivotal role in supporting data exploration, further adjustment and customization of auto-generated grouping criteria is non-trivial. Such adjustments are currently performed either programmatically or through menus and dialogues which require specific parameter adjustments over several steps. In response, we introduce Embedded Merge & Split (EMS), a new interaction technique for direct adjustment of data grouping criteria. We demonstrate how the EMS technique can be designed to directly manipulate width and position in bar charts and histograms, as a means for adjustment of data grouping criteria. We also offer a set of design guidelines for supporting EMS. Finally, we present the results of two user studies, providing initial evidence that EMS can significantly reduce interaction time compared to WIMP-based technique and was subjectively preferred by participants.","Authors":"A. Sarvghad; B. Saket; A. Endert; N. Weibel","DOI":"10.1109\/TVCG.2018.2865075","Keywords":"Data Visualization;Direct Manipulation;Embedded Merge & Split;Data Grouping;Embedded Interaction","Title":"Embedded Merge & Split: Visual Adjustment of Data Grouping","Keywords_Processed":"embed interaction;embed Merge Split;datum grouping;direct manipulation;Data visualization","Keyword_Vector":[0.1447582765,-0.0371033013,0.0300067699,0.0124441794,-0.0275075842,0.0621789192,-0.0712996731,-0.0634097232,-0.0739236302,-0.1074384507,0.0169595997,0.0474860867,-0.098195187,0.0643278861,-0.1713570668,0.0237155318,-0.1980495341,0.0465023686,-0.0906667874,0.1204796505,0.0071605069,-0.039997552,0.038566764,0.1703394589,0.1973439558,-0.1797178245,0.1111925808,-0.1978927297,0.2349638367,0.1701219297,0.1158214868,-0.1516569266,0.0537755073,-0.0608542647,-0.0768634862,-0.1159906427,-0.1198395105,0.0126398316,-0.0431671059,0.1029104833,0.1231189314,0.1246889441,-0.032978524,-0.0670946304,-0.0095931474,-0.02503714,-0.0262348759,-0.0284905655,0.0981978375,-0.0046454501,0.0185956568,-0.0242543891],"Abstract_Vector":[0.2174587459,-0.1147663394,0.074865898,0.027001714,0.083494552,0.0649696436,0.0493433206,-0.0782466817,-0.0636621858,-0.1507658799,0.216133513,0.2152425843,-0.094256926,0.018166404,-0.0189400373,-0.0401313862,0.1088711216,-0.0475113951,-0.0057896172,0.0797566664,-0.0245714011,-0.0158305594,0.0006805001,-0.0404155033,-0.0285743765,-0.0137526099,-0.0667788102,0.0626206224,-0.0150441174,0.037745214,-0.0534493542,0.0017242904,-0.0486841081,0.0188704558,0.0165494251,-0.0107168576,-0.0272832983,0.0503239471,-0.0253329145,0.027856483,0.0061912139,-0.0022949519,0.0029116998,0.0364485504,0.0098845371,-0.0141330194,0.0201454416,0.0054285713,0.0441197089,0.0064242155,0.0116221748,0.0408418168,0.0336500469,0.0196740469,-0.0216792375,-0.0622518205,-0.0253642914,0.0362106195,-0.014466759,0.0023738508,-0.0054438151,-0.0267154371,-0.0438479995,0.0150009954,-0.0259458515,0.0055882134,0.0245205378,-0.0323435484,-0.003765128,-0.0040377712,0.0253718097,-0.0147406035,0.0027428567,-0.0323796188,-0.019630533,-0.0309923863,-0.0338158124,0.018992015,0.0124900034,0.0152165729,-0.0018065084,0.0074918421,0.0120247997,0.009221944,-0.0030731898,0.0085026736,-0.0125180989,-0.000781967,-0.0259097628,-0.0526628349,-0.0051951718,-0.0028950833,0.0004439923,0.0611121416,-0.0362345467,0.0193829155,-0.0238850391,-0.0186702238,-0.0029017621,-0.0150450899,-0.0317291207,0.0084565158,0.0240592099,0.0248979615,-0.0522392232,0.0018654691,0.0091209882,0.0083529527,-0.0138224355,-0.0258189433,-0.0404863888,0.0054809604,0.0217864211,0.0351091288,-0.0402076952,-0.0060596945]},"216":{"Abstract":"We describe an experiment that explores the contribution of auditory and other features to the illusion of plausibility in a virtual environment that depicts the performance of a string quartet. `Plausibility' refers to the component of presence that is the illusion that the perceived events in the virtual environment are really happening. The features studied were: Gaze (the musicians ignored the participant, the musicians sometimes looked towards and followed the participant's movements), Sound Spatialization (Mono, Stereo, Spatial), Auralization (no sound reflections, reflections corresponding to a room larger than the one perceived, reflections that exactly matched the virtual room), and Environment (no sound from outside of the room, birdsong and wind corresponding to the outside scene). We adopted the methodology based on color matching theory, where 20 participants were first able to assess their feeling of plausibility in the environment with each of the four features at their highest setting. Then five times participants started from a low setting on all features and were able to make transitions from one system configuration to another until they matched their original feeling of plausibility. From these transitions a Markov transition matrix was constructed, and also probabilities of a match conditional on feature configuration. The results show that Environment and Gaze were individually the most important factors influencing the level of plausibility. The highest probability transitions were to improve Environment and Gaze, and then Auralization and Spatialization. We present this work as both a contribution to the methodology of assessing presence without questionnaires, and showing how various aspects of a musical performance can influence plausibility.","Authors":"I. Bergstr\u00f6m; S. Azevedo; P. Papiotis; N. Saldanha; M. Slater","DOI":"10.1109\/TVCG.2017.2657138","Keywords":"Presence;plausibility;place illusion;user studies;experimental methods;multimodal interaction;entertainment","Title":"The Plausibility of a String Quartet Performance in Virtual Reality","Keywords_Processed":"user study;entertainment;presence;place illusion;plausibility;multimodal interaction;experimental method","Keyword_Vector":[0.0307911301,0.0002111312,-0.0211511022,0.0131247577,0.0153064923,-0.0360329966,-0.0112592672,-0.0279720507,-0.0337901147,0.0071385451,-0.0065977079,0.0289126981,-0.0173814004,0.0452995875,-0.0308028115,-0.0190269581,-0.0836655035,-0.0301408411,-0.0106891099,0.0647182087,-0.0072421903,-0.000473414,-0.0672989045,0.0899378036,0.0170375519,-0.1325629759,-0.0721256684,0.0010222094,-0.0276901802,-0.0738897426,0.0328866101,-0.0708745225,-0.0605975768,0.0123742924,0.005712269,0.0305431704,-0.039463037,0.004393249,0.0419064831,0.0456438557,-0.0229979249,-0.0178791164,-0.0041962105,0.0034323767,-0.0175621863,-0.0008258684,-0.0286988543,0.0070868591,0.0134795804,0.0108707989,0.0106266139,-0.0414920076],"Abstract_Vector":[0.1771116856,-0.0648123424,0.0041931377,0.0293944832,-0.0463445705,-0.026841439,0.0416694631,0.0005167516,-0.0372357002,0.0377210566,0.017606099,-0.0047187248,-0.0823687532,0.0119899312,0.0192496639,0.0019977277,0.037916748,0.0450591635,-0.0358526077,-0.0776184069,0.030823761,0.005430471,-0.0134535578,0.0198328647,-0.0232544415,0.0121759615,0.0445430075,-0.0313255341,0.0021056527,-0.0036285125,-0.0205363747,0.0165469729,-0.0175015098,-0.0023919691,-0.0100394809,-0.0393435761,0.0020195484,-0.0371601969,0.0178985525,-0.0054638984,0.0004752175,0.0122178368,-0.045380212,0.023994083,0.0086593015,0.0225838008,0.0161617067,0.0107128864,0.0338332876,0.0131157798,0.0111369487,-0.0048228287,0.0062946411,0.0272737637,0.0067372426,-0.014488887,0.0690229374,-0.0125118713,0.016059403,-0.0154537381,-0.0000578132,-0.0020014417,-0.0183668167,0.0189159671,-0.0122494938,0.0083427048,0.0218421247,-0.0079288621,0.0180485515,0.0269238823,-0.0433702764,-0.0215142075,-0.0337941831,-0.0131957187,-0.0286217774,0.0170142894,-0.004513363,0.0113034404,0.0028874622,-0.0189486263,-0.0086247404,0.0184371496,-0.0004172266,-0.0007623304,-0.0212655911,-0.0023493669,-0.0029604584,0.0474277365,-0.0068886999,-0.0141729078,-0.0142248404,-0.038043388,0.0125651859,0.0090430397,-0.0059033965,0.0012892111,-0.0064138389,-0.0065412226,0.0065426512,0.0088634156,0.0069539537,0.0000772232,-0.0000556947,0.0126925298,0.0119885986,-0.0423156068,-0.0086375659,0.0102255401,-0.0048382328,0.0255875929,0.0060168015,-0.0055711983,0.023615908,-0.0142165667,0.0370261103,0.0109522812]},"217":{"Abstract":"Decades of research have repeatedly shown that people perform poorly at estimating and understanding conditional probabilities that are inherent in Bayesian reasoning problems. Yet in the medical domain, both physicians and patients make daily, life-critical judgments based on conditional probability. Although there have been a number of attempts to develop more effective ways to facilitate Bayesian reasoning, reports of these findings tend to be inconsistent and sometimes even contradictory. For instance, the reported accuracies for individuals being able to correctly estimate conditional probability range from 6% to 62%. In this work, we show that problem representation can significantly affect accuracies. By controlling the amount of information presented to the user, we demonstrate how text and visualization designs can increase overall accuracies to as high as 77%. Additionally, we found that for users with high spatial ability, our designs can further improve their accuracies to as high as 100%. By and large, our findings provide explanations for the inconsistent reports on accuracy in Bayesian reasoning tasks and show a significant improvement over existing methods. We believe that these findings can have immediate impact on risk communication in health-related fields.","Authors":"A. Ottley; E. M. Peck; L. T. Harrison; D. Afergan; C. Ziemkiewicz; H. A. Taylor; P. K. J. Han; R. Chang","DOI":"10.1109\/TVCG.2015.2467758","Keywords":"Bayesian Reasoning;Visualization;Spatial Ability;Individual Differences;Bayesian Reasoning;Visualization;Spatial Ability;Individual Differences","Title":"Improving Bayesian Reasoning: The Effects of Phrasing, Visualization, and Spatial Ability","Keywords_Processed":"spatial ability;visualization;individual difference;Bayesian reasoning","Keyword_Vector":[0.231243394,-0.129122713,0.0038395021,0.2673383113,-0.1194502013,0.3748764588,-0.0387668112,-0.1390614302,-0.0992829689,0.0109415514,-0.0016736156,-0.0083112434,0.0487644518,-0.0297634175,-0.0782885417,-0.1028699741,0.0492723136,-0.0438914712,0.0499386309,-0.0873043974,-0.0349121339,-0.0683145606,0.060773339,0.0054294286,-0.0396794323,-0.1488303342,-0.088056305,0.0926770506,-0.0327275537,0.0413309443,0.0337942564,0.0228444614,0.0450003224,-0.0190103692,-0.0016188265,-0.0698195228,0.0143530807,-0.0432891301,-0.0709130584,-0.0026261275,0.0883215527,-0.0289266103,0.0082283672,-0.0531033627,0.0824943374,0.0242162998,0.0086259229,-0.0211862872,0.0593927429,0.0135555588,-0.0027339809,-0.0400469493],"Abstract_Vector":[0.2750128973,0.0239640377,0.0166136297,0.0625428868,0.0071344863,-0.1073023672,0.012387713,0.0431759152,-0.095174319,0.0473379123,-0.0369364167,-0.0197782915,0.0606929377,-0.0528053731,0.023664089,0.0120792785,0.0116093789,0.046922686,-0.0056707627,0.0789117456,-0.0428588668,0.0288275818,0.0590863054,-0.0197146109,-0.0572429279,0.0538446798,-0.0811168998,-0.0135050904,-0.043137225,0.031045305,-0.0073781057,-0.0388599146,-0.0128132582,0.0492299037,-0.032419048,-0.00650983,-0.0348898142,-0.0061844266,0.0201239065,-0.01808635,0.0046019582,0.0045467628,0.0227298346,0.0361747682,-0.0466024682,0.0055690736,0.0441619169,0.0048005678,0.0127794135,-0.0254939804,0.0168890476,-0.0371884928,-0.0582443082,0.0064002932,0.0033164384,-0.0088665227,0.0147895413,-0.0186311824,-0.0035559443,-0.0679060146,-0.0599920346,-0.0232613447,0.0441286656,-0.0073049219,0.0234490538,0.0300647727,-0.0995775639,0.0281014866,-0.0328946311,0.0509391196,0.0226354052,0.0232770815,-0.0177693808,0.0581119463,-0.0429335829,0.0395089128,-0.0180758079,-0.0023965629,0.0576399818,0.0727961523,-0.043318835,-0.0431040674,0.0082547745,-0.0122629601,-0.0214642057,0.0193556458,0.0137265989,0.0291130527,0.020017966,0.0266456322,0.0179193622,0.0583837488,0.0431287827,-0.0321968214,-0.0080454314,0.0236702061,-0.0329327181,-0.028789829,0.0041703809,0.0046635577,-0.0382855577,-0.0287008273,-0.0251616055,0.0204032297,-0.0047101825,-0.0090230909,0.0163779597,-0.0142223853,0.0101822013,-0.0438403059,0.0264895252,0.014190221,0.0380721822,-0.021285315,-0.0072563835,0.0242784543]},"218":{"Abstract":"The Parallel Coordinates plot is a popular tool for the visualization of high-dimensional data. One of the main challenges when using parallel coordinates is occlusion and overplotting resulting from large data sets. Brushing is a popular approach to address these challenges. Since its conception, limited improvements have been made to brushing both in the form of visual design and functional interaction. We present a set of novel, smart brushing techniques that enhance the standard interactive brushing of a parallel coordinates plot. We introduce two new interaction concepts: Higher-order, sketch-based brushing, and smart, data-driven brushing. Higher-order brushes support interactive, flexible, n-dimensional pattern searches involving an arbitrary number of dimensions. Smart, data-driven brushing provides interactive, real-time guidance to the user during the brushing process based on derived meta-data. In addition, we implement a selection of novel enhancements and user options that complement the two techniques as well as enhance the exploration and analytical ability of the user. We demonstrate the utility and evaluate the results using a case study with a large, high-dimensional, real-world telecommunication data set and we report domain expert feedback from the data suppliers.","Authors":"R. C. Roberts; R. S. Laramee; G. A. Smith; P. Brookes; T. D'Cruze","DOI":"10.1109\/TVCG.2018.2808969","Keywords":"Multivariate visualization;parallel coordinates;call center;glyph;brushing;interaction techniques","Title":"Smart Brushing for Parallel Coordinates","Keywords_Processed":"call center;glyph;brush;parallel coordinate;multivariate visualization;interaction technique","Keyword_Vector":[0.2000038442,-0.0654760514,0.1002479109,-0.0992506289,-0.0093219434,0.0176621049,0.0159681766,0.0119965037,-0.0150719997,-0.1174246184,0.0251788831,-0.0231806892,-0.0593232808,0.0093812501,-0.146502256,0.0537364006,-0.0916165814,0.0675409512,-0.0389627685,0.0851912031,-0.0550849645,-0.0133765851,0.0402658419,0.1542277918,0.1092214312,0.02706142,0.1400236322,-0.0668883993,0.2668229764,0.1100740082,-0.0002939568,0.0552422151,-0.0808493369,0.0315916087,-0.060819888,-0.1060553131,-0.0361755952,-0.0157238884,0.0946022816,0.0150726435,-0.0151948611,0.0616614851,-0.0333175222,0.0198451273,-0.0289380611,-0.0688107227,-0.0401987411,-0.0101469901,0.0391445334,-0.0002795775,-0.0016652425,-0.0449701283],"Abstract_Vector":[0.1890892354,-0.0858128909,0.0247827851,0.0085135854,0.0261511587,-0.0483294756,-0.021645678,-0.0253103521,-0.0156264262,-0.037231978,0.0317651025,0.0256446801,-0.1218441295,-0.0163084335,-0.0110814225,-0.0070572604,0.0464891269,-0.0167695826,-0.0273654133,0.0091033398,-0.025755383,-0.0366688072,-0.0215138393,0.055107108,0.0211287049,0.0941526925,-0.0701295627,0.019827492,0.0789711003,0.0036795014,0.0741681586,0.0581337946,0.0068069192,-0.1087950193,0.0202694267,-0.0219377158,-0.0189928721,-0.0134499403,-0.0710828213,0.0106965708,-0.0325227057,0.0175031106,-0.0187948857,-0.0488845279,-0.0060913779,-0.0227554898,-0.0690496982,0.0198806519,-0.0000078157,-0.0416680883,-0.0525524093,0.1010618683,-0.0058069945,-0.011847231,-0.0846739644,0.0698943754,0.0438580997,0.0376102382,-0.0047667826,0.0366102015,0.0436848026,-0.0499791788,-0.0343224869,-0.0702893836,0.0508810965,0.0103947586,-0.0391743004,-0.0285364553,0.0316027487,0.0323643064,0.0472226141,-0.0330866553,-0.0549581055,-0.028555857,0.0789817378,0.0620858954,0.02417369,0.0151296622,-0.0666097365,-0.053203147,0.087159516,0.0223520497,0.0519683032,0.0243558274,0.0201423766,0.0489869352,0.0433058859,0.0148752655,0.008595438,0.0528494565,0.0056566443,-0.0147571895,0.0889802438,-0.0257653834,0.1155320197,0.0523685347,-0.0248171297,0.1093625781,-0.0047943572,0.0810069629,-0.0139401373,-0.0773556728,-0.0704887447,-0.0142915613,-0.0033828854,0.0087665494,-0.0374556338,0.027796135,-0.0084914716,-0.0485668936,-0.0420139723,0.0230844331,0.0102705381,-0.0075937555,0.0068648137,0.0743314718]},"219":{"Abstract":"We present the results of a two-year design study to developing virtual reality (VR) flow visualization tools for the analysis of dinosaur track creation in a malleable substrate. Using Scientific Sketching methodology, we combined input from illustration artists, visualization experts, and domain scientists to create novel visualization methods. By iteratively improving visualization concepts at multiple levels of abstraction we helped domain scientists to gain insights into the relationship between dinosaur foot movements and substrate deformations. We involved over 20 art and computer science students from a VR design course in a rapid visualization sketching cycle, guided by our paleontologist collaborators through multiple critique sessions. This allowed us to explore a wide range of potential visualization methods and select the most promising methods for actual implementation. Our resulting visualization methods provide paleontologists with effective tools to analyze their data through particle, pathline and time surface visualizations. We also introduce a set of visual metaphors to compare foot motion in relation to substrate deformation by using pathsurfaces. This is one of the first large-scale projects using Scientific Sketching as a development methodology. We discuss how the research questions of our collaborators have evolved during the sketching and prototyping phases. Finally, we provide lessons learned and usage considerations for Scientific Sketching based on the experiences gathered during this project.","Authors":"J. Novotny; J. Tveite; M. L. Turner; S. Gatesy; F. Drury; P. Falkingham; D. H. Laidlaw","DOI":"10.1109\/TVCG.2019.2898796","Keywords":"Virtual Reality;Scientific Visualization;Flow Visualization;Design Study","Title":"Developing Virtual Reality Visualizations for Unsteady Flow Analysis of Dinosaur Track Formation using Scientific Sketching","Keywords_Processed":"flow visualization;scientific visualization;virtual reality;Design Study","Keyword_Vector":[0.1333667766,-0.0839247105,-0.0018814194,-0.0864915937,0.0330570546,-0.0218493381,-0.0282339989,-0.0423001501,-0.0657883527,0.0479145837,-0.0616357821,0.0548650764,0.0158366552,0.0707560375,-0.1120290245,0.0205760637,-0.0318184895,0.0082136074,-0.0529454968,0.0019682446,0.0869741353,0.0587854379,0.013456405,-0.1438865766,0.0009930488,0.0583753225,0.0510878364,0.0303906351,-0.0247194393,-0.1246650776,-0.0207901711,-0.0182173043,-0.0061367804,0.0545076301,0.1291876886,-0.1017511494,0.0359782072,-0.0876246676,-0.0579680643,0.0362567803,0.0365756243,-0.0048891782,-0.0365864361,0.0230292695,0.0740788412,-0.0379991143,-0.0253002071,0.0017084206,0.0842350958,0.0335070311,0.0100389295,-0.0082329455],"Abstract_Vector":[0.1782982553,-0.0717288081,-0.0142984579,0.0708459558,0.0690716733,0.037824722,-0.0217593919,-0.0167413832,-0.0088287712,0.0362362785,-0.0229253535,0.0475812452,0.0406453434,0.0029824342,0.0006866595,0.0591753588,0.026329641,0.0642829906,0.0427330176,0.0296256504,0.0933161101,0.0577712333,0.055424326,-0.0813172982,0.080781027,0.0953161546,-0.0553984708,-0.0418031774,-0.0227780063,-0.0599338939,-0.0224652558,-0.0156902471,-0.071144649,0.0847913429,-0.0550330176,-0.0625875331,-0.0240778539,-0.0581802697,0.0562050304,0.054678248,-0.0134482376,-0.0033603073,0.0577123366,-0.0199847155,0.0179711433,0.0732216186,-0.0138026705,-0.0487011044,-0.0452119425,-0.0599855117,0.0009467984,0.0244918574,-0.0085475522,0.0098507582,-0.0056169765,0.0141074451,0.0010629348,0.0543402292,-0.0134567624,-0.0175013815,-0.0244046511,-0.0641771661,0.0035758855,0.026179666,0.0166200567,-0.0015362664,-0.0304963884,-0.0065238984,0.0025582421,0.0402518098,-0.0233241051,0.0286755431,-0.0424327708,-0.0204095858,0.0648425867,-0.0234802604,-0.0426891039,0.0307485591,-0.0288713866,-0.0262592339,0.0314162242,0.0148794719,-0.0083224691,-0.0215706939,0.015732117,-0.0099892894,0.0031632731,0.0030536141,0.0438994586,-0.0205506571,0.0259014638,-0.0335927126,-0.026441814,-0.013576936,-0.0013834305,0.0152013658,-0.009721403,-0.0432202131,-0.0061896496,-0.0178916713,0.042202389,-0.003657202,0.0259896117,-0.0169767901,0.0028027523,-0.0229111368,-0.0162947052,-0.0194593099,-0.0100814725,0.0126252595,-0.0043688419,-0.0073313411,0.0098258059,-0.0216747654,-0.0134255366,0.0058972749]},"22":{"Abstract":"We present Charticulator, an interactive authoring tool that enables the creation of bespoke and reusable chart layouts. Charticulator is our response to most existing chart construction interfaces that require authors to choose from predefined chart layouts, thereby precluding the construction of novel charts. In contrast, Charticulator transforms a chart specification into mathematical layout constraints and automatically computes a set of layout attributes using a constraint-solving algorithm to realize the chart. It allows for the articulation of compound marks or glyphs as well as links between these glyphs, all without requiring any coding or knowledge of constraint satisfaction. Furthermore, thanks to the constraint-based layout approach, Charticulator can export chart designs into reusable templates that can be imported into other visualization tools. In addition to describing Charticulator's conceptual framework and design, we present three forms of evaluation: a gallery to illustrate its expressiveness, a user study to verify its usability, and a click-count comparison between Charticulator and three existing tools. Finally, we discuss the limitations and potentials of Charticulator as well as directions for future research. Charticulator is available with its source code at https:\/\/charticulator.com.","Authors":"D. Ren; B. Lee; M. Brehmer","DOI":"10.1109\/TVCG.2018.2865158","Keywords":"Interactive visualization authoring;Chart layout design;Glyph design;Constraint-based design;Reusable chart layout","Title":"Charticulator: Interactive Construction of Bespoke Chart Layouts","Keywords_Processed":"reusable chart layout;Glyph design;interactive visualization author;constraint base design;chart layout design","Keyword_Vector":[0.1225258584,-0.0551503188,0.0574719985,-0.0289839158,-0.0105661584,0.0059452113,0.0561381287,0.0399302709,-0.0258792256,-0.045120533,0.0420428673,-0.0325223961,-0.0149629859,0.0074848567,-0.0668249769,0.0098518431,0.0130215044,0.0080041114,-0.0085747898,0.0316035349,-0.0391024305,-0.0020849478,0.0120740696,0.1275396402,-0.0099978001,0.0441172042,0.053012307,0.0410207323,0.0882624705,0.0245048899,-0.0596942991,0.1450975654,-0.1058475497,0.080705555,-0.0200416677,-0.077993238,0.0306774424,-0.0685140665,0.1362672133,-0.0021391325,-0.0798737684,0.0046863477,-0.035508617,0.0319088895,0.0013306976,-0.0582736925,-0.0432252629,0.0056782425,0.0259572954,0.016369068,0.0178208106,-0.0464842593],"Abstract_Vector":[0.2267449498,-0.0126979492,0.0858584076,-0.0442033177,-0.0055691087,-0.0218632657,-0.0084435864,-0.0369392174,-0.0335394745,0.0060922784,-0.0600239942,0.0086319335,-0.079261312,0.0441862646,0.0076416357,0.0207245587,0.1018651326,0.0618054588,0.0111376899,-0.048815001,-0.0089645146,-0.045933391,-0.0056541915,0.0063502365,-0.0292498572,0.0439961927,-0.0131628873,0.0392389965,0.0226805991,-0.0360406059,0.0076626297,0.0284824492,-0.0229381596,0.0057327586,-0.0478101566,-0.034431866,0.0236369396,0.0364590414,-0.0092311247,-0.0126230713,-0.0243257939,-0.0457807818,0.0369757228,-0.0395003596,-0.0037438903,-0.0131646856,0.0134230949,-0.0275354632,0.0275519978,-0.0501009007,-0.0248536603,0.0045420205,-0.062145573,0.0089997682,-0.0345904381,0.0059742023,0.0426177798,0.0166394319,-0.0197383322,0.0158768241,0.0392923254,0.0289651396,-0.0052214598,-0.0331622384,0.0154415721,0.0414660718,-0.0200968715,0.0166801258,-0.0239793195,-0.0207568089,0.0096630209,0.0338084207,-0.0057095782,-0.0289502461,-0.0033822517,0.0049878441,-0.0001385173,0.0260048209,-0.0512799036,-0.0162251464,0.0111594773,-0.0008002184,0.0244522647,-0.0116332373,0.0303357992,0.0294287505,0.008653886,-0.0174102107,-0.0269143739,0.0041228467,0.0215817861,-0.0294399891,0.0244077876,0.0002790457,0.0610368944,0.0195400173,0.0062889204,-0.0118259095,-0.0075384922,0.0127303644,0.0305954429,-0.0226029712,0.000505394,-0.0118969288,0.0049590084,0.0194922197,0.0037047041,0.0117840787,-0.0254335387,-0.0117870554,-0.0337371824,-0.0322826737,0.0086885461,-0.0046897692,0.012046511,0.0180026295]},"220":{"Abstract":"Finding good projections of n-dimensional datasets into a 2D visualization domain is one of the most important problems in Information Visualization. Users are interested in getting maximal insight into the data by exploring a minimal number of projections. However, if the number is too small or improper projections are used, then important data patterns might be overlooked. We propose a data-driven approach to find minimal sets of projections that uniquely show certain data patterns. For this we introduce a dissimilarity measure of data projections that discards affine transformations of projections and prevents repetitions of the same data patterns. Based on this, we provide complete data tours of at most n\/2 projections. Furthermore, we propose optimal paths of projection matrices for an interactive data exploration. We illustrate our technique with a set of state-of-the-art real high-dimensional benchmark datasets.","Authors":"D. J. Lehmann; H. Theisel","DOI":"10.1109\/TVCG.2015.2467132","Keywords":"Multivariate Projections;Star Coordinates;Radial Visualization;High-dimensional Data;Multivariate Projections;Star Coordinates;Radial Visualization;High-dimensional Data","Title":"Optimal Sets of Projections of High-Dimensional Data","Keywords_Processed":"Multivariate projection;high dimensional datum;Radial visualization;Star coordinate","Keyword_Vector":[0.073657925,-0.0475603189,-0.0407700052,-0.0591677452,0.0271636505,-0.0435394916,0.0033546639,-0.0245836238,-0.0764400972,0.1011274597,-0.0830081008,0.1621190392,0.0110090888,0.011229085,-0.040959316,0.009714925,0.0305611896,-0.0521480472,-0.0377949311,0.0333100983,0.0588759584,-0.032902605,-0.0625567788,0.0249168357,-0.0304128549,0.040567084,0.0149894553,-0.0203859338,-0.0038098628,-0.0197470942,-0.0470748377,-0.0774487986,-0.0668381861,0.0080282855,-0.0065106031,-0.0486523629,0.0956303241,-0.0678642715,-0.0340934375,-0.0149486139,0.1071165581,0.01267324,0.0620644637,-0.0529047407,-0.044369487,0.0466180376,0.0251517159,-0.0179259003,-0.0096637747,-0.030212378,-0.0168541046,0.0695150112],"Abstract_Vector":[0.1452158645,-0.0628632306,-0.0009979822,0.0564208615,0.0058665187,0.0558958829,-0.0124288144,-0.0065168142,0.0125240232,-0.0078560958,-0.0367459612,0.0787190241,0.0103637442,-0.0294083345,0.0056631116,0.0513593259,-0.0615505319,-0.0225643524,-0.0438337741,-0.0315116284,-0.0239076096,0.0174701339,0.0090520885,0.008740067,-0.0154859745,-0.0220565091,0.000516967,0.0149534394,0.0094484805,-0.0169508219,-0.0243737406,0.0050708316,-0.0066420327,0.0025472438,0.0083512641,0.0163312425,0.0080178025,0.0175037267,-0.0101242197,0.0263735179,0.0176413404,-0.0002230346,0.0311305269,0.0140702727,-0.0097627051,-0.0088831573,0.0577726765,-0.0148762816,-0.073902677,-0.0071479276,-0.0057898703,0.0342409008,0.001529054,0.0182788364,-0.0449691393,0.0057197477,0.0444959639,-0.0071110487,0.0098941599,0.0012658499,0.0207994129,-0.0710376558,0.0280758554,0.0126950798,-0.0154508371,0.0076983017,0.0004887119,0.0525510803,0.0403437679,0.0230444909,-0.0265133643,0.0325966848,-0.0203695772,0.0031509526,0.0072422662,-0.0178642222,0.0136711736,-0.0123888901,0.0419563039,-0.0377122405,0.0032518106,-0.0015557057,-0.0284721915,0.0490049151,0.0519615503,-0.0208859936,0.0267829525,0.0204595178,-0.0183829647,0.015048434,-0.0455337952,-0.0019176435,-0.044003819,0.0041643049,0.0627551275,-0.0058013645,-0.0077249124,-0.0307256853,-0.0059223411,0.0134071314,-0.0236603741,0.0561943177,0.0273770701,0.0093273204,-0.0294646464,-0.0167340734,-0.0336326143,-0.0472188298,-0.022650582,0.0037898123,0.0020426088,0.0051969926,-0.0032400625,0.0087568012,-0.0329573141,-0.013775024]},"221":{"Abstract":"We propose an occlusion compensation method for optical see-through head-mounted displays (OST-HMDs) equipped with a singlelayer transmissive spatial light modulator (SLM), in particular, a liquid crystal display (LCD). Occlusion is an important depth cue for 3D perception, yet realizing it on OST-HMDs is particularly difficult due to the displays' semitransparent nature. A key component for the occlusion support is the SLM-a device that can selectively interfere with light rays passing through it. For example, an LCD is a transmissive SLM that can block or pass incoming light rays by turning pixels black or transparent. A straightforward solution places an LCD in front of an OST-HMD and drives the LCD to block light rays that could pass through rendered virtual objects at the viewpoint. This simple approach is, however, defective due to the depth mismatch between the LCD panel and the virtual objects, leading to blurred occlusion. This led existing OST-HMDs to employ dedicated hardware such as focus optics and multi-stacked SLMs. Contrary to these viable, yet complex and\/or computationally expensive solutions, we return to the single-layer LCD approach for the hardware simplicity while maintaining fine occlusion-we compensate for a degraded occlusion area by overlaying a compensation image. We compute the image based on the HMD parameters and the background scene captured by a scene camera. The evaluation demonstrates that the proposed method reduced the occlusion leak error by 61.4% and the occlusion error by 85.7%.","Authors":"Y. Itoh; T. Hamasaki; M. Sugimoto","DOI":"10.1109\/TVCG.2017.2734427","Keywords":"Occlusion support;optical see-through HMD;occlusion leak;spatial light modulator;depth cue","Title":"Occlusion Leak Compensation for Optical See-Through Displays Using a Single-Layer Transmissive Spatial Light Modulator","Keywords_Processed":"depth cue;occlusion support;optical see through HMD;occlusion leak;spatial light modulator","Keyword_Vector":[0.0549973647,-0.0059993096,-0.0147991449,0.0212949754,0.024026552,-0.017264452,-0.0329622335,-0.0160095863,-0.0094333815,0.0073001229,0.0160868777,0.0344558775,0.0036439962,0.1001541234,-0.02430819,0.0376965642,-0.0479830008,-0.0696285844,0.0353919289,0.0855626235,0.0273834283,0.0390330375,-0.0657865795,0.0381781214,-0.0293187296,-0.071028132,-0.0316294129,-0.0237169248,-0.0299279256,-0.0837017748,0.0720491507,-0.0254896335,-0.0672037235,-0.0549751161,0.0432540433,0.0311411324,-0.0594709227,-0.0144552292,0.0531552255,0.0831974943,0.0142518524,0.0052519503,-0.0051006325,-0.0454008015,-0.0111841103,-0.0171784164,-0.0304000582,-0.0194825414,0.0051017682,0.0174239565,-0.020891594,-0.0251454129],"Abstract_Vector":[0.1623801604,0.0219080582,0.001315409,-0.0062556286,-0.0426979528,0.0271448438,0.0207511245,0.0101664877,0.0627090656,0.0126019787,-0.0508327809,0.0111759741,-0.0449356064,0.0102190395,-0.0319531209,-0.0436983951,0.0116161805,0.0315957223,0.0175736172,-0.0410453124,-0.0731000751,0.0210539315,0.1639863299,0.04948196,-0.0169052131,-0.0438557453,0.018333642,0.0570975933,0.0406969325,-0.0318450868,0.0020386329,-0.0188213763,-0.0545947307,-0.0045813868,-0.0127429753,-0.0045983769,-0.0268201682,-0.015871551,0.0101352577,0.0178845675,0.0119708039,-0.020731968,0.049076406,-0.0118115806,-0.0563900289,0.0005848706,0.0365728036,0.0919197529,-0.0095335798,0.0384979975,0.0418819164,-0.0008759194,0.0109416429,0.0271163608,0.0241960827,0.0350279533,0.0360217569,-0.0239817357,0.0018221477,0.0060605191,0.0117607868,-0.007180256,0.0643783804,-0.0507433597,0.0031515565,0.05071186,0.049342748,-0.0172766683,-0.0521358612,0.0569090539,0.0173822751,0.0347793116,0.0130087286,0.020126075,-0.0226720246,-0.0319897945,0.0246955839,-0.0089821248,0.0066983598,-0.0431944345,-0.0024586037,0.0381091765,0.0320874648,-0.0199872951,-0.0949786374,0.0155192588,-0.0118367314,-0.0029727161,0.0302780764,-0.0331085881,0.0282320596,0.0260130748,0.0130789713,-0.0747271117,0.0044657339,0.0028172686,0.0188475098,0.0233457835,0.0002072543,-0.0439757956,-0.0208511437,0.0217192644,0.0321986712,0.0179584699,-0.0398550703,-0.0454157105,-0.0004238449,0.029328874,-0.0137619813,-0.0144444832,-0.00665381,-0.0175142566,-0.0350689647,-0.01737143,0.0440167541,-0.0157609376]},"222":{"Abstract":"Skyline queries have wide-ranging applications in fields that involve multi-criteria decision making, including tourism, retail industry, and human resources. By automatically removing incompetent candidates, skyline queries allow users to focus on a subset of superior data items (i.e., the skyline), thus reducing the decision-making overhead. However, users are still required to interpret and compare these superior items manually before making a successful choice. This task is challenging because of two issues. First, people usually have fuzzy, unstable, and inconsistent preferences when presented with multiple candidates. Second, skyline queries do not reveal the reasons for the superiority of certain skyline points in a multi-dimensional space. To address these issues, we propose SkyLens, a visual analytic system aiming at revealing the superiority of skyline points from different perspectives and at different scales to aid users in their decision making. Two scenarios demonstrate the usefulness of SkyLens on two datasets with a dozen of attributes. A qualitative study is also conducted to show that users can efficiently accomplish skyline understanding and comparison tasks with SkyLens.","Authors":"X. Zhao; Y. Wu; W. Cui; X. Du; Y. Chen; Y. Wang; D. L. Lee; H. Qu","DOI":"10.1109\/TVCG.2017.2744738","Keywords":"Skyline query;skyline visualization;multi-dimensional data;visual analytics;multi-criteria decision making","Title":"SkyLens: Visual Analysis of Skyline on Multi-Dimensional Data","Keywords_Processed":"multi dimensional datum;skyline visualization;multi criterion decision make;Skyline query;visual analytic","Keyword_Vector":[0.2220242048,-0.1075800783,0.1813026275,0.1204896637,-0.081212707,0.2358649196,0.2547450968,-0.098389583,-0.0699028628,0.22571586,0.0034645307,-0.0032775889,0.0627137031,0.002456442,0.0522745201,0.1699396173,0.020717096,-0.0108780526,-0.0719327794,0.0121763523,-0.0687489043,0.0181560573,0.1099923312,0.2155153613,-0.0655204187,0.0198112932,-0.008484884,0.0586795074,0.0730874021,0.0525611364,-0.0646819481,0.1644047409,-0.1777428195,-0.0042578108,0.1077312777,-0.0774946744,-0.0414132367,-0.0594621443,0.0842572069,0.0153501719,-0.044260325,0.1860560894,-0.0526725255,0.0056929433,-0.0149107425,-0.0427521272,0.0092279287,0.0010654328,-0.050909484,-0.0132446889,0.0161176728,-0.1148188267],"Abstract_Vector":[0.2005876857,-0.0072696139,0.1080329234,-0.0383595611,0.0233169701,-0.0299416808,0.0405315722,-0.0394643174,0.0073813582,0.0686598863,-0.0291147702,0.0466745016,-0.0761586967,0.0352764619,-0.0368052266,-0.0132041284,-0.0109053528,-0.0169012868,-0.00309717,-0.035258085,-0.0250787843,-0.0104762942,-0.0123917591,0.0396725243,-0.0065944256,0.035346609,0.0079241559,-0.0200973375,-0.0296890579,-0.0415577418,0.061728617,0.002949673,-0.0288375961,-0.0572827368,0.0105120278,-0.0404000196,-0.0316260198,0.0146194635,0.0097195297,0.0745925466,0.0168752188,-0.0205531012,0.0194132974,-0.016194913,0.0020905463,-0.0119038369,0.0515279645,0.0599242625,0.0437620063,-0.0137107904,-0.0186139699,-0.0119877042,-0.0184668251,0.0337489022,-0.0046433321,-0.023552562,0.0259061901,-0.0153205471,-0.0311640496,0.0326406231,0.0093955937,0.0221791543,0.0057254836,-0.0072821917,0.0041991485,0.0028177856,-0.0644794427,0.0175027195,-0.0589702339,0.0432770768,0.0066568908,0.0029859058,-0.044867417,-0.030094191,-0.0182886283,0.0229816063,-0.0069493328,0.0202668212,0.0117600562,0.0117499775,0.0082901018,0.002788953,-0.0106022855,-0.024355721,-0.0189149428,0.0229077513,0.0219630687,-0.0103691523,-0.0143007865,0.0475818477,0.0093781307,0.0071671895,0.0479613184,-0.0161163589,0.0180567602,-0.032270559,0.0075206733,-0.0036784354,-0.0253444074,0.0106609723,-0.0156044454,-0.0019060396,-0.0056436358,0.0387554979,-0.0051224507,0.0101558741,0.0204134109,0.0286062792,-0.0266986077,-0.0093628181,0.0067721695,0.0345057153,0.0281608885,-0.0007891682,-0.0307983081,-0.0304770252]},"223":{"Abstract":"We extend the popular brushing and linking technique by incorporating personal agency in the interaction. We map existing research related to brushing and linking into a design space that deconstructs the interaction technique into three components: source (what is being brushed), link (the expression of relationship between source and target), and target (what is revealed as related to the source). Using this design space, we created MyBrush, a unified interface that offers personal agency over brushing and linking by giving people the flexibility to configure the source, link, and target of multiple brushes. The results of three focus groups demonstrate that people with different backgrounds leveraged personal agency in different ways, including performing complex tasks and showing links explicitly. We reflect on these results, paving the way for future research on the role of personal agency in information visualization.","Authors":"P. Koytek; C. Perin; J. Vermeulen; E. Andr\u00e9; S. Carpendale","DOI":"10.1109\/TVCG.2017.2743859","Keywords":"Brushing;linking;personal agency;coordinated multiple views;interaction;design space;information visualization","Title":"MyBrush: Brushing and Linking with Personal Agency","Keywords_Processed":"coordinate multiple view;link;design space;personal agency;brush;interaction;information visualization","Keyword_Vector":[0.1106027767,-0.0475273003,0.0422357254,-0.0525413416,0.0054964397,0.0153959638,0.045903603,0.0326320455,-0.041589649,-0.0626598114,0.0449186372,0.0053902777,-0.0400616333,-0.0476801989,-0.014902556,0.0716783092,0.0065193881,0.0286979317,-0.0645263053,-0.0736765594,0.0096923535,0.0423890827,-0.0185600894,-0.0201028061,0.1430637635,-0.0170463663,0.041093587,0.0344738033,-0.0779069145,-0.1018170551,-0.0280303398,-0.0512958722,0.0239969828,0.0223702768,-0.0542489817,-0.0578489778,-0.0250359449,0.00102509,0.1052702279,0.0208105687,0.0097943413,0.0071391265,-0.0537730688,-0.0235127513,-0.0078255946,0.0022514063,0.0199183035,-0.003462918,-0.0519812461,0.0235131627,0.0036050839,0.0417753741],"Abstract_Vector":[0.2295191302,0.0132497348,-0.0609603331,-0.0164685396,0.0632006921,0.0998646786,-0.0056863011,0.0355192668,-0.0386084986,0.0742053432,0.117340501,-0.1275362301,-0.1202470575,0.0489197568,0.0712951607,-0.1191688114,-0.0721311259,0.1134785822,0.0305978816,-0.0457685902,-0.0979287051,0.1300981984,-0.055366397,0.0278649272,-0.0366400579,0.0383003222,-0.0595959348,-0.0850346965,0.0109187841,0.0144119585,-0.0517841374,-0.032783827,0.0356190494,0.0763416599,0.0544920156,-0.0215464518,0.0289720603,-0.0145215301,-0.0264163753,0.0157111855,-0.0215478554,0.0455572926,0.0066628346,0.0021556502,0.0058438956,-0.0217008695,0.0060470696,0.0115124599,0.0029762709,0.0495539176,0.0363638601,-0.0062631728,-0.0094134463,-0.0241951403,0.0303307458,0.001194381,-0.0051594682,-0.0110754769,0.030695333,-0.0056808208,-0.0263374291,-0.0065455606,0.004100528,0.0190489387,-0.0047724703,-0.0089167175,-0.0030350513,0.0079704443,0.054194484,0.011672504,0.0402747495,-0.0376690208,0.0004852046,-0.0147406592,0.0340275784,0.0252264916,0.0515948938,-0.0505805628,0.0070224877,0.010120899,0.0756412307,0.0164494188,-0.0148584039,0.0158519191,0.0114113973,0.0290668991,-0.0273907527,0.0001400436,0.0171996239,-0.0200079386,-0.0175625794,0.0043340941,0.0445076252,-0.0016593679,-0.0364691237,-0.0038462223,-0.0059193107,-0.0241652056,-0.0024686067,0.006261228,-0.0008381142,0.0058376111,0.023271487,0.0426535243,-0.0221618959,0.0230196967,-0.0148203114,-0.0178835195,0.0170931603,0.0344541491,-0.0240125091,0.0321665064,-0.0176143536,-0.0349865391,-0.0076367517,-0.0335869738]},"224":{"Abstract":"Visual exploration of flow fields is important for studying dynamic systems. We introduce semantic flow graph (SFG), a novel graph representation and interaction framework that enables users to explore the relationships among key objects (i.e., field lines, features, and spatiotemporal regions) of both steady and unsteady flow fields. The objects and their relationships are organized as a heterogeneous graph. We assign each object a set of attributes, based on which a semantic abstraction of the heterogeneous graph is generated. This semantic abstraction is SFG. We design a suite of operations to explore the underlying flow fields based on this graph representation and abstraction mechanism. Users can flexibly reconfigure SFG to examine the relationships among groups of objects at different abstraction levels. Three linked views are developed to display SFG, its node split criteria and history, and the objects in the spatial volume. For simplicity, we introduce SFG construction and exploration for steady flow fields with critical points being the only features. Then we demonstrate that SFG can be naturally extended to deal with unsteady flow fields and multiple types of features. We experiment with multiple data sets and conduct an expert evaluation to demonstrate the effectiveness of our approach.","Authors":"J. Tao; C. Wang; N. V. Chawla; L. Shi; S. H. Kim","DOI":"10.1109\/TVCG.2017.2773071","Keywords":"Flow visualization;heterogeneous graph;semantic abstraction;critical points;vortex cores;FTLE;field lines","Title":"Semantic Flow Graph: A Framework for Discovering Object Relationships in Flow Fields","Keywords_Processed":"semantic abstraction;field line;heterogeneous graph;FTLE;flow visualization;vortex core;critical point","Keyword_Vector":[0.0628489924,0.0046482511,-0.0142469023,0.0147563098,0.0612695012,0.0188241179,-0.0081081695,0.0692509222,-0.0772858286,-0.0307080102,-0.0572148558,-0.0158731738,0.0107052603,0.0069572176,0.0912931942,-0.0496490453,0.0398025269,0.0939032829,-0.0105857597,0.0187107021,0.0682924806,-0.0133000882,-0.082466088,0.0291627528,-0.017115535,-0.0158800138,-0.0049852404,-0.025855237,-0.000820426,-0.0291602871,0.0439771512,0.0389375836,0.0050600853,0.0938106716,0.0451265985,-0.0618244144,-0.0641858319,-0.0045554931,-0.0528814873,-0.0267090357,0.0164390494,-0.0275004594,-0.005502291,-0.0149448214,0.0095252638,-0.0218225345,0.0319989775,-0.0483660583,-0.0119649074,0.0473073319,0.0285385334,0.0243696087],"Abstract_Vector":[0.1748484073,0.0720104085,-0.0467731184,0.0403295946,0.056517413,-0.0106505129,0.0172925971,0.0201651351,0.0278261567,0.0405446826,0.0199913576,0.0127368937,-0.068337742,-0.0785615176,-0.0140499895,0.0117075707,-0.030606046,-0.031071318,-0.0225187662,-0.0153704658,-0.001335708,-0.0089579832,-0.0053595215,0.070773869,-0.0286154384,0.0427326689,-0.0039756801,0.0320238266,0.0644534119,0.0304536511,0.0176321373,-0.0381796689,0.026190493,-0.1179910235,-0.0004347724,0.0231300746,0.0017450419,-0.0432392207,-0.0027780508,0.0642158932,-0.0422180026,-0.0008965673,-0.0435524113,-0.0516374098,-0.0518913227,0.0189785628,-0.0048542965,0.0289867266,0.0801323916,-0.017560695,-0.0189827005,-0.0071731117,-0.0229876078,0.0318130623,-0.0127868101,0.0250460495,0.0212700742,-0.0712788759,-0.0074234119,-0.0193098169,0.0153298511,-0.063178865,0.0353478697,-0.0467230998,-0.0040443318,0.0290808418,-0.0905233286,0.0120381621,0.0121123725,0.0008511047,0.0615350746,0.0513745367,-0.010120583,-0.0099156773,0.0717718364,0.0380435873,0.0160654705,-0.0202859476,-0.0057067837,-0.0147440481,0.041234611,0.0008582216,0.0147971547,0.0074467266,-0.0203227879,0.0288533717,-0.0133346269,0.0252131693,0.0357913974,-0.0210789455,-0.0091706534,0.0103743096,-0.0137550861,0.0283819571,0.0067724685,-0.0223779421,0.0465995457,0.006127295,0.0126567068,0.0037492,-0.0064494848,-0.0098665823,-0.0092122775,-0.0231798658,-0.0126163989,0.0013271372,0.0227445292,-0.011393795,-0.0262671923,-0.0144919964,-0.0155450153,0.0055594556,0.0044361919,-0.0229048897,0.0024892823,-0.0229956814]},"225":{"Abstract":"Famous examples such as Anscombe's Quartet highlight that one of the core benefits of visualizations is allowing people to discover visual patterns that might otherwise be hidden by summary statistics. This visual inspection is particularly important in exploratory data analysis, where analysts can use visualizations such as histograms and dot plots to identify data quality issues. Yet, these visualizations are driven by parameters such as histogram bin size or mark opacity that have a great deal of impact on the final visual appearance of the chart, but are rarely optimized to make important features visible. In this paper, we show that data flaws have varying impact on the visual features of visualizations, and that the adversarial or merely uncritical setting of design parameters of visualizations can obscure the visual signatures of these flaws. Drawing on the framework of Algebraic Visualization Design, we present the results of a crowdsourced study showing that common visualization types can appear to reasonably summarize distributional data while hiding large and important flaws such as missing data and extraneous modes. We make use of these results to propose additional best practices for visualizations of distributions for data quality tasks.","Authors":"M. Correll; M. Li; G. Kindlmann; C. Scheidegger","DOI":"10.1109\/TVCG.2018.2864907","Keywords":"Graphical perception;data quality;univariate visualizations","Title":"Looks Good To Me: Visualizations As Sanity Checks","Keywords_Processed":"data quality;graphical perception;univariate visualization","Keyword_Vector":[0.1992778245,-0.2386123565,-0.2120122227,0.2415761172,-0.1805287579,-0.026945158,0.0724596532,-0.0216072196,0.0490343826,-0.0278283926,0.0792696887,-0.0105692979,0.0306021929,0.0787505788,-0.0258772047,-0.0151808513,0.0087743896,0.0205528475,-0.001719024,-0.0453030343,-0.0164591474,-0.0526408346,0.0416900256,-0.0205033167,-0.0172263513,-0.0051621406,-0.0255953428,-0.0222153034,-0.0088697223,0.0182730258,0.015925402,-0.0121480961,0.0470660907,-0.0227029059,0.0335681185,-0.0035996426,0.0189694833,-0.0352128763,-0.0751478548,-0.0798119651,-0.0449707888,-0.044267017,-0.0054995696,-0.0589827985,0.0157236343,-0.0059253733,-0.0402403238,-0.0090054805,0.0286993169,-0.0076808699,-0.0040539107,-0.0151716426],"Abstract_Vector":[0.1083625224,-0.0732294918,0.0120333912,0.0139873852,0.0046073609,0.0036149152,0.0182443513,-0.0087654737,-0.0112044209,0.044373762,0.0053329361,-0.0098549267,-0.0569212631,0.0190699702,0.0115524908,-0.0234781865,-0.0076927097,-0.0064666146,-0.0083045309,-0.0004393109,0.0142756759,-0.0228208573,-0.0013809734,0.0260948939,0.0012563433,0.0081605515,0.0035422507,0.0313677287,-0.0136459697,-0.0478262514,0.0208853573,-0.019030028,0.0071680136,-0.0158029989,-0.0435694731,0.0136583017,-0.0152066145,0.0013581075,-0.0166414212,0.0171732751,0.0129205639,-0.0004001085,-0.0016502622,0.0214987134,0.0111083393,-0.0136309203,0.002594444,-0.0202202384,-0.0017501636,-0.0378004867,-0.0151168062,-0.0390938334,0.0078972431,0.0178469494,-0.0075281909,-0.0018807719,0.0041843264,-0.0090303356,0.0091648596,0.0115891041,-0.0026301028,0.0031903266,-0.0057448069,0.0178091447,0.012049009,-0.0193772946,0.0211527494,-0.0061705802,-0.0094898248,-0.0075501509,0.0008435863,-0.0066730653,-0.0099422156,0.0052900991,0.0096600588,0.0233189834,-0.0142496701,-0.0122128143,0.0103164261,-0.0061837371,0.019716809,-0.0156857279,0.00232343,0.0051848318,-0.0121121391,0.0054103536,0.0031364366,-0.0013102981,-0.0101232643,-0.0118448659,-0.010991891,0.0070331505,0.0164514983,0.0006393444,-0.028636794,0.0050151203,0.0124556663,0.0134144027,0.0179310758,-0.0089834304,0.0056830753,0.0157104333,-0.0138392539,0.0147125025,-0.0256122703,0.0049975432,-0.0004814015,-0.0050642877,0.0070277836,0.0046001107,-0.0093421525,-0.0027439032,0.0003852011,0.0043677508,0.0014495258,0.0106821464]},"226":{"Abstract":"There currently exist two dominant strategies to reduce data sizes in analysis and visualization: reducing the precision of the data, e.g., through quantization, or reducing its resolution, e.g., by subsampling. Both have advantages and disadvantages and both face fundamental limits at which the reduced information ceases to be useful. The paper explores the additional gains that could be achieved by combining both strategies. In particular, we present a common framework that allows us to study the trade-off in reducing precision and\/or resolution in a principled manner. We represent data reduction schemes as progressive streams of bits and study how various bit orderings such as by resolution, by precision, etc., impact the resulting approximation error across a variety of data sets as well as analysis tasks. Furthermore, we compute streams that are optimized for different tasks to serve as lower bounds on the achievable error. Scientific data management systems can use the results presented in this paper as guidance on how to store and stream data to make efficient use of the limited storage and bandwidth in practice.","Authors":"D. Hoang; P. Klacansky; H. Bhatia; P. Bremer; P. Lindstrom; V. Pascucci","DOI":"10.1109\/TVCG.2018.2864853","Keywords":"data compression;bit ordering;multi-resolution;data analysis","Title":"A Study of the Trade-off Between Reducing Precision and Reducing Resolution for Data Analysis and Visualization","Keywords_Processed":"datum analysis;multi resolution;data compression;bit order","Keyword_Vector":[0.0510975748,-0.0410628836,-0.0320373996,0.0253027254,-0.0260773121,0.0043465999,-0.0020405866,-0.0178588619,-0.0179521404,-0.0226747356,0.0291525167,-0.0366317374,0.0250238588,0.1497133888,-0.0651641885,-0.0144663986,0.0559585114,0.0104399773,0.011296543,-0.0070228464,-0.006085312,-0.0199465712,-0.0061885149,-0.0114985599,0.016084773,0.035099868,-0.0185552176,0.0009686154,-0.0267353386,0.0623460549,-0.0099071165,-0.0072447505,0.0384317861,-0.0136712776,-0.0310017614,-0.0038014026,0.0131040784,0.0047443014,-0.0054090735,0.0119298727,0.0003529363,0.0149335965,-0.0416036336,0.0059822088,-0.0382738323,0.0388937111,-0.0369480052,0.0232978377,-0.0167120788,0.011303775,-0.039244515,-0.0042253819],"Abstract_Vector":[0.1234791255,-0.0167088679,-0.0087682705,0.0105900644,-0.0150895854,0.0333154854,0.0033593668,0.0162086463,-0.0004433434,0.0021573903,-0.0174560049,0.0389724367,-0.0181163699,-0.020298483,-0.0196789305,0.0089812165,-0.0105451834,0.0332262348,-0.0073243245,0.0066352655,-0.0211756699,0.0159576967,-0.0265561063,0.0161162897,0.0075513706,-0.0243995197,-0.0212050685,0.0192063328,-0.000756611,0.0192589081,0.0089484612,0.0238400204,-0.0019473816,0.0080258817,-0.0062475713,-0.0177901321,0.0004733465,-0.0323012078,0.0065217211,0.045415784,-0.0313236425,-0.0037716221,0.0005710082,0.0050840492,-0.0239795926,-0.0284590337,0.0265125974,-0.001459895,0.0246441822,-0.0132203615,0.01851091,0.0105745219,0.0072581881,-0.0029867987,0.0010182534,0.0021374078,-0.0042066593,0.0115854453,0.0117306416,-0.0145051526,0.008419646,-0.022399438,0.0289506235,0.0064682781,-0.0252926623,0.0086607578,-0.0116738446,-0.0179673229,-0.0024116851,0.0299234934,-0.0170206026,-0.0003235906,0.0005665067,0.0154591757,0.009838801,-0.0221509105,-0.0262138564,0.0032306949,0.0134655013,-0.001841624,-0.0035463367,0.0319572733,0.0139786421,0.0181270148,0.018880097,-0.0226418579,0.0244110055,0.0063204299,0.0170554459,0.0066840486,-0.0289735638,-0.0225085007,-0.0425208539,0.0117149429,0.0016191315,0.0042674567,0.0171788334,-0.0166838441,-0.0197792004,-0.0059939454,-0.0165558755,0.0052094147,0.002025937,0.009886264,-0.0107789198,0.0033967184,0.0040780283,-0.005832798,-0.0244379034,0.0012646666,0.0063163024,-0.0077616607,-0.0139662924,-0.0036757991,-0.0302914201,-0.0109549294]},"227":{"Abstract":"We present a framework for the analysis of uncertainty in isocontour extraction. The marching squares (MS) algorithm for isocontour reconstruction generates a linear topology that is consistent with hyperbolic curves of a piecewise bilinear interpolation. The saddle points of the bilinear interpolant cause topological ambiguity in isocontour extraction. The midpoint decider and the asymptotic decider are well-known mathematical techniques for resolving topological ambiguities. The latter technique investigates the data values at the cell saddle points for ambiguity resolution. The uncertainty in data, however, leads to uncertainty in underlying bilinear interpolation functions for the MS algorithm, and hence, their saddle points. In our work, we study the behavior of the asymptotic decider when data at grid vertices is uncertain. First, we derive closed-form distributions characterizing variations in the saddle point values for uncertain bilinear interpolants. The derivation assumes uniform and nonparametric noise models, and it exploits the concept of ratio distribution for analytic formulations. Next, the probabilistic asymptotic decider is devised for ambiguity resolution in uncertain data using distributions of the saddle point values derived in the first step. Finally, the confidence in probabilistic topological decisions is visualized using a colormapping technique. We demonstrate the higher accuracy and stability of the probabilistic asymptotic decider in uncertain data with regard to existing decision frameworks, such as deciders in the mean field and the probabilistic midpoint decider, through the isocontour visualization of synthetic and real datasets.","Authors":"T. Athawale; C. R. Johnson","DOI":"10.1109\/TVCG.2018.2864505","Keywords":"Isocontour visualization;topological uncertainty;marching squares;asymptotic decider;bilinear interpolation;probabilistic computation","Title":"Probabilistic Asymptotic Decider for Topological Ambiguity Resolution in Level-Set Extraction for Uncertain 2D Data","Keywords_Processed":"bilinear interpolation;topological uncertainty;asymptotic decider;march square;probabilistic computation;isocontour visualization","Keyword_Vector":[0.1056980869,-0.0126179878,0.021562606,-0.0538448433,-0.0012275221,-0.023315244,-0.0502883943,0.0150251827,-0.1073682462,-0.0477518667,0.0609928856,0.056975398,-0.0219124794,0.0074930501,0.0374974578,0.1004904281,0.0745361603,0.0870323056,0.1570402946,-0.0103188165,-0.1129930825,0.0863204796,0.1280242655,0.0742440291,-0.0955488627,-0.0519308884,0.0225541508,-0.0080175585,0.0572208992,-0.1373314604,-0.0163074818,-0.0611209618,0.0676046787,0.0367780658,-0.010733851,-0.0519959347,0.0176771184,-0.0342365602,-0.0157523331,-0.024050332,-0.0427151707,0.0499316764,0.0288721813,0.0142661197,0.0185407138,-0.0642430711,0.0502872809,0.0482217795,-0.020192083,0.0645926968,0.0170076277,0.0176855214],"Abstract_Vector":[0.1534344067,-0.0587560338,0.0013584593,-0.0153398,-0.0613472414,0.0710863542,0.0017136437,0.0231155303,0.0303744712,0.0016181882,-0.0316395743,0.055944884,-0.0178771413,0.0065617714,0.0062400654,-0.0103678679,-0.0590644469,-0.0012521342,0.0585632773,-0.0168262297,-0.0154627727,-0.0353053856,-0.0031156391,0.0412206944,-0.0231500255,0.0276092269,-0.0206520798,-0.0262825748,-0.0165571534,-0.0323750903,0.0710290672,-0.0247562953,-0.0181896428,-0.02323546,-0.0508264002,-0.0713924629,-0.0562849421,-0.0571374771,-0.0145565137,-0.0213917067,0.0025823875,0.0354015774,-0.0230538204,-0.0565343408,-0.0011064819,0.0182565608,0.0139015347,-0.0110158557,0.0294994834,0.0196363271,-0.0179795819,0.0071325167,0.0189475799,-0.0502116715,0.0477487203,-0.0135731295,-0.0097295939,0.0350031631,-0.0504411884,-0.0130980333,-0.0193670586,-0.0194305919,-0.0565758818,-0.0333773565,-0.0422572396,-0.0180543783,0.0008204078,0.0076731875,-0.0365540547,0.0331065496,0.0268978172,0.031468078,0.0188726138,0.0428371779,0.0018822218,-0.0393874296,-0.0019778722,0.0197994694,-0.0111534767,-0.0046974418,0.0125070712,-0.0372471712,-0.0352217748,-0.0260515541,-0.0192900743,0.0084081759,0.0152939514,-0.029778092,0.0166615712,0.0209649115,-0.0159672412,-0.013710984,0.0047585206,-0.0380573881,-0.0253991434,0.001176393,0.0007740971,-0.0215077216,0.022954243,-0.0335519932,-0.0146048316,0.0246343087,-0.0268765351,-0.0035018899,-0.0089457838,0.0210805089,-0.029782432,0.043061096,0.0262915746,0.0092496155,0.0376520117,0.041207003,0.0183855239,0.0315829668,-0.0104523736,0.0281214232]},"228":{"Abstract":"The stated goal for visual data exploration is to operate at a rate that matches the pace of human data analysts, but the ever increasing amount of data has led to a fundamental problem: datasets are often too large to process within interactive time frames. Progressive analytics and visualizations have been proposed as potential solutions to this issue. By processing data incrementally in small chunks, progressive systems provide approximate query answers at interactive speeds that are then refined over time with increasing precision. We study how progressive visualizations affect users in exploratory settings in an experiment where we capture user behavior and knowledge discovery through interaction logs and think-aloud protocols. Our experiment includes three visualization conditions and different simulated dataset sizes. The visualization conditions are: (1) blocking, where results are displayed only after the entire dataset has been processed; (2) instantaneous, a hypothetical condition where results are shown almost immediately; and (3) progressive, where approximate results are displayed quickly and then refined over time. We analyze the data collected in our experiment and observe that users perform equally well with either instantaneous or progressive visualizations in key metrics, such as insight discovery rates and dataset coverage, while blocking visualizations have detrimental effects.","Authors":"E. Zgraggen; A. Galakatos; A. Crotty; J. Fekete; T. Kraska","DOI":"10.1109\/TVCG.2016.2607714","Keywords":"Exploratory analysis;interactive visualization;progressive visualization;scalability;insight-based evaluation","Title":"How Progressive Visualizations Affect Exploratory Analysis","Keywords_Processed":"interactive visualization;exploratory analysis;progressive visualization;scalability;insight base evaluation","Keyword_Vector":[0.2346627737,0.0962874565,-0.067647055,-0.1254807586,-0.0111393193,0.0325820442,0.1070719055,-0.009899068,0.0153311562,-0.0610413074,-0.0143973988,-0.0317120495,-0.0253431027,-0.0606848746,-0.029383586,0.0643706963,-0.0155468081,0.1075648459,-0.0623476976,-0.0942280902,-0.0656812486,0.0489942094,-0.0150127326,-0.0868743415,0.2779848803,-0.0240980381,-0.0176120465,0.0448395728,-0.1329707902,-0.1716488949,-0.0310638655,-0.0769059492,-0.0251797333,0.0170522177,-0.0721730747,-0.0289271058,-0.0255660873,-0.0242696095,0.1727444804,-0.0118168899,0.0353367943,-0.0090538249,-0.0295950176,-0.0034543653,-0.0204469424,-0.0257423728,0.0313108678,-0.0338648683,-0.0006672984,0.0114324243,-0.0305556144,-0.0166043234],"Abstract_Vector":[0.2852357477,0.0030845262,-0.0632971286,-0.087734144,0.0108305346,0.0530555039,-0.0437090628,-0.0110836122,-0.0655386781,0.0335384092,0.0110670408,-0.0237934353,0.0239846013,0.0190866981,-0.0127136524,0.0147466775,-0.0167424401,0.0060860613,0.0412605301,-0.0347888569,-0.04786985,0.0483795327,-0.0482329623,-0.0544560745,-0.0106476566,0.0443828228,0.0296952164,-0.0269985808,0.0796164428,-0.0446061552,-0.0411028905,-0.0087062199,-0.0192051919,0.043523954,0.0261858491,-0.0071278065,-0.0183633546,0.0386640208,-0.0294240334,0.0716200281,-0.0491245437,0.0133974262,0.0071660445,-0.0195347354,-0.0255346185,-0.0240520496,0.0400424137,-0.0023678921,-0.0125312606,0.0167535859,-0.0130239334,-0.006864459,-0.0170449902,-0.0258288214,0.0059747417,0.0064666899,-0.0082721864,0.0130629213,0.0411381534,-0.0270505381,-0.0156531176,-0.0324119355,0.0217153016,0.0219990471,-0.0002246928,0.017655437,0.0051174653,-0.0019541318,0.0081867347,0.0061715578,-0.000641664,-0.0067138329,-0.002682653,-0.0203772762,-0.0110957494,0.002950817,-0.032929239,0.0052443319,0.0033032689,-0.0249645695,0.0057845184,0.0408144021,-0.0247336455,0.0348798666,-0.0033347664,0.0198933736,-0.025573469,-0.0312391014,-0.0103553281,-0.030564146,-0.0004980073,0.0204909817,-0.0163268423,0.0299453308,-0.0213929595,0.0145291893,-0.0239475201,-0.0204058996,0.0158502077,-0.0145759668,0.0060924714,0.014664975,-0.0022829363,-0.0174905194,0.0348669437,0.0079268561,-0.0148201632,0.0119121089,0.0140795784,0.0114440459,-0.0374521172,0.0021609684,0.0015650156,-0.0177627222,-0.0031454847,0.0188736544]},"229":{"Abstract":"Animated transitions can be effective in explaining and exploring a small number of visualizations where there are drastic changes in the scene over a short interval of time. This is especially true if data elements cannot be visually distinguished by other means. Current research in animated transitions has mainly focused on linear transitions (all elements follow straight line paths) or enhancing coordinated motion through bundling of linear trajectories. In this paper, we introduce animated transition design, a technique to build smooth, non-linear transitions for clustered data with either minimal or no user involvement. The technique is flexible and simple to implement, and has the additional advantage that it explicitly enhances coordinated motion and can avoid crowding, which are both important factors to support object tracking in a scene. We investigate its usability, provide preliminary evidence for the effectiveness of this technique through metric evaluations and user study and discuss limitations and future directions.","Authors":"Y. Wang; D. Archambault; C. E. Scheidegger; H. Qu","DOI":"10.1109\/TVCG.2017.2750689","Keywords":"Information visualization;animated transitions;vector field design","Title":"A Vector Field Design Approach to Animated Transitions","Keywords_Processed":"vector field design;animate transition;information visualization","Keyword_Vector":[0.0278095622,-0.0138848638,-0.0079941711,0.0189998792,0.005952272,-0.0134163066,-0.016646059,-0.0116085344,-0.0102523081,0.000043584,0.0027140115,0.0099699557,-0.0034263394,0.0319127538,-0.0137888205,-0.0046525341,-0.0319304423,-0.0048236189,-0.0027554981,-0.0014986197,-0.0015140652,-0.0294574991,0.0144969054,-0.0123274284,-0.0166458241,-0.0025276486,-0.0098082127,0.0261818934,-0.0190412865,0.0347760445,0.002692352,-0.0146146084,-0.0045498133,0.0056016137,-0.0118330893,0.0247551588,-0.0559332704,-0.0101028722,-0.0012504004,-0.0354550919,0.0033362829,0.0004638908,-0.037082077,-0.043017502,0.0378628037,-0.014440655,0.0137498081,-0.0041206848,-0.0209991565,0.0142976781,0.0127311942,0.0546959411],"Abstract_Vector":[0.2247274831,-0.0912282369,-0.0082056369,0.00113483,-0.0808453376,-0.0339913462,-0.0098649392,0.0184258549,-0.01205158,-0.0704954508,0.0298668798,0.0143106726,0.0066101464,0.0004226665,0.0155282022,-0.0312529185,0.0466943409,0.067620408,-0.0243671064,-0.1132861954,0.0722702345,0.032593036,0.0238720104,-0.027263445,-0.0719985488,0.0228884904,0.0396560445,-0.0305483581,0.0023318152,-0.0443971296,0.0059522136,-0.036654893,0.0377295692,-0.0251910087,-0.0217293039,-0.0246793707,0.0082231413,-0.0113032088,-0.0037211185,-0.0323019335,-0.0454227338,-0.0064148735,0.0399197561,-0.0142386776,-0.0151362516,0.0514368702,-0.0299555048,0.0044192147,-0.0006975745,-0.0030450794,-0.0312863569,-0.0183243013,-0.0213880436,0.0033854182,-0.0295860382,0.0024411582,-0.0107833739,0.0287624871,0.0227395252,0.0583284685,0.0271509809,-0.009553563,-0.0390527374,0.0054432821,-0.0101468674,0.0461131139,-0.0214305585,-0.0132249634,-0.0320573786,0.0389120755,-0.0194458725,0.0089656285,-0.023435712,0.0015945636,-0.0397761189,0.0041815413,0.0189866529,0.012808144,-0.0355480186,0.0114541576,-0.0125831003,0.0009057199,0.0146316626,0.0093985639,-0.0003424299,-0.0058372476,0.0022280619,0.0335493325,-0.0400407455,0.0110576631,0.0232997357,0.0197479981,-0.012258217,-0.0114799388,0.0049028566,-0.0082348876,-0.01277218,0.0423768349,-0.0302433742,0.0074990662,0.0500601826,-0.0056041994,-0.0021010754,-0.0030614731,-0.0026347814,-0.0180994993,-0.0297471833,0.0225701924,-0.0164329614,-0.0232884507,-0.0087018919,0.0141644151,-0.0108031639,0.016443135,-0.0004000759,0.0087232537]},"23":{"Abstract":"As an ensemble model that consists of many independent decision trees, random forests generate predictions by feeding the input to internal trees and summarizing their outputs. The ensemble nature of the model helps random forests outperform any individual decision tree. However, it also leads to a poor model interpretability, which significantly hinders the model from being used in fields that require transparent and explainable predictions, such as medical diagnosis and financial fraud detection. The interpretation challenges stem from the variety and complexity of the contained decision trees. Each decision tree has its unique structure and properties, such as the features used in the tree and the feature threshold in each tree node. Thus, a data input may lead to a variety of decision paths. To understand how a final prediction is achieved, it is desired to understand and compare all decision paths in the context of all tree structures, which is a huge challenge for any users. In this paper, we propose a visual analytic system aiming at interpreting random forest models and predictions. In addition to providing users with all the tree information, we summarize the decision paths in random forests, which eventually reflects the working mechanism of the model and reduces users' mental burden of interpretation. To demonstrate the effectiveness of our system, two usage scenarios and a qualitative user study are conducted.","Authors":"X. Zhao; Y. Wu; D. L. Lee; W. Cui","DOI":"10.1109\/TVCG.2018.2864475","Keywords":"Interpretable Machine Learning;Random Forests;Random Forest Visualization;Visual Analytics","Title":"iForest: Interpreting Random Forests via Visual Analytics","Keywords_Processed":"Random Forest Visualization;Random forest;visual analytic;interpretable machine Learning","Keyword_Vector":[0.0601383774,0.0247989006,0.0499897043,0.0336330432,-0.0144248054,-0.0117110329,0.0155462346,0.0039863218,-0.0202548635,0.0521408786,0.021296897,-0.0081921736,-0.0145003999,0.0184617208,0.0528945765,0.018663761,-0.0603326451,-0.0681756697,0.0620327398,-0.0466795811,0.0456792364,-0.0245673421,-0.0580383385,0.0208931505,0.0209076943,-0.020922086,0.0894104885,0.0697439344,0.0055915753,0.0213634736,-0.0497690383,-0.0093412801,-0.0221499629,-0.0329293175,0.004240528,0.0039267583,-0.0477782242,-0.016296636,-0.0412927761,-0.0126794953,0.00307369,0.0037454909,-0.0167143273,0.0000732642,0.0591771425,0.0148850768,-0.0074625477,-0.0276501788,-0.006701007,0.0021787157,-0.0190934238,-0.0062017601],"Abstract_Vector":[0.2367705562,0.0327292715,0.0542072281,-0.0048142619,0.0729754147,-0.0143299272,-0.083245704,-0.0205948391,0.0516106885,-0.0407152592,-0.0264611362,0.0251046119,-0.0584075196,-0.0252209449,0.0143044681,-0.0560328917,-0.0534625778,0.000398691,-0.0656270027,-0.0665650496,-0.039835616,-0.0112403963,-0.0342822426,0.0318095272,0.0285514044,-0.0594266693,0.0257237961,0.0554118135,0.006167973,-0.0296518768,-0.0367951862,-0.0474266882,-0.0676905424,0.059765701,-0.1055842103,0.0679797827,0.0371185157,-0.1043895053,0.0202698054,-0.0129259865,0.041235566,0.024164351,0.0297382895,-0.0218851412,0.0123616873,-0.0552439631,0.0689231607,-0.0179491181,0.0369428144,-0.0615540624,0.0074251618,0.0510402824,-0.0438458741,-0.0463247353,0.0357344285,-0.0421579054,0.0322642974,-0.0525379752,0.0938582923,-0.1069375667,0.0089210692,0.0412034792,-0.0302624067,-0.0092905817,0.0242846199,0.0083862243,-0.0394023223,-0.1036857962,0.0209168609,0.0302505422,0.0708489009,0.0086714681,-0.0509218882,0.059534735,0.0246693596,0.0472614617,0.0401818256,-0.0648465554,-0.0676046701,-0.0610855354,-0.0243473613,0.0007311502,-0.0612400247,-0.0187581177,0.0599267509,0.0540422454,-0.0483552156,-0.0308904731,-0.0622377575,0.0115634735,0.0016078762,0.0340816871,-0.0627219912,-0.0325087657,0.0062859373,0.0296229485,-0.018391271,-0.0306460062,0.0011503163,-0.0251489864,-0.0379992982,-0.0319648697,-0.0056960878,-0.0101678763,-0.0070660581,-0.0143578696,-0.054875474,-0.0249463032,-0.0238459146,-0.050413091,0.0407150377,-0.0198425942,0.0554316965,0.0425639551,0.0085846743,0.0051166216]},"230":{"Abstract":"We present the results of two perception studies to assess how quickly people can perform a simple data comparison task for small-scale visualizations on a smartwatch. The main goal of these studies is to extend our understanding of design constraints for smartwatch visualizations. Previous work has shown that a vast majority of smartwatch interactions last under 5 s. It is still unknown what people can actually perceive from visualizations during such short glances, in particular with such a limited display space of smartwatches. To shed light on this question, we conducted two perception studies that assessed the lower bounds of task time for a simple data comparison task. We tested three chart types common on smartwatches: bar charts, donut charts, and radial bar charts with three different data sizes: 7, 12, and 24 data values. In our first study, we controlled the differences of the two target bars to be compared, while the second study varied the difference randomly. For both studies, we found that participants performed the task on average in <;300 ms for the bar chart, <;220 ms for the donut chart, and in <; 1780 ms for the radial bar chart. Thresholds in the second study per chart type were on average 1.14-1.35\u00d7 higher than in the first study. Our results show that bar and donut charts should be preferred on smartwatch displays when quick data comparisons are necessary.","Authors":"T. Blascheck; L. Besan\u00e7on; A. Bezerianos; B. Lee; P. Isenberg","DOI":"10.1109\/TVCG.2018.2865142","Keywords":"Glanceable visualization;smartwatch;perception;quantitative evaluation;data comparison","Title":"Glanceable Visualization: Studies of Data Comparison Performance on Smartwatches","Keywords_Processed":"glanceable visualization;datum comparison;quantitative evaluation;perception;smartwatch","Keyword_Vector":[0.1474266338,-0.0801723148,0.0163604697,-0.0194065475,-0.0197867888,0.0453072143,-0.0500084273,-0.0618991212,-0.0246088071,0.0868987329,-0.0375481472,0.0692014416,-0.0183405506,0.0238723183,-0.0371622657,0.1491288183,-0.0613835101,0.0258906885,-0.128142829,-0.0019941808,0.0813261011,0.0500996778,0.0149776698,0.02822438,-0.0640630763,0.0780582046,0.0215274069,-0.0341682015,-0.0796191948,0.0056143916,-0.0408105476,-0.0547785616,-0.0112458725,0.0069067337,0.066582155,-0.0284328961,0.1032532502,-0.0200144921,0.0290153943,-0.0550467882,0.0085998018,-0.0036758731,0.056651514,0.01555081,0.0490657099,-0.0731885407,0.0363633495,-0.0850004751,0.0176550231,-0.0011912827,-0.0334544204,0.0012376726],"Abstract_Vector":[0.1661628739,-0.1085914497,0.0195102629,-0.024412855,-0.0077522439,-0.0685687664,0.0096841886,-0.0008370141,-0.0191714759,0.0421042073,-0.0096437596,0.0006739803,-0.0507968541,0.0246248113,-0.0354623103,-0.0673351083,0.0117848058,-0.0463588768,-0.0508137112,0.0039123366,-0.0025732779,-0.0086844371,0.0222816468,-0.000668343,-0.0272466268,0.0087616478,-0.0042487437,0.0355034325,-0.0831510812,-0.0647623027,0.0685440186,-0.017926115,-0.025479545,0.0136909134,-0.017984617,-0.00246753,-0.0469047505,0.0245402875,0.0253366021,-0.0154359302,-0.0371573734,0.017175404,0.0561100743,-0.0071303671,0.016340048,-0.0039879248,0.002140334,0.0324673696,-0.0137613872,0.0215150383,-0.0521029443,-0.0217780842,-0.0031972111,0.0153350907,0.0069714702,0.0559297256,0.0103018628,0.0041745038,0.0613968024,0.0446604186,0.0614523312,0.0359648645,0.0900703529,-0.0281384886,-0.0398781903,0.0013895326,-0.0348171343,0.1048861563,0.0671271823,-0.0468217269,0.0306728687,0.125094953,0.0261965992,0.0291800449,0.032317814,-0.0239591146,0.0372316453,-0.0290764395,0.0013852132,0.0220849082,-0.0312443342,-0.0171443096,0.0176216191,0.0129756161,0.0830036029,0.0665451673,-0.0211044651,0.0424874288,-0.0517417999,-0.0076405768,-0.0052091655,-0.0221734256,0.0152257848,0.0412859508,-0.0348962985,-0.0265145328,0.0049070165,0.0329509186,-0.0432497844,0.029240407,0.0470579027,0.0075314663,-0.0761481908,0.0267216682,0.0372246788,-0.0369878413,-0.0532421825,-0.0157221474,0.0271414702,0.0016108929,0.0131474227,-0.0954243236,-0.0173609035,0.0361109557,0.0203870927,0.0347724482]},"231":{"Abstract":"Multiple time series are a set of multiple quantitative variables occurring at the same interval. They are present in many domains such as medicine, finance, and manufacturing for analytical purposes. In recent years, streamgraph visualization (evolved from ThemeRiver) has been widely used for representing temporal evolution patterns in multiple time series. However, streamgraph as well as ThemeRiver suffer from scalability problems when dealing with several time series. To solve this problem, multiple time series can be organized into a hierarchical structure where individual time series are grouped hierarchically according to their proximity. In this paper, we present a new streamgraph-based approach to convey the hierarchical structure of multiple time series to facilitate the exploration and comparisons of temporal evolution. Based on a focus+context technique, our method allows time series exploration at different granularities (e.g., from overview to details). To illustrate our approach, two usage examples are presented.","Authors":"E. Cuenca; A. Sallaberry; F. Y. Wang; P. Poncelet","DOI":"10.1109\/TVCG.2018.2796591","Keywords":"Streamgraph;stacked graph;time series;aggregation;multiresolution visualization;overview+detail;focus+context;fisheye","Title":"MultiStream: A Multiresolution Streamgraph Approach to Explore Hierarchical Time Series","Keywords_Processed":"overview detail;stack graph;aggregation;streamgraph;multiresolution visualization;time series;fisheye;focus context","Keyword_Vector":[0.1041178555,-0.0914093339,-0.0493594103,0.0966545401,-0.0419686672,0.0990466919,-0.0250610577,-0.038443943,-0.0290859512,-0.009556339,0.0210480316,-0.0173000165,0.0394492499,0.0993968065,-0.0435566912,-0.0350927052,0.0114008784,0.00586467,-0.0002157225,-0.0584257773,-0.0189533586,-0.0715873369,-0.0019264684,-0.0099862964,-0.0228707069,-0.0049918573,-0.010706322,-0.0334925584,-0.005978378,-0.002683313,-0.0024543897,-0.019568139,0.0288095086,0.0221628098,-0.000209366,-0.0065774168,0.0006584406,-0.0205251841,-0.0676534506,-0.0183048423,0.0119597712,-0.0304836217,-0.0060524093,-0.0333306071,0.0145716475,0.0097187491,0.0030003046,-0.0199555895,0.0075210571,-0.0104418553,-0.0123697362,-0.0045991869],"Abstract_Vector":[0.14445995,-0.0822452584,-0.0134694636,-0.000954482,-0.0289300537,0.0054224612,-0.012070316,0.002109086,-0.0159301802,-0.0316406693,0.00739129,-0.0131710894,-0.0070114405,-0.0024485244,-0.0068143582,-0.0261553113,-0.0160833012,-0.0285803134,0.004764969,-0.0173664412,0.0184131146,-0.0136892412,0.0186549924,-0.032490277,-0.014085127,0.0081068651,-0.0014357283,-0.0597085102,0.0025802292,-0.0158987998,0.1079666092,-0.0073240691,0.0018461542,-0.0721266789,0.0095194385,-0.0050839802,-0.0130281374,0.0014711151,0.0113024228,0.0099836352,0.0088745564,-0.012907993,0.0123748708,-0.0362785301,0.0342696697,0.0088873649,0.0132811165,-0.0381639634,-0.0035633969,0.0178901436,0.0013895913,0.0195854893,-0.0241452058,-0.0248012226,-0.0427964269,-0.0321288684,-0.0362590797,0.0740341768,0.0088865213,0.0459733902,0.0148306139,-0.0398008613,0.0056629358,0.0118628719,-0.0047929936,-0.0327765551,-0.0297823908,-0.0020515408,0.0250148951,0.0097170745,-0.004946722,-0.0324345263,-0.0449464086,-0.0234816342,0.0092220241,-0.0042466162,-0.0272791406,-0.0001335384,0.0214965914,-0.0111588625,-0.0341197615,-0.0129190117,0.0242167911,0.0165164715,-0.0038479261,-0.0193497519,0.0167720048,0.0176148981,-0.0042286052,-0.0106117551,-0.0095819102,0.0726137659,-0.0013105133,-0.0189717727,0.0123268114,-0.0203465142,-0.0017004818,0.0676186015,-0.0588490512,0.0195073585,0.0409571666,-0.0154443654,-0.0038180524,0.0150568195,0.0221407926,0.0331574359,-0.0146793288,-0.0216578466,0.0197841156,0.0009686396,0.0217719725,-0.0098533824,0.0370450408,-0.0013108894,0.0316237892,0.0223016113]},"232":{"Abstract":"A new method is presented for producing photo collages that preserve content correlation of photos. We use deep learning techniques to find correlation among given photos to facilitate their embedding on the canvas, and develop an efficient combinatorial optimization technique to make correlated photos stay close to each other. To make efficient use of canvas space, our method first extracts salient regions of photos and packs only these salient regions. We allow the salient regions to have arbitrary shapes, therefore yielding informative, yet more compact collages than by other similar collage methods based on salient regions. We present extensive experimental results, user study results, and comparisons against the state-of-the-art methods to show the superiority of our method.","Authors":"L. Liu; H. Zhang; G. Jing; Y. Guo; Z. Chen; W. Wang","DOI":"10.1109\/TVCG.2017.2703853","Keywords":"Photo collage;image saliency;irregular shaped packing;image classification","Title":"Correlation-Preserving Photo Collage","Keywords_Processed":"image classification;image saliency;photo collage;irregular shaped packing","Keyword_Vector":[0.1182289083,0.0122331489,0.0337217589,-0.0376026028,0.0201327431,-0.0490378037,-0.0644095582,0.0287683149,-0.0963068672,-0.0104595829,0.015468841,0.0966763619,0.0233437375,0.0383036927,0.0967072166,0.0874756781,-0.0213482836,-0.0008752277,0.2075728713,-0.0017414584,-0.0871725109,-0.0510382231,0.1463396462,0.0481263332,-0.022133042,0.037843675,0.0154712127,0.0088604298,0.0250138525,-0.1114935387,0.1153404363,-0.0748306367,0.0045128069,0.0511159382,-0.0555718746,0.0023181582,0.021436802,0.0801937343,-0.0065116449,-0.046763329,-0.0081357467,0.080002776,-0.1024512644,-0.1047457114,-0.0704930612,0.0180354052,0.1189494337,0.1246436658,-0.1069647828,0.0586966971,-0.0288461252,0.1340397995],"Abstract_Vector":[0.1695084247,-0.0192144297,-0.0112627646,-0.0222007999,-0.0709775877,0.1172155939,-0.056162707,0.0393760157,0.1839965882,-0.0118472877,-0.0674033345,0.050520531,0.0149612415,0.0447198664,0.1116449272,0.0054801551,0.0088987563,0.024473267,0.1190623333,0.0591550588,0.0140239399,-0.0981976367,-0.0287021629,-0.0150843295,-0.0636635236,0.0203965176,0.0430324462,-0.0165026997,0.0077400123,-0.0474152823,0.0627737918,-0.1011470914,0.0022597222,0.0102651007,0.0254196151,-0.0364516934,0.0296902041,0.0368984548,-0.0271902327,-0.0567567898,0.0328681495,0.0774981334,-0.0539813871,-0.032281112,-0.010000027,0.0986332845,-0.0359938128,-0.0211061413,0.0414348994,-0.0138837385,-0.0200097969,-0.0086089548,-0.0093419552,-0.0432110251,-0.01298835,-0.0301287308,0.0277070023,0.0083505343,-0.0288124677,0.0089668419,-0.0372188332,0.0687889913,0.0194452936,-0.0213346737,0.0171899285,0.0040043389,0.0045573725,-0.0422314253,0.0297319915,-0.0173219881,-0.0350557019,-0.0502842633,0.0138936656,0.0038963714,-0.0208633905,-0.018972294,-0.0003385008,-0.0365266924,0.025654256,-0.0167144365,-0.0211741069,0.0427250987,-0.0051502272,-0.0409481324,0.0328433533,-0.0231926434,0.0008977464,0.0240551279,-0.0022306124,0.0212944597,-0.0020264284,-0.0330702158,-0.0036578011,0.0199692616,0.0503397888,-0.0293076629,0.0024410147,-0.0094093226,0.0051669492,0.007532399,0.0261906415,-0.0149737271,-0.0382180217,-0.0503031201,0.0693324109,-0.023671477,0.0070419516,0.0017919341,-0.0017009595,-0.0049938974,0.0001004811,-0.0170333222,0.0288421754,0.0409135175,-0.0241560947,-0.0164986044]},"233":{"Abstract":"We present a method for the fast computation of the intersection between a ray and the geometry of a scene. The scene geometry is simplified with a 2D array of voxelizations computed from different directions, sampling the space of all possible directions. The 2D array of voxelizations is compressed using a vector quantization approach. The ray-scene intersection is approximated using the voxelization whose rows are most closely aligned with the ray. The voxelization row that contains the ray is looked up, the row is truncated to the extent of the ray using bit operations, and a truncated row with non-zero bits indicates that the ray intersects the scene. We support dynamic scenes with rigidly moving objects by building a separate 2D array of voxelizations for each type of object, and by using the same 2D array of voxelizations for all instances of an object type. We support complex dynamic scenes and scenes with deforming geometry by computing and rotating a single voxelization on the fly. We demonstrate the benefits of our method in the context of interactive rendering of scenes with thousands of moving lights, where we compare our method to ray tracing, to conventional shadow mapping, and to imperfect shadow maps.","Authors":"L. Wang; X. Liang; C. Meng; V. Popescu","DOI":"10.1109\/TVCG.2018.2828422","Keywords":"Real time rendering;many lights;visibility determination;photorealism","Title":"Fast Ray-Scene Intersection for Interactive Shadow Rendering with Thousands of Dynamic Lights","Keywords_Processed":"visibility determination;real time render;many light;photorealism","Keyword_Vector":[0.0670955641,0.0958341326,-0.0324996036,0.0197542524,0.0057301334,-0.0316089949,0.0012497382,-0.0381484377,-0.027678115,-0.0251812118,0.0206785354,0.0208134877,-0.0412834109,0.0609661803,0.0946532725,-0.0249655777,-0.0429570044,-0.1023091766,0.0386968206,-0.0495703255,-0.0142593184,-0.0278302057,-0.0041945058,0.0144130698,-0.0073784285,-0.0322130715,0.1376422857,0.0330270871,-0.0636626328,0.0405764401,-0.053516585,0.0313341954,-0.0134016964,-0.0639522919,0.0292427884,0.0563146808,-0.0652348603,-0.0510627911,-0.0278881269,-0.0358920477,0.0272482686,0.0173286152,0.0327058212,0.0352394987,0.0031863388,-0.0435107408,0.0129563219,-0.0112847815,0.0305526722,0.0132589126,-0.0169235901,0.0301435758],"Abstract_Vector":[0.2262800216,0.1534212025,-0.1180213625,0.0063966832,-0.0381963489,0.1052485746,0.0792200853,0.0653890917,0.0948052406,0.0388510592,0.0263387049,-0.003763086,-0.1006669581,-0.1219071748,-0.0542770252,-0.065564658,-0.0399956199,0.0740700741,-0.0745616795,0.0079742487,-0.0272298384,0.0798603081,-0.0571508493,-0.0741019045,0.0068938126,-0.0364608238,-0.0728640903,-0.0185999126,-0.0343003418,0.0399717078,-0.0160482263,-0.0032073519,-0.0026866173,-0.0274757475,-0.0071636046,-0.0339635873,-0.0320473056,-0.0530019514,-0.0107658616,-0.084757294,0.021474175,-0.003626649,0.0048437174,-0.0253474845,-0.0057350343,-0.0463767432,0.0045828667,-0.029661336,0.0188252653,-0.0210043227,0.0112875986,-0.0176213664,-0.0088520733,-0.0448284077,-0.0236260471,-0.0535713346,0.0470161987,0.0274136366,-0.0030156305,-0.0084781826,-0.0460244021,0.0219060755,-0.008454129,0.0321176916,0.0084129832,-0.0311101105,0.0014368202,0.0177638871,0.0086737561,-0.026261903,-0.0005930188,-0.0020655286,0.0267178757,0.0593068199,0.0042030365,-0.0504394274,-0.0091057406,-0.0170317312,0.0136124232,-0.0357190828,0.0592931992,-0.0032422059,0.0589311922,0.0239803919,0.0057382004,0.0150933644,0.0123262923,0.0382890619,0.0167489809,0.026117733,0.04380914,0.0134162353,-0.0750907842,0.0220832768,0.04446225,-0.0271160101,-0.0101313023,0.0026013543,-0.0102893599,0.0103487449,-0.0093553897,-0.0104428827,-0.0052811274,0.0029693211,0.0663632098,0.016313829,-0.0407415776,0.0181447566,0.0185490733,0.0167845803,-0.0190204656,-0.0008229413,0.0118039748,-0.0179978129,-0.0346788703,0.0220137766]},"234":{"Abstract":"We provide a qualitative and quantitative evaluation of eight clear sky models used in Computer Graphics. We compare the models with each other as well as with measurements and with a reference model from the physics community. After a short summary of the physics of the problem, we present the measurements and the reference model, and how we \u201cinvert\u201d it to get the model parameters. We then give an overview of each CG model, and detail its scope, its algorithmic complexity, and its results using the same parameters as in the reference model. We also compare the models with a perceptual study. Our quantitative results confirm that the less simplifications and approximations are used to solve the physical equations, the more accurate are the results. We conclude with a discussion of the advantages and drawbacks of each model, and how to further improve their accuracy.","Authors":"E. Bruneton","DOI":"10.1109\/TVCG.2016.2622272","Keywords":"Clear sky;atmospheric scattering;model;measurements;evaluation","Title":"A Qualitative and Quantitative Evaluation of 8 Clear Sky Models","Keywords_Processed":"clear sky;measurement;atmospheric scattering;evaluation;model","Keyword_Vector":[0.0342469936,0.0246400169,-0.0273881238,0.0033110122,0.017644336,-0.0379452664,-0.0104867314,-0.0064521984,-0.0323925208,-0.0113369434,-0.0263593153,0.0217531306,-0.0145780954,0.0320692061,-0.0011027953,0.006391704,-0.0748381647,-0.0334897039,-0.0188784179,0.0122175097,-0.0027817681,-0.0748115753,0.039869953,0.0008682679,-0.04198834,0.0071859472,-0.014359372,0.0441564987,-0.0529273361,0.0231812961,0.0377932925,-0.0120124991,0.0007194592,0.0453105343,-0.0231469463,0.0835459228,-0.1468767476,-0.0262482393,0.0799011406,-0.0780637145,-0.0001541965,0.0396570158,-0.1219610767,-0.0850098458,-0.0278815065,0.0337654699,0.1231783183,0.0457785804,-0.065885683,0.016917426,0.065769902,0.065986418],"Abstract_Vector":[0.1414846009,0.0271618929,0.0290971597,-0.002327937,-0.0611414327,0.0727780304,-0.0172080739,-0.0102317381,-0.0022658876,-0.001087608,-0.0206436791,0.0251662081,-0.0230456582,-0.0059639203,0.0093401786,0.0067170633,0.0285490179,0.0055572224,-0.0024086711,-0.0602759301,0.0304673788,-0.0132161792,0.0064300779,-0.0069711635,-0.0058847195,-0.0026483048,0.0193181654,0.0163646128,-0.0096610787,-0.0202258978,0.0002186347,-0.0161133758,0.0071177518,0.0240093638,-0.0251825263,0.0162808406,0.0254789385,-0.0076477297,-0.006781849,0.0174279139,0.0046444047,0.0326146558,0.0089959207,0.0032677014,-0.0252525036,0.0149454911,0.0153596466,0.0169630767,-0.0120576913,0.0221879556,-0.010256521,0.0343489658,0.0009428024,0.0078289539,-0.0069103671,0.0189583713,-0.0033485058,0.0248885422,-0.0198807053,0.0172783428,-0.0021288626,0.0025225919,-0.0076882928,-0.0322361108,0.0055845205,-0.0083000042,0.0016452311,-0.0044747776,0.0158201216,0.0079080662,-0.000119805,0.0137643791,-0.0142874716,-0.0140705915,0.0335237294,0.0211860951,0.0202680938,-0.0045255953,-0.0004529223,0.0342690298,-0.0036095506,0.0066116453,0.0338510786,0.0104994956,-0.0012856662,-0.0193914298,0.008746461,-0.0122247461,-0.0363929914,-0.0126390107,-0.0093576896,0.0203840333,0.0036740416,0.0035735857,0.0263093072,-0.003439886,-0.0098735776,0.0036866326,-0.0012675046,0.0124325366,0.0146067638,-0.0032038455,-0.0030492317,-0.0343963824,0.0156900459,-0.0428026815,-0.001139886,-0.0611183068,-0.0195897032,-0.0082261711,0.0106504885,-0.0120964025,0.0042042086,-0.0152140464,-0.0160103473,0.0038418149]},"235":{"Abstract":"Pattern analysis of human motions, which is useful in many research areas, requires understanding and comparison of different styles of motion patterns. However, working with human motion tracking data to support such analysis poses great challenges. In this paper, we propose MotionFlow, a visual analytics system that provides an effective overview of various motion patterns based on an interactive flow visualization. This visualization formulates a motion sequence as transitions between static poses, and aggregates these sequences into a tree diagram to construct a set of motion patterns. The system also allows the users to directly reflect the context of data and their perception of pose similarities in generating representative pose states. We provide local and global controls over the partition-based clustering process. To support the users in organizing unstructured motion data into pattern groups, we designed a set of interactions that enables searching for similar motion sequences from the data, detailed exploration of data subsets, and creating and modifying the group of motion patterns. To evaluate the usability of MotionFlow, we conducted a user study with six researchers with expertise in gesture-based interaction design. They used MotionFlow to explore and organize unstructured motion tracking data. Results show that the researchers were able to easily learn how to use MotionFlow, and the system effectively supported their pattern analysis activities, including leveraging their perception and domain knowledge.","Authors":"S. Jang; N. Elmqvist; K. Ramani","DOI":"10.1109\/TVCG.2015.2468292","Keywords":"Human motion visualization;interactive clustering;motion tracking data;expert reviews;user study;Human motion visualization;interactive clustering;motion tracking data;expert reviews;user study","Title":"MotionFlow: Visual Abstraction and Aggregation of Sequential Patterns in Human Motion Tracking Data","Keywords_Processed":"user study;human motion visualization;motion tracking datum;interactive clustering;expert review","Keyword_Vector":[0.0609081058,-0.0434298246,-0.0284291195,0.055389247,-0.0244010142,0.0148732926,-0.004825022,0.0071747556,-0.0150153063,-0.0372660603,0.0228026851,-0.0007257778,0.0163641394,0.1237094673,-0.0635778817,0.0080866906,0.0121748163,-0.0046330676,0.0121580692,0.0163777338,0.0042753521,-0.0356504313,0.0222813529,-0.007248621,0.0489537774,-0.0168904255,-0.0184326255,-0.025077972,-0.0071018454,0.0611110245,-0.0509760084,-0.0193550935,0.017955881,-0.0093656274,0.0084380575,0.0592659657,0.075576925,-0.0080770938,-0.0171510896,0.0127556602,-0.0011877835,0.0059619611,-0.0253384821,0.0746501474,0.0154371099,0.0494846694,-0.1035699024,-0.0161972767,-0.0724423301,0.0396007073,-0.0152197097,0.0662182883],"Abstract_Vector":[0.1991913261,-0.0898554935,0.0151183162,-0.0284623795,0.016185263,-0.0988035359,-0.0175832389,-0.0354224858,0.0254897101,0.0035474341,0.0247691405,-0.0056605729,-0.0734545798,0.017605793,0.0055426638,-0.027248738,0.000440427,0.0304808787,-0.0365665015,0.0026137552,0.0356879189,-0.0478572024,-0.0021128654,-0.0574915361,-0.0025880643,0.0206127497,-0.0045831822,0.0310668549,-0.0102415394,-0.0126210015,0.0043139247,0.0073861562,0.0030296447,0.0548950058,-0.0783522771,0.0153506442,0.0117689601,0.0331669554,-0.0709541676,0.0422890477,-0.0189578106,0.0217370071,0.0079268865,-0.0084463908,0.0836534614,-0.0219022361,-0.0229978585,-0.033502435,-0.0647771979,-0.0475761261,-0.054808493,0.0152345822,0.0170268853,0.0000090301,0.0122959545,-0.0047205715,0.0436974255,-0.0689048446,-0.0022650286,0.0275114604,0.0150374623,0.0707126159,-0.0029073258,-0.0244676848,0.098235026,-0.0154301706,-0.0413519985,0.0517798698,0.015119231,-0.0201014525,0.0549704981,0.0659022319,0.0858204069,-0.0165617688,-0.0203231223,0.0191585704,-0.0367344912,0.0348741807,-0.0353250001,0.0213436149,0.0607222135,0.0233304274,0.033981576,0.0034144817,-0.0478116131,0.0194010944,0.0502991418,-0.0371913986,-0.0036853493,0.021227223,-0.0095919302,-0.0329584862,0.0020887961,-0.0505054509,0.0214888078,-0.0211996004,0.0335418478,-0.0433035362,-0.0256543816,0.0094320841,0.0108312319,0.021707513,0.0068569692,0.020697199,-0.0043913242,-0.0141886065,0.0548861145,-0.0579274593,0.0144118988,-0.0228656134,0.0240585273,0.0385043481,0.061431388,0.0182870007,-0.0017883437,0.0105028614]},"236":{"Abstract":"Moving sand pictures are interesting devices that can be used to generate an infinite number of unique scenes when repeatedly being flipped over. However, little work has been done on attempting to simulate the process of picture formulation. In this paper, we present an approach capable of generating images in the style of moving sand pictures. Our system defines moving sand pictures in a few steps, such as initialization, segmentation and physical simulation, so that a variety of moving sand pictures including mountain ridges, desert, clouds and even regular patterns can be generated by either automatic or semi-automatic via interaction during initialization and segmentation. Potential applications of our approach range from advertisements, posters, post cards, packaging, to digital arts.","Authors":"M. Zhang; H. Lin; K. Zhang; J. Yu","DOI":"10.1109\/TVCG.2017.2779799","Keywords":"Moving sand picture;tessellation;segmentation;color arrangement;physical simulation","Title":"Computer Simulation and Generation of Moving Sand Pictures","Keywords_Processed":"move sand picture;physical simulation;tessellation;segmentation;color arrangement","Keyword_Vector":[0.2992147009,-0.1920028019,-0.0378977425,-0.1921168289,0.0144416709,-0.0391935792,-0.182352117,-0.0649760599,-0.0690921444,0.0633667379,-0.0110697968,-0.1579051473,0.0686229676,0.1287729472,-0.0813990881,-0.0553887638,0.0932908314,0.030692102,0.0662030122,-0.130757028,-0.0134150452,0.1221722562,0.0541367997,0.0080775738,-0.0012132148,-0.0283010641,0.0373748578,-0.1015973105,-0.0244674939,0.0948760345,-0.0563531744,0.0126830501,-0.078178336,-0.0055637647,0.0486080299,0.1166850958,-0.0173641569,0.1002081558,0.0483048312,0.0216848139,-0.0675694983,-0.0992345668,-0.0007110124,-0.083520366,0.0560192756,0.0067143747,-0.0471270762,0.0210410316,-0.0432868152,-0.0416268733,0.018015375,0.0625194502],"Abstract_Vector":[0.190224927,-0.1456910294,-0.0054538672,-0.0243115888,-0.0681685236,-0.0132137696,0.0109148513,0.0099891397,-0.0390272353,-0.0384462754,0.001800511,0.004879521,0.0673968156,-0.01772433,-0.0039119846,-0.0261995292,-0.0449896327,0.0134122299,-0.0062741387,-0.0070887817,-0.0165248188,-0.0064928377,-0.0271440665,-0.0124811855,-0.0210967177,-0.0293495745,-0.0246054813,0.0009978573,-0.0343525989,-0.0314176156,-0.002873482,-0.0299362421,0.0320720099,-0.0026013371,-0.0345021653,0.0485283489,0.0170812088,0.0171724243,0.0206075277,0.0058308214,-0.0167073084,-0.0319546646,0.0514765295,-0.0471989736,0.0193222711,-0.0063906446,0.0089602467,-0.0377729941,-0.0094619996,0.004943686,-0.0107521726,-0.0091333688,0.0266591726,-0.026977424,0.016834729,0.0008487368,-0.0285319905,-0.0034322659,0.0083510235,-0.0067938231,-0.0037687465,-0.0201210249,-0.0197809497,-0.0170348204,0.0066368659,0.0435480565,0.0361952983,0.0079309585,-0.0242475192,-0.0017343011,0.0083271145,0.0278627716,-0.0223600032,-0.0057352085,-0.0074467974,0.03866341,-0.0088323248,-0.0113177326,0.0068366619,0.0273584593,-0.0174878675,-0.0094948234,0.0111709892,0.0024054985,0.0125882132,0.0117149126,0.0654759298,-0.0034394354,-0.056541346,0.0116920398,0.0122484767,0.0027909216,0.0050094106,-0.0301352165,0.0002821925,-0.0359598496,0.0101169653,-0.0205088961,0.0252805289,-0.0084568644,0.0419651664,-0.0169700595,0.0227636603,-0.0247388661,0.0276702878,-0.0079190854,-0.0054871468,-0.0023813765,-0.0057882606,0.0131393969,0.0137985504,-0.0051477615,0.0038733915,-0.0002180797,0.0181248425,0.0079646166]},"237":{"Abstract":"Wall-displays allow multiple users to simultaneously view and analyze large amounts of information, such as the increasingly complex graphs present in domains like biology or social network analysis. We focus on how pairs explore graphs on a touch enabled wall-display using two techniques, both adapted for collaboration: a basic localized selection, and a propagation selection technique that uses the idea of diffusion\/transmission from an origin node. We assess in a controlled experiment the impact of selection technique on a shortest path identification task. Pairs consistently divided space even if the task is not spatially divisible, and for the basic selection technique that has a localized visual effect, it led to parallel work that negatively impacted accuracy. The large visual footprint of the propagation technique led to close coordination, improving speed and accuracy for complex graphs only. We then observed the use of propagation on additional graph topology tasks, confirming pair strategies on spatial division and coordination.","Authors":"A. Prouzeau; A. Bezerianos; O. Chapuis","DOI":"10.1109\/TVCG.2016.2592906","Keywords":"Wall-displays;multi-user interaction;graph visualization;selection techniques;co-located collaboration","Title":"Evaluating Multi-User Selection for Exploring Graph Topology on Wall-Displays","Keywords_Processed":"graph visualization;selection technique;co locate collaboration;multi user interaction;Wall display","Keyword_Vector":[0.0614878202,-0.004888811,-0.0323391123,-0.0526362181,0.0142539756,-0.0646678167,-0.093869837,-0.0235147737,-0.1595902625,0.030610373,0.0082589695,0.1162363289,0.0309180772,0.0374737887,0.0298592912,0.063686876,0.0555638242,0.1018693981,0.1985031726,-0.0625647424,-0.065642941,0.1735112577,0.2235486556,0.064712653,-0.1237325702,-0.0771833325,0.0685854677,-0.0489243484,0.0919876743,-0.1741501625,-0.0490613154,-0.110064355,0.061581035,0.0302549191,-0.0178776699,-0.0272372887,0.0244085647,-0.0145150754,-0.0320459871,-0.0320838716,-0.0558452107,0.0458776759,0.0754239584,0.0695704166,-0.0232551981,-0.0338915726,0.0209949104,0.0557750247,0.0015719653,0.0862925809,-0.0172612192,0.038813911],"Abstract_Vector":[0.1344803999,-0.0399770565,-0.0094585648,-0.0039732878,-0.0273440854,0.0592026797,0.0187606279,0.02902523,0.0407984835,-0.0007746765,-0.0342810817,0.0590481107,-0.0140047149,-0.0019465808,0.0174072101,-0.0032958886,-0.0206833833,0.006190158,0.0227851537,-0.0255406966,0.0368134017,-0.0318039296,0.0277561934,-0.004082743,-0.007660631,0.0138680188,0.0039027836,-0.0223525778,-0.0182347395,0.0019608196,0.055049794,0.0337955911,-0.0380417337,-0.0154293571,0.0269682654,-0.0245225633,-0.0288905562,-0.0199140952,-0.0200476634,-0.0081608946,-0.0030774346,0.0161514646,0.0280955817,-0.0117845522,0.0308927706,-0.0033734531,-0.01492339,0.0209419592,0.0117998642,-0.039805033,0.0088770656,-0.0248595546,-0.0088910074,-0.03196818,0.0010188566,-0.0023599194,-0.0009263024,0.0063020062,-0.031131979,-0.0361718573,0.0074960317,-0.0005090913,-0.0112753167,0.004094674,-0.0009647589,-0.0637562467,0.0087266733,-0.0269087452,-0.0239193728,0.0317760335,0.0019292312,0.0097784024,0.0024530022,0.0167171747,0.0073346614,-0.0903839393,0.0635482539,0.0356747122,-0.0003404841,-0.0337112933,0.0052040829,0.0065848535,-0.018513109,0.038968503,0.0403100102,0.0351560713,0.0039291986,0.0076822477,-0.049532193,-0.0132203247,-0.003921376,0.0033143314,0.0011078854,0.045364162,0.0000846779,-0.0452301224,0.0342896327,-0.0083298292,-0.0121825236,0.0035788629,0.002794733,0.021212916,-0.0336453965,0.0163507742,0.0353540444,0.0128245745,0.0272903021,0.0119791583,0.012535009,0.0020049061,-0.0182855452,0.046042962,0.0376147663,-0.0135182707,0.0347368425,0.0377378712]},"238":{"Abstract":"Shader lamp systems augment the real environment by projecting new textures on known target geometries. In dynamic scenes, object tracking maintains the illusion if the physical and virtual objects are well aligned. However, traditional trackers based on texture or contour information are often distracted by the projected content and tend to fail. In this paper, we present a model-based tracking strategy, which directly takes advantage from the projected content for pose estimation in a projector-camera system. An iterative pose estimation algorithm captures and exploits visible distortions caused by object movements. In a closed-loop, the corrected pose allows the update of the projection for the subsequent frame. Synthetic frames simulating the projection on the model are rendered and an optical flow-based method minimizes the difference between edges of the rendered and the camera image. Since the thresholds automatically adapt to the synthetic image, a complicated radiometric calibration can be avoided. The pixel-wise linear optimization is designed to be easily implemented on the GPU. Our approach can be combined with a regular contour-based tracker and is transferable to other problems, like the estimation of the extrinsic pose between projector and camera. We evaluate our procedure with real and synthetic images and obtain very precise registration results.","Authors":"N. Gard; A. Hilsmann; P. Eisert","DOI":"10.1109\/TVCG.2019.2932223","Keywords":"Projector-camera systems;projector-camera calibration;shader lamp systems;object tracking;object registration;spatial augmented reality;projection mapping","Title":"Projection Distortion-based Object Tracking in Shader Lamp Scenarios","Keywords_Processed":"spatial augmented reality;projector camera calibration;projector camera system;shad lamp system;projection mapping;object registration;object tracking","Keyword_Vector":[0.2146233783,-0.2001223641,-0.0501330663,0.0760163403,-0.0746216054,0.0184540638,0.2735606818,-0.067704176,0.0139040037,-0.1145314127,0.0083575295,0.0533951327,0.1134506411,-0.1455603689,-0.0204173286,-0.0515597697,0.004077119,-0.095765291,-0.0544170567,-0.0449391539,-0.0472257659,0.0429813486,0.0871909827,0.0610817178,0.0394071897,0.0132264189,-0.0200664194,0.0312258492,0.0726451789,-0.1331588143,-0.0194509591,0.0951423598,-0.1541220562,-0.0114553689,0.0797694984,-0.1021045957,-0.081492034,-0.0141236398,0.0653217733,-0.1357216879,-0.0137229433,0.2013333318,-0.0039173966,-0.0189527039,-0.0370332824,0.0165977024,-0.0335371874,0.0015532029,-0.0095719518,-0.0173431673,0.0128905531,-0.0388543432],"Abstract_Vector":[0.2309478693,-0.0806621596,0.0971409014,-0.0370129223,0.06398515,-0.0440401332,0.1328080639,-0.0860388507,0.0397845699,0.0340457436,0.0076215444,-0.0035474422,-0.0806402853,0.0431232702,-0.0458802834,0.0115978601,-0.0019991436,-0.0385051897,-0.0255377805,0.0274516903,-0.0119608546,-0.0210801365,0.0206695982,0.0508800827,-0.0073844624,0.009365208,0.0764529431,-0.0131631827,-0.0981842272,-0.0353590674,0.0427834666,0.0458493569,0.0014471367,-0.0093784512,-0.0091809301,-0.0044347981,-0.055852621,-0.0134060186,-0.009824715,0.0111733005,0.0227672687,0.0074368514,-0.0058390758,-0.0081395863,0.0338779373,0.0163853671,-0.0238160712,0.0238809841,0.0069918567,0.0172825268,-0.0279364303,-0.0028632592,-0.0541260795,0.0156689557,-0.0352018103,-0.0501185849,0.0286698928,0.0009059288,-0.0332707683,0.0393770374,-0.0160299648,0.006165085,0.018598269,-0.0100051821,-0.0114471788,-0.0255812428,-0.003653638,-0.0186272727,0.0010252479,-0.009167488,-0.0250014815,-0.0258724843,-0.0112304001,-0.0195477908,-0.0497681651,0.0147232619,0.0151353039,0.011387971,0.0069969447,0.0343851009,0.0325031669,0.0304849436,-0.0353712142,-0.0149434297,-0.0399051883,0.0024865567,-0.0229285746,0.0048890846,0.0165565461,0.0285253792,-0.0299899886,0.0423027335,0.0567121061,-0.00905548,-0.0056046351,0.0041596981,-0.0071066383,0.0341764897,-0.004654216,-0.0233897144,-0.0130651565,0.0065025725,-0.0046000573,-0.0106681883,-0.0186035233,-0.0140119549,-0.0212093568,0.0063169903,-0.0397475652,-0.0092967215,0.0244345605,0.0309508354,0.0126698337,0.0257968064,0.0043701767,0.0064154742]},"239":{"Abstract":"Semi-automatic text analysis involves manual inspection of text. Often, different text annotations (like part-of-speech or named entities) are indicated by using distinctive text highlighting techniques. In typesetting there exist well-known formatting conventions, such as bold typeface, italics, or background coloring, that are useful for highlighting certain parts of a given text. Also, many advanced techniques for visualization and highlighting of text exist; yet, standard typesetting is common, and the effects of standard typesetting on the perception of text are not fully understood. As such, we surveyed and tested the effectiveness of common text highlighting techniques, both individually and in combination, to discover how to maximize pop-out effects while minimizing visual interference between techniques. To validate our findings, we conducted a series of crowd-sourced experiments to determine: i) a ranking of nine commonly-used text highlighting techniques; ii) the degree of visual interference between pairs of text highlighting techniques; iii) the effectiveness of techniques for visual conjunctive search. Our results show that increasing font size works best as a single highlighting technique, and that there are significant visual interferences between some pairs of highlighting techniques. We discuss the pros and cons of different combinations as a design guideline to choose text highlighting techniques for text viewers.","Authors":"H. Strobelt; D. Oelke; B. C. Kwon; T. Schreck; H. Pfister","DOI":"10.1109\/TVCG.2015.2467759","Keywords":"Text highlighting techniques;visual document analytics;text annotation;crowdsourced study;Text highlighting techniques;visual document analytics;text annotation;crowdsourced study","Title":"Guidelines for Effective Usage of Text Highlighting Techniques","Keywords_Processed":"text highlight technique;visual document analytic;text annotation;crowdsourced study","Keyword_Vector":[0.0529392978,-0.0009447434,-0.0273177933,0.0183649055,0.0261072489,-0.0810686205,-0.0255982568,-0.0500896622,-0.0393748024,0.0093610105,-0.0247168695,0.0468202039,-0.0439851197,0.0916039353,-0.0484410665,-0.0140872241,-0.1456063125,-0.0344416919,-0.0132010232,0.1207841405,0.0133731193,-0.0023815691,-0.1484898389,0.1636527426,-0.0000035991,-0.1999903976,-0.1686355304,-0.0196908393,-0.0328840632,-0.0881997019,0.0646749332,-0.1150591624,-0.0913070739,0.0053521993,0.0119127722,0.0750775828,-0.0465124493,-0.0322552369,0.0421192092,0.0537926199,-0.0441818557,0.0019295231,-0.010129397,-0.0469283152,-0.0475453708,0.0127807803,-0.0009967409,-0.0000852603,0.03217122,-0.0056405733,-0.0004510907,-0.0334788748],"Abstract_Vector":[0.0769219053,-0.0429406591,0.0114311193,0.0005392059,-0.0098013228,0.0198757967,0.0099464584,0.0061179891,-0.0300158676,0.0039500188,-0.0101696294,0.0183321227,-0.0051167529,-0.0067274694,0.0091982006,-0.0169994588,-0.0051666634,0.0093956845,-0.0093816681,0.007950109,-0.0047791703,-0.0096175171,0.0094816725,-0.0001929799,-0.0162544347,0.0111401252,0.0043762582,0.0037843957,0.0005671798,0.0022656701,0.02677756,0.0156823462,0.0070306856,-0.0023020324,-0.0173369376,0.0121050197,0.0055032699,0.0230105115,-0.0043652264,0.004673674,-0.0042418071,-0.009418085,0.0025256951,-0.0054084058,0.0254018395,-0.0218928853,0.0236060379,-0.0048851693,0.0013907394,0.0042280675,0.0029707148,0.0288222653,-0.0096023791,0.0288086379,-0.0277321219,0.0149371355,-0.0303013273,0.0174296711,-0.0035525863,0.0030076439,0.024247081,0.0140985528,0.0180169956,0.0040093653,0.0135633277,-0.0327008345,-0.0003579818,0.0086783727,-0.0095288282,0.0049874851,0.0012626587,0.0126156556,-0.0265248693,0.0247052717,0.0078378619,0.0142197722,0.0054726438,0.0247158002,-0.0044780621,-0.0131661009,0.0222189123,0.0049898642,-0.0085941807,-0.0206467984,-0.0199646964,-0.0117811955,0.0177443033,0.0147353435,0.0234063167,0.0157262123,0.006061517,-0.0074604402,0.0117475767,0.0154878236,0.0208480023,-0.0057055666,-0.0081592833,0.0270197231,0.0020799158,0.0125031125,-0.0010712315,-0.0061888123,0.0386349186,0.0161073123,-0.005368947,-0.0099876179,0.0012891273,-0.0053272308,-0.0045375985,0.0089989236,0.0065827207,0.017275346,0.0201495551,-0.0012020107,-0.0131014326,0.0156159735]},"24":{"Abstract":"We propose a unified mathematical model for multilayer-multiframe compressive light field displays that supports both attenuation-based and polarization-based architectures. We show that the light field decomposition of such a display can be cast as a bound constrained nonlinear matrix optimization problem. Efficient light field decomposition algorithms are developed using the limited-memory BFGS (L-BFGS) method for automultiscopic displays with high resolution and high image fidelity. In addition, this framework is the first to support multilayer polarization-based compressive light field displays with time multiplexing. This new architecture significantly reduces artifacts compared with attenuation-based multilayer-multiframe displays; thus, it can allow the requirements regarding the number of layers or the refresh rate to be relaxed. We verify the proposed methods by constructing two 3-layer prototypes using high-speed LCDs, one based on the attenuation architecture and one based on the polarization architecture. Moreover, an efficient CUDA-based program is implemented. Our displays can produce images with higher spatial resolution with thinner form factors compared with traditional automultiscopic displays in both simulations and experiments.","Authors":"J. Zhang; Z. Fan; D. Sun; H. Liao","DOI":"10.1109\/TVCG.2018.2810279","Keywords":"Compressive light field display;multilayer-multiframe LCD display;optimization methods;polarization-based display","Title":"Unified Mathematical Model for Multilayer-Multiframe Compressive Light Field Displays Using LCDs","Keywords_Processed":"optimization method;compressive light field display;multilayer multiframe lcd display;polarization base display","Keyword_Vector":[0.2840027551,-0.2378303309,-0.1176602213,-0.0601863619,0.0184371072,-0.1265851072,0.316271561,-0.0703108203,-0.03236372,-0.096678527,-0.0001556999,0.1160801602,-0.0559151991,-0.1133440318,0.0408556508,0.1356449341,-0.0646698365,0.0317362408,-0.0659402815,-0.1619860677,0.1665087569,0.1366647405,-0.0947303361,-0.050361857,0.1913697631,0.0190996276,0.0269314253,0.1474280284,-0.073532874,-0.1703184363,-0.1275364031,-0.1462620277,0.0778438255,-0.132205937,-0.1461663406,-0.1303090764,-0.0518448159,-0.0918621631,0.1083167647,-0.05302002,0.0299824456,0.0041889581,-0.1321096456,-0.0068210014,-0.0512362366,0.0711104951,-0.0748307851,-0.0322585848,-0.048827126,0.0119702932,-0.028202806,0.0244620446],"Abstract_Vector":[0.2123644273,-0.1738165998,0.021272087,0.1287528502,0.1106902044,0.0943066889,0.0515044796,-0.0420319222,-0.1230796675,0.1457506451,0.087627812,-0.1536114413,-0.0837727547,0.1112793756,0.2045851287,-0.1137924895,-0.0029050823,0.0423615789,0.0383585761,-0.0592993358,-0.0463597239,0.0195962709,-0.017685776,-0.0052662546,0.0240647784,-0.0341901863,0.0218988154,-0.0489557065,-0.0115069537,0.039431273,0.0119691689,-0.0165952466,-0.0104864951,-0.0558776362,0.0622669852,-0.0200737316,0.0121724831,0.0156554287,-0.0198460578,0.0289207474,-0.0456039064,0.0192013644,-0.0420005325,0.0340321163,0.0048944058,-0.0390884528,0.0527207003,-0.0235331655,-0.0376562964,0.0180733274,0.0363883881,0.006602729,-0.0080635855,0.010313098,0.0028655781,-0.0052210501,0.0325262868,0.0140126531,-0.0137770339,0.0809336236,-0.0676691236,0.0284322227,-0.0099317636,-0.031503782,0.0041363245,-0.0299489318,-0.0071333332,-0.0080758462,-0.012715593,0.0386693372,-0.0121738692,-0.0110240337,-0.0337439292,-0.0129975112,-0.0142889718,-0.0250468728,-0.0116837148,-0.037943225,-0.0172850459,-0.0143382174,0.0101578364,-0.0027728061,-0.017747377,-0.0526188507,0.0116391117,0.043393502,-0.0112562195,0.0324253633,0.0398935968,-0.018213942,-0.008086202,-0.0124987417,0.0151908334,-0.0031040225,-0.0094530595,0.0025182823,0.023258104,-0.0318133348,-0.0218666126,0.0003048695,-0.018737705,0.0130754544,-0.0177495972,0.0156020601,0.0124505099,-0.0208418103,-0.017172381,0.0037884058,-0.0001927402,0.0169129806,0.0041427309,0.0173647405,-0.0104868895,-0.0275734067,0.0217096961,0.0006026701]},"240":{"Abstract":"Illumination effects in translucent materials are a combination of several physical phenomena: refraction at the surface, absorption and scattering inside the material. Because refraction can focus light deep inside the material, where it will be scattered, practical illumination simulation inside translucent materials is difficult. In this paper, we present an a Point-Based Global Illumination method for light transport on homogeneous translucent materials with refractive boundaries. We start by placing light samples inside the translucent material and organizing them into a spatial hierarchy. At rendering, we gather light from these samples for each camera ray. We compute separately the sample contributions for single, double and multiple scattering, and add them. We present two implementations of our algorithm: an offline version for high-quality rendering and an interactive GPU implementation. The offline version provides significant speed-ups and reduced memory footprints compared to state-of-the-art algorithms, with no visible impact on quality. The GPU version yields interactive frame rates: 30 fps when moving the viewpoint, 25 fps when editing the light position or the material parameters.","Authors":"B. Wang; N. Holzschuch","DOI":"10.1109\/TVCG.2017.2768525","Keywords":"Participating media;point-based rendering;interactive editing","Title":"Point-Based Rendering for Homogeneous Participating Media with Refractive Boundaries","Keywords_Processed":"interactive editing;participate medium;point base render","Keyword_Vector":[0.1477118139,-0.1166199288,-0.0425020466,0.0807220474,-0.0968366718,-0.0324619774,0.053024188,-0.0222728733,0.0753340101,-0.0091444798,0.0247577827,0.0081061526,-0.0383420547,-0.0402538231,0.0454868146,0.1598784892,-0.0086132819,0.0308854862,-0.0825396251,-0.0218930358,-0.0132449112,0.0537038522,0.0167050862,0.0934877592,-0.058172308,0.0182601543,-0.0165576818,-0.0006289879,-0.0794790067,0.0308562877,-0.0024434266,-0.0195147384,-0.0323419175,0.0627408267,0.0381107924,-0.0061457779,-0.0148419738,0.1314301518,0.0244909244,0.1236437579,0.0544672048,-0.0236496515,0.0229012576,0.1146175377,0.0432787617,-0.0418873337,0.1062366408,-0.0111657747,-0.0426165144,0.0008695376,-0.0599754599,0.0457067384],"Abstract_Vector":[0.1429468461,-0.0573562311,-0.0011809082,0.0062326365,-0.0289202003,-0.0096648496,0.0231267461,0.0050953896,-0.0441133221,0.0381045555,0.0196846157,-0.008014126,-0.062411281,-0.0223605079,-0.0214545105,0.0166103798,-0.0071342732,-0.0210858433,-0.0236352922,0.0254718532,-0.0095638076,0.0056238652,-0.0183123405,-0.0210302878,-0.0401296832,0.0117517765,-0.0086046454,0.0032733044,0.0274295033,-0.0070342406,0.0123086046,0.0346697483,-0.0431874052,-0.0228892359,-0.0251111805,0.0160141173,-0.0011874552,-0.0135289855,0.0475076489,-0.0072402112,0.0235643349,-0.0297237361,-0.0404669339,0.0336876645,0.0239135342,0.0127984677,-0.0055017905,-0.0197405024,-0.0203974063,-0.037806897,0.0072383954,-0.0022101442,0.0256502206,-0.0107971255,0.0603933327,0.0206510978,0.0296575921,-0.0320229303,0.0094804828,-0.0135275554,0.0116181527,-0.0158567875,-0.0356076832,-0.0445023636,-0.0126503352,-0.0227750402,0.0233396205,-0.0173151296,0.0674332462,0.0160193435,-0.0108178609,-0.0010754035,-0.0106868939,-0.0299866905,-0.0212457834,-0.0052957232,-0.0359874493,0.0075094321,0.0056227371,0.0124121555,0.0007798679,-0.0099103738,0.0069308256,0.0064542442,0.0085116257,0.0222990848,0.0276611514,-0.0283798017,-0.0228554784,-0.0410861867,0.0101840264,0.0196029337,-0.0003140127,0.0326628071,0.0218085867,0.0072298829,-0.0071673275,-0.0213945356,0.0168805031,0.0169993499,-0.0256164138,-0.0199117903,-0.0321355059,0.0246639992,-0.010896146,-0.0150395962,-0.0181184614,-0.021056702,-0.0116212203,0.0094890146,-0.0045549768,-0.0006331138,0.0066915245,0.0150036844,0.0002953414,-0.0289788289]},"241":{"Abstract":"Diffusion Tensor Imaging (DTI) is a magnetic resonance imaging modality that enables the in-vivo reconstruction and visualization of fibrous structures. To inspect the local and individual diffusion tensors, glyph-based visualizations are commonly used since they are able to effectively convey full aspects of the diffusion tensor. For several applications it is necessary to compare tensor fields, e.g., to study the effects of acquisition parameters, or to investigate the influence of pathologies on white matter structures. This comparison is commonly done by extracting scalar information out of the tensor fields and then comparing these scalar fields, which leads to a loss of information. If the glyph representation is kept, simple juxtaposition or superposition can be used. However, neither facilitates the identification and interpretation of the differences between the tensor fields. Inspired by the checkerboard style visualization and the superquadric tensor glyph, we design a new glyph to locally visualize differences between two diffusion tensors by combining juxtaposition and explicit encoding. Because tensor scale, anisotropy type, and orientation are related to anatomical information relevant for DTI applications, we focus on visualizing tensor differences in these three aspects. As demonstrated in a user study, our new glyph design allows users to efficiently and effectively identify the tensor differences. We also apply our new glyphs to investigate the differences between DTI datasets of the human brain in two different contexts using different b-values, and to compare datasets from a healthy and HIV-infected subject.","Authors":"C. Zhang; T. Schultz; K. Lawonn; E. Eisemann; A. Vilanova","DOI":"10.1109\/TVCG.2015.2467435","Keywords":"Glyph Design;Comparative Visualization;Diffusion Tensor Field;Glyph Design;Comparative Visualization;Diffusion Tensor Field","Title":"Glyph-Based Comparative Visualization for Diffusion Tensor Fields","Keywords_Processed":"Glyph Design;comparative visualization;diffusion Tensor Field","Keyword_Vector":[0.1247793492,0.1226289605,-0.0739679137,0.0048279328,0.0126677749,-0.0200020655,-0.0114024579,-0.0230049658,-0.0665145555,-0.0535896785,0.0547636193,0.0386539952,-0.0235128525,0.0561409154,0.1200417148,0.1357261143,-0.105279413,-0.1975932501,0.1955671791,-0.156808496,0.0910602155,-0.0515502448,-0.1083949776,0.0918256408,-0.0475595359,-0.0956172832,0.3021936461,0.0955336825,-0.0126552858,-0.0237399369,-0.1615730336,0.0338157235,0.002183025,-0.1165704044,0.047407177,0.0204657046,-0.0952143004,-0.0301384349,-0.0728739865,-0.0197126862,-0.0088060257,-0.0231904799,0.0334454721,0.01072552,-0.0143104424,0.0464955116,-0.0020663364,-0.0114552863,0.0088561274,0.0199569578,-0.0427468699,-0.065795278],"Abstract_Vector":[0.156241211,0.018973609,-0.0621677773,-0.0564564315,-0.0831534346,0.1271134475,0.0619028451,0.0291434003,0.0927855703,0.0952716887,-0.0996885713,0.0551120188,-0.0027272871,0.0149651843,-0.0433872889,-0.0721646932,0.0782602874,-0.0100704411,-0.0291289126,-0.0170033871,-0.1616990092,0.0114440674,0.3914507589,-0.0734301688,-0.071834037,-0.0725697902,0.0410934503,-0.0382818309,0.1768586127,0.0482000505,0.0527186335,-0.0172843013,-0.0367071885,0.0182963122,-0.0148223808,0.0103895559,0.0301727077,0.047351401,-0.0868079335,0.0223855308,0.0913301216,0.0307418468,-0.0080358271,0.028456874,-0.0567373812,0.0179525663,-0.041392504,0.0162636389,-0.026353583,-0.0267239781,0.0467017664,0.0744143111,0.0283746577,-0.0283794277,0.0414853178,-0.0540197368,-0.0487976668,0.0045332879,-0.0848974975,-0.0691957092,-0.0117818201,0.0352647581,-0.0214448135,0.0229323989,0.0434654316,0.0876504056,0.0024081863,-0.0071776688,-0.001753497,0.0574519385,0.0276206747,-0.0076504589,-0.0302184686,-0.0399385816,-0.029207013,-0.0046547375,0.0238270478,0.0071291896,0.011577095,0.007787736,0.0314024318,-0.0528599027,0.0339228227,0.0011101933,0.0041425723,0.0177151213,-0.0120766927,0.0336236735,-0.0251695028,0.0486331202,-0.0425371243,0.0132819639,-0.0178266467,0.0419338827,-0.0216393604,-0.0587816556,0.0232513424,-0.0139798935,0.0384585599,0.0028083424,0.0010760027,-0.008534308,-0.0080574882,0.0084953154,0.0188326016,0.0245198274,-0.0193096813,0.0517622692,0.0146746612,-0.0152241093,0.0678439721,-0.0569595739,0.0133926749,0.0052589675,0.0186212798,0.0246428906]},"242":{"Abstract":"Face sketch synthesis shows great applications in a lot of fields such as online entertainment and suspects identification. Existing face sketch synthesis methods learn the patch-wise sketch style from the training dataset containing photo-sketch pairs. These methods manipulate the whole process directly in the field of RGB space, which unavoidably results in unsmooth noises at patch boundaries. If denoising methods are used, the sketch edges would be blurred and face structures could not be restored. Recent researches of feature maps, which are the outputs of a certain neural network layer, have achieved great success in texture synthesis and artistic image generation. In this paper, we reformulate the face sketch synthesis problem into a neural network feature maps based optimization task. Our results accurately capture the sketch drawing style and make full use of the whole stylistic information hidden in the training dataset. Unlike former feature map based methods, we utilize the Enhanced 3D PatchMatch and cross-layer cost aggregation methods to obtain the target feature maps for the final results. Multiple experiments have shown that our approach imitates hand-drawn sketch style vividly, and has high-quality visual effects on CUHK, AR, XM2VTS and CUFSF face sketch datasets.","Authors":"B. Sheng; P. Li; C. Gao; K. Ma","DOI":"10.1109\/TVCG.2018.2866090","Keywords":"Non-photorealistic rendering;face sketch synthesis;convolutional neural network (CNN);style transformation","Title":"Deep Neural Representation Guided Face Sketch Synthesis","Keywords_Processed":"convolutional neural network CNN;style transformation;face sketch synthesis;non photorealistic rendering","Keyword_Vector":[0.4355919658,0.1798449747,-0.0302348931,-0.1634784727,-0.087559189,-0.0005227927,-0.0227650574,0.1043815471,-0.191393372,-0.0205032781,0.1419509758,0.0918420888,-0.1233966371,-0.0960471304,-0.0937287739,0.1196799562,-0.0541670633,-0.1078247456,-0.1224865506,0.0817170026,0.0281891143,-0.0704836319,-0.1363032535,-0.0548176,0.1599967273,-0.0110379529,0.154813944,-0.1664189854,0.1797161354,0.0701541334,-0.0055692443,-0.0360720989,0.0846207401,-0.0866106808,-0.0960545585,-0.0820326505,-0.0395593641,0.1569202837,-0.0042325344,0.0875200153,-0.1263758443,0.0589853885,-0.0723355871,-0.0113458134,-0.0768987887,0.107725708,0.0561015513,-0.0729721102,0.0053400546,0.0364843573,0.002020146,0.0763826272],"Abstract_Vector":[0.2395961073,0.0279519174,0.2065925715,-0.1289146917,0.0843003032,0.0209072532,-0.1662744349,0.3553149395,-0.0576701612,0.0999906063,0.1412319676,0.0576801706,-0.026337366,-0.0040414655,0.032163474,0.0516099713,0.0710537124,-0.047501888,-0.042612627,0.0542260049,-0.0184838452,-0.0607479654,0.0591917548,-0.0621519642,0.0569495497,-0.0640665033,0.0280726511,0.0313379593,-0.0965007783,-0.031109445,-0.0298098708,0.0193177874,0.0766722501,-0.0872311409,-0.0214615499,0.0029631488,-0.0202401365,0.0159325145,0.029945836,-0.0047940372,-0.0418945193,0.0100711354,0.0011156931,0.0400483284,0.0523991134,-0.0041281482,0.0084318549,-0.0111535582,-0.0241985345,0.0114585801,0.0168423021,0.0357120758,-0.0100513139,0.0171757376,0.0024368552,0.0127415142,0.0145783381,-0.0025954723,0.0245019999,0.036631926,-0.0102379653,0.0050967705,-0.0237335615,0.0007752725,0.0141220623,0.0138457965,0.0512332196,-0.0160541249,0.0183183046,0.0169193671,0.0211035629,-0.0004895389,-0.0283510371,0.0010487511,-0.011755008,-0.0007373438,-0.0040095746,0.0057186293,0.0054099798,0.0394669262,0.0011986984,-0.014957282,-0.0077729122,-0.0095594489,-0.0179397581,-0.0007088999,0.0114838795,0.0188289063,-0.0144802027,-0.0292463951,-0.0211396872,-0.0435535892,0.0285476554,0.001392243,-0.0068872853,0.0190817444,0.0008845397,0.0612461829,0.0169332969,0.0084577932,-0.0259694201,-0.0220580271,0.0152942874,0.009317364,0.0068891557,-0.0161740533,0.0168127532,-0.001503525,-0.0151448298,-0.0204922653,0.0239276261,-0.0055043288,-0.0328174648,0.014770204,0.0110173294,0.0192852059]},"243":{"Abstract":"We propose a novel type of low distortion radial embedding which focuses on one specific entity and its closest neighbors. Our embedding preserves near-exact distances to the focus entity and aims to minimize distortion between the other entities. We present an interactive exploration tool SolarView which places the focus entity at the center of a \u201csolar system\u201d and embeds its neighbors guided by concentric circles. SolarView provides an implementation of our novel embedding and several state-of-the-art dimensionality reduction and embedding techniques, which we adapted to our setting in various ways. We experimentally evaluated our embedding and compared it to these state-of-the-art techniques. The results show that our embedding competes with these techniques and achieves low distortion in practice. Our method performs particularly well when the visualization, and hence the embedding, adheres to the solar system design principle of our application. Nonetheless-as with all dimensionality reduction techniques-the distortion may be high. We leverage interaction techniques to give clear visual cues that allow users to accurately judge distortion. We illustrate the use of SolarView by exploring the high-dimensional metric space of bibliographic entity similarities.","Authors":"T. Castermans; K. Verbeek; B. Speckmann; M. A. Westenberg; R. Koopman; S. Wang; H. van den Berg; A. Betti","DOI":"10.1109\/TVCG.2018.2865361","Keywords":"Dimensionality reduction;radial embedding;visualizing distortion","Title":"SolarView: Low Distortion Radial Embedding with a Focus","Keywords_Processed":"dimensionality reduction;radial embed;visualize distortion","Keyword_Vector":[0.1267397089,-0.0498099124,-0.0282625732,0.071806055,0.0604359503,-0.0703001219,0.0606274344,0.0232664895,-0.0112673696,-0.0028007268,-0.0330901608,0.0330355818,0.0135152592,0.0423167499,0.0334789146,0.0010235062,-0.0217430798,-0.0054753847,-0.0492738088,-0.0201098759,-0.0250981484,0.0418975964,-0.0334819749,-0.0085640498,0.1761567601,-0.0599724813,0.0474007929,0.0603276656,-0.063624091,-0.1630599097,0.0627497735,-0.0949493848,-0.0597113779,0.0130760167,-0.0319228274,0.0230911426,-0.009085427,0.0493563307,0.1312966283,0.0158115695,0.0807403042,0.0187907281,-0.0998158613,-0.0375083111,-0.0450739013,-0.003025174,-0.0238400784,0.028557855,-0.0493674489,0.0260401519,-0.0178378259,0.0374690994],"Abstract_Vector":[0.169347654,0.0058195525,-0.030854185,-0.0071885718,-0.0091704184,0.0847333409,-0.0109038588,-0.0370294937,0.0362722655,-0.0188911206,-0.0529904489,-0.0538835281,-0.0455958452,0.0631598767,0.0616762851,0.0423825701,0.0587494044,0.0627891451,0.0724341119,-0.0478547801,-0.0144944136,-0.0239075413,0.0348436111,0.0154931955,0.0040049669,-0.0287433056,-0.0296949332,0.0720576358,-0.0914844482,-0.0343813448,0.0203218741,-0.041019569,0.0013496587,-0.1425600095,-0.0394274443,0.0323714681,0.0938494134,0.0557060511,-0.0087203042,-0.0348417113,-0.0294289634,-0.0058113291,0.0522637596,-0.0066040799,-0.0635773473,0.0186968196,-0.0523604014,0.0067914534,-0.0552173573,0.0266552636,0.0330506965,-0.0224392922,-0.0215747398,0.051329967,0.0318747026,0.015240973,0.033303837,-0.0536342887,0.0498501087,-0.0329085828,-0.0055386868,-0.0137108164,-0.0327138238,0.0051238969,-0.0096863802,-0.0450185566,0.0733705846,0.014876385,0.0056199384,0.028606223,0.0138913271,-0.0356269341,-0.0298478148,0.0149949179,-0.0696569133,0.0484933785,-0.0597169479,-0.0268475279,0.024712872,-0.0011872182,0.0615910564,-0.0111374969,0.0413414168,0.0041160308,0.0501647924,0.0265614158,-0.0199495882,-0.055020033,0.0202321175,0.0292510207,-0.0021644176,0.010573992,-0.0320716978,-0.0318672461,-0.0247609478,0.0138540605,-0.0774327746,0.0285427795,-0.0172485057,-0.0100459288,0.0134752194,-0.0051764301,-0.0224286485,0.0321441631,0.01040897,-0.03226361,-0.0047057977,0.0041354163,0.0316462065,-0.0237666923,-0.005890992,0.0187583554,0.0093648528,-0.0271161381,0.0510774119,-0.0000029241]},"244":{"Abstract":"Progressive Visual Analytics aims at improving the interactivity in existing analytics techniques by means of visualization as well as interaction with intermediate results. One key method for data analysis is dimensionality reduction, for example, to produce 2D embeddings that can be visualized and analyzed efficiently. t-Distributed Stochastic Neighbor Embedding (tSNE) is a well-suited technique for the visualization of high-dimensional data. tSNE can create meaningful intermediate results but suffers from a slow initialization that constrains its application in Progressive Visual Analytics. We introduce a controllable tSNE approximation (A-tSNE), which trades off speed and accuracy, to enable interactive data exploration. We offer real-time visualization techniques, including a density-based solution and a Magic Lens to inspect the degree of approximation. With this feedback, the user can decide on local refinements and steer the approximation level during the analysis. We demonstrate our technique with several datasets, in a real-world research scenario and for the real-time analysis of high-dimensional streams to illustrate its effectiveness for interactive data analysis.","Authors":"N. Pezzotti; B. P. F. Lelieveldt; L. v. d. Maaten; T. H\u00f6llt; E. Eisemann; A. Vilanova","DOI":"10.1109\/TVCG.2016.2570755","Keywords":"High dimensional data;dimensionality reduction;progressive visual analytics;approximate computation","Title":"Approximated and User Steerable tSNE for Progressive Visual Analytics","Keywords_Processed":"dimensionality reduction;progressive visual analytic;high dimensional datum;approximate computation","Keyword_Vector":[0.0257169493,0.0103497823,-0.0099871284,0.0123723304,0.0111966005,-0.0077221516,0.0122331424,0.0045899757,-0.025109236,-0.0075280684,-0.0072204127,0.0091517228,-0.0152343932,0.026936071,0.0217063678,0.0054886808,-0.0201839286,-0.0491467348,0.0431825663,-0.0304752052,0.0133987344,-0.0387999834,-0.0292632893,0.0269098076,0.006127852,-0.0234001753,0.1071572901,0.0286024975,-0.0273158922,0.0326878666,-0.0165167042,0.0127280741,0.0020650198,-0.0474812452,0.0342329223,0.0521113177,-0.0470434606,-0.0529836461,-0.030999521,-0.0147348556,0.0195628403,0.0097943853,0.0262227304,0.0095849892,0.0232516779,-0.0491359317,0.030361935,0.0102685748,0.0219958716,0.0154638206,-0.0221817673,-0.0032034168],"Abstract_Vector":[0.1626914895,0.0039661473,0.0064975903,0.016662269,-0.0303935462,0.0449136236,-0.0327940771,0.0282190458,0.002077546,-0.0514887506,-0.0281865613,0.040566475,-0.1120655261,-0.0194705724,-0.0459426047,-0.0328262846,-0.088256891,0.0125911604,0.0058177848,-0.040849016,-0.0369055731,0.0602829308,-0.0567041356,-0.0112919244,0.0205723199,-0.0357587866,-0.0125197008,-0.02164891,0.0291554375,-0.0600748854,0.0198557621,0.020777533,-0.038952651,-0.0255908135,-0.0174993463,-0.0442179734,-0.0202955144,0.0101674128,0.0089211091,0.0250070097,-0.034719725,0.0360081389,0.0047303173,0.0159312453,-0.028882626,0.0574642539,0.0246747541,-0.0151560067,-0.0049606516,-0.0183362679,-0.0077607109,0.0332674314,0.0302150026,-0.0122570859,0.00168515,-0.0351108397,0.0329654147,-0.0387038305,0.0278797617,0.0064119152,0.006924423,0.0075826283,0.0427481089,-0.0135230086,-0.0351066883,0.00463714,-0.0073952806,-0.0281682517,-0.0560259819,0.0126769363,-0.0043834197,0.0286898598,0.0037746847,0.0101317498,0.0032858004,0.0247709672,-0.0006899178,-0.0131967027,-0.0065412165,0.0241273458,-0.0283463225,0.0128105495,-0.0063288304,-0.0100666253,-0.0213212204,-0.0108534956,0.0088750159,-0.0138144811,-0.0270472851,-0.019711507,-0.0047182023,-0.0248592583,0.0091026886,-0.0118823718,0.0002085545,-0.0129720053,-0.0065291765,-0.0006309587,0.0004851331,0.0190009038,0.0044519881,0.0198018487,0.0218123011,0.0006670953,0.0124211454,0.0047023014,-0.0041154429,0.0210597409,0.0061253504,0.0017411811,-0.0276828547,0.0134046596,0.0517928795,-0.0205103907,-0.0069158087,0.0147105577]},"245":{"Abstract":"Over the last 50 years a wide variety of automatic network layout algorithms have been developed. Some are fast heuristic techniques suitable for networks with hundreds of thousands of nodes while others are multi-stage frameworks for higher-quality layout of smaller networks. However, despite decades of research currently no algorithm produces layout of comparable quality to that of a human. We give a new \u201chuman-centred\u201d methodology for automatic network layout algorithm design that is intended to overcome this deficiency. User studies are first used to identify the aesthetic criteria algorithms should encode, then an algorithm is developed that is informed by these criteria and finally, a follow-up study evaluates the algorithm output. We have used this new methodology to develop an automatic orthogonal network layout method, HOLA, that achieves measurably better (by user study) layout than the best available orthogonal layout algorithm and which produces layouts of comparable quality to those produced by hand.","Authors":"S. Kieffer; T. Dwyer; K. Marriott; M. Wybrow","DOI":"10.1109\/TVCG.2015.2467451","Keywords":"Graph layout;orthogonal layout;automatic layout algorithms;user-generated layout;graph-drawing aesthetics;Graph layout;orthogonal layout;automatic layout algorithms;user-generated layout;graph-drawing aesthetics","Title":"HOLA: Human-like Orthogonal Network Layout","Keywords_Processed":"graph layout;graph drawing aesthetic;user generate layout;orthogonal layout;automatic layout algorithm","Keyword_Vector":[0.1948479347,0.3535022149,-0.1973444853,-0.0600590357,-0.0777971181,0.0160187543,0.0890642737,0.0228693927,-0.0135042073,0.0431435726,-0.0503908544,-0.0244544718,0.012959948,0.0051674348,-0.0260064802,-0.0188816875,-0.0105544141,-0.0389815714,-0.021495485,0.0168286479,-0.0001251923,0.0286077073,0.0073831194,-0.0399538619,-0.0081703323,0.0079570854,-0.0524565632,-0.0773847972,-0.0232074092,-0.0234873144,0.0153222021,-0.0039442847,-0.0143969418,-0.0236987945,-0.0115522257,0.0117943801,0.066766626,-0.0063197245,0.0422753792,0.0315960085,-0.029300169,0.0163859889,-0.0646790496,0.029308512,-0.0432333205,-0.0069366792,0.0820622184,-0.014404077,0.0485642243,-0.0192401054,0.038426132,0.0111900701],"Abstract_Vector":[0.190020513,0.0977016318,-0.0847045284,-0.0541896813,-0.0062199049,0.0655264929,-0.0219770932,-0.0303208362,0.0051903919,0.0279927469,-0.0711073182,0.0192526492,-0.0767771831,-0.0309311621,0.0282628891,0.007841412,0.001659695,0.0621504135,0.0047249356,-0.0744902512,-0.0442532043,-0.0836859161,0.0011105068,-0.039363506,-0.025779753,0.0272989377,0.0578106173,0.0341316357,-0.0456034132,0.018426182,-0.0208760272,0.0041550437,0.0067713152,-0.0094533093,-0.0232421909,0.0146253387,0.0160895962,0.0640387864,0.0815844606,0.0539766414,0.0900437331,0.0717356763,-0.0106494039,0.0698645927,-0.0282876245,0.0128837587,-0.0179354873,0.0067459684,-0.0725386156,-0.0405562469,0.0412364362,-0.0144859756,-0.0026858936,0.1033104571,-0.0189469931,0.0104219597,-0.0206295335,0.0044228766,-0.0555133894,-0.0231045981,-0.0343436204,-0.0350037431,0.0051350745,-0.0430817694,-0.0111418436,0.0357554128,-0.0069775244,0.0367278629,-0.0208241213,-0.0291921451,0.1014339475,-0.0024455418,0.0009527951,-0.0335977449,-0.0260554054,-0.013245316,0.0658676506,-0.0357321394,-0.0068229258,-0.0224093624,-0.0007131381,-0.0375857101,0.0155967128,-0.0162598267,-0.0325778583,0.0271818106,-0.0103906496,0.0308198824,0.0194000645,0.0260608915,-0.0091160879,0.0206166933,0.0063241194,-0.0126024349,0.0315088347,0.0024386583,0.052040558,0.0019485867,0.0468769815,0.0027223624,-0.0237545885,0.0146256712,0.0066755935,-0.0147810556,0.0085258413,0.0104792898,0.0119065512,0.0056958622,-0.024993951,0.0045091974,0.0158388049,-0.0100294875,-0.0252752186,-0.0161597119,0.0305241186,-0.0270193423]},"246":{"Abstract":"This paper presents a method to reconstruct a functional mechanical assembly from raw scans. Given multiple input scans of a mechanical assembly, our method first extracts the functional mechanical parts using a motion-guided, patch-based hierarchical registration and labeling algorithm. The extracted functional parts are then parameterized from the segments and their internal mechanical relations are encoded by a graph. We use a joint optimization to solve for the best geometry, placement, and orientation of each part, to obtain a final workable mechanical assembly. We demonstrated our algorithm on various types of mechanical assemblies with diverse settings and validated our output using physical fabrication.","Authors":"M. Lin; T. Shao; Y. Zheng; N. J. Mitra; K. Zhou","DOI":"10.1109\/TVCG.2017.2662238","Keywords":"3D scanning;mechanical assembly;functionality;mechanical constraints;motion","Title":"Recovering Functional Mechanical Assemblies from Raw Scans","Keywords_Processed":"3d scan;functionality;mechanical constraint;motion;mechanical assembly","Keyword_Vector":[0.2036323114,-0.1465277227,-0.0968479804,0.061164828,-0.0925977775,-0.0710504736,0.0192634317,0.0004517681,0.0009834565,-0.0681061867,0.0353422883,-0.0522264286,0.0055736566,0.232865269,-0.110057778,0.0177990721,0.0387348156,0.0866131721,0.0013651593,-0.0531023173,-0.0355410158,-0.0651413498,-0.0249129202,-0.0317947043,0.0241098648,0.0218886348,0.0278039153,-0.012773881,0.0014276086,0.0334649337,0.0008770013,0.0157909743,-0.0017827375,-0.0010934179,0.0184161777,-0.0079645497,0.0373316274,-0.0231295592,-0.0217729709,-0.0046341817,-0.0606142199,-0.0277432973,-0.0156925025,-0.0044447977,-0.0246730611,0.0126746387,0.0242327347,0.011458486,-0.0179552738,0.0036789935,0.0082307884,-0.0071633873],"Abstract_Vector":[0.1722942651,-0.1390896665,0.0296410133,0.0022978076,-0.062989136,0.0264903573,0.0027031793,0.0237019894,-0.0545288381,-0.0186097983,-0.0213866936,0.0443742787,-0.0286301379,-0.0034771148,0.017978948,-0.0275719509,-0.0613828528,-0.0580185317,0.0256752961,-0.0077626334,0.0335774979,-0.02762268,-0.0244055631,0.010153543,-0.0166551859,0.0188831245,-0.0393874243,-0.0446976067,-0.0256501202,-0.0469944609,0.0433867704,-0.0794530911,-0.0442586299,-0.0018521417,-0.0397692172,-0.0438275821,0.0053419539,0.0075930368,0.0108626791,-0.061977708,-0.0561297603,0.0484349555,-0.0811848446,-0.0492108687,0.0124047412,-0.0104395972,0.0156123658,0.0443226978,-0.0281679591,-0.0135057499,-0.0246243013,0.0079470617,-0.0102098139,-0.0309836452,-0.0044011956,-0.0065499883,0.003708507,-0.0014834387,0.0148735219,0.0296717524,-0.0002816223,-0.0229325729,-0.0119417868,0.0163739626,0.0070758462,0.0381941034,-0.0183102085,0.0555656464,-0.0028338601,-0.0392471492,-0.0067776132,0.0165789492,-0.0007903013,0.0057965744,-0.0108421517,0.0322937662,-0.0631830092,-0.0283853473,-0.0425435524,0.0481706976,0.0217778388,-0.0144783219,-0.0263202684,0.0093003629,0.0145911374,-0.0040220669,0.0429083384,-0.053573206,0.0121991698,0.0108117278,0.0140583489,-0.0314149217,-0.0162805067,-0.0345874311,-0.0287105066,-0.0268930981,0.014588233,0.0350683851,0.0375556306,-0.0123516905,-0.0157712865,0.004965678,0.0167921113,0.0013390435,0.0274313037,0.0059958881,-0.0247002455,-0.0215491918,0.0156845738,-0.0215662075,-0.005088692,0.0218778309,0.0064059595,0.0174908248,-0.004717271,-0.0112474901]},"247":{"Abstract":"This paper presents a novel interactive system that provides users with virtual reality (VR) experiences, wherein users feel as if they are ascending\/descending stairs through passive haptic feedback. The passive haptic stimuli are provided by small bumps under the feet of users; these stimuli are provided to represent the edges of the stairs in the virtual environment. The visual stimuli of the stairs and shoes, provided by head-mounted displays, evoke a visuo-haptic interaction that modifies a user's perception of the floor shape. Our system enables users to experience all types of stairs, such as half-turn and spiral stairs, in a VR setting. We conducted a preliminary user study and two experiments to evaluate the proposed technique. The preliminary user study investigated the effectiveness of the basic idea associated with the proposed technique for the case of a user ascending stairs. The results demonstrated that the passive haptic feedback produced by the small bumps enhanced the user's feeling of presence and sense of ascending. We subsequently performed an experiment to investigate an improved viewpoint manipulation method and the interaction of the manipulation and haptics for both the ascending and descending cases. The experimental results demonstrated that the participants had a feeling of presence and felt a steep stair gradient under the condition of haptic feedback and viewpoint manipulation based on the characteristics of actual stair walking data. However, these results also indicated that the proposed system may not be as effective in providing a sense of descending stairs without an optimization of the haptic stimuli. We then redesigned the shape of the small bumps, and evaluated the design in a second experiment. The results indicated that the best shape to present haptic stimuli is a right triangle cross section in both the ascending and descending cases. Although it is necessary to install small protrusions in the determined direction, by using this optimized shape the users feeling of presence of the stairs and the sensation of walking up and down was enhanced.","Authors":"R. Nagao; K. Matsumoto; T. Narumi; T. Tanikawa; M. Hirose","DOI":"10.1109\/TVCG.2018.2793038","Keywords":"Virtual reality;locomotion;haptic feedback;perception;stairs;staircase","Title":"Ascending and Descending in Virtual Reality: Simple and Safe System Using Passive Haptics","Keywords_Processed":"virtual reality;stair;locomotion;staircase;perception;haptic feedback","Keyword_Vector":[0.2089123205,-0.1374369961,-0.0292245337,0.0167618361,-0.0631964522,0.0930185472,-0.0930450876,-0.1059452831,-0.0846784064,0.0831074021,-0.039121129,0.0551911217,0.0075524297,0.1649085801,-0.1268078085,0.1673662026,-0.1012690765,0.0481250387,-0.1542682141,-0.0491437504,0.062172242,-0.0474549596,0.0695359893,-0.0368468899,-0.1321299924,0.068005427,-0.0013784633,0.0423741402,-0.0543343317,-0.0776307549,-0.0599614931,0.0762022606,0.0389292071,0.0568797551,0.00859101,0.006457291,0.0487283984,-0.0179473512,-0.0200962134,-0.0529266765,0.0242778071,0.03276599,-0.0145310265,0.0609010859,0.016841678,-0.1109450084,0.0123111402,-0.008472563,0.0285231055,-0.0561374241,0.0121771226,-0.0100903537],"Abstract_Vector":[0.1708851583,-0.0993984059,-0.0093880875,-0.0232136909,-0.0357864055,-0.0386704807,0.0345160033,0.0066574905,-0.012370578,0.0023288524,0.0157315029,0.0186077916,0.0034025036,-0.0007125292,-0.014285162,-0.0368568647,-0.0278447864,-0.0048182479,-0.0156283639,-0.0072657515,-0.0154726705,0.004785992,0.0260522492,0.0141184036,-0.0446647456,0.0347155108,0.0053722318,-0.0114510463,-0.0677547265,-0.0594346928,0.0442240803,0.0073452152,0.0018362958,-0.0099951907,-0.0453325576,0.0072572326,-0.0337902631,-0.0141016447,-0.0184750006,-0.0030606712,-0.0050824705,0.0078574892,0.022289882,-0.0530825881,0.0350351561,0.0325831193,0.0043376207,-0.0556924585,0.0012685179,-0.0207411434,-0.0527638597,0.014088515,-0.0048809367,0.0199851536,0.0265587331,-0.0049169312,0.0140147543,0.0326285387,-0.0005578921,0.0205609396,-0.0465307498,-0.0265457704,-0.0293323305,-0.0244757105,-0.0631887455,-0.0180533233,0.0129997696,0.0512405595,-0.0159695613,-0.0168890563,-0.0000867613,-0.0378976965,-0.0035416021,0.0442468226,0.0367697238,-0.0386101971,0.0255825611,-0.0059333371,-0.0057813203,0.0389167204,-0.0076505912,-0.0174096935,-0.0171385201,-0.0265697347,0.0735939164,-0.0028735061,-0.0055164993,0.0033891317,0.0219178016,-0.0166722399,0.0115161709,0.0173772426,-0.0188353769,-0.0400171451,-0.0538382528,0.0124491846,0.031136971,-0.0019638311,0.0058758084,0.0000120251,0.003416164,0.0069712799,0.0001962031,-0.0243117286,-0.0029753322,-0.004985954,-0.013434628,-0.0091017705,0.0023928482,-0.0057474609,0.0013493791,-0.0007862259,-0.0277141429,-0.0079917445,-0.0088960638,0.010668672]},"248":{"Abstract":"This paper presents an interactive visualization interface-HiPiler-for the exploration and visualization of regions-of-interest in large genome interaction matrices. Genome interaction matrices approximate the physical distance of pairs of regions on the genome to each other and can contain up to 3 million rows and columns with many sparse regions. Regions of interest (ROIs) can be defined, e.g., by sets of adjacent rows and columns, or by specific visual patterns in the matrix. However, traditional matrix aggregation or pan-and-zoom interfaces fail in supporting search, inspection, and comparison of ROIs in such large matrices. In HiPiler, ROIs are first-class objects, represented as thumbnail-like \u201csnippets\u201d. Snippets can be interactively explored and grouped or laid out automatically in scatterplots, or through dimension reduction methods. Snippets are linked to the entire navigable genome interaction matrix through brushing and linking. The design of HiPiler is based on a series of semi-structured interviews with 10 domain experts involved in the analysis and interpretation of genome interaction matrices. We describe six exploration tasks that are crucial for analysis of interaction matrices and demonstrate how HiPiler supports these tasks. We report on a user study with a series of data exploration sessions with domain experts to assess the usability of HiPiler as well as to demonstrate respective findings in the data.","Authors":"F. Lekschas; B. Bach; P. Kerpedjiev; N. Gehlenborg; H. Pfister","DOI":"10.1109\/TVCG.2017.2745978","Keywords":"Interactive Small Multiples;Matrix Comparison;Biomedical Visualization;Genomics","Title":"HiPiler: Visual Exploration of Large Genome Interaction Matrices with Interactive Small Multiples","Keywords_Processed":"Interactive small Multiples;Biomedical visualization;genomic;Matrix Comparison","Keyword_Vector":[0.0821549182,-0.0966791433,-0.0846983392,0.0503137544,-0.0688380948,-0.1042759076,0.0066310854,0.0114314653,-0.0400161725,0.0034436801,-0.0047126068,0.0425700639,-0.0321314201,-0.0179112064,-0.0421139855,0.0283783377,-0.0739138294,0.0007277497,-0.0303846616,-0.0283620034,0.0546664942,-0.029000126,0.0713495211,-0.0218827449,-0.0775758311,-0.0220824861,0.0124229472,0.0414359701,-0.0142704842,-0.0187813184,0.0568679688,0.1482265893,0.0933059371,0.1373025898,-0.0260981094,0.0482031291,-0.0328718342,-0.018323467,0.035735702,-0.0004976346,0.0167372737,0.0401820702,-0.0618944252,0.051452471,0.0319516084,-0.0744486255,-0.0192080376,0.0359274195,0.0098210086,0.0360067198,0.0401239633,-0.0292988036],"Abstract_Vector":[0.2224661067,-0.0938490936,0.0142252214,-0.000238839,-0.0930735198,0.0184038574,-0.0025165022,0.0264198327,-0.0143317507,-0.0709767694,0.011395561,0.0132634796,0.0340751954,0.0037532056,-0.0097752943,-0.003017726,0.1025856519,0.0435015592,-0.0273192096,-0.1480507259,0.076094264,0.0284428795,0.0071424583,-0.0100404209,-0.0584721007,-0.0020429168,0.0869407593,-0.0337680446,0.0022581486,-0.0069988446,-0.0166158196,0.0094765071,0.0090837624,0.0081239126,-0.024947164,0.0103869362,0.0061628393,-0.0627744162,0.0488414115,-0.0529638029,-0.0502144429,-0.0097573841,-0.0006693639,-0.0069593075,-0.0516111299,0.0425432207,-0.0039285295,0.0623229338,-0.0133544854,-0.007884869,0.0195021541,-0.0285746688,-0.0513284251,-0.0184441231,-0.0238275275,-0.0197472683,0.0253893421,0.0158975715,0.00672705,0.0403164551,0.0204502199,-0.0142943372,-0.0500532189,-0.016002197,-0.0250203965,0.0261138211,-0.0159105151,-0.0012577985,-0.0344091649,-0.0156506582,0.001721481,0.0381926004,-0.0057737292,-0.023341313,-0.0197841745,0.021426657,0.009469727,0.0329171909,-0.0071402277,0.0009728513,-0.0039043042,-0.0099045666,-0.0391042835,-0.0113625016,-0.0185446784,-0.0040762374,-0.0078551669,-0.064172587,-0.0347351719,0.0062947839,-0.0325995793,-0.0397993409,0.0174621209,-0.0203959542,-0.0172843921,-0.0093967591,-0.006767515,-0.0297031843,-0.0015640812,-0.0075440814,-0.0045062442,-0.0034022545,0.0203468048,-0.0246237353,-0.0039221254,-0.0134692104,0.0001208847,0.0297291935,0.0009648315,-0.0190849157,-0.0384430283,0.0269921718,0.0001310338,0.0289793692,0.0014452315,-0.0557236437]},"249":{"Abstract":"Real-time 3D scene reconstruction from RGB-D sensor data, as well as the exploration of such data in VR\/AR settings, has seen tremendous progress in recent years. The combination of both these components into telepresence systems, however, comes with significant technical challenges. All approaches proposed so far are extremely demanding on input and output devices, compute resources and transmission bandwidth, and they do not reach the level of immediacy required for applications such as remote collaboration. Here, we introduce what we believe is the first practical client-server system for real-time capture and many-user exploration of static 3D scenes. Our system is based on the observation that interactive frame rates are sufficient for capturing and reconstruction, and real-time performance is only required on the client site to achieve lag-free view updates when rendering the 3D model. Starting from this insight, we extend previous voxel block hashing frameworks by introducing a novel thread-safe GPU hash map data structure that is robust under massively concurrent retrieval, insertion and removal of entries on a thread level. We further propose a novel transmission scheme for volume data that is specifically targeted to Marching Cubes geometry reconstruction and enables a 90% reduction in bandwidth between server and exploration clients. The resulting system poses very moderate requirements on network bandwidth, latency and client-side computation, which enables it to rely entirely on consumer-grade hardware, including mobile devices. We demonstrate that our technique achieves state-of-the-art representation accuracy while providing, for any number of clients, an immersive and fluid lag-free viewing experience even during network outages.","Authors":"P. Stotko; S. Krumpen; M. B. Hullin; M. Weinmann; R. Klein","DOI":"10.1109\/TVCG.2019.2899231","Keywords":"Remote collaboration;live telepresence;real-time reconstruction;voxel hashing;RGB-D;real-time streaming","Title":"SLAMCast: Large-Scale, Real-Time 3D Reconstruction and Streaming for Immersive Multi-Client Live Telepresence","Keywords_Processed":"live telepresence;rgb;real time stream;voxel hashing;remote collaboration;real time reconstruction","Keyword_Vector":[0.0940593219,-0.0609914057,-0.0052805497,-0.0994685535,0.0349116118,-0.0395372721,-0.0536783253,-0.019058099,-0.0143235975,0.0269415916,-0.020458612,-0.0439934065,-0.0143721136,0.0177257086,-0.0108344426,0.0112976062,-0.0835638615,-0.0368953178,-0.0172361291,-0.0019005374,0.018021904,-0.0582472687,0.03106149,-0.0423558072,-0.0545196777,0.0064900582,-0.0166482928,0.0528539631,-0.0035213665,-0.0052556454,0.0275805085,0.0427435222,0.053988015,0.0728887372,-0.0551791795,0.0487706943,-0.0652946084,0.0101061006,0.0215167457,-0.045104223,0.0155380416,0.0377668163,-0.0417772085,0.0097429052,-0.0008868154,-0.0168998059,0.0059097932,0.0044317549,-0.0171665687,0.0046334263,-0.0008072361,0.0275184727],"Abstract_Vector":[0.1935686228,-0.0733882676,0.0215501163,0.0008387434,-0.0780130356,0.0245055975,-0.0413403603,-0.0192450105,-0.0572962873,-0.0414156257,-0.045640253,0.0068039099,-0.071561874,0.0128538847,0.0107223403,0.0277983874,0.0697303,0.0477608907,-0.033523418,-0.1284402701,0.0978319592,-0.0424552486,-0.0079376839,-0.0798815805,-0.0633972544,0.0029128488,0.0490585202,-0.0455324753,0.0311269816,-0.0291669589,-0.0198369018,-0.0434238328,0.0201435395,0.0368715423,-0.0818394159,0.0003641262,0.0815316386,-0.0191748395,-0.0062723089,-0.0263228131,0.0317968707,-0.0728561236,0.044705625,-0.0085873094,-0.0733109645,-0.0337635492,0.0004887211,-0.0211195737,-0.0001491609,-0.019079353,-0.0435523434,-0.0196092666,-0.034320036,-0.0071250305,0.0264310274,-0.0069622381,0.056253077,-0.0068982432,-0.0146953899,-0.0345066094,0.028163633,-0.0308474769,-0.018677716,0.0217052846,-0.0151827987,0.0090962127,-0.0119142264,0.0139809025,0.0056099865,0.0274396642,-0.0099472964,0.0058416441,0.0253615496,-0.0896116645,-0.0725977327,0.0044909225,0.1004052191,0.0161793482,-0.0207985067,0.0391779455,0.0195895281,-0.0040626405,-0.0006803553,-0.0167718506,0.0096004841,-0.004327351,0.0129979701,-0.0445180984,-0.0300244717,-0.0313219832,0.0681533078,0.0113753911,0.0232487132,0.0517969903,0.0282775806,-0.0039473149,0.0323252399,0.0270123859,-0.0201625945,-0.0221788868,0.0099713308,-0.047089049,0.0091255321,-0.0355413458,-0.0228167058,-0.0298475193,0.0133916856,0.0062003063,-0.0336390831,-0.0251183182,0.0204926567,0.0190261352,0.0245851724,0.0464718219,0.0008159076,-0.0362025667]},"25":{"Abstract":"A variety of human movement datasets are represented in an Origin-Destination(OD) form, such as taxi trips, mobile phone locations, etc. As a commonly-used method to visualize OD data, flow map always fails to discover patterns of human mobility, due to massive intersections and occlusions of lines on a 2D geographical map. A large number of techniques have been proposed to reduce visual clutter of flow maps, such as filtering, clustering and edge bundling, but the correlations of OD flows are often neglected, which makes the simplified OD flow map present little semantic information. In this paper, a characterization of OD flows is established based on an analogy between OD flows and natural language processing (NPL) terms. Then, an iterative multi-objective sampling scheme is designed to select OD flows in a vectorized representation space. To enhance the readability of sampled OD flows, a set of meaningful visual encodings are designed to present the interactions of OD flows. We design and implement a visual exploration system that supports visual inspection and quantitative evaluation from a variety of perspectives. Case studies based on real-world datasets and interviews with domain experts have demonstrated the effectiveness of our system in reducing the visual clutter and enhancing correlations of OD flows.","Authors":"Z. Zhou; L. Meng; C. Tang; Y. Zhao; Z. Guo; M. Hu; W. Chen","DOI":"10.1109\/TVCG.2018.2864503","Keywords":"Visual abstraction;human mobility;origin-destination;flow map;representation learning","Title":"Visual Abstraction of Large Scale Geospatial Origin-Destination Movement Data","Keywords_Processed":"human mobility;flow map;representation learning;visual abstraction;origin destination","Keyword_Vector":[0.2002064761,-0.0990720854,-0.0318780595,0.0597114686,-0.0703797107,0.1256074692,-0.0716540414,-0.0511855652,-0.0532456827,-0.0727854984,0.0385757782,-0.031891138,0.0444602974,0.1088436544,-0.0976404632,-0.0100044524,-0.0059621284,0.0372390895,0.0609956116,-0.0689629878,-0.0560307721,-0.0257140612,-0.0417564152,-0.008898434,-0.0307982716,0.0218330342,0.0016417616,0.0396517074,-0.0585025329,0.0784402272,-0.0462449623,-0.0444723347,-0.0555062341,0.0008695221,0.0103027714,0.0579684988,-0.0176004815,0.0341030601,-0.0576360075,-0.0558589355,0.0248210209,-0.0209801285,0.0203536623,-0.0278505861,0.0890019144,-0.0149153872,-0.0172167261,-0.0285265652,0.0260297325,-0.0815910786,0.0631302651,0.0252246516],"Abstract_Vector":[0.2653990336,-0.1372540413,-0.0199989922,-0.0239386758,-0.0815636928,-0.0332986529,0.049505134,0.0355232935,-0.0666836109,-0.0049981519,0.017817938,0.0102308677,0.0952514629,-0.0456832923,-0.0149923803,-0.0147239735,-0.048978342,0.0442419185,-0.0160843137,0.0088301389,-0.0561881976,0.042422916,-0.0312411978,0.0268517772,-0.033949827,-0.0293420349,-0.0486752069,0.0058552367,0.0080447393,-0.0037921933,-0.0062792201,0.0274547187,0.0343330113,-0.029316711,-0.012693964,-0.0214821743,-0.0075620039,-0.016057231,0.047310583,0.0196514336,-0.0054050353,-0.0033644442,0.0329159015,-0.0303608513,-0.0195762741,0.0205377809,0.0577011386,0.0303026953,-0.0048096282,0.0003613716,0.0079182006,-0.0116349783,0.0084746167,0.0098876868,0.0056347481,-0.0220068519,0.0109735466,-0.012280813,0.0095403201,0.007394926,0.0320706234,0.021719386,0.0082356098,-0.0141886999,-0.0267314017,0.0150328571,0.0276100153,0.0229873523,0.0195197851,0.005229326,0.0054972607,-0.0059229616,-0.0231628195,-0.0161890921,0.0185798524,0.0019029357,-0.0360007031,0.0487596465,0.0178229417,-0.0031563778,-0.0240096802,-0.0049149258,0.0220104837,-0.0152967396,-0.0045002222,0.0084327817,-0.0073086745,0.0138583732,-0.0219969939,0.0270331582,-0.0203769,0.0468686604,0.0115242211,-0.0048073891,0.0454895482,-0.0034532461,-0.0025299294,-0.0146704116,0.0053479339,0.0255514248,0.0299382211,-0.0016436837,-0.0219004274,-0.0111814872,-0.0064701733,-0.0118463229,-0.051779355,0.0023294694,-0.0234846268,-0.0277242713,0.0132719324,-0.0020973414,-0.0156241805,0.0262719634,-0.0406227493,-0.0065739587]},"250":{"Abstract":"Economic globalization is increasing connectedness among regions of the world, creating complex interdependencies within various supply chains. Recent studies have indicated that changes and disruptions within such networks can serve as indicators for increased risks of violence and armed conflicts. This is especially true of countries that may not be able to compete for scarce commodities during supply shocks. Thus, network-induced vulnerability to supply disruption is typically exported from wealthier populations to disadvantaged populations. As such, researchers and stakeholders concerned with supply chains, political science, environmental studies, etc. need tools to explore the complex dynamics within global trade networks and how the structure of these networks relates to regional instability. However, the multivariate, spatiotemporal nature of the network structure creates a bottleneck in the extraction and analysis of correlations and anomalies for exploratory data analysis and hypothesis generation. Working closely with experts in political science and sustainability, we have developed a highly coordinated, multi-view framework that utilizes anomaly detection, network analytics, and spatiotemporal visualization methods for exploring the relationship between global trade networks and regional instability. Requirements for analysis and initial research questions to be investigated are elicited from domain experts, and a variety of visual encoding techniques for rapid assessment of analysis and correlations between trade goods, network patterns, and time series signatures are explored. We demonstrate the application of our framework through case studies focusing on armed conflicts in Africa, regional instability measures, and their relationship to international global trade.","Authors":"H. Wang; Y. Lu; S. T. Shutters; M. Steptoe; F. Wang; S. Landis; R. Maciejewski","DOI":"10.1109\/TVCG.2018.2864844","Keywords":"Global trade network;anomaly detection;visual analytics","Title":"A Visual Analytics Framework for Spatiotemporal Trade Network Analysis","Keywords_Processed":"anomaly detection;global trade network;visual analytic","Keyword_Vector":[0.1926219766,0.0159034745,0.1239671457,-0.1106168932,-0.0667583218,-0.0216756584,-0.0605369045,0.0419853448,0.0785673229,-0.1492416885,0.0211751008,0.0619126813,-0.0198874342,-0.0275953585,-0.0665375203,0.0181311983,-0.0182146702,-0.016431395,0.0003361189,-0.0082997611,-0.0875513595,-0.0361094442,0.0204161365,-0.0376185178,-0.042469491,0.0264377056,0.0048034323,-0.0638210676,0.0168894868,-0.0907246522,0.0145802889,0.0375874448,0.0260860581,-0.0133510608,0.0367161126,0.0319491614,-0.0001873228,-0.0308214207,0.0525553449,-0.0538869588,0.0418115451,-0.0523675213,0.0320986921,-0.00181465,0.0199134516,0.0160319049,0.0271139958,-0.0532374038,0.0173174597,-0.0851383931,-0.0456859336,0.0702532749],"Abstract_Vector":[0.2138587503,-0.0275788367,0.1158009203,-0.0590865671,-0.0349414159,0.0631160888,0.098919193,-0.0700492357,-0.0019939257,-0.0117690888,0.0550203188,-0.0216088261,0.0227121469,0.0369275193,-0.0612664576,0.0508525608,-0.065310142,-0.0126442945,0.0265082854,0.0208982835,-0.0097009748,0.0270946444,-0.0051758642,-0.0143341706,-0.0099721201,0.0375934586,0.0418283208,-0.0234946712,-0.0398448342,-0.033403862,0.00633296,0.0019102948,-0.0432605745,-0.0405306757,-0.0569377355,0.0078195979,-0.041761262,0.0149921122,0.0339397302,-0.007809794,-0.0141647444,-0.005804942,-0.039076703,-0.0012619376,-0.0045173337,-0.0032369039,-0.024499269,-0.0295682506,0.0307817624,0.02651641,0.0146622911,0.0482633943,-0.0215401981,-0.0078792356,0.0737725166,-0.0226018392,0.011475675,-0.0151177459,-0.0573134008,0.0283443972,0.0118420168,-0.0532880259,-0.0692900617,-0.0293869873,-0.0294546036,0.0109585019,-0.0079507304,0.0645135063,-0.0025413786,-0.0160473967,0.0103252714,0.0306693867,0.0640355684,0.0462169812,-0.0007779862,0.0087870188,0.0198640818,-0.0348895008,-0.031439354,0.0171040751,0.0237471177,0.0093656755,0.024838363,-0.0290636079,0.0042048312,-0.0229324909,-0.0060691906,0.0292308464,-0.0087193325,0.0025152446,-0.0140038366,0.0284900117,-0.0035171906,0.0004615564,0.0113643254,0.0021854972,0.0152096736,-0.0204310344,0.0145868929,0.0001443062,-0.0404543916,0.0061507665,-0.0153182209,0.0344664776,0.0068222424,0.0184112413,0.0051299568,-0.0029733412,-0.0136416882,-0.0315657777,-0.0038452792,0.0409150666,-0.0449826759,0.0078593904,0.0021433961,-0.0120212677]},"251":{"Abstract":"Vulnerabilities represent one of the main weaknesses of IT systems and the availability of consolidated official data, like CVE (Common Vulnerabilities and Exposures), allows for using them to compute the paths an attacker is likely to follow. However, even if patches are available, business constraints or lack of resources create obstacles to their straightforward application. As a consequence, the security manager of a network needs to deal with a large number of vulnerabilities, making decisions on how to cope with them. This paper presents VULNUS (VULNerabilities visUal aSsessment), a visual analytics solution for dynamically inspecting the vulnerabilities spread on networks, allowing for a quick understanding of the network status and visually classifying nodes according to their vulnerabilities. Moreover, VULNUS computes the approximated optimal sequence of patches able to eliminate all the attack paths and allows for exploring sub-optimal patching strategies, simulating the effect of removing one or more vulnerabilities. VULNUS has been evaluated by domain experts using a lab-test experiment, investigating the effectiveness and efficiency of the proposed solution.","Authors":"M. Angelini; G. Blasilli; T. Catarci; S. Lenti; G. Santucci","DOI":"10.1109\/TVCG.2018.2865028","Keywords":"Visual Analytics;Network security;Vulnerability analysis;CVE;CVSS;Attack Graph;Vulnerability triage and management","Title":"Vulnus: Visual Vulnerability Analysis for Network Security","Keywords_Processed":"visual analytic;CVE;Attack Graph;network security;cvss;vulnerability triage and management;vulnerability analysis","Keyword_Vector":[0.1283155519,-0.0018511608,0.1252454126,-0.0029653649,-0.0712336822,-0.0226157853,0.0040953507,0.0944579326,-0.1071925181,0.0567981012,0.1103968083,-0.0275431986,-0.0213323806,-0.0161407314,-0.0176779952,-0.024190683,-0.0066851234,0.0338175003,0.0415516833,0.10296967,-0.0016030151,0.0715981475,-0.0040374019,-0.0253976664,0.0128645903,0.0202714662,0.0150886753,0.0451209042,-0.0802408987,0.0745171044,-0.0042150627,-0.0107430959,-0.0152187835,0.0018124104,-0.0265168021,0.0256194733,-0.0478882769,0.0184906766,-0.0135458236,0.0190303582,-0.018775706,-0.0245333044,0.0406934329,-0.043112293,-0.0436266833,0.0673419986,0.0160281592,0.0475733237,-0.0166100778,-0.0044685077,-0.0164148742,-0.0761998821],"Abstract_Vector":[0.168174924,0.0167002158,0.0167878258,-0.0091797782,0.029199072,-0.0103472773,0.0092549187,0.1164145912,-0.0240098574,0.0136833786,-0.0037475913,-0.0211401358,0.0201749452,-0.0698367037,0.0003087167,-0.0129018212,0.0008743072,0.0271208503,-0.0386750172,0.0178961678,0.0346653617,0.0156701256,-0.0131461542,0.0072303778,-0.0265784613,-0.0399264228,-0.0438472975,-0.0179011845,-0.0057140887,0.0226105448,-0.0322083099,-0.0326453505,-0.0288465925,-0.0436826544,-0.0224123609,0.0064178166,0.0502590461,-0.0121674421,-0.012026615,0.019260931,-0.0436470044,0.0184007178,0.0342103685,-0.0337136565,-0.0041186732,-0.0560319988,0.0296876365,-0.0281000507,-0.0573242695,-0.0084664319,-0.0099222241,0.0294452229,-0.0813729692,0.0184454058,0.0067795621,-0.0302857981,0.0439245244,-0.0021541176,-0.0036892557,0.0035223985,-0.0169905459,0.0398939426,0.003622825,0.0430075975,-0.046905795,0.0297936725,0.0334330755,0.0262128931,-0.009060022,0.0077711137,-0.0048585008,-0.0411836857,-0.0199705625,-0.0118969999,0.005012883,-0.059937163,-0.0146725226,-0.0123886186,0.027695196,-0.0006836216,-0.0005187082,-0.0169803523,-0.0061832607,-0.0218293758,0.0060351619,-0.0474731722,0.0013372298,0.0629578615,-0.0059943539,-0.0481696015,-0.0116223824,0.0422527039,-0.0075066518,-0.0022944804,-0.0314129521,0.0057252124,0.0206821109,0.0245552003,-0.0026757312,0.0342817368,-0.0305249966,0.0166495189,-0.0274212414,0.0277917731,-0.0061931538,-0.0414947162,0.0483896764,0.0025805779,-0.0169743534,0.0018419693,-0.0173444041,-0.0357330746,-0.0019665006,-0.0259418684,-0.0118779113,0.048542019]},"252":{"Abstract":"Evaluation has become a fundamental part of visualization research and researchers have employed many approaches from the field of human-computer interaction like measures of task performance, thinking aloud protocols, and analysis of interaction logs. Recently, eye tracking has also become popular to analyze visual strategies of users in this context. This has added another modality and more data, which requires special visualization techniques to analyze this data. However, only few approaches exist that aim at an integrated analysis of multiple concurrent evaluation procedures. The variety, complexity, and sheer amount of such coupled multi-source data streams require a visual analytics approach. Our approach provides a highly interactive visualization environment to display and analyze thinking aloud, interaction, and eye movement data in close relation. Automatic pattern finding algorithms allow an efficient exploratory search and support the reasoning process to derive common eye-interaction-thinking patterns between participants. In addition, our tool equips researchers with mechanisms for searching and verifying expected usage patterns. We apply our approach to a user study involving a visual analytics application and we discuss insights gained from this joint analysis. We anticipate our approach to be applicable to other combinations of evaluation techniques and a broad class of visualization applications.","Authors":"T. Blascheck; M. John; K. Kurzhals; S. Koch; T. Ertl","DOI":"10.1109\/TVCG.2015.2467871","Keywords":"visual analytics;qualitative evaluation;thinking aloud;interaction logs;eye tracking;time series data;visual analytics;qualitative evaluation;thinking aloud;interaction logs;eye tracking;time series data","Title":"VA2: A Visual Analytics Approach for Evaluating Visual Analytics Applications","Keywords_Processed":"interaction log;eye tracking;qualitative evaluation;think aloud;time series datum;visual analytic","Keyword_Vector":[0.070714364,-0.0171302229,0.0870032836,0.0559935209,-0.0584233464,0.0505778874,0.002719347,-0.0454761181,0.0243093908,0.0296557512,-0.0356711612,0.0151641623,-0.0486320486,0.0386029448,0.0368000997,0.1606837555,0.092666117,-0.0329962527,-0.0627051015,-0.0026031682,0.0115288531,0.0180269808,0.0447322067,0.1400896063,-0.0081000781,-0.0372724959,0.001259142,0.018169305,-0.0897404287,0.0218936755,-0.032559155,0.0290952888,0.072538322,0.065429677,-0.0137096796,-0.0185181966,0.0238892708,-0.0006638057,0.031137803,0.08397301,-0.0069685979,0.006548939,-0.0477122244,-0.0356533106,0.0474474815,-0.0285884845,0.0238644711,0.0000979575,0.0130830549,0.0364064044,0.0507869834,0.0175224434],"Abstract_Vector":[0.2333664918,0.0614661932,-0.0701831041,0.0152265498,0.0809503823,-0.1080343736,0.1022797445,0.0422197019,0.067209441,0.0844197974,0.0144903031,-0.0121225363,-0.0408646672,-0.0256983459,-0.000187161,0.0026790414,0.0252954689,-0.0110224487,-0.0405714463,0.0396247626,-0.0116992037,-0.0173055043,-0.0870531403,-0.0319452645,-0.0314579693,0.0760328107,-0.0886199543,-0.0336115792,0.0362398851,0.0315556607,-0.056439156,0.0420179591,-0.006402152,-0.0435528063,-0.1007564809,0.0110248777,0.057727306,0.0250554479,-0.0035033461,0.0249607017,-0.0235968218,0.0027974109,0.0027801197,-0.067088673,-0.0543169427,0.063438793,0.0016138929,0.053427422,-0.0014731554,0.0421039963,-0.0136022023,-0.0065552581,-0.0555373254,0.0454904406,-0.0130248525,0.0221382985,0.0739937049,0.0067214998,-0.0296480561,-0.027633357,-0.0128603307,-0.0181515087,0.0027348516,0.0367661038,-0.0123971737,0.0800962201,0.0086914969,0.0076173165,-0.0036171118,0.0484899099,0.0032266196,-0.0275400033,-0.0599534334,-0.0203885648,0.0181025713,0.0080272372,-0.0379183145,-0.0022575931,-0.0450409702,-0.0852058976,-0.0214950686,0.0098800824,0.023468078,-0.0018729956,-0.0046768365,0.0344550684,-0.0530982774,0.0243907271,0.0033303078,-0.0013695788,-0.0397939111,-0.0189770309,0.0602327899,0.0161122024,0.006976729,-0.0239364272,-0.0484507198,0.0045899104,0.0063742784,-0.0264457138,0.0090890653,0.0045861509,0.05373975,-0.0234932286,0.0019556015,0.017019765,0.0377485442,0.0385057001,0.0098797267,0.0191331431,-0.0064359842,-0.0204354575,0.0081039649,-0.0387547808,0.026461456,-0.0155191754]},"253":{"Abstract":"In this paper we present a novel GPU-friendly real-time voxelization technique for rendering homogeneous media that is defined by particles, e.g., fluids obtained from particle-based simulations such as Smoothed Particle Hydrodynamics (SPH). Our method computes view-adaptive binary voxelizations with on-the-fly compression of a tiled perspective voxel grid, achieving higher resolutions than previous approaches. It allows for interactive generation of realistic images, enabling advanced rendering techniques such as ray casting-based refraction and reflection, light scattering and absorption, and ambient occlusion. In contrast to previous methods, it does not rely on preprocessing such as expensive, and often coarse, scalar field conversion or mesh generation steps. Our method directly takes unsorted particle data as input. It can be further accelerated by identifying fully populated simulation cells during simulation. The extracted surface can be filtered to achieve smooth surface appearance. Finally, we provide a new scheme for accelerated ray casting inside the voxelization.","Authors":"T. Zirr; C. Dachsbacher","DOI":"10.1109\/TVCG.2017.2656897","Keywords":"Surface extraction;interactive particle visualization;ray tracing","Title":"Memory-Efficient On-the-Fly Voxelization and Rendering of Particle Data","Keywords_Processed":"surface extraction;interactive particle visualization;ray trace","Keyword_Vector":[0.0777335851,-0.0032926264,0.0268758176,0.1087063151,0.0211823937,-0.0592998533,-0.029850867,-0.1759095504,-0.0501931097,-0.0298097931,0.0383433537,0.0268275996,-0.1250122133,0.2235793798,0.0849332262,-0.155035637,-0.0407566387,0.0077026507,-0.072821302,-0.0225591808,-0.1320746002,-0.0250306539,0.0415181485,-0.0425502848,0.0303322484,0.0343200634,-0.0586248034,-0.0108747934,-0.0121738768,0.0105686776,-0.0357777988,0.0101147007,0.0469019549,0.0230073737,0.032604614,-0.0470471411,-0.0692132904,-0.0002702913,0.0245131307,-0.0023793829,-0.0145735541,-0.0075952532,-0.0010819493,-0.0096009603,0.0075900532,-0.0544950862,0.0123885706,-0.0556212832,-0.0914802406,-0.0062933867,0.012867419,0.0488020727],"Abstract_Vector":[0.157095999,0.1005179331,0.0447549635,0.1186992517,-0.1032904889,-0.0151999234,-0.0277442212,-0.0171139719,-0.015173933,-0.0188641001,0.0202877242,0.0106066455,-0.037792839,0.0107132026,-0.0310319288,-0.000986847,-0.0655236604,-0.0142745811,-0.0253916092,0.0059428245,-0.0112843642,0.0069725141,-0.0050571825,0.0378205901,0.0106853246,0.0034490964,0.0153309084,0.0063735147,-0.0075582682,-0.0332823293,0.023861139,-0.002447935,-0.0090518217,-0.0352266551,-0.0053832423,-0.0184374968,0.0362099537,0.0060210715,-0.005359768,0.038762315,-0.0240249062,0.0174078792,0.0284482847,-0.0007942773,0.0012092198,0.025442749,-0.0357428368,0.0254655796,0.0057095787,-0.0172424292,-0.0200017697,0.0272659308,0.0085013039,0.0164505517,0.0026905044,0.0078296294,-0.0219126881,-0.0044042886,-0.0005959094,-0.012338753,0.0246476073,0.005922471,0.0086889489,0.0167476575,0.0442624741,-0.0288297485,-0.0311925011,-0.0255803824,0.0159378223,0.0179985268,0.0324842895,0.0005744693,-0.0005983449,0.0114439107,-0.0028821626,0.0102889521,0.0064471834,-0.0177073036,-0.0189718513,-0.0210387802,0.0166739592,0.0063302974,-0.0117892082,-0.0239639184,0.0200113308,0.0014354463,-0.0005943065,-0.0034979186,-0.0185222619,-0.0095159857,0.0145871949,0.0066390815,-0.0103587038,0.0169630208,0.0167160799,0.0083041784,0.0096683268,-0.0017089404,-0.0027576892,-0.0100983726,-0.011134864,0.0046585421,-0.0034901399,-0.0249962592,0.0207743806,-0.0132464588,0.0172395163,-0.0279965554,-0.0118594141,-0.0019636582,0.0045159543,-0.0143679247,0.0025373969,0.0080694389,-0.0083946591,0.0463450057]},"254":{"Abstract":"In the field of connectomics, neuroscientists acquire electron microscopy volumes at nanometer resolution in order to reconstruct a detailed wiring diagram of the neurons in the brain. The resulting image volumes, which often are hundreds of terabytes in size, need to be segmented to identify cell boundaries, synapses, and important cell organelles. However, the segmentation process of a single volume is very complex, time-intensive, and usually performed using a diverse set of tools and many users. To tackle the associated challenges, this paper presents NeuroBlocks, which is a novel visualization system for tracking the state, progress, and evolution of very large volumetric segmentation data in neuroscience. NeuroBlocks is a multi-user web-based application that seamlessly integrates the diverse set of tools that neuroscientists currently use for manual and semi-automatic segmentation, proofreading, visualization, and analysis. NeuroBlocks is the first system that integrates this heterogeneous tool set, providing crucial support for the management, provenance, accountability, and auditing of large-scale segmentations. We describe the design of NeuroBlocks, starting with an analysis of the domain-specific tasks, their inherent challenges, and our subsequent task abstraction and visual representation. We demonstrate the utility of our design based on two case studies that focus on different user roles and their respective requirements for performing and tracking the progress of segmentation and proofreading in a large real-world connectomics project.","Authors":"A. K. Ai-Awami; J. Beyer; D. Haehn; N. Kasthuri; J. W. Lichtman; H. Pfister; M. Hadwiger","DOI":"10.1109\/TVCG.2015.2467441","Keywords":"Neuroscience;Segmentation;Proofreading;Data and Provenance Tracking;Neuroscience;Segmentation;Proofreading;Data and Provenance Tracking","Title":"NeuroBlocks \u2013 Visual Tracking of Segmentation and Proofreading for Large Connectomics Projects","Keywords_Processed":"Data and Provenance Tracking;proofread;segmentation;Neuroscience","Keyword_Vector":[0.1451956288,-0.1844859721,-0.1712612223,0.1959059215,-0.1416835118,-0.0141816218,0.0602178781,-0.014570844,0.0409171754,-0.0128580187,0.035297102,0.0085543426,0.0239745712,-0.1314748132,0.0380580491,-0.0365108837,-0.032998562,-0.0357558112,-0.020105744,-0.0048524353,-0.0070533207,0.0168006799,0.0676018583,0.0110143754,-0.0202838021,-0.0197433848,-0.0209229131,0.021348943,0.0239521227,-0.0470022814,0.0529614318,0.0421620328,-0.0127254368,-0.0429575674,0.0757345199,-0.0513662008,-0.0258859335,-0.0373284599,-0.0558976129,-0.0716981164,-0.0107359784,0.1092979969,0.0098921125,-0.0746329655,-0.0141472368,-0.0062599388,-0.0232943665,-0.0341525514,0.0106534557,-0.0236561608,0.0404103609,-0.009651408],"Abstract_Vector":[0.208010439,-0.0836474459,0.0273855876,-0.0243107373,0.0031059139,-0.0700182645,-0.0123353307,-0.0258717922,0.0340102464,0.0358255372,0.010019725,-0.0366074752,-0.0717956309,-0.0180171408,-0.0395247868,-0.0246334691,0.0121889679,-0.0518443539,0.0272407051,-0.0646409077,-0.01272019,-0.0677142084,0.0554902342,-0.0456963758,-0.0002569604,0.0206090628,-0.0005652687,-0.0011689103,-0.0623860381,-0.023448996,0.1078105096,0.010101503,-0.0924736077,-0.002897286,0.0321732198,0.0324379216,-0.0313670095,0.0193300841,0.0158965377,0.0114695458,0.0201412746,-0.063498096,0.0278659025,-0.0043891888,0.0857824085,-0.0003180491,0.0544805717,0.0463769495,-0.0139669962,0.0309112331,-0.0199803545,0.0615186583,-0.0122200968,0.0125065455,0.0346789737,0.0029638811,-0.0044061637,0.0669979335,0.094588629,0.0633198324,-0.0230145482,-0.0029796701,0.0135646866,-0.0166037333,-0.0037614555,0.0014403883,-0.0497208794,-0.0275458847,-0.0239918629,0.0109906916,0.0479456692,-0.0105742609,-0.0292037144,-0.0031082893,-0.0216993145,-0.0049237524,-0.0668566716,0.0037350474,0.0383350448,0.0414759801,-0.0013738139,-0.0178787106,0.007336406,-0.0402558793,-0.0573422384,-0.0083582214,-0.0357367835,-0.0253991494,0.0622822721,0.0926594525,-0.0032403185,-0.0003163861,0.0287175373,0.0124303235,0.0495947002,0.0094702229,0.0365838346,0.051401403,-0.1371691511,-0.0051527933,-0.0642625354,-0.0053798644,0.0748340103,0.0513495148,0.0549112174,0.0652305529,0.0075918263,0.0740365079,-0.0490837137,0.0093222726,-0.0248669001,0.0433975501,0.0048792673,0.0375539007,-0.0283547689,-0.0734029041]},"255":{"Abstract":"Motion in depth is commonly misperceived in Virtual Reality (VR), making it difficult to intercept moving objects, for example, in games. We investigate whether motion cues could be modified to improve these interactions in VR. We developed a time-to-contact estimation task, in which observers (n = 18) had to indicate by button press when a looming virtual object would collide with their head. We show that users consistently underestimate speed. We construct a user-specific model of motion-in-depth perception, and use this model to propose a novel method to modify monocular depth cues tailored to the specific user, correcting individual response errors in speed estimation. A user study was conducted in a simulated baseball environment and observers were asked to hit a looming baseball back in the direction of the pitcher. The study was conducted with and without intervention and demonstrates the effectiveness of the method in reducing interception errors following cue modifications. The intervention was particularly effective at fast ball speeds where performance is most limited by the user's sensorimotor constraints. The proposed approach is easy to implement and could improve the user experience of interacting with dynamic virtual environments.","Authors":"R. A. Rolin; J. Fooken; M. Spering; D. K. Pai","DOI":"10.1109\/TVCG.2018.2859987","Keywords":"Virtual reality;motion perception;time to contact;augmented reality;games","Title":"Perception of Looming Motion in Virtual Reality Egocentric Interception Tasks","Keywords_Processed":"virtual reality;game;augmented reality;motion perception;time to contact","Keyword_Vector":[0.0218988543,0.0122430991,-0.0116854955,-0.0069395118,0.0091269035,-0.0189768518,-0.0301231833,-0.0011160163,-0.0693793784,-0.014981908,0.0350593061,0.0520996342,-0.001462507,0.0125741481,0.0531372128,0.0545887529,0.0416248845,0.0510494286,0.1453197552,-0.0109034242,-0.079167762,0.0586729217,0.0948855347,0.0492626513,-0.0641536439,-0.0291256557,-0.0147257551,-0.0208884189,0.0733823812,-0.1091582019,-0.0198603132,-0.0893226706,0.0495919763,0.0291294751,-0.0348065103,-0.023019498,-0.0111595247,0.0024671586,-0.031467839,-0.0374060056,-0.0174938152,0.0324187753,0.0205800213,0.0171833095,-0.0049977241,-0.0283394816,0.0039063912,0.0411134543,-0.0176578835,0.0257924102,-0.0256749613,0.0030123757],"Abstract_Vector":[0.093449425,-0.0210351883,0.0022133751,0.0007606811,-0.0433620275,0.0376646597,0.011104282,0.0144254297,0.0397987475,-0.0012742701,-0.009563777,0.0320068822,-0.0102916118,0.0120383171,0.0162940193,0.0212209274,-0.0117820988,-0.0027813968,0.0236324649,-0.0344981527,0.0116907483,-0.0401473562,0.0387232336,-0.0103640415,-0.0303652211,-0.0016070438,0.0045950434,-0.0118575276,-0.0145165301,0.0052242182,0.0611965694,0.0050320999,-0.0318281719,0.0005977106,0.0483848828,-0.0275302786,-0.0278916879,0.0087429252,0.0002271025,-0.0078299541,0.0607551609,0.0376459493,0.0209303588,0.0038788203,0.0003066972,0.0007759741,0.0182779246,0.0249360169,0.0088275598,0.0011123682,-0.0198959297,-0.0290691192,-0.0057463528,-0.0250091992,0.0325744302,-0.0029756954,0.0060601097,0.0403977353,-0.0271539741,-0.0076981869,0.0198146697,-0.0043626657,-0.0117881183,0.0075965424,-0.0200834801,-0.0329555792,-0.0214060798,-0.0439655102,-0.0351169105,0.0550890706,-0.0118456653,0.0213546062,0.0242108884,0.0042244713,0.0573807731,-0.0651308459,0.0409479586,0.0028394844,0.0334520382,-0.030955821,-0.0098338844,0.0336246718,0.0510025451,0.0036228075,-0.0222594335,-0.0067533189,0.0025970816,0.0000151775,-0.0108804252,-0.0244060327,0.0180067279,-0.032853293,0.0088738671,-0.0049730588,-0.0136599987,-0.0170551686,-0.0036550296,-0.0114173504,-0.0111404819,-0.0298646308,-0.0491711372,-0.0073104594,-0.0385797622,0.0061996,-0.0224299554,0.0343220342,-0.0280819091,0.0068802963,-0.0232583175,-0.0449137574,0.0326339423,0.0266054214,0.0268716588,-0.0262136887,-0.0215001931,-0.0115918756]},"256":{"Abstract":"Recent popularity of consumer-grade virtual reality devices, such as the Oculus Rift and the HTC Vive, has enabled household users to experience highly immersive virtual environments. We take advantage of the commercial availability of these devices to provide an immersive and novel virtual reality training approach, designed to teach individuals how to survive earthquakes, in common indoor environments. Our approach makes use of virtual environments realistically populated with furniture objects for training. During a training, a virtual earthquake is simulated. The user navigates in, and manipulates with, the virtual environments to avoid getting hurt, while learning the observation and self-protection skills to survive an earthquake. We demonstrated our approach for common scene types such as offices, living rooms and dining rooms. To test the effectiveness of our approach, we conducted an evaluation by asking users to train in several rooms of a given scene type and then test in a new room of the same type. Evaluation results show that our virtual reality training approach is effective, with the participants who are trained by our approach performing better, on average, than those trained by alternative approaches in terms of the capabilities to avoid physical damage and to detect potentially dangerous objects.","Authors":"C. Li; W. Liang; C. Quigley; Y. Zhao; L. Yu","DOI":"10.1109\/TVCG.2017.2656958","Keywords":"Virtual reality;modeling and simulation;virtual worlds training simulations","Title":"Earthquake Safety Training through Virtual Drills","Keywords_Processed":"virtual reality;virtual world training simulation;modeling and simulation","Keyword_Vector":[0.1583527223,-0.1736119454,-0.1845691035,0.141392216,-0.1383566608,-0.2407541621,0.0929856821,0.0328654072,0.1038193767,-0.0776226672,0.0386042311,0.0142526371,-0.0667602875,-0.1194427227,0.0105161857,0.0220927624,-0.0632871851,0.0668898204,-0.0131219004,-0.0359379378,-0.0170550645,0.0890019202,0.0198282761,0.0277489132,-0.0694191505,0.0269613385,-0.0148395689,0.0841007296,-0.0056849916,0.0350121195,0.0317774773,-0.0150535508,-0.1103310227,0.0133066839,0.0470345015,-0.0669674947,-0.0601729033,0.2153446311,-0.0786702574,0.2078346207,0.0885984492,-0.0474755111,0.0242301252,0.2170620338,-0.0029355965,0.0191253441,0.0677228499,0.011871586,-0.0473983814,0.0200487414,-0.0802579595,0.0212965983],"Abstract_Vector":[0.2232048891,-0.1283600932,0.0049099394,-0.0354413752,-0.0406246952,-0.0553015751,0.0134488363,0.0001848984,-0.0345947952,0.0163958577,0.0304215849,-0.0153471,-0.0207212424,-0.0218550841,-0.0314565215,-0.0330299306,-0.0167561292,-0.0031794715,-0.0321944186,0.0285044904,-0.0224146906,0.0107939983,0.0577822938,0.0333986688,-0.0292126801,-0.0049980108,-0.0073034401,0.0566335641,0.0399652651,-0.0442243937,0.0499276536,-0.0174792589,-0.0056667161,-0.0141588348,-0.0646911502,0.0186118631,-0.0279134793,0.000146824,-0.0010364951,0.0337130124,0.0148585915,-0.0331995069,-0.0039734769,0.0269398653,0.0150955738,0.0252132445,0.0286928578,0.0026027525,-0.0104103458,-0.0165667586,0.0068235736,-0.0422341411,0.0525981505,0.0087590128,0.0283323223,-0.0271196149,-0.0290148771,0.0436172347,0.0148320212,0.0271160137,-0.0155671007,-0.0050132098,-0.019817921,0.0280910772,0.0195168112,0.0121827261,-0.0432519461,-0.00456387,-0.0552553067,0.0041836277,0.0010757558,-0.0257366202,-0.0330746003,0.0004871672,-0.0226402769,0.0124096375,-0.0142255109,-0.0033264754,0.008190893,-0.0294152414,0.0149304142,-0.0548980141,0.031960306,-0.0105086842,-0.0276102123,0.0030024963,-0.0158504308,0.0150740422,-0.0232604136,-0.0429988273,-0.0241251651,0.0144584888,0.0022007117,-0.0090511468,0.018759145,0.0182549389,-0.0192316185,0.0272894165,0.0048137602,-0.0258580016,0.0194399027,0.0076772739,-0.0136938875,0.0148350418,0.0155973627,-0.0568114287,-0.0057165591,-0.000233082,-0.0298985771,0.0107502502,-0.0088386313,-0.0090142147,-0.0438088876,-0.024554413,-0.0228380243,0.0210767954]},"257":{"Abstract":"Anomalous runtime behavior detection is one of the most important tasks for performance diagnosis in High Performance Computing (HPC). Most of the existing methods find anomalous executions based on the properties of individual functions, such as execution time. However, it is insufficient to identify abnormal behavior without taking into account the context of the executions, such as the invocations of children functions and the communications with other HPC nodes. We improve upon the existing anomaly detection approaches by utilizing the call stack structures of the executions, which record rich temporal and contextual information. With our call stack tree (CSTree) representation of the executions, we formulate the anomaly detection problem as finding anomalous tree structures in a call stack forest. The CSTrees are converted to vector representations using our proposed stack2vec embedding. Structural and temporal visualizations of CSTrees are provided to support users in the identification and verification of the anomalies during an active anomaly detection process. Three case studies of real-world HPC applications demonstrate the capabilities of our approach.","Authors":"C. Xie; W. Xu; K. Mueller","DOI":"10.1109\/TVCG.2018.2865026","Keywords":"Call Stack;Performance Visualization;Representation Learning;Active Learning;Anomaly Detection","Title":"A Visual Analytics Framework for the Detection of Anomalous Call Stack Trees in High Performance Computing Applications","Keywords_Processed":"Representation Learning;call stack;performance visualization;Anomaly Detection;Active Learning","Keyword_Vector":[0.0725779283,0.030110545,0.0083726136,0.0518863678,0.126801232,-0.0332207819,-0.0317098723,0.0981169253,-0.0386403063,-0.0200405965,-0.0514088274,-0.0295841599,0.0380309034,0.0169196534,0.0918927025,0.0140199046,0.0500350143,0.1185305164,-0.0111298582,0.0210170692,0.0682593426,0.0164832176,-0.0091055151,0.0391270382,0.0536952298,-0.0200347929,-0.0721866464,-0.0224769914,0.0169358237,0.0193960845,-0.0242652183,0.0685726316,-0.0064052072,-0.0000727763,0.0476625357,0.0049298013,-0.0329878321,0.0574614273,-0.0513527893,-0.0716909463,0.0397980734,0.0166929903,-0.0477280749,-0.0017601339,0.0188245856,0.0055433423,-0.0167666762,-0.1020142269,0.0338851901,0.0179070279,0.0104353776,-0.0281105458],"Abstract_Vector":[0.1568590433,-0.033904776,-0.0233869125,0.0180005565,-0.0510811148,0.012664652,0.0159611537,0.0280162044,0.0078536937,-0.0212282201,-0.0162012701,0.0387184934,-0.0451984335,-0.0274083488,0.0076671854,-0.0079764397,-0.0395030824,0.0003952399,-0.0014819414,-0.019168185,0.0282213072,-0.0400182973,-0.0085715382,0.0201267231,-0.0182105133,-0.014794801,0.00975478,-0.0399860841,0.0154228103,-0.0070670603,0.035480349,0.0479362815,-0.0324494955,-0.0076526376,-0.0054196686,-0.0424285907,0.0090361688,0.0115586604,0.0296949176,-0.0092505884,-0.0031561372,0.0534195626,0.020341323,0.0520921335,0.0071858729,0.0450310583,-0.0254119926,-0.0091619727,0.0054907724,-0.0119973756,-0.0022509964,0.0246046054,0.0027844926,-0.0149492682,-0.000492788,-0.0243269955,0.0255766433,0.0078478266,0.0224480098,-0.0284317147,-0.0122800847,0.0246877062,-0.0051698171,0.0044907974,-0.032780674,-0.0406244453,0.0148854804,-0.0284640188,-0.0025549371,0.0127583816,0.0010839593,0.0185780411,-0.0295630549,0.0029182054,-0.0108301857,0.0010291799,0.0400326241,0.023962411,-0.0021307858,0.0019997436,0.0340465101,0.0284969305,0.0133355829,-0.0006707139,0.0046874632,-0.0003118322,0.05556238,-0.0248098365,-0.0246075034,-0.0244324785,0.006932782,-0.0205814319,-0.0071378552,0.0143680549,0.0005245497,0.0024699712,-0.0031403666,-0.0219771038,-0.0006698183,-0.03169703,0.0038986249,-0.007381765,0.0069296918,-0.0024388935,-0.0146820611,0.0230493413,-0.0236531096,-0.0138733328,-0.0152837454,0.0019681492,-0.0038942711,0.0001352389,-0.0211140912,-0.0378376499,-0.0223777187,-0.0080017316]},"258":{"Abstract":"We present a new algorithm for calculating the external labeling of ghosted views of moderately complex 3D models. The algorithm uses multiple criteria decision making, based on fuzzy logic, to optimize positions of the labels associated with different parts of the input model. The proposed method can be used with various existing algorithms for creating ghosted views from 3D models. The method operates in real-time, which allows the user to acquire a good understanding of the structure of the input model by studying the model and its labels from different viewpoints. We have conducted a user study to evaluate label layouts produced by our algorithm and those created by humans. The results show that the proposed method can significantly improve user understanding of labeled ghosted views of complicated 3D models, and its label layouts are comparable with label layouts created by humans.","Authors":"L. \u010cmol\u00edk; J. Bittner","DOI":"10.1109\/TVCG.2018.2833479","Keywords":"External labeling;ghosted views;illustrative visualization;empirical evaluation;visualization for the masses","Title":"Real-Time External Labeling of Ghosted Views","Keywords_Processed":"external labeling;empirical evaluation;ghost view;visualization for the masse;illustrative visualization","Keyword_Vector":[0.1137754086,0.1005111099,-0.0590587646,0.0086001646,0.0273888188,-0.0166296755,-0.038807982,-0.0283126087,-0.0670857024,-0.0339799247,0.0822076242,0.0624169437,0.0155984575,0.0934545834,0.1206475979,0.1138651267,-0.0013237796,-0.1114571966,0.2015118343,-0.0255728564,0.0107357702,0.0624793977,-0.0365136779,0.0494808651,-0.0795486731,-0.0715759186,0.1472392547,0.0399789676,0.0723455267,-0.1181256585,-0.0590296968,-0.0551697448,0.0515449554,-0.0875514041,0.0148630756,0.0066362451,-0.1070703363,-0.0357541322,-0.0329078837,-0.0111493197,-0.0094873049,0.0119799612,0.0426801596,0.0434737447,0.0337583073,-0.0102902835,-0.0341648399,0.0195037238,0.0135159979,0.0667375667,-0.0222601808,-0.0211611057],"Abstract_Vector":[0.1663899442,0.0242153191,-0.0061063889,-0.0282425491,-0.0398136104,0.0884962724,0.0559754229,0.0041972353,0.1029031549,0.0247164164,-0.0494681101,-0.0017087423,-0.0460193542,0.0305067713,-0.0190156053,-0.02478367,0.0131601596,-0.0027262286,0.0401317064,-0.0192555958,-0.0389361138,-0.0191460139,0.0987133552,0.02827292,-0.0042938491,-0.055021986,-0.006095504,0.0321829364,-0.0357441274,-0.0563131147,0.0491976153,-0.0239624276,-0.0494692333,-0.0583832285,0.007099045,0.0075016849,0.0011141654,-0.013399042,0.010457299,-0.0123735743,0.0289324997,0.0191617801,0.0583133658,0.0492039788,-0.0328576284,-0.0101166897,0.0021664039,0.0367588208,-0.0064956405,0.0279384227,0.0111611611,-0.0082737871,-0.0129594425,0.0132340182,0.0437353156,0.0291249489,0.018016982,-0.00679641,-0.0379085289,-0.0351532413,0.0346147251,-0.0133841213,0.0246402009,-0.0366077375,0.0062399692,-0.0136237604,0.0467365315,-0.0217218359,-0.0419781699,0.0356391881,0.0084284801,0.0519310056,0.063775161,0.045625485,0.0086886365,-0.0146797717,0.0007825438,0.0157456412,0.0334841067,-0.0505397879,-0.0026037066,0.024894737,0.0276189027,-0.0169607007,-0.0692820836,0.031271767,-0.0015433627,-0.0207037524,0.0462551007,-0.0390834307,0.0313061742,0.0298014317,0.0232170351,-0.0751064062,-0.0014563671,-0.0056312337,-0.0067498943,-0.0139108014,-0.0085229941,-0.0502994458,-0.0000453843,-0.0025409608,0.0157976529,-0.0172386321,-0.0633862755,-0.0332034198,-0.00579106,0.0551860276,-0.0285186988,-0.0346522382,-0.0238167898,0.0097114177,-0.0049833515,-0.0245530251,0.0859652041,-0.0021863995]},"259":{"Abstract":"We present a temporal 6-DOF tracking method which leverages deep learning to achieve state-of-the-art performance on challenging datasets of real world capture. Our method is both more accurate and more robust to occlusions than the existing best performing approaches while maintaining real-time performance. To assess its efficacy, we evaluate our approach on several challenging RGBD sequences of real objects in a variety of conditions. Notably, we systematically evaluate robustness to occlusions through a series of sequences where the object to be tracked is increasingly occluded. Finally, our approach is purely data-driven and does not require any hand-designed features: robust tracking is automatically learned from data.","Authors":"M. Garon; J. Lalonde","DOI":"10.1109\/TVCG.2017.2734599","Keywords":"Tracking;Deep Learning;Augmented Reality","Title":"Deep 6-DOF Tracking","Keywords_Processed":"augmented reality;tracking;deep Learning","Keyword_Vector":[0.0916285757,-0.0104269845,0.0734104178,0.0408823007,-0.0243575347,-0.023303273,-0.0287384548,0.0246361072,-0.022763589,0.1035490743,-0.0098225621,0.0656591711,0.0526291754,0.0392476313,0.0118169203,0.0735047478,0.0070334464,-0.0293409113,0.1359135917,0.0384401479,-0.0214690469,0.0295050883,-0.0127169949,-0.03373172,0.0233442495,0.0567077729,-0.0007637948,0.0093065643,-0.0056834667,0.0003555366,0.0423334194,-0.0022554046,-0.0083780351,-0.0617353265,0.0173085144,-0.0143357061,-0.0156835614,-0.0574570272,0.0073514491,0.0466211148,0.0072742761,-0.0384426907,0.0072016098,0.0450986627,0.0539039807,-0.0326220731,-0.0408638581,-0.0229379674,-0.0883086619,0.0008858355,0.0794165712,-0.0819203093],"Abstract_Vector":[0.2108375976,-0.0025336746,0.0113213942,-0.0089721182,-0.0405007273,0.0633261759,0.003949061,0.0590230735,0.0708152933,0.0106881236,-0.0819467017,0.0471351697,-0.0324758445,-0.0260947339,0.0233263411,-0.0163476187,0.0166234136,0.035173259,0.0081980754,0.0183746374,-0.0241521897,0.0088625616,0.0341944107,-0.013371284,-0.0215834772,-0.018186763,0.0169456037,0.0052429566,0.0494129894,0.0197110619,0.0086730009,0.0024098623,-0.0189696101,-0.0259744567,0.0280191402,-0.030891317,-0.0247494086,-0.0277850817,-0.0125253153,-0.0390807496,0.016860653,0.0187179502,0.0012939486,0.0109183405,0.0045736837,0.025223915,-0.0303447356,-0.0063256659,-0.0454769791,-0.0437161513,-0.0054540026,0.002680857,0.0241608234,-0.0112601881,0.0627194102,0.0011885856,-0.0096925524,-0.0318816921,0.0184456664,-0.0327289865,0.0223044442,0.0070142945,-0.0278964333,0.0078168187,-0.0152932662,-0.0130460105,-0.0081227673,0.0036065239,-0.0158369992,-0.0102429664,0.0007725592,-0.0107911326,0.0068007251,0.0176622228,0.002344517,-0.0040137317,0.0486782908,0.0236164066,0.0008714994,-0.0303962376,-0.0174705093,0.0129020827,0.0074129093,-0.0152921172,0.00500727,-0.0341697444,-0.0256285373,-0.0351388238,-0.0567411763,-0.0307274517,0.0190971196,-0.0141601926,-0.0075153851,-0.0010978581,0.0022603577,-0.0182604992,0.0147572678,-0.022845659,0.0077645951,0.0172224632,-0.0011184903,0.0100336882,0.0052117597,0.0117163576,-0.0001161345,0.0112345777,-0.0211324435,-0.0076888768,0.0086855228,0.0069833582,-0.0207046029,0.0181968797,-0.0044186138,-0.0119638773,0.0235914282,-0.0028083205]},"26":{"Abstract":"The joint bilateral filter, which enables feature-preserving signal smoothing according to the structural information from a guidance, has been applied for various tasks in geometry processing. Existing methods either rely on a static guidance that may be inconsistent with the input and lead to unsatisfactory results, or a dynamic guidance that is automatically updated but sensitive to noises and outliers. Inspired by recent advances in image filtering, we propose a new geometry filtering technique called static\/dynamic filter, which utilizes both static and dynamic guidances to achieve state-of-the-art results. The proposed filter is based on a nonlinear optimization that enforces smoothness of the signal while preserving variations that correspond to features of certain scales. We develop an efficient iterative solver for the problem, which unifies existing filters that are based on static or dynamic guidances. The filter can be applied to mesh face normals followed by vertex position update, to achieve scale-aware and feature-preserving filtering of mesh geometry. It also works well for other types of signals defined on mesh surfaces, such as texture colors. Extensive experimental results demonstrate the effectiveness of the proposed filter for various geometry processing applications such as mesh denoising, geometry feature enhancement, and texture color filtering.","Authors":"J. Zhang; B. Deng; Y. Hong; Y. Peng; W. Qin; L. Liu","DOI":"10.1109\/TVCG.2018.2816926","Keywords":"Geometry processing;mesh filtering;mesh denoising","Title":"Static\/Dynamic Filtering for Mesh Geometry","Keywords_Processed":"mesh filter;geometry processing;mesh denoise","Keyword_Vector":[0.006057425,-0.0065060724,-0.0059576436,0.0009580874,-0.0010266586,-0.0020164295,-0.0041070239,-0.0005010224,-0.0057833246,0.0004781113,-0.0027002851,0.0045645847,0.0011442518,-0.0104705427,0.0028456575,-0.000071759,-0.0043331021,-0.0047238267,0.0095732178,-0.0026771323,-0.0034891293,-0.002684348,0.0014434183,-0.0017010458,0.0040311242,0.0012159776,0.0144740834,0.0082297058,-0.0022641994,0.0004108122,-0.0006284176,0.0085166767,-0.0113867272,0.0073792907,-0.0027807601,-0.0047066238,-0.0155580645,0.0077896569,0.0087530113,0.0219794003,0.0047183165,-0.0111782292,0.0116632911,0.0191093644,-0.0093326184,0.0039906819,-0.0096116773,0.0013850687,0.002755722,0.012813406,-0.0069793665,-0.0099786274],"Abstract_Vector":[0.1575342014,-0.0192677313,-0.0435939956,0.0107496276,-0.0179080325,0.0502449913,0.0439878903,0.0192605089,0.0101031362,0.0448953946,-0.010826516,0.0058093265,-0.0754087698,-0.0004023618,0.0053672522,-0.0358595546,0.0153002374,0.0845502556,-0.0132823437,0.0187709237,-0.016158071,-0.027781439,0.0001509368,0.0480165479,0.0223758594,-0.0357125924,-0.0065851109,-0.0248658167,0.0251909572,0.0206806573,-0.0116948326,-0.0044014128,0.018003371,-0.0162562123,-0.0095721104,-0.0255106301,0.010714166,-0.0299141791,0.0422403703,0.0242778561,0.0444005027,0.0044994451,-0.0003355664,0.0651945711,0.0482139104,-0.009580894,-0.0032462831,0.0037248844,-0.0009556248,0.0076035044,-0.0255565347,0.0067072187,0.0042863921,-0.0102909005,0.0087776644,0.0143279747,0.0015172886,-0.0057444429,0.0036285855,-0.035465954,0.0110610968,-0.0067173119,0.0094825016,-0.0137947446,-0.0414866103,-0.0019125748,0.0071610254,-0.0232956374,-0.0325494027,-0.0083975609,0.0077653455,0.0289492969,-0.021464552,0.0456606339,-0.0243580351,-0.0057016293,0.0156657187,0.0414505943,-0.0234593086,-0.0035617669,-0.0057250361,0.0106922951,-0.000800904,-0.002572946,-0.00621971,-0.0439945233,-0.0090344732,-0.0261536856,-0.0141878649,-0.0434980146,0.0219337002,0.0006606703,0.0033562633,0.0179952079,-0.0024411124,-0.0108661136,0.0256855612,0.0009425026,-0.006449512,0.0020272022,-0.0020585927,0.0018154632,-0.0096693177,-0.0279496541,-0.0038312429,-0.0211823531,-0.0095215898,-0.0005756694,-0.0000386014,-0.0010430163,0.0018856483,0.0229865527,-0.0127281062,-0.0033579922,-0.0023757018,0.0261095915]},"260":{"Abstract":"Node-link diagrams provide an intuitive way to explore networks and have inspired a large number of automated graph layout strategies that optimize aesthetic criteria. However, any particular drawing approach cannot fully satisfy all these criteria simultaneously, producing drawings with visual ambiguities that can impede the understanding of network structure. To bring attention to these potentially problematic areas present in the drawing, this paper presents a technique that highlights common types of visual ambiguities: ambiguous spatial relationships between nodes and edges, visual overlap between community structures, and ambiguity in edge bundling and metanodes. Metrics, including newly proposed metrics for abnormal edge lengths, visual overlap in community structures and node\/edge aggregation, are proposed to quantify areas of ambiguity in the drawing. These metrics and others are then displayed using a heatmap-based visualization that provides visual feedback to developers of graph drawing and visualization approaches, allowing them to quickly identify misleading areas. The novel metrics and the heatmap-based visualization allow a user to explore ambiguities in graph layouts from multiple perspectives in order to make reasonable graph layout choices. The effectiveness of the technique is demonstrated through case studies and expert reviews.","Authors":"Y. Wang; Q. Shen; D. Archambault; Z. Zhou; M. Zhu; S. Yang; H. Qu","DOI":"10.1109\/TVCG.2015.2467691","Keywords":"Visual Ambiguity;Visualization;Node-link diagram;Graph layout;Graph visualization;Visual Ambiguity;Visualization;Node-link diagram;Graph layout;Graph visualization","Title":"AmbiguityVis: Visualization of Ambiguity in Graph Layouts","Keywords_Processed":"graph layout;visual ambiguity;graph visualization;visualization;node link diagram","Keyword_Vector":[0.0462902854,0.0360382234,-0.0048000976,0.0482801227,0.0541739208,-0.0436932596,-0.0133974266,-0.0843318367,-0.0294606965,0.024346348,0.0104473202,-0.0004646318,-0.0523742823,0.0199769056,-0.0424614702,-0.0553703186,-0.0626415732,-0.006992471,-0.0269278289,0.0803636288,-0.0015036564,-0.0201813778,-0.0563343401,0.0711381835,0.0240312213,-0.0902383219,-0.0237909846,-0.0322332035,-0.0505779503,-0.0729122606,0.0356499925,-0.0970600502,-0.0706882214,-0.0014812867,0.0118003026,0.0231849148,0.0213229923,0.0739450257,-0.0527426611,0.0064955235,0.0036435705,0.0223690724,-0.1304208667,-0.0147749012,0.0341507017,-0.0678407852,-0.0711582595,0.1736160837,0.0201696064,-0.0789940162,-0.0072244486,-0.0588766849],"Abstract_Vector":[0.2044316458,0.1317465601,-0.0708806374,0.0232112643,0.010896202,-0.0860614836,-0.0103665914,0.080844478,-0.1243755028,-0.058131288,-0.056150825,0.016174311,0.0169037075,0.1117651158,0.0126022191,0.0070740891,-0.0785998831,0.0071583446,0.018979293,0.027463621,0.0283117174,-0.0036487237,0.051873745,-0.0073541537,0.0330883135,-0.0560419727,0.0118044822,0.0313658059,-0.0575074411,0.0235839887,-0.051996757,-0.030312465,-0.0061272288,-0.0041579398,-0.0111211313,-0.0213322717,0.0393263691,0.0175204884,-0.0217488898,-0.0127362286,0.0309269327,-0.0355001449,-0.0158237153,0.0193703034,0.034696952,-0.0675753568,0.0054987544,0.0047609792,-0.0095699367,0.0471544546,-0.0010592031,0.025532618,0.0183351356,0.0016731046,-0.0241502297,0.0654343791,0.0166114176,0.0709005895,-0.0066398022,-0.0092155718,0.0035365365,0.0011723681,-0.0131979484,-0.002223356,0.0664044158,-0.0139747012,-0.0225608919,-0.0507131518,0.0065048981,-0.0115337337,-0.0253015547,0.0208546475,0.0070886399,0.0097939432,0.0154447706,0.0102205475,0.0558431848,0.0030388821,0.0129286645,0.0073120178,-0.016104151,-0.0103416022,0.0112196454,-0.0233143717,0.0023598525,-0.1027579016,0.0129453538,-0.0088687268,0.0106889085,-0.0386171971,0.0024888519,-0.0263456602,-0.0093063504,-0.0186995749,-0.0077475061,-0.0257862634,0.0243381316,-0.0306511267,-0.0014415334,-0.0104684955,-0.0218620364,0.0112124862,-0.0365183805,-0.0115432207,0.0350850462,-0.0158243982,-0.0303678025,-0.0276785744,-0.0015960251,-0.0677947501,-0.0288118086,-0.0215249604,0.017357825,0.0084443797,-0.0289410927,-0.0057221855]},"261":{"Abstract":"Realistic Rendering of thin transparent layers bounded by rough surfaces involves substantial expense of computation time to account for multiple internal reflections. Resorting to Monte Carlo rendering for such material is usually impractical since recursive importance sampling is inevitable. To reduce the burden of sampling for simulating subsurface scattering and hence improve rendering performance, we adapt the microfacet model to the material with a single thin layer by introducing the extended normal distribution function (ENDF), a new representation of this model, to express visually perceived roughness due to multiple bounces of reflections and refractions. With such a representation, both surface reflection and subsurface scattering can be treated in the same microfacet framework, and the sampling process can be reduced to only once for each bounce of scattering. We derive analytical expressions of the ENDF for several cases using joint spherical warping. We also show how to choose proper shadowing-masking and Fresnel terms to make the proposed bidirectional scattering distribution function (BSDF) model energy-conserving. Experiments demonstrate that our model can be easily incorporated into a Monte Carlo path tracer with little extra computational and storage overhead, enabling some real-time applications.","Authors":"J. Guo; J. Qian; Y. Guo; J. Pan","DOI":"10.1109\/TVCG.2016.2617872","Keywords":"Microfacet;layered material;normal distribution function;BSDF","Title":"Rendering Thin Transparent Layers with Extended Normal Distribution Functions","Keywords_Processed":"normal distribution function;BSDF;layer material;Microfacet","Keyword_Vector":[0.1524242659,-0.1559707417,-0.1277336498,0.074806264,-0.0963487095,-0.1686882672,0.0734244938,0.0340627605,0.0741314857,-0.0339291777,0.0172524474,-0.0022234983,-0.0429997217,-0.1040991242,0.0359188119,0.0169331355,-0.0406027064,0.0275023833,-0.0094613668,0.0270110904,0.0045209164,0.0233253925,0.0168416302,0.0014788414,-0.0001049176,0.0185943273,0.029464124,0.0064771566,0.0212893811,-0.02398004,0.0306171441,0.020794139,0.0049359264,0.0023425614,0.0083515633,-0.0001435542,0.0110568598,-0.0096459468,0.0014517252,-0.0228591162,-0.0152231193,-0.0153084932,0.0253132205,-0.014610703,-0.0039306063,-0.0191321287,-0.0242493182,-0.0009019066,0.0107009486,0.0095861266,0.0195866408,0.0156748712],"Abstract_Vector":[0.1656245282,-0.1000735277,0.0011067705,0.0136347729,-0.0950518343,0.03755502,0.0297675083,0.0336937862,-0.0525623809,-0.0198843381,-0.0153617727,-0.0000220441,-0.0180493936,-0.013663986,0.0006886694,0.0444361672,0.0604164013,0.061775921,-0.0080124618,-0.0603498055,0.0498841679,0.0042959798,0.0235409003,0.0193548528,-0.0129036584,-0.0033498084,0.0203845959,-0.0194393011,0.0389293811,0.0187829403,-0.0267305371,-0.0195970068,0.0250188194,0.0089215785,-0.0137212176,-0.05507309,0.0255765827,-0.0718003875,-0.0145660041,-0.0143692212,-0.0429705699,0.0195344048,-0.0222263036,0.0021767747,0.0045447429,0.017390798,0.0031225286,0.0219319885,0.0011516939,-0.0265909405,-0.010451695,-0.0076767316,0.0306757803,0.0259718686,0.0156144558,-0.0161990113,0.0331668733,-0.014625073,0.0208488356,-0.0178441465,0.0316928183,-0.0143123751,-0.0296172609,0.000816069,0.0165954739,-0.0001407009,0.0199987389,0.0053366935,-0.0424997859,-0.0241703323,-0.0430762185,-0.028975163,0.0123575085,0.004431052,-0.0203002454,0.0461263845,-0.0331380923,0.0226553705,0.001458019,-0.0093781067,-0.029346317,0.0067862904,0.0163938058,0.0210808288,0.0277018715,-0.0083066882,-0.0063639587,-0.0118670383,-0.0460946428,-0.0228776489,0.0093169773,0.0029004997,0.0241627425,-0.0158547218,-0.0043708681,0.0007448637,-0.0185925932,0.0130164777,-0.0558886226,-0.0154027156,-0.0027404018,0.0147108098,0.0139672027,-0.0517899416,0.0279682626,-0.0172505991,-0.0031762972,0.0075992447,0.015834425,-0.0061111883,-0.02276511,-0.003675475,0.0261633532,0.0296649035,0.0192727283,-0.020438324]},"262":{"Abstract":"Olfactory feedback for analytical tasks is a virtually unexplored area in spite of the advantages it offers for information recall, feature identification, and location detection. Here we introduce the concept of information olfactation as the fragrant sibling of information visualization, and discuss how scent can be used to convey data. Building on a review of the human olfactory system and mirroring common visualization practice, we propose olfactory marks, the substrate in which they exist, and their olfactory channels that are available to designers. To exemplify this idea, we present viScent: A six-scent stereo olfactory display capable of conveying olfactory glyphs of varying temperature and direction, as well as a corresponding software system that integrates the display with a traditional visualization display. Finally, we present three applications that make use of the viScent system: A 2D graph visualization, a 2D line and point chart, and an immersive analytics graph visualization in 3D virtual reality. We close the paper with a review of possible extensions of viScent and applications of information olfactation for general visualization beyond the examples in this paper.","Authors":"B. Patnaik; A. Batch; N. Elmqvist","DOI":"10.1109\/TVCG.2018.2865237","Keywords":"Olfaction;smell;scent;olfactory display;immersive analytics;immersion","Title":"Information Olfactation: Harnessing Scent to Convey Data","Keywords_Processed":"smell;olfaction;immersive analytic;scent;olfactory display;immersion","Keyword_Vector":[0.1656549977,-0.1796191415,-0.1603624955,0.1908224221,-0.1284674299,-0.1441495164,0.1014043982,-0.0325535422,0.1067768271,-0.0127667851,0.0544932612,0.0328527837,-0.0537703239,-0.0875527009,0.0622092476,0.1080078753,-0.0467529033,0.0325973032,-0.082612004,-0.0387356194,0.0127084633,0.096986567,0.0402661923,0.1001757617,-0.0704941439,0.0363213521,-0.0006721681,-0.0034217317,-0.0774044579,0.0842317748,0.013545622,-0.0682340216,-0.0559658206,0.0516228902,0.0819525734,0.0112030681,-0.0150384182,0.171970321,-0.0034521595,0.0719421954,0.0989844776,-0.0589301167,0.0032340668,0.1149255292,0.070894598,-0.0923908174,0.0321017848,-0.0554006629,-0.0023668281,-0.0138216514,-0.0617672937,0.0223923847],"Abstract_Vector":[0.2895197324,-0.1433175351,0.0188053861,-0.0111869174,-0.0469598505,-0.0736667492,0.0113836404,0.0084051199,-0.0178998572,0.0137765561,0.0045256044,-0.0172981816,-0.0546445652,0.0475793654,-0.0760577215,-0.0752751101,-0.0007025732,-0.0400538669,-0.0374958242,0.0117928037,-0.0206213177,0.0220008424,0.0666072786,0.0234631166,-0.0428242631,0.0215441169,0.0535449352,0.0273894324,-0.0350088238,-0.0864604942,0.0031824909,0.0207823602,-0.1018199058,-0.0407586863,0.0080822628,0.0712744878,-0.0808778533,0.0350023169,-0.0042590004,0.0092631822,-0.0148485427,0.0054633622,0.0719803634,-0.0488040761,0.0322116834,0.0141619655,0.0360675761,0.0099975579,0.021083404,-0.047422946,-0.0547264264,-0.0007962823,-0.0573661247,0.0557614124,-0.0320148116,0.0150719819,0.004563572,0.0475347844,0.0559839568,0.0425247583,-0.0545198648,0.0130932914,0.0840058747,-0.0543793302,-0.0848831012,-0.0143542838,-0.0037358201,0.1421189248,0.0055861659,-0.0729877712,0.0111989167,0.0096424715,0.0334738486,0.039458378,0.0282353515,-0.0338554456,0.0267120988,-0.0101788362,-0.0022387055,-0.0355480235,-0.0501528771,0.024137773,-0.0300898723,-0.0411072435,0.1529603034,-0.0305234638,0.0119091343,-0.0318199634,0.0435274052,0.0165045615,0.0066506983,-0.0217478286,-0.0226352745,-0.0202106446,-0.0326802277,0.0048248301,0.0238396004,0.0442131975,0.0002204449,0.0253676407,-0.0144504306,0.0056863591,-0.0542382512,0.0285759366,0.0016678246,-0.0250985543,0.0411246078,-0.0008604086,0.0477743376,-0.0376050179,-0.042809091,-0.0225857881,0.00739759,-0.0159226233,-0.0220222329,0.0052703253]},"263":{"Abstract":"Working with noisy meshes and aiming at providing high-fidelity 3D object models without tampering the metric quality of the acquisitions, we propose a mesh denoising technique that, through a normal-diffusion process guided by a curvature saliency map, is able to preserve and emphasize the natural object features, concurrently allowing the introduction of a bound on the maximum distance from the original model. Moreover, both the position of the mesh vertices and the edge orientations are optimized through a tailored geometric-aliasing correction. Thanks to an efficiently parallelized procedure, we are able to process even large models almost instantly with a parameter configuration that does not depend on the scale of the object. An essential survey on mesh denoising is also presented which is functional to the definition of a common framework where to set up our solutions and the related technical and experimental comparisons. The proposed results prove the effectiveness of our method, especially on the challenging target application profiles. Where competing techniques tend to inappropriately recover sharp edges while deforming the surrounding geometry or, on the contrary, to oversmooth shallow features, our method protects and enhances the natural object features and effectively reduces scanning noise on the smooth parts, while guaranteeing the prescribed metric-fidelity to the input model.","Authors":"M. Centin; A. Signoroni","DOI":"10.1109\/TVCG.2017.2731771","Keywords":"Mesh denoising;high-fidelity 3D modeling;feature preservation;saliency-driven geometry processing;scale-invariance;surface normal diffusion;geometric aliasing","Title":"Mesh Denoising with (Geo)Metric Fidelity","Keywords_Processed":"scale invariance;saliency drive geometry processing;surface normal diffusion;geometric aliasing;feature preservation;Mesh denoise;high fidelity 3d modeling","Keyword_Vector":[0.1826932108,-0.0596740592,0.0472645012,-0.1216940405,0.0138707951,-0.016109825,-0.0727354233,-0.0045048435,-0.0237672932,-0.0386432283,-0.0268985526,0.0215408077,-0.0501091265,-0.009532626,-0.0748990472,0.0569289086,-0.1307910862,0.0468057287,-0.0282868174,0.0052150054,-0.0062718542,-0.0498162988,0.0065161137,-0.0769394513,-0.0317573665,0.011733097,-0.0114428576,0.0831319009,0.0299649174,-0.0397196205,0.0659917358,0.1264342537,0.0619972409,0.1035180773,-0.039936439,0.0796093008,-0.0612556743,0.0599620114,0.004084404,-0.0424446294,-0.0419752101,0.0451983262,-0.0070888264,0.0733754411,0.0017670638,-0.0331824094,-0.0483216966,0.0291920886,0.0189483843,-0.0027858429,0.0033326578,-0.0016249058],"Abstract_Vector":[0.2231924622,-0.0947092546,0.0114623025,-0.0429976894,-0.0368768119,-0.0246062821,-0.0313080667,-0.0024757504,-0.0072500056,-0.0653710566,-0.005147585,0.0465140136,0.0836859218,0.021846971,0.0173610356,-0.0785991108,-0.0780396342,0.0097225145,-0.0185735281,-0.042743636,0.0174917751,0.0017112273,0.0013124179,0.0176846539,0.0003233046,0.0276562016,-0.0138593521,-0.0647041499,-0.0035747722,0.0329820613,0.0259900232,-0.0092062611,0.0084562719,-0.0399856303,-0.0268771505,-0.0270555578,0.0163453465,0.0009001706,0.0040606883,0.0197928945,-0.0157091714,-0.0321327584,0.0102360716,-0.0156359403,0.0368607599,0.0094470456,0.0136645919,0.0260364155,0.0179613765,0.014141348,0.0436044363,0.0198014542,0.0111193186,0.0096571296,-0.0606453267,-0.0064140954,-0.0059661145,0.0167630944,-0.0143524012,0.0215192143,0.0050651187,0.0134640164,-0.009349488,0.0093718379,-0.0160902082,0.0193653593,0.0203970409,0.0022740489,-0.0171292993,0.0067920772,0.0560567104,0.0214901649,0.0423537588,-0.0312724158,-0.0567786999,-0.0377119552,0.0139290674,0.009455985,-0.0262408457,-0.001453988,-0.0060218545,0.0091844757,-0.0166994361,0.0463537929,0.0105041633,-0.03428089,0.0292753839,-0.0507406446,0.0096804497,-0.0233292438,-0.0427297227,-0.0115778488,-0.0379056484,-0.0344601316,-0.0142170788,-0.0202792181,-0.0265499197,-0.0162394097,-0.0232775076,0.018927197,0.0199301179,0.021970495,0.0127669041,-0.0432490291,-0.0122144287,-0.0238919775,0.0444982531,0.041786568,-0.0268128266,-0.016280704,-0.0230845822,0.0237265179,-0.0272203086,0.0253240166,-0.0580496367,0.0000366683]},"264":{"Abstract":"This article reports the impact of the degree of personalization and individualization of users' avatars as well as the impact of the degree of immersion on typical psychophysical factors in embodied Virtual Environments. We investigated if and how virtual body ownership (including agency), presence, and emotional response are influenced depending on the specific look of users' avatars, which varied between (1) a generic hand-modeled version, (2) a generic scanned version, and (3) an individualized scanned version. The latter two were created using a state-of-the-art photogrammetry method providing a fast 3D-scan and post-process workflow. Users encountered their avatars in a virtual mirror metaphor using two VR setups that provided a varying degree of immersion, (a) a large screen surround projection (L-shape part of a CAVE) and (b) a head-mounted display (HMD). We found several significant as well as a number of notable effects. First, personalized avatars significantly increase body ownership, presence, and dominance compared to their generic counterparts, even if the latter were generated by the same photogrammetry process and hence could be valued as equal in terms of the degree of realism and graphical quality. Second, the degree of immersion significantly increases the body ownership, agency, as well as the feeling of presence. These results substantiate the value of personalized avatars resembling users' real-world appearances as well as the value of the deployed scanning process to generate avatars for VR-setups where the effect strength might be substantial, e.g., in social Virtual Reality (VR) or in medical VR-based therapies relying on embodied interfaces. Additionally, our results also strengthen the value of fully immersive setups which, today, are accessible for a variety of applications due to the widely available consumer HMDs.","Authors":"T. Waltemate; D. Gall; D. Roth; M. Botsch; M. E. Latoschik","DOI":"10.1109\/TVCG.2018.2794629","Keywords":"Avatars;presence;virtual body ownership;emotion;personalization;immersion","Title":"The Impact of Avatar Personalization and Immersion on Virtual Body Ownership, Presence, and Emotional Response","Keywords_Processed":"emotion;presence;virtual body ownership;personalization;avatar;immersion","Keyword_Vector":[0.1731384801,-0.1846337253,-0.1238821456,-0.0090091438,-0.0545213981,-0.1598909514,0.0350682396,0.0168467209,0.0792920836,0.0214017394,0.0166174776,-0.0983513227,-0.0261573731,-0.0898764055,0.0662715485,-0.016062279,-0.0235674692,-0.0353030589,-0.0089273797,0.0240515831,-0.0006372651,-0.000674297,0.0231662141,0.0009452101,-0.0118440779,0.0007711914,0.0136508963,0.0015861612,0.0295913812,-0.0074280669,0.016937942,0.003112706,0.0121294194,0.0222425809,-0.0263999889,-0.0197697904,0.0032885877,-0.0060757426,0.0134414126,-0.0012269786,0.0101360474,-0.0190176595,0.0210299327,0.0167400694,-0.004815007,-0.0078785531,-0.0159469308,-0.0078271877,0.0073535147,0.0202391818,-0.0096608962,0.0041957252],"Abstract_Vector":[0.1881470239,-0.1136927644,-0.0154006315,0.0521752624,-0.0801677159,0.0483888004,0.0473352064,0.037128629,-0.0407797516,-0.021662686,0.0129440634,-0.0354091773,-0.0277381708,-0.0005572493,0.0295268676,0.0442056124,0.1003821787,0.1034878601,-0.0123497047,-0.0782023987,0.0775712597,-0.0217996919,0.0205195209,0.0071258664,-0.0373689846,-0.0004128166,0.0537442719,-0.0206042462,0.0088813758,-0.018666381,-0.0598922507,-0.0204441458,0.0153049411,0.0139217444,-0.0536978741,-0.0008057142,0.0232356698,-0.0145058742,-0.0602720304,-0.0397257473,-0.0756803419,0.0460968705,-0.0202076138,-0.0236403608,-0.0173651565,0.0339724735,0.0081178934,0.050739127,-0.0321818758,-0.0001631802,0.0031135068,-0.0090141695,-0.0035749443,0.0214310857,-0.0043069615,-0.0354935397,0.0090544234,-0.0041546366,0.0399258087,0.0259293219,0.0189414807,-0.0056272784,-0.0293182598,0.0028533744,0.0103955736,-0.0036710851,0.0112032959,-0.0055811683,0.0019763092,-0.014675434,-0.0257439946,-0.0000942116,0.0148107997,0.0034104768,-0.0225078657,0.0472908757,-0.0053425024,-0.0289808418,0.0281693873,0.0131297706,0.0093967258,-0.0182101893,-0.0267500098,0.0099034748,-0.0003796758,0.0206123034,-0.000136142,0.0028181231,-0.0025930158,-0.0415288132,-0.0050144058,0.0086716064,-0.0053176814,-0.0210551657,-0.0419303798,0.050714466,0.0054771173,-0.008996398,-0.0100586689,-0.0063690678,0.0010403175,0.0420426147,0.0461918167,-0.0205273983,-0.0126827948,-0.0121843759,-0.0109695634,0.0088140682,-0.005425011,-0.0053983338,-0.0234934521,0.0054983911,0.030692747,0.0185784859,0.0265668879,-0.0232194471]},"265":{"Abstract":"Social media data with geotags can be used to track people's movements in their daily lives. By providing both rich text and movement information, visual analysis on social media data can be both interesting and challenging. In contrast to traditional movement data, the sparseness and irregularity of social media data increase the difficulty of extracting movement patterns. To facilitate the understanding of people's movements, we present an interactive visual analytics system to support the exploration of sparsely sampled trajectory data from social media. We propose a heuristic model to reduce the uncertainty caused by the nature of social media data. In the proposed system, users can filter and select reliable data from each derived movement category, based on the guidance of uncertainty model and interactive selection tools. By iteratively analyzing filtered movements, users can explore the semantics of movements, including the transportation methods, frequent visiting sequences and keyword descriptions. We provide two cases to demonstrate how our system can help users to explore the movement patterns.","Authors":"S. Chen; X. Yuan; Z. Wang; C. Guo; J. Liang; Z. Wang; X. Zhang; J. Zhang","DOI":"10.1109\/TVCG.2015.2467619","Keywords":"Spatial temporal visual analytics;Geo-tagged social media;Sparsely sampling;Uncertainty;Movement;Spatial temporal visual analytics;Geo-tagged social media;Sparsely sampling;Uncertainty;Movement","Title":"Interactive Visual Discovering of Movement Patterns from Sparsely Sampled Geo-tagged Social Media Data","Keywords_Processed":"spatial temporal visual analytic;movement;Geo tag social medium;uncertainty;sparsely sampling","Keyword_Vector":[0.0715784243,0.0152357506,-0.0211712368,-0.023659818,0.0674192622,-0.0245228233,0.1193943431,-0.0122361158,-0.0757295877,-0.0748126053,0.0290191937,0.0484310896,0.0306062179,0.0777837008,0.0647433044,0.035347393,-0.0357029202,-0.1451873507,0.1296226542,-0.0370920401,0.0189422827,-0.116075025,-0.0664547452,0.116474701,-0.017231108,-0.1357862445,0.2479447789,0.0839163355,-0.0949874106,0.0719781825,-0.0390300142,0.0563404704,-0.004130965,-0.1771367486,0.0612375318,0.1420516935,-0.1038016634,-0.0916054558,-0.0634605193,-0.0777068784,0.108705982,-0.0559770014,0.0505762422,0.0657032146,0.029196568,-0.1146852123,0.0468583063,-0.0033824478,0.0641143362,-0.0051645582,-0.0377407349,-0.0125659245],"Abstract_Vector":[0.180742475,0.0239248483,-0.048003986,0.0349330805,0.0136883661,0.1783767893,-0.0309387707,0.0275279992,0.0924303094,-0.0366045676,-0.0442261399,-0.0073861579,-0.1379121633,-0.0526198785,-0.1978795646,-0.1316849745,0.0614948602,0.0838830356,-0.0633376071,-0.0189795375,-0.0526862594,0.1326345885,-0.043762202,-0.083248737,0.0502351639,-0.0575298209,-0.0307957456,-0.0142553405,-0.0706858182,0.0117969906,0.0439941553,-0.0335839426,0.0379986197,-0.0089302992,0.0503515392,0.0217835769,-0.0214528852,-0.0062187913,-0.0627081474,0.0163366973,0.0294692274,-0.0275413563,-0.0170662711,-0.0249508746,-0.0526353244,-0.0128136603,0.1241603689,0.0093822964,0.026297044,0.0430903615,-0.0146388479,-0.0155170232,0.0204041426,0.0451976511,-0.0333174251,-0.0164273045,-0.0244799486,0.02584685,0.0317817702,-0.0483012556,0.029817056,-0.0057298625,-0.0208531991,-0.0143024569,0.029401197,0.0250595715,-0.0300653146,-0.0293408966,0.0157804771,-0.0075368884,0.0598611769,-0.0102723638,-0.0471545371,0.0066776273,0.0243755582,0.0106525508,-0.0389604605,-0.0351604192,-0.0183627535,-0.0125763448,-0.0115186611,0.0289654179,0.0108185418,0.0285157414,0.0570525971,0.0098432758,0.0159313624,0.0200946846,-0.010119176,-0.0103273779,0.0340817522,0.038013455,-0.0192882776,-0.0074107124,0.0025655891,-0.007141263,0.0368187634,-0.007691328,-0.0241796203,0.031546178,-0.0203278357,0.0122087663,-0.0011293746,0.0101894339,0.0020197273,-0.004400975,-0.0451718542,-0.0022403496,0.0178624401,0.0315388346,0.0089843752,0.0063676429,0.0229887978,0.0076469318,0.0134209973,-0.0025309263]},"266":{"Abstract":"This paper presents the extended work on LazyNav, a head-free, eyes-free and hands-free mid-air ground navigation control model presented at the IEEE 3D User Interfaces (3DUI) 2015, in particular with a new application to the head-mounted display (HMD). Our mid-air interaction metaphor makes use of only a single pair of the remaining tracked body elements to tailor the navigation. Therefore, the user can navigate in the scene while still being able to perform other interactions with her hands and head, e.g., carrying a bag, grasping a cup of coffee, or observing the content by moving her eyes and locally rotating her head. We design several body motions for navigation by considering the use of non-critical body parts and develop assumptions about ground navigation techniques. Through the user studies, we investigate the motions that are easy to discover, easy to control, socially acceptable, accurate and not tiring. Finally, we evaluate the desired ground navigation features with a prototype application in both a large display (LD) and a HMD navigation scenarios. We highlight several recommendations for designing a particular mid-air ground navigation technique for a LD and a HMD.","Authors":"P. Punpongsanon; E. Guy; D. Iwai; K. Sato; T. Boubekeur","DOI":"10.1109\/TVCG.2016.2586071","Keywords":"3D user interface;spatial interaction;virtual reality;navigation","Title":"Extended LazyNav: Virtual 3D Ground Navigation for Large Displays and Head-Mounted Displays","Keywords_Processed":"spatial interaction;virtual reality;3d user interface;navigation","Keyword_Vector":[0.1099940136,-0.0248195336,-0.0666951021,-0.0750020307,0.0297771228,-0.0941719331,-0.1282170572,-0.0198186419,-0.166896226,0.08709717,-0.1045141542,0.0486097288,0.0654032776,0.0257540606,0.0217416482,-0.0523880653,-0.0027650963,0.0187178913,0.0364695713,-0.1206593816,0.1668374921,0.0777890177,0.0651029826,0.1288317624,0.0313750052,-0.0275236669,0.1072194535,0.0239910068,-0.0661076133,0.0366905616,-0.0363379336,0.0654981096,-0.0759341366,-0.0279175505,0.108300447,0.0779487933,-0.0927986872,-0.0033545936,0.009499848,-0.0550032698,0.0382659182,-0.0940139539,-0.0007498167,0.0241504048,0.0109722492,-0.0129750035,0.0890207301,-0.0656322603,-0.0276148542,0.0330330018,0.0147478109,0.0047848387],"Abstract_Vector":[0.2341319716,-0.1130023605,-0.0392958765,-0.0190036351,-0.1080096971,0.0956713295,-0.0019511262,0.0519346224,-0.0114437961,-0.0432736115,-0.074001114,0.0580114844,-0.0425506741,-0.0049242074,0.0010137342,-0.0331282917,-0.0312508293,0.0453456923,-0.0634805351,-0.043547201,0.0378030552,0.0304784995,-0.0285687993,-0.0433302382,0.0053155146,-0.0240515798,-0.0403307286,-0.0622234366,-0.0664960897,-0.0210039222,-0.0145462187,0.009917841,0.0359526396,0.0546565362,-0.0129354659,0.0032285175,-0.0652725194,-0.0635156566,-0.0317291282,-0.0409607489,-0.0726641635,0.0269332605,0.0537211488,-0.1088322575,-0.0120777384,0.0060189887,0.0209688226,0.0837761704,0.0049873039,0.014228911,0.0509158939,0.0085457552,0.0574725485,0.0568917497,0.0692942476,0.0093669541,-0.023870289,-0.0590910703,0.0050343993,-0.0386691237,-0.0087194478,-0.009617961,-0.0057570065,-0.0412369745,-0.0610304338,0.0509448061,-0.0175830471,-0.0254137649,-0.0476853004,-0.0366895443,-0.0439741548,-0.0565077459,0.0142335298,0.0635678701,0.0434622045,-0.000318825,-0.0378034407,0.0021927446,-0.0029718096,0.0412552022,-0.0167790769,0.021251564,-0.0013202922,0.0197065723,0.0500193948,0.0125461661,0.0081505582,0.0563014269,-0.0202277185,-0.0488053592,-0.0050191145,0.0088559412,0.0119302856,0.0191051388,-0.0494730357,0.0247733298,0.0267614179,0.0224106444,0.0094372351,-0.0462208063,0.0166126755,-0.0061498439,0.0024642292,-0.0693413225,0.0460424212,0.0043798086,-0.0300337303,-0.0302135215,-0.0044331017,-0.0255175469,0.0008608235,0.0112116271,0.0512340214,-0.008028906,0.0259113628,0.0059297503]},"267":{"Abstract":"We introduce the Hierarchical Poisson Disk Sampling Multi-Triangulation (HPDS-MT) of surfaces, a novel structure that combines the power of multi-triangulation (MT) with the benefits of Hierarchical Poisson Disk Sampling (HPDS). MT is a general framework for representing surfaces through variable resolution triangle meshes, while HPDS is a well-spaced random distribution with blue noise characteristics. The distinguishing feature of the HPDS-MT is its ability to extract adaptive meshes whose triangles are guaranteed to have good shape quality. The key idea behind the HPDS-MT is a preprocessed hierarchy of points, which is used in the construction of a MT via incremental simplification. In addition to proving theoretical properties on the shape quality of the triangle meshes extracted by the HPDS-MT, we provide an implementation that computes the HPDS-MT with high accuracy. Our results confirm the theoretical guarantees and outperform similar methods. We also prove that the Hausdorff distance between the original surface and any (extracted) adaptive mesh is bounded by the sampling distribution of the radii of Poisson-disks over the surface. Finally, we illustrate the advantages of the HPDS-MT in some typical problems of geometry processing.","Authors":"E. Medeiros; M. Siqueira","DOI":"10.1109\/TVCG.2017.2704078","Keywords":"Multiresolution;poisson disk sampling;triangulation","Title":"Good Random Multi-Triangulation of Surfaces","Keywords_Processed":"multiresolution;triangulation;poisson disk sampling","Keyword_Vector":[0.068970112,-0.0207203714,-0.0213567828,-0.0036406778,0.0096128581,-0.044032667,-0.0666491921,0.0068634976,-0.0900626283,0.0498532138,-0.0317294368,0.1734909215,0.0693838196,-0.0266242935,0.0629901635,0.0530457555,0.0515653334,-0.0592506724,0.1438668703,0.0552837683,-0.0535749464,-0.1019486401,0.052045869,0.0189900545,0.0636186455,0.0726500417,-0.015029813,-0.0396206386,0.0455999789,-0.0537918051,0.0542480331,-0.0034330498,-0.041111734,0.0171377255,-0.0495774653,-0.0605104744,0.0241932517,0.0901729723,0.0073350159,-0.0106457716,-0.0207570819,0.0592978817,-0.0158621216,-0.0099759662,-0.0771478142,0.0104862629,0.0301285367,0.1102950184,-0.0197769656,0.0409311404,0.0271440139,0.0663438731],"Abstract_Vector":[0.1621782166,0.0134595719,-0.0276028241,-0.0342406814,-0.0419719349,0.0777957929,-0.0233356012,-0.0053244107,0.0605050826,-0.0131502112,-0.0445282573,0.0107138278,-0.0336190087,-0.0159376864,0.0189447055,0.0116687408,0.0144797654,-0.0423366332,0.0804026862,-0.051245814,0.0141803876,-0.0392792018,0.0463729835,-0.0516461257,-0.0109700722,0.0058440814,-0.0048564208,-0.0199698454,-0.0416098297,0.0443316986,0.0572112381,-0.0123505721,0.0205727818,-0.0058761551,0.057782319,-0.0118422987,-0.0372236447,0.0249282628,-0.0354778611,-0.021634258,0.0439261443,-0.0052873871,0.0027829382,0.0207360908,0.0677680476,0.0119645332,0.0628078156,0.0157103185,-0.0026285962,0.0053164864,-0.0197383406,0.0230882395,0.0061135188,-0.0435467049,0.0261163405,0.0298767342,0.0110840349,-0.0018434712,-0.007635613,-0.0441710511,0.0028618163,0.0121213674,-0.0235702293,0.0305994064,0.0536195181,-0.0013385054,-0.0154506093,-0.0188212514,0.0094128314,0.000692805,-0.0017190214,-0.0150859731,-0.0130188011,-0.019625953,0.0415830327,-0.0099014801,-0.0200452214,-0.0419982811,-0.0324116809,0.0022767926,0.0402213916,0.0045090138,-0.0197620907,-0.0326017595,0.0178328994,-0.0174267516,-0.0336737209,0.0052835883,-0.0093024505,-0.0001104962,-0.0134621127,0.0048819984,0.0185211195,0.0237657717,-0.0056279907,0.0058349695,0.0037019328,0.0045949136,0.0047904119,-0.028328438,-0.0225491465,0.0009702013,-0.0134881279,-0.0136019924,-0.0153437512,-0.0115816294,0.0099250041,-0.0113890536,0.0353122914,-0.0091141935,0.0413384017,-0.0336853563,-0.0131103395,0.003214903,0.0293282361,0.0434609664]},"268":{"Abstract":"Modeling virtual textiles has long been an appealing topic in computer graphics. To date, considerable effort has been devoted to their distinctive appearance and physically-based simulation. The apperance of staining patterns, commonly seen on textiles, has received comparatively little attention. This paper introduces techniques for simulating staining effects on fabric. Based on the microstructure of yarn, we propose a triple-layer model (TLM) to handle the liquid-yarn interaction for the wetting and wicking computation, and we formalize the liquid spreading in woven cloth into two typical actions, the in-yarn diffusion and the cross-yarn diffusion. The dye diffusion is driven by the liquid diffusion and the concentration distribution of pigments. The warp-weft anisotropy is handled by simulation of the yarn's structure in the two directions. Experimental results demonstrate that a wide range of fabric stain phenomenon on different textile materials, such as the water ring effect, the high saturate stain contour, and the dynamic wash away effect, can be simulated effectively without loss of visual realism. The realism of our simulation results is comparable to effects shown in photographs of real-world examples.","Authors":"Y. Zheng; Y. Chen; G. Fei; J. Dorsey; E. Wu","DOI":"10.1109\/TVCG.2018.2832039","Keywords":"Fabric appearance;capillary action;simulation;texture","Title":"Simulation of Textile Stains","Keywords_Processed":"Fabric appearance;simulation;capillary action;texture","Keyword_Vector":[0.1377791182,-0.1287436801,-0.1085045712,0.2052522214,-0.030882059,0.1036295507,0.0223134634,0.130097435,-0.0572282001,-0.0339940385,-0.0890240173,-0.0124173156,-0.0214498271,-0.0460851092,0.053159185,-0.1450938834,-0.0135399789,0.0523614871,-0.0034788959,-0.0476281431,0.035465135,-0.0746077006,-0.0433544764,-0.0117853479,-0.1118904827,-0.0174471007,0.0416633234,-0.0655274151,0.0231116651,-0.0273531747,0.0111431266,-0.0256944811,-0.0037488731,0.0663493406,-0.0743183955,-0.0632395046,-0.0422383538,-0.0556295642,0.0375587193,0.0183564313,-0.0061432423,-0.0566548626,0.0224441292,-0.0502687735,0.0041886256,-0.0346076758,0.0210552512,0.0519444543,-0.0298958035,0.051795347,-0.0248591068,0.0394497337],"Abstract_Vector":[0.1755681397,-0.0208459974,0.0036621112,-0.0007306338,-0.0065559872,-0.0384925475,-0.0540591703,-0.0126141352,0.023557201,-0.041039814,-0.0021299245,0.0029697688,-0.0590286275,-0.0261982385,-0.0244736857,-0.0093224572,-0.0417668271,0.0183761762,-0.0036623259,-0.0375970718,0.0233273861,0.0144883407,0.0241550359,-0.0096369699,-0.0465218563,0.0248827984,0.0106326052,0.014950215,0.0100017804,-0.0164544707,0.0290864064,0.0121788818,-0.0422440645,-0.0068184054,0.0086030931,0.0088093417,-0.0095694142,0.0090300191,0.0070175107,0.07194007,0.0262914017,0.0106624898,0.0388436492,0.0250703517,-0.0288275345,0.0280120706,0.0083430738,0.0097204557,-0.0122854682,0.0073269628,-0.0118544913,0.0255326445,0.0105841867,-0.0313894437,-0.0008200893,-0.0102035025,0.0053957221,-0.0196744082,0.0080148845,-0.0379457929,-0.0086762928,0.0175606483,0.0026735975,0.0214860644,-0.0082581627,-0.0282252378,0.0717780349,0.0379695593,-0.0127993821,0.0210440804,0.0031663453,0.0277467798,-0.0503112939,-0.0170315903,-0.0232704869,0.0002127669,0.0279675905,-0.0036594297,-0.0455532212,0.0049336862,0.0115005561,0.0395348974,-0.0157133392,-0.0074431658,0.0121873393,-0.0096652799,-0.0022005099,0.0036980254,-0.036795773,0.0029058632,0.0155613328,0.002397621,-0.0375429823,0.0021318619,0.0409611911,-0.0117354582,0.0132561007,0.0060736349,-0.0365999593,-0.0305948058,0.0034885839,0.01944421,-0.0062659164,-0.0502674589,0.0379948347,-0.0098882969,0.0301892794,-0.005842069,-0.022576324,-0.0436385342,-0.0163830921,0.0324734671,-0.0144010937,0.0432581049,0.0015404087,0.0184837964]},"269":{"Abstract":"Stress analysis is a crucial tool for designing structurally sound shapes. However, the expensive computational cost has hampered its use in interactive shape editing tasks. We augment the existing example-based shape editing tools, and propose a fast subspace stress analysis method to enable stress-aware shape editing. In particular, we construct a reduced stress basis from a small set of shape exemplars and possible external forces. This stress basis is automatically adapted to the current user edited shape on the fly, and thereby offers reliable stress estimation. We then introduce a new finite element discretization scheme to use the reduced basis for fast stress analysis. Our method runs up to two orders of magnitude faster than the full-space finite element analysis, with average L2 estimation errors less than 2 percent and maximum L2 errors less than 6 percent. Furthermore, we build an interactive stress-aware shape editing tool to demonstrate its performance in practice.","Authors":"X. Chen; C. Zheng; K. Zhou","DOI":"10.1109\/TVCG.2016.2618875","Keywords":"Shape editing;elastostatic stress analysis;reduced stress basis;finite element method;shape deformation","Title":"Example-Based Subspace Stress Analysis for Interactive Shape Design","Keywords_Processed":"finite element method;shape edit;reduce stress basis;elastostatic stress analysis;shape deformation","Keyword_Vector":[0.1297903332,0.0894159623,-0.0294484745,-0.0564320783,-0.0045866271,0.0097996915,0.005485793,0.0133375148,0.024391221,-0.029650055,0.0078536272,-0.0100252908,0.0056520008,-0.0168289923,0.0013471716,0.0338561006,-0.0301159772,-0.0017277166,-0.0189571098,-0.0158431796,-0.0242654437,0.0171091432,-0.0079661121,-0.0224780982,-0.0013183329,0.0105606639,0.0042222236,-0.0042229365,-0.0171170608,0.0186162141,-0.0323159196,0.0089973778,-0.0070367762,-0.0075502783,-0.0059222943,-0.0039135272,-0.0115822558,-0.0088923898,-0.0145080297,-0.0073479965,-0.027706131,0.027349489,0.034754209,-0.0103262321,-0.0034017066,-0.0041755018,-0.005860748,0.0068243033,-0.008311695,0.0181048184,0.0032589348,0.0550983646],"Abstract_Vector":[0.226646359,-0.0804988836,-0.0706032975,-0.0616055251,-0.0462776018,0.0132759398,0.0810576736,0.0317050749,-0.0392225175,0.0382684488,0.0154907577,-0.0251346663,0.0465129721,-0.0105421596,-0.0219912178,0.0264095765,0.0375063681,-0.0042172175,-0.0147482659,0.0523918577,-0.0244811555,-0.033237071,-0.0246651529,0.0511806605,0.0075159777,-0.0209591837,-0.0251320038,-0.0193537189,0.0436243997,-0.0088234679,-0.0314486238,0.0004682387,0.0131714841,-0.0069974493,0.0149571933,0.0394091509,0.0049109166,-0.0150825864,-0.0278582156,0.0733997139,0.0311372782,-0.0075784682,-0.0106203058,0.0451935513,0.0083905636,-0.0265424298,0.0235550237,-0.0007748798,0.0120101073,-0.016324793,-0.0202156932,0.0267422699,-0.0492477999,-0.0267997227,0.0419896861,0.0086054747,0.0075724793,0.0296407146,-0.0230665838,-0.0481007425,0.0055304804,0.0267228413,0.0248136455,0.0472051686,-0.0430126823,-0.0087920131,0.0241504869,-0.005088619,0.0438863465,0.0318198433,-0.0262592638,0.0659896263,-0.0747077494,0.0226529656,-0.0063449688,0.010692382,-0.0489999383,0.0311472503,0.0376440728,-0.0396680989,-0.014546439,-0.0408547556,-0.0423992585,0.0103106456,0.0306963985,0.0025146524,0.0097814422,-0.0338554956,0.0148606585,-0.0026122572,0.0035381492,0.0356916713,0.0242131005,-0.0020091512,0.0017121342,0.0485582811,-0.0200718027,-0.0561265106,-0.0370217665,-0.0252634233,0.0358339212,0.02579473,0.022587628,0.0382666629,0.0097430008,-0.0052676927,0.0141362957,-0.0105578594,0.0014509177,0.0222633989,0.0436218079,-0.0311725601,0.0132963262,-0.0422212636,-0.0177644804,-0.0023937727]},"27":{"Abstract":"Storing analytical provenance generates a knowledge base with a large potential for recalling previous results and guiding users in future analyses. However, without extensive manual creation of meta information and annotations by the users, search and retrieval of analysis states can become tedious. We present KnowledgePearls, a solution for efficient retrieval of analysis states that are structured as provenance graphs containing automatically recorded user interactions and visualizations. As a core component, we describe a visual interface for querying and exploring analysis states based on their similarity to a partial definition of a requested analysis state. Depending on the use case, this definition may be provided explicitly by the user by formulating a search query or inferred from given reference states. We explain our approach using the example of efficient retrieval of demographic analyses by Hans Rosling and discuss our implementation for a fast look-up of previous states. Our approach is independent of the underlying visualization framework. We discuss the applicability for visualizations which are based on the declarative grammar Vega and we use a Vega-based implementation of Gapminder as guiding example. We additionally present a biomedical case study to illustrate how KnowledgePearls facilitates the exploration process by recalling states from earlier analyses.","Authors":"H. Stitz; S. Gratzl; H. Piringer; T. Zichner; M. Streit","DOI":"10.1109\/TVCG.2018.2865024","Keywords":"Visualization provenance;interaction provenance;retrieval","Title":"KnowledgePearls: Provenance-Based Visualization Retrieval","Keywords_Processed":"retrieval;visualization provenance;interaction provenance","Keyword_Vector":[0.1344473798,-0.0547739806,0.0196382345,-0.1072125777,-0.0079743381,-0.0320767056,-0.0984099128,-0.0297185619,-0.096958348,0.0043415472,-0.0730369674,0.0529164561,-0.0448728061,0.0237467438,-0.1613362684,0.0053849987,-0.1022192224,0.0807054979,-0.0998175321,-0.019663116,0.0856421369,0.095587986,0.1787101522,0.0267666583,0.1194758792,-0.0637605332,0.2123051541,-0.1482026727,0.1042590928,0.0997734606,0.0314829194,-0.0012073952,0.0341817417,-0.0228770848,0.0311989888,-0.0729422917,0.021088068,-0.0311047735,-0.013573081,0.09594492,0.0058858715,0.0109884366,0.033650415,-0.0187586828,-0.0099152353,-0.0491300379,-0.0064455556,-0.011218375,0.0285149344,0.0334565482,0.0057979046,0.0580600262],"Abstract_Vector":[0.178738143,-0.1150726112,0.062053091,-0.0612247368,-0.0755050224,0.0081111334,0.0588260087,-0.0160059673,-0.0961123397,-0.2570469917,0.2497763823,0.3585394817,-0.0648697435,-0.0221207127,0.1442487051,-0.0738546504,0.1493331673,-0.0512082734,0.0476301239,0.0188871443,-0.0730737699,0.0174896339,-0.0021549376,-0.0416821094,-0.004931294,0.0197464173,-0.1635213517,0.0239367733,0.0042389148,0.0406328068,-0.0362789702,-0.0594300429,-0.0480370211,-0.0132186785,0.0340897032,-0.0033137534,-0.0170404734,0.0414934477,0.0184425422,0.0241720235,-0.0099785908,0.0318132991,0.0011658151,-0.0105721103,0.0059390778,-0.0079492998,0.011529131,-0.0019076262,0.0172052917,0.0154893698,0.0131163441,0.028882205,-0.0207818971,-0.0401511069,-0.037611736,0.0383551811,0.0132290051,-0.0205379069,-0.0370396053,-0.0148173714,-0.0094485689,-0.0486846475,-0.0476424346,0.0277956273,-0.0190683576,-0.0091308004,0.0108019536,-0.0005737668,-0.035782775,-0.005741248,0.0580459222,-0.0200619358,0.0248807884,0.0138845573,-0.0358524848,0.0059070721,0.020918927,0.0020466095,0.004818218,0.0409577242,0.0197012239,-0.0068518535,-0.0020858324,-0.000199303,0.0010669058,-0.0106766074,0.000787915,-0.0203290248,0.0154517364,-0.01297665,-0.0028225947,0.0140003871,0.0101874138,-0.0297332476,-0.0214442315,0.0007052576,-0.0116159054,0.0060379464,-0.0138980062,-0.0245139096,-0.0027249733,0.0148823164,0.0035542435,-0.0221149814,0.0294714277,0.0200697622,0.0098233875,0.0220864667,-0.0187425361,0.0058116447,-0.0382637074,0.0167512177,-0.0207574446,0.0365574229,-0.000037931,-0.0054886346]},"270":{"Abstract":"Working with prescribed velocity gradients is a promising approach to efficiently and robustly simulate highly viscous SPH fluids. Such approaches allow to explicitly and independently process shear rate, spin, and expansion rate. This can be used to, e.g., avoid interferences between pressure and viscosity solvers. Another interesting aspect is the possibility to explicitly process the vorticity, e.g., to preserve the vorticity. In this context, this paper proposes a novel variant of the prescribed-gradient idea that handles vorticity in a physically motivated way. In contrast to a less appropriate vorticity preservation that has been used in a previous approach, vorticity is diffused. The paper illustrates the utility of the vorticity diffusion. Therefore, comparisons of the proposed vorticity diffusion with vorticity preservation and additionally with vorticity damping are presented. The paper further discusses the relation between prescribed velocity gradients and prescribed velocity Laplacians which improves the intuition behind the prescribed-gradient method for highly viscous SPH fluids. Finally, the paper discusses the relation of the proposed method to a physically correct implicit viscosity formulation.","Authors":"A. Peer; M. Teschner","DOI":"10.1109\/TVCG.2016.2636144","Keywords":"Physically based modeling;fluid simulation;smoothed-particle hydrodynamics;viscosity;prescribed velocity gradients","Title":"Prescribed Velocity Gradients for Highly Viscous SPH Fluids with Vorticity Diffusion","Keywords_Processed":"prescribe velocity gradient;fluid simulation;smoothed particle hydrodynamic;viscosity;physically base modeling","Keyword_Vector":[0.0977490476,-0.019457095,-0.021697396,-0.0204428219,-0.0127784938,-0.0613496186,-0.0897855149,-0.0265552212,-0.1254315128,0.1017129138,-0.0431237938,0.2492925022,0.0968413952,-0.0270626657,0.098141253,0.0417997784,0.1659747856,-0.0296901826,0.1143232217,0.1173801933,-0.1664783676,-0.055969717,-0.0312358884,-0.007691864,0.0174951615,-0.0361657127,0.0542821336,-0.0291396538,0.0826948665,-0.0854351501,0.0400651924,-0.0591671636,0.0270868946,0.021106203,-0.0722091384,-0.0169385261,0.0710256103,0.1453470097,0.0043748878,-0.0345857683,0.0073293651,0.0449852242,-0.0408642375,0.0098409058,-0.1074158454,0.0394648051,0.0323139374,0.088750052,-0.0433778356,0.0949484696,-0.0463162562,0.1192892798],"Abstract_Vector":[0.1629854705,-0.0207653576,-0.0130884407,-0.0116036337,-0.0543543876,0.0850041959,0.0026947897,0.0100090744,0.0912195912,0.0109845132,-0.0053317842,0.0203474628,-0.0130456039,0.0133961702,-0.003266059,0.0509234093,0.0174357316,-0.0121955692,0.0867540099,-0.0698254308,-0.001927296,-0.0433452947,0.0547824596,-0.064933259,-0.0010848787,0.007248491,0.0282693619,0.0140789008,-0.0044895697,0.0589020909,0.0997568073,0.0171815916,-0.0464109805,-0.0441208364,0.0849921074,0.0004710555,-0.0501487084,-0.0207207228,-0.0543499558,0.0072730795,0.0465538885,-0.0051756158,-0.0130751774,-0.0196097132,0.0883194476,-0.0156786384,0.0668236039,0.0387759733,-0.0345413958,-0.0260952738,-0.0480554395,-0.0286800535,-0.0072075706,-0.0370945246,0.0382757837,0.0181616714,0.0160224514,0.0557677652,0.0240684977,0.0011562936,0.0360456869,-0.0007822831,-0.0059232952,0.0340826792,0.0170827285,-0.036954831,-0.0097126044,-0.0344897013,-0.0133925521,0.0466080133,-0.0141365575,0.0427559405,0.0055832786,-0.0124333258,0.0212917378,-0.1109642906,0.0520505081,0.012424879,0.0530084104,-0.0391306104,0.0189495382,-0.0088356386,-0.0092363147,0.0759298669,0.003320307,0.0042063694,-0.0215706667,0.0277633844,-0.0122069255,-0.0223489202,0.0063901691,0.0378536818,0.0125645388,0.0071777676,-0.0082609226,-0.0261825664,0.0282459616,-0.0437365507,-0.0008192582,-0.0059181686,0.0155574465,-0.0554497895,-0.0306971937,0.0330917125,-0.0057586414,0.0474691856,0.0023232656,0.0335115647,0.0288281802,-0.0468395945,0.0095115359,-0.0194498281,0.0403206965,-0.0358568634,0.0022119379,0.007356713]},"271":{"Abstract":"In 2014, more than 10 million people in the US were affected by an ambulatory disability. Thus, gait rehabilitation is a crucial part of health care systems. The quantification of human locomotion enables clinicians to describe and analyze a patient's gait performance in detail and allows them to base clinical decisions on objective data. These assessments generate a vast amount of complex data which need to be interpreted in a short time period. We conducted a design study in cooperation with gait analysis experts to develop a novel Knowledge-Assisted Visual Analytics solution for clinical Gait analysis (KAVAGait). KAVAGait allows the clinician to store and inspect complex data derived during clinical gait analysis. The system incorporates innovative and interactive visual interface concepts, which were developed based on the needs of clinicians. Additionally, an explicit knowledge store (EKS) allows externalization and storage of implicit knowledge from clinicians. It makes this information available for others, supporting the process of data inspection and clinical decision making. We validated our system by conducting expert reviews, a user study, and a case study. Results suggest that KAVAGait is able to support a clinician during clinical practice by visualizing complex gait data and providing knowledge of other clinicians.","Authors":"M. Wagner; D. Slijepcevic; B. Horsak; A. Rind; M. Zeppelzauer; W. Aigner","DOI":"10.1109\/TVCG.2017.2785271","Keywords":"Design study;interface design;knowledge generation;knowledge-assisted;visualization;visual analytics;gait analysis","Title":"KAVAGait: Knowledge-Assisted Visual Analytics for Clinical Gait Analysis","Keywords_Processed":"knowledge generation;gait analysis;interface design;design study;knowledge assist;visualization;visual analytic","Keyword_Vector":[0.2746826991,-0.192443178,-0.0406067379,-0.1532603781,0.0404436904,0.1000073022,-0.2075829986,-0.0833550616,-0.1062380442,0.1622360034,-0.0636817992,-0.118607458,0.0907240124,-0.094840833,0.0368163027,-0.1444520856,0.0374626157,-0.0182573624,0.0116716425,-0.1240491201,0.0105417621,0.1385857196,0.1520249079,0.0580610424,0.0106310669,-0.1300847307,0.0539884488,-0.0897289124,0.0014700488,0.031657537,0.0089660683,0.0448711302,-0.0112990355,-0.025187777,0.0350319112,0.0797597133,-0.0100823473,0.0516808778,0.036743594,0.0347043597,-0.0459244986,-0.1078713657,0.0454458255,-0.044142859,-0.0465852032,0.055343487,0.0068373731,0.0304208737,-0.0019987557,-0.025614648,-0.0192733046,0.0192283603],"Abstract_Vector":[0.2242095467,-0.0848888696,0.0290627291,-0.0057465401,-0.062797718,0.0283296242,0.0098091974,0.0332494179,-0.0047265189,0.0004290929,-0.0256526076,0.027068706,0.0727230665,-0.0346679059,-0.0128927243,0.0110104591,-0.0054927627,-0.0143210694,0.0145715237,-0.0590967228,0.0233356189,0.0075076899,-0.017193579,-0.0374291098,0.0222366596,-0.0074079216,0.0148904267,-0.0094717914,0.0029399739,0.0645317044,-0.0111677272,-0.0081337872,-0.0178152394,-0.0196276404,0.0749430394,0.0130280777,-0.0228270866,0.020274645,-0.0132152154,0.0219890213,0.0119414046,-0.0595441736,-0.0104109361,-0.0081319671,0.0100068628,-0.0302614726,0.0306366305,0.023685886,-0.0082729356,-0.0103773323,-0.0283344773,-0.0031723511,0.0238000248,-0.0295396836,-0.0106983527,0.0301732587,0.0268082713,-0.0009330413,0.0216092197,0.0445160038,0.0151640262,-0.0252443417,-0.013636898,0.0402943576,0.0356421956,0.0118269806,-0.0299815466,0.0019416463,-0.0154174662,0.0283999686,0.0006258726,-0.0364571064,0.0014004199,-0.0082801261,-0.0251235259,-0.0551407574,0.0167580107,-0.006268402,-0.0488388555,-0.0262396016,-0.0180276177,0.0176583957,-0.0005003926,0.0099613464,-0.0413282908,0.0198213896,-0.0273350141,-0.0249081454,-0.0025901992,0.0118645439,-0.0302236509,0.0166623901,-0.0241618906,0.0109897137,-0.0206630653,-0.0259476537,-0.0270848404,0.0018037958,-0.0008565512,0.0089823457,0.0218378233,-0.0211364855,-0.0002187258,-0.0097616205,-0.0092470515,-0.0237158454,-0.0222707732,-0.0119974805,0.0262401482,-0.0076405149,-0.0482788656,-0.022206603,0.0260445686,-0.0098522693,0.0043119114,0.0086191494]},"272":{"Abstract":"A wide variety of color schemes have been devised for mapping scalar data to color. We address the challenge of color-mapping multivariate data. While a number of methods can map low-dimensional data to color, for example, using bilinear or barycentric interpolation for two or three variables, these methods do not scale to higher data dimensions. Likewise, schemes that take a more artistic approach through color mixing and the like also face limits when it comes to the number of variables they can encode. Our approach does not have these limitations. It is data driven in that it determines a proper and consistent color map from first embedding the data samples into a circular interactive multivariate color mapping display (ICD) and then fusing this display with a convex (CIE HCL) color space. The variables (data attributes) are arranged in terms of their similarity and mapped to the ICD's boundary to control the embedding. Using this layout, the color of a multivariate data sample is then obtained via modified generalized barycentric coordinate interpolation of the map. The system we devised has facilities for contrast and feature enhancement, supports both regular and irregular grids, can deal with multi-field as well as multispectral data, and can produce heat maps, choropleth maps, and diagrams such as scatterplots.","Authors":"S. Cheng; W. Xu; K. Mueller","DOI":"10.1109\/TVCG.2018.2808489","Keywords":"Multivariate data;color mapping;color space;high dimensional data;pseudo coloring","Title":"ColorMapND: A Data-Driven Approach and Tool for Mapping Multivariate Data to Color","Keywords_Processed":"pseudo color;multivariate datum;color mapping;high dimensional datum;color space","Keyword_Vector":[0.2445241637,0.0216408208,0.2224643959,0.1710165657,-0.0737010601,0.1076791521,0.0368391943,-0.0580467091,0.0161236162,0.2326103863,0.0243709176,-0.022648365,0.0644194402,-0.0821614796,0.0146748901,-0.031947753,-0.1576038997,0.1035668589,0.0974549936,-0.0937442729,0.010189405,-0.0098537211,-0.1534652603,0.011404141,0.0073667707,0.0526827542,0.0879072021,0.0258114812,0.0600704865,-0.1009965538,0.0184344157,0.0167858528,0.0641210749,-0.0376289857,-0.0614421069,0.0035257794,-0.0571291922,-0.0028260454,-0.1071400715,0.0445502571,-0.0203444616,-0.053708583,-0.0845457568,-0.0265136569,-0.010320546,0.1243076672,-0.056449186,-0.00739529,-0.0019520401,-0.0463766871,0.0043747174,-0.040190932],"Abstract_Vector":[0.2444807016,0.1081405163,0.0873266017,0.0655782423,0.0326501699,-0.131616555,0.0059240417,0.079130436,-0.0804843327,-0.0164575483,-0.1160677531,0.0219964038,0.0089063113,-0.0442806592,0.0385445041,-0.043177149,-0.0249262921,-0.0378025271,-0.0098219387,-0.0269553039,-0.028598223,0.0020555639,-0.0696409445,-0.013669943,-0.0157785442,-0.0258293769,-0.0143879452,0.0472330681,0.0296176303,0.0797850826,0.0345323964,-0.0380113519,-0.0568838537,0.0210900278,-0.041512764,-0.0215234691,0.0030140677,0.0493931994,-0.0541384754,0.0165014146,0.0066471736,-0.0410932092,-0.0505160932,-0.0351441259,-0.043936628,-0.0159993359,-0.0405323645,0.0151180167,0.0011959198,-0.021605813,-0.0409701063,-0.0152143489,0.0060817185,-0.010385337,-0.0240829842,0.0344709778,-0.0153835729,0.0873512768,0.0032294701,-0.0119779134,0.0002805672,0.0122872762,-0.0762063562,0.0052408391,0.0428325062,0.0576857502,-0.0419880505,-0.006204959,-0.0038052275,-0.0028024507,0.0538733794,0.0513497786,0.0641622939,0.0117166275,0.0028004675,-0.0779838871,0.0285839786,-0.0572653093,-0.0216225458,-0.0076197385,-0.0043348176,0.0094116414,0.0075210073,-0.0204188638,-0.0164731204,-0.0068716457,0.0287427669,0.0139659965,-0.0392174908,0.0104734014,-0.0308662315,-0.014872288,-0.0600766243,-0.0756745829,0.0898959202,-0.0093608826,-0.0500219601,-0.0259081241,-0.0505630645,-0.0324944418,-0.0322752903,0.0300716231,0.0594597961,0.0648142053,-0.039864502,-0.0023914988,0.0096040117,0.0018906307,0.0038414755,-0.0226529285,0.0042277188,-0.0116689824,-0.0288650216,-0.0046766708,0.0008189378,0.0792478844]},"273":{"Abstract":"A common challenge faced by many domain experts working with time series data is how to identify and compare similar patterns. This operation is fundamental in high-level tasks, such as detecting recurring phenomena or creating clusters of similar temporal sequences. While automatic measures exist to compute time series similarity, human intervention is often required to visually inspect these automatically generated results. The visualization literature has examined similarity perception and its relation to automatic similarity measures for line charts, but has not yet considered if alternative visual representations, such as horizon graphs and colorfields, alter this perception. Motivated by how neuroscientists evaluate epileptiform patterns, we conducted two experiments that study how these three visualization techniques affect similarity perception in EEG signals. We seek to understand if the time series results returned from automatic similarity measures are perceived in a similar manner, irrespective of the visualization technique; and if what people perceive as similar with each visualization aligns with different automatic measures and their similarity constraints. Our findings indicate that horizon graphs align with similarity measures that allow local variations in temporal position or speed (i.e., dynamic time warping) more than the two other techniques. On the other hand, horizon graphs do not align with measures that are insensitive to amplitude and y-offset scaling (i.e., measures based on z-normalization), but the inverse seems to be the case for line charts and colorfields. Overall, our work indicates that the choice of visualization affects what temporal patterns we consider as similar, i.e., the notion of similarity in time series is not visualization independent.","Authors":"A. Gogolou; T. Tsandilas; T. Palpanas; A. Bezerianos","DOI":"10.1109\/TVCG.2018.2865077","Keywords":"Time series;similarity perception;automatic similarity search;line charts;horizon graphs;colorfields;evaluation","Title":"Comparing Similarity Perception in Time Series Visualizations","Keywords_Processed":"similarity perception;line chart;colorfield;time series;automatic similarity search;evaluation;horizon graphs","Keyword_Vector":[0.1338811118,-0.1615994888,-0.0805440206,-0.0747054866,0.0518982368,-0.1011014405,0.1951952737,-0.0409449396,0.0106566741,0.0431440334,-0.0060368643,-0.0565889144,0.014762372,-0.0163280609,0.0569843206,-0.0401861905,-0.0224120027,-0.0666833977,-0.0009084739,0.0198776663,0.0146599396,-0.0363365121,0.0250487985,-0.0073581217,-0.078755787,-0.0260725956,-0.0972994915,-0.003717294,0.0612010514,0.0507957515,-0.023772371,0.0025092716,0.1189811817,0.0018036818,-0.0098376976,-0.0002922691,-0.0284344309,0.0099389824,0.020118167,0.0303002704,0.0598313023,0.004293824,-0.0099555731,-0.0275971728,0.0223907762,-0.0184452811,0.0409154788,0.0457199557,-0.0814214981,-0.018061226,-0.0064284284,-0.0222702513],"Abstract_Vector":[0.3008990204,-0.1436754413,-0.0098666502,0.1393081184,0.1152844205,0.0151202124,-0.0098967783,-0.0080701525,-0.0063278177,-0.0080094771,0.0553205207,-0.0177827898,-0.0423236144,0.0063638675,-0.0906022333,-0.0835520399,0.0458455359,-0.0210778278,-0.0395865799,0.0436514595,0.0348245883,-0.0602745009,-0.0183971659,0.003026943,-0.0509472404,0.0327464216,0.068960358,-0.0041637349,-0.0349431854,-0.0427624398,-0.0161094643,0.0474045124,-0.0733423353,0.0097400355,0.0000353109,0.0213857255,-0.0031113528,0.0319993985,-0.0632638695,0.0186362903,0.010603674,0.0245108545,-0.0149935998,-0.0345780211,0.0028399149,0.0343730616,0.019686815,0.0778667098,0.0267752104,0.0135394348,0.0113629546,-0.0056492891,0.0013972538,0.0340886119,0.0029183613,-0.0596491615,-0.020264908,0.0053493668,0.0098153836,0.0204784858,-0.0018776826,0.0267486335,-0.0207629448,-0.0118235517,-0.0135904776,-0.0693937084,-0.0002898698,0.0070570982,0.047767888,-0.0375637341,0.0009104917,0.0116178571,-0.0182358251,0.0270665173,-0.0411702677,-0.0184731669,-0.0167259539,0.0113286331,-0.0282188216,0.0323858338,-0.0047758293,0.0233021562,0.0007107314,0.0120855826,-0.0483548009,-0.0008218283,-0.0034750324,0.022116889,0.0546993394,-0.0259784252,0.01475789,0.0058017174,-0.0134717008,-0.0033339039,-0.0197401087,0.0199695486,0.0145803061,-0.0158820367,-0.0183155465,0.0180765454,0.0044649083,-0.0081570415,0.0413700299,0.0163161124,-0.0068794443,0.0219804485,0.0204784605,-0.0119813027,0.0052784699,-0.0594210599,-0.0032434439,-0.0255163658,0.0331267023,-0.0189227687,0.0118694407,0.0042512855]},"274":{"Abstract":"Deep Q-Network (DQN), as one type of deep reinforcement learning model, targets to train an intelligent agent that acquires optimal actions while interacting with an environment. The model is well known for its ability to surpass professional human players across many Atari 2600 games. Despite the superhuman performance, in-depth understanding of the model and interpreting the sophisticated behaviors of the DQN agent remain to be challenging tasks, due to the long-time model training process and the large number of experiences dynamically generated by the agent. In this work, we propose DQNViz, a visual analytics system to expose details of the blind training process in four levels, and enable users to dive into the large experience space of the agent for comprehensive analysis. As an initial attempt in visualizing DQN models, our work focuses more on Atari games with a simple action space, most notably the Breakout game. From our visual analytics of the agent's experiences, we extract useful action\/reward patterns that help to interpret the model and control the training. Through multiple case studies conducted together with deep learning experts, we demonstrate that DQNViz can effectively help domain experts to understand, diagnose, and potentially improve DQN models.","Authors":"J. Wang; L. Gou; H. Shen; H. Yang","DOI":"10.1109\/TVCG.2018.2864504","Keywords":"Deep Q-Network (DQN);reinforcement learning;model interpretation;visual analytics","Title":"DQNViz: A Visual Analytics Approach to Understand Deep Q-Networks","Keywords_Processed":"Deep network dqn;model interpretation;visual analytic;reinforcement learning","Keyword_Vector":[0.1666182272,0.1401233845,-0.023987243,0.0773200557,-0.056165756,0.0853735476,-0.005928892,-0.089495447,-0.0184385082,0.0001686276,0.0473579107,-0.0159506425,0.1643320648,-0.0318447732,0.1293424005,0.0363020075,0.0105822385,0.090382178,-0.0832546203,0.0352271077,0.1562993015,-0.1110525273,-0.0172484483,0.0939141074,-0.0556912598,0.0161721466,0.0398397823,-0.0062658724,-0.095410539,0.0369337241,0.0064481753,-0.0724971676,0.038471604,0.022660435,0.0438528622,-0.0308302522,0.0470208148,0.1211115601,0.1112868801,-0.0590389041,-0.0147030355,0.1166010925,0.1597019363,0.0237066282,0.1130257195,-0.1217824971,-0.1030934829,0.0976048873,0.0291666685,-0.05110084,-0.1509230886,0.0361851089],"Abstract_Vector":[0.2560833355,0.1295818595,-0.1610142341,0.0157578612,0.0365916234,-0.0588486783,0.1338588184,0.0898149749,0.0493463536,-0.0411550618,0.0595958177,-0.0609309838,0.0175736986,0.0428066045,-0.0085296444,-0.0394640865,0.0470476581,-0.137445832,-0.0584121454,-0.0075805098,0.0447172629,-0.006739667,-0.0221968812,0.0434852428,-0.0434638898,-0.0186542008,-0.0360153626,-0.0180177622,-0.0187446146,-0.0199882976,0.0100646074,0.0050633857,-0.0066464743,0.0053058719,0.0189094912,-0.0251944646,-0.0200469367,-0.0323909616,0.0403046853,-0.0028785192,0.0715792984,0.0309169493,0.0353680559,0.056601318,-0.0973958867,0.036362034,-0.0001166307,-0.0125197272,0.082722272,0.0304752607,-0.0157167042,0.0391409481,-0.005192829,0.0301081806,0.0525988402,-0.008356408,-0.0032185407,-0.0354203975,-0.0191052064,0.0594928329,0.0638329669,0.0500535049,-0.0239772321,-0.0493665068,0.0403591295,0.0546270838,0.0418296817,0.0247620633,-0.1151026049,0.0013133852,0.0027308251,0.0066747253,-0.0348736651,-0.0042723316,0.0031233557,-0.0198770642,-0.0666050315,-0.0128402375,-0.0753709059,-0.0142171947,-0.0065087239,0.0090764794,-0.0506079581,-0.0339047604,-0.0088622379,-0.0575574879,0.0569160662,-0.0591662367,0.0283788122,0.0399696449,-0.0180110837,-0.002360688,0.0344672975,0.0744573804,0.0161876368,0.0296930463,0.0714776861,-0.06486008,0.0220231763,-0.0019911261,0.0440678349,-0.0507364519,0.0008483957,-0.009605471,-0.0112405542,0.0159228967,-0.0260614107,-0.0087716208,-0.0420394859,0.024030157,-0.0143399112,0.0201565805,0.0532760604,0.0024015108,0.0115429677,0.0196682144]},"275":{"Abstract":"Recent advances in data acquisition produce volume data of very high resolution and large size, such as terabyte-sized microscopy volumes. These data often contain many fine and intricate structures, which pose huge challenges for volume rendering, and make it particularly important to efficiently skip empty space. This paper addresses two major challenges: (1) The complexity of large volumes containing fine structures often leads to highly fragmented space subdivisions that make empty regions hard to skip efficiently. (2) The classification of space into empty and non-empty regions changes frequently, because the user or the evaluation of an interactive query activate a different set of objects, which makes it unfeasible to pre-compute a well-adapted space subdivision. We describe the novel SparseLeap method for efficient empty space skipping in very large volumes, even around fine structures. The main performance characteristic of SparseLeap is that it moves the major cost of empty space skipping out of the ray-casting stage. We achieve this via a hybrid strategy that balances the computational load between determining empty ray segments in a rasterization (object-order) stage, and sampling non-empty volume data in the ray-casting (image-order) stage. Before ray-casting, we exploit the fast hardware rasterization of GPUs to create a ray segment list for each pixel, which identifies non-empty regions along the ray. The ray-casting stage then leaps over empty space without hierarchy traversal. Ray segment lists are created by rasterizing a set of fine-grained, view-independent bounding boxes. Frame coherence is exploited by re-using the same bounding boxes unless the set of active objects changes. We show that SparseLeap scales better to large, sparse data than standard octree empty space skipping.","Authors":"M. Hadwiger; A. K. Al-Awami; J. Beyer; M. Agus; H. Pfister","DOI":"10.1109\/TVCG.2017.2744238","Keywords":"Empty Space Skipping;Volume Rendering;Segmented Volume Data;Hybrid Image\/Object-Order Approaches","Title":"SparseLeap: Efficient Empty Space Skipping for Large-Scale Volume Rendering","Keywords_Processed":"volume Rendering;empty space skipping;Segmented Volume Data;Hybrid image Object order approach","Keyword_Vector":[0.1243793565,-0.0691739369,-0.0201235327,0.1024361654,-0.0648604763,0.1243106713,-0.0085014504,0.0304811518,-0.1625316505,0.0165434799,0.1728182848,-0.0085679682,0.0476944219,-0.0368791496,-0.0174185558,-0.0605544913,0.062764135,-0.0834210788,-0.0091874286,-0.0884962247,0.0152541245,0.0457497463,-0.0526769752,-0.0064973213,0.0719090847,0.0110894752,-0.0586249289,0.0299201382,0.0426155133,-0.0185946264,0.043214947,0.0651780657,-0.0613675805,-0.001948717,0.0862553273,0.0581803989,0.0016363605,0.007900726,-0.0394011463,0.0076937412,0.0624023824,0.3073260704,0.0213943623,-0.0633221057,-0.0986572274,0.0093745369,0.1211106052,-0.0456055494,0.0027098499,-0.0263133773,-0.0170485208,-0.0197362387],"Abstract_Vector":[0.3096124235,-0.1268337293,0.0206686571,-0.0241253842,0.0177906364,-0.1445172375,-0.0518848704,0.0571485304,-0.0142004382,0.0354370612,0.0414510222,-0.0746149073,-0.1327309903,0.0186918435,-0.0450853386,0.0592225384,0.0322359616,0.002264668,-0.0453057693,0.0494598497,0.0781627069,-0.0250736299,0.0321930681,-0.0159816083,-0.0837860324,-0.047068669,0.0225853399,0.0361854622,-0.0407464697,-0.0750600763,0.0159122039,-0.027292758,-0.0154577038,-0.0925026781,-0.044925807,0.0311102231,-0.0094721487,-0.0204682386,-0.0443424715,0.0405274031,-0.0366011016,-0.0398504672,0.0372607501,-0.0066986985,0.0368526384,0.0301256685,-0.011449421,-0.0419430881,-0.0211335559,0.0182927912,-0.0209953584,0.0355679198,-0.0392147822,0.0292424006,0.0321553955,-0.0006528044,0.0135831556,0.0139800671,0.0368245637,0.0355006352,-0.0662492491,0.0154715379,-0.0456205732,-0.0154526276,0.0376749144,-0.0409675711,0.0692917628,0.0131045639,-0.0264177083,0.0009223627,0.0085387261,-0.0492831697,-0.0689708335,0.0489640727,-0.045297967,-0.0084093294,-0.064200596,0.0190280925,0.028280279,0.0686257694,-0.0377676936,-0.0671732657,0.0275572545,0.0017677259,-0.0143056261,0.0441675296,-0.0156487738,-0.0066654597,0.030378016,-0.0099073669,-0.0101614966,-0.0072586312,0.0191206239,-0.0293269028,0.010911456,0.0379823329,0.0022165717,-0.0096382953,-0.0327448883,0.0239894337,0.0009365851,0.0235954688,0.0280502912,-0.01255942,-0.0013693045,0.0265354151,0.017912899,-0.074679036,-0.0464636038,0.017902662,0.0091198973,-0.0133671697,-0.0368228707,-0.0067147798,0.0231263346,-0.0210897698]},"276":{"Abstract":"The comparison of many members of an ensemble is difficult, tedious, and error-prone, which is aggravated by often just subtle differences. In this paper, we introduce Dynamic Volume Lines for the interactive visual analysis and comparison of sets of 3D volumes. Each volume is linearized along a Hilbert space-filling curve into a 1D Hilbert line plot, which depicts the intensities over the Hilbert indices. We present a nonlinear scaling of these 1D Hilbert line plots based on the intensity variations in the ensemble of 3D volumes, which enables a more effective use of the available screen space. The nonlinear scaling builds the basis for our interactive visualization techniques. An interactive histogram heatmap of the intensity frequencies serves as overview visualization. When zooming in, the frequencies are replaced by detailed 1D Hilbert line plots and optional functional boxplots. To focus on important regions of the volume ensemble, nonlinear scaling is incorporated into the plots. An interactive scaling widget depicts the local ensemble variations. Our brushing and linking interface reveals, for example, regions with a high ensemble variation by showing the affected voxels in a 3D spatial view. We show the applicability of our concepts using two case studies on ensembles of 3D volumes resulting from tomographic reconstruction. In the first case study, we evaluate an artificial specimen from simulated industrial 3D X-ray computed tomography (XCT). In the second case study, a real-world XCT foam specimen is investigated. Our results show that Dynamic Volume Lines can identify regions with high local intensity variations, allowing the user to draw conclusions, for example, about the choice of reconstruction parameters. Furthermore, it is possible to detect ring artifacts in reconstructions volumes.","Authors":"J. Weissenb\u00f6ck; B. Fr\u00f6hler; E. Gr\u00f6ller; J. Kastner; C. Heinzl","DOI":"10.1109\/TVCG.2018.2864510","Keywords":"Ensemble data;comparative visualization;visual analysis;Hilbert curve;nonlinear scaling;X-ray computed tomography","Title":"Dynamic Volume Lines: Visual Comparison of 3D Volumes through Space-filling Curves","Keywords_Processed":"ensemble datum;ray compute tomography;nonlinear scaling;comparative visualization;Hilbert curve;visual analysis","Keyword_Vector":[0.1559624042,-0.0421863106,0.102568783,-0.0521965702,-0.0597947612,-0.0664138534,-0.0478270245,-0.0192068414,0.0120410961,-0.0650379782,-0.0608477277,0.1471839498,0.057273353,-0.0621566021,-0.0366866559,-0.0222708335,0.0838339842,-0.0839304786,-0.0463605499,0.0304504125,-0.0529725503,-0.0641851821,-0.0797155847,-0.0375395893,0.0527955283,0.0199763283,-0.014300572,-0.0973188549,-0.0124849084,-0.0541328157,-0.0583420902,-0.0208089103,-0.0186863737,-0.0168664672,-0.0121187788,-0.0202107506,-0.0207508453,0.0502429379,0.0094464626,-0.0480544596,-0.0046032691,-0.031081126,0.0145094844,0.0352424161,-0.0619003948,0.024891773,-0.0157238808,-0.0156538562,0.0234175404,0.0353477334,-0.05772,-0.0081934022],"Abstract_Vector":[0.23680298,-0.0590635814,0.1551785552,-0.0583440125,0.0553529642,0.0610206244,0.1166557735,-0.1047988699,0.0185094978,-0.018868079,0.013150946,-0.0488412755,-0.0921971162,0.0540125601,-0.0539898947,0.073248497,-0.0297528449,-0.01366039,0.0272663703,0.021724643,-0.0161743366,-0.0206707989,0.0334853296,0.0200036869,0.021096656,-0.0440329155,0.0266060417,-0.0174896649,-0.0513710935,0.056192382,0.0370361638,0.0474278584,0.0321243174,0.0390819407,0.016273624,0.0446728975,0.0410847127,-0.0163583843,-0.0197211336,-0.0354558324,0.0065724295,-0.0417249436,0.0084011951,-0.0222493348,0.0388634282,-0.0171545505,-0.0341716551,-0.0239022624,0.0021472058,-0.0195110616,-0.0295827345,0.0243893566,-0.0041017373,-0.0245213114,0.0238054381,-0.0417064699,-0.0038984145,0.0659832019,0.0278006964,0.0333734084,-0.0004594071,-0.0518106346,-0.0054382044,-0.0005100055,-0.0321707742,0.0158311283,0.014951566,0.0054118273,-0.0412002176,0.0141628021,0.0013733346,-0.0383092585,-0.0152186969,-0.0110871304,-0.0281167469,0.0071905713,-0.0250122482,-0.0260295298,0.0255189052,0.0275265555,-0.024962865,0.0099344919,-0.0076796866,-0.0269214591,0.0334998198,0.001000166,-0.0053422503,0.0213157074,-0.0245492576,0.0031021115,-0.0075048936,-0.0037945239,0.0268870293,0.0111042586,-0.0262749556,-0.0173050691,-0.015268104,0.0163806593,-0.0324773921,-0.0122383355,-0.0432459993,-0.0057369685,0.0091330185,0.0150004534,0.0019787476,0.0105913988,0.0028628383,0.0403320073,-0.0393998587,-0.025639908,-0.009428826,-0.0146253124,0.0007898669,-0.0142127713,0.0013331573,-0.0291155666]},"277":{"Abstract":"Applied visualization researchers often work closely with domain collaborators to explore new and useful applications of visualization. The early stages of collaborations are typically time consuming for all stakeholders as researchers piece together an understanding of domain challenges from disparate discussions and meetings. A number of recent projects, however, report on the use of creative visualization-opportunities (CVO) workshops to accelerate the early stages of applied work, eliciting a wealth of requirements in a few days of focused work. Yet, there is no established guidance for how to use such workshops effectively. In this paper, we present the results of a 2-year collaboration in which we analyzed the use of 17 workshops in 10 visualization contexts. Its primary contribution is a framework for CVO workshops that: 1) identifies a process model for using workshops; 2) describes a structure of what happens within effective workshops; 3) recommends 25 actionable guidelines for future workshops; and 4) presents an example workshop and workshop methods. The creation of this framework exemplifies the use of critical reflection to learn about visualization in practice from diverse studies and experience.","Authors":"E. Kerzner; S. Goodwin; J. Dykes; S. Jones; M. Meyer","DOI":"10.1109\/TVCG.2018.2865241","Keywords":"User-centered visualization design;design studies;creativity workshops;critically reflective practice","Title":"A Framework for Creative Visualization-Opportunities Workshops","Keywords_Processed":"design study;critically reflective practice;creativity workshop;User center visualization design","Keyword_Vector":[0.142598623,-0.0515926067,0.051239796,-0.0309199235,-0.0066792075,0.0373714072,0.0100327153,0.0420532459,-0.0248104798,-0.0791746349,0.0010869223,-0.0138224902,-0.052213279,0.0044507789,-0.0971137203,-0.0099197101,-0.0711506092,0.0411871702,-0.0194507716,0.069315254,0.003454105,-0.0240545057,0.0112358955,0.0918786641,0.0702246914,0.0154867244,0.1116794565,-0.0580152293,0.1651771009,0.0779968199,-0.0238518891,0.016784222,-0.0524042927,0.0689393406,-0.0700456183,-0.1025210788,-0.0302004569,-0.019276611,0.0666101974,0.0293005395,-0.019527705,0.0249855613,0.0177748183,-0.0027794364,-0.0054394524,-0.0516984245,-0.0225981354,0.005498523,-0.0027677377,-0.0181710828,-0.0008227975,-0.0149667545],"Abstract_Vector":[0.1623823776,-0.0482473857,0.0185308861,-0.0210793855,0.0149341726,-0.0513694685,-0.0125630192,-0.0243217633,-0.0254758181,-0.0956814098,0.0914449856,0.0674104306,-0.0553458211,-0.0127635169,0.0266205326,-0.0181919351,0.0629181502,0.0289785838,0.0279460946,0.0043903697,-0.0056419343,-0.0455783376,0.0130348875,0.0129466315,0.0366889593,0.0251976993,-0.0667122862,0.0407951809,0.0387543805,0.0081591717,0.0044739455,0.0020813519,-0.0428226768,0.0073315254,-0.010397497,0.0226940744,-0.0053620825,0.0318657125,-0.0096725613,-0.0130891252,-0.0028009189,0.0192626472,0.0287389137,0.0078881202,0.0354172788,-0.0608899829,-0.0366659325,0.0024237619,-0.0158073684,-0.048209732,0.003093714,0.0749195391,-0.0346352016,-0.0073100858,-0.0096208214,0.0084870444,-0.0178214201,-0.0244557829,0.0523387294,0.0171472961,0.0194447862,-0.0089736909,-0.0277517922,-0.0065535356,0.0053827674,0.0302011725,0.0005779979,-0.0376965135,-0.01592803,-0.0042086665,0.0311628207,0.0155455126,-0.0177451322,0.0343236289,0.0395789383,0.0554044649,-0.0041689833,-0.0047562697,-0.0592686493,-0.0347929765,0.0947208207,0.0633807326,0.0111527039,0.0076761824,0.0600615583,0.0032363831,-0.024817469,0.0451964454,-0.0122929819,0.0667111585,-0.0307472053,-0.0577584918,0.117244385,-0.0641968778,0.1088789406,0.0595689632,-0.013596715,0.0523874624,0.0375453589,0.1036344849,-0.0648063599,-0.0666301706,-0.0495446336,-0.0129339374,0.0185927964,0.0146070504,-0.0623983484,0.0731993998,-0.0103800116,-0.0814716596,-0.0399825572,-0.00470758,-0.0120453961,-0.0226499525,-0.0207340216,0.0566842861]},"278":{"Abstract":"We present a visual analysis method for interactively recomposing a large number of photos based on example photos with high-quality composition. The recomposition method is formulated as a matching problem between photos. The key to this formulation is a new metric for accurately measuring the composition distance between photos. We have also developed an earth-mover-distance-based online metric learning algorithm to support the interactive adjustment of the composition distance based on user preferences. To better convey the compositions of a large number of example photos, we have developed a multi-level, example photo layout method to balance multiple factors such as compactness, aspect ratio, composition distance, stability, and overlaps. By introducing an EulerSmooth-based straightening method, the composition of each photos is clearly displayed. The effectiveness and usefulness of the method has been demonstrated by the experimental results, user study, and case studies.","Authors":"Y. Liang; X. Wang; S. Zhang; S. Hu; S. Liu","DOI":"10.1109\/TVCG.2017.2764895","Keywords":"Photo recomposition;example-based learning;earth mover\u2019s distance;metric learning;photo summarization","Title":"PhotoRecomposer: Interactive Photo Recomposition by Cropping","Keywords_Processed":"earth mover distance;photo recomposition;metric learning;example base learning;photo summarization","Keyword_Vector":[0.186127092,-0.104565364,-0.0576568624,0.2195787056,-0.0667997583,0.3224292354,-0.0405753609,-0.1254306517,-0.0777176525,0.0216688455,0.032932279,-0.0406697376,0.0283214357,-0.0010635774,-0.0877747399,-0.0831477105,0.0350850797,0.011554209,0.064371035,-0.028426028,0.0179428503,-0.0853882432,0.0447455243,-0.0571173889,-0.0237414271,-0.1006733364,-0.0199701325,0.0226579733,-0.0022680203,0.019173318,-0.0111734958,-0.0476107655,-0.0231502187,-0.0197526998,-0.0119896084,-0.020018231,-0.0187625837,-0.0240129253,-0.0375366912,-0.0915191532,0.0760559279,-0.0242109418,-0.0056343714,0.0306955074,0.0044810176,-0.0043878084,-0.0004081757,0.0040029559,0.0244358622,-0.0281645047,-0.0215770192,0.0084359505],"Abstract_Vector":[0.2529128126,0.1405256254,0.0546152095,0.0860657947,-0.081991405,-0.0265713346,-0.0643209038,0.0324219885,0.0196337189,0.1337308083,0.0623287492,0.0363817337,-0.0289558412,-0.0045774059,-0.0541954359,0.0699344971,-0.0166414056,0.0221257589,0.0566391946,-0.0093889002,-0.0109875542,0.0264225268,-0.003383774,0.0576244405,-0.1192428454,0.0806160302,-0.1577271183,-0.1311284973,0.0194030273,0.0771320988,0.024201538,-0.0550428746,0.0426764594,0.0750907285,0.0236538179,0.0287699261,0.0105420407,-0.0094309936,0.0272477377,-0.0498851779,-0.0027202564,-0.0472809773,0.0705918679,0.0196641043,0.0349626311,-0.0458369812,-0.0765570636,0.0016614816,0.0023803669,0.0462830204,0.0002984205,-0.0188241294,-0.0000785974,0.0661371914,-0.0025754483,-0.0801695863,-0.0431737775,0.0014191697,0.022744222,0.0276034811,-0.031632599,0.0081545656,-0.0209835781,0.0372210056,0.0466659142,0.0206960732,-0.0293828133,0.0191334353,-0.0056561125,-0.102320329,-0.0160378731,-0.000037421,0.0318620347,-0.0130233513,0.001727088,0.0049761348,-0.0339479046,-0.0514427686,0.0215462851,0.0012748132,0.0268857219,0.01781438,0.0127737284,-0.0319100762,-0.0338274406,0.0458818451,0.0230808355,0.0192172692,-0.0092636506,-0.015599616,-0.0176091855,-0.0054299658,-0.0392818075,0.0083641377,0.0189451858,-0.0174526644,0.0104883059,-0.0274947562,-0.0572524315,0.0025298228,-0.0118967723,-0.042956413,0.0326193871,0.0087286439,-0.0622925887,0.0033542447,0.0082183494,-0.0019606507,-0.0126662564,-0.0060311727,-0.0084059974,0.0461774228,0.0129409983,0.0034514001,-0.0048597802,0.0011801657]},"279":{"Abstract":"The proliferation of high resolution and affordable virtual reality (VR) headsets is quickly making room-scale VR experiences available in our homes. Most VR experiences strive to achieve complete immersion by creating a disconnect from the real world. However, due to the lack of a standardized notification management system and minimal context awareness in VR, an immersed user may face certain situations such as missing an important phone call (digital scenario), tripping over wandering pets (physical scenario), or losing track of time (temporal scenario). In this paper, we present the results of 1) a survey across 61 VR users to understand common interruptions and scenarios that would benefit from some form of notifications; 2) a design exercise with VR professionals to explore possible notification methods; and 3) an empirical study on the noticeability and perception of 5 different VR interruption scenarios across 6 modality combinations (e.g., audio, visual, haptic, audio + haptic, visual + haptic, and audio + visual) implemented in Unity and presented using the HTC Vive headset. Finally, we combine key learnings from each of these steps along with participant feedback to present a set of observations and recommendations for notification design in VR.","Authors":"S. Ghosh; L. Winston; N. Panchal; P. Kimura-Thollander; J. Hotnog; D. Cheong; G. Reyes; G. D. Abowd","DOI":"10.1109\/TVCG.2018.2793698","Keywords":"Virtual Reality;Notifications;Interruptibility;Multi-Modal;Feedback;Context Awareness","Title":"NotifiVR: Exploring Interruptions and Notifications in Virtual Reality","Keywords_Processed":"feedback;virtual reality;notification;Multi Modal;interruptibility;Context Awareness","Keyword_Vector":[0.0377731225,-0.0126519366,0.0112490577,0.024870805,-0.005485729,0.0296288294,0.007964065,-0.0179375235,0.0109603394,0.0432096957,0.0129796517,0.0014469304,-0.0201016988,0.0435157108,0.0169590054,0.0667019058,0.0192194739,-0.0207349912,-0.0221583175,0.0412734954,0.0371063067,0.0125051683,0.0028029313,0.0293999275,-0.0399018729,0.0432666653,0.0010748807,-0.0273525406,-0.0147296801,-0.0309124008,-0.0093830495,-0.0113249322,-0.0159408308,0.0143893623,0.0270694719,0.032967252,0.0309789048,-0.0125573911,0.0087192051,0.0269025168,0.0261931317,-0.0019914413,0.0185198952,-0.0188652545,-0.0073003628,-0.0360655523,-0.0046147176,-0.0236100715,-0.0152143715,-0.0048788703,-0.0076045991,0.0468157808],"Abstract_Vector":[0.1611261101,0.0888749628,-0.0008620073,0.0313597034,-0.1492677337,0.1156489308,-0.035576697,-0.0166362172,0.0277359706,0.0022992808,-0.0221916352,0.0065433195,-0.0421243631,0.0208641931,-0.0260208758,-0.0037144239,0.0528204248,0.0782331457,-0.0726508387,-0.0744400206,-0.0045956891,-0.0058744085,0.0302961872,-0.0473636586,0.0150418894,-0.0086404731,0.0443178916,-0.0026715642,-0.0044356096,0.0083344206,0.0422103079,-0.0170189686,0.0206175447,0.0139984642,-0.0192117131,0.0253539421,0.0163490198,0.0261347841,0.0211388593,0.0602624102,0.125143709,0.0879141721,0.0026176393,0.0416311009,-0.0598057503,0.0087820825,-0.0467640655,0.0396428373,-0.0468441922,-0.0227124728,0.0012028058,0.0314386382,-0.0027824346,0.0647242771,0.0039326122,-0.0058226874,-0.0567247808,0.0327553993,-0.0359306056,0.0218992365,-0.0411051618,0.0041961717,-0.0025285317,-0.0141223652,-0.0089309699,0.040091941,0.0279841065,0.018321487,-0.034871172,0.0306127139,0.0386390754,-0.0069168282,-0.0082361158,-0.0129129867,0.0342702657,0.020574456,0.0371318124,0.0387043056,-0.0054759731,0.0538944569,0.0264411714,-0.0582072079,-0.0213908554,-0.0254204578,-0.0166961093,0.0169564394,0.0214681272,-0.0159740326,0.0156856602,0.0423186215,-0.0332041195,0.0401955214,0.0001468785,0.0492703653,-0.0119671232,-0.0524622679,0.0225096155,-0.0109535281,-0.033174946,0.0142445482,-0.0015052384,-0.004121197,0.0044682171,-0.0143058749,-0.0416876652,0.0055529562,-0.0019676312,0.0165703693,0.0236106554,-0.0010760636,0.0319497174,-0.0163391607,-0.0252518086,0.0038628867,-0.0091733406,0.0176221964]},"28":{"Abstract":"Visualizations are nowadays appearing in popular media and are used everyday in the workplace. This democratisation of visualization challenges educators to develop effective learning strategies, in order to train the next generation of creative visualization specialists. There is high demand for skilled individuals who can analyse a problem, consider alternative designs, develop new visualizations, and be creative and innovative. Our three-stage framework, leads the learner through a series of tasks, each designed to develop different skills necessary for coming up with creative, innovative, effective, and purposeful visualizations. For that, we get the learners to create an explanatory visualization of an algorithm of their choice. By making an algorithm choice, and by following an active-learning and project-based strategy, the learners take ownership of a particular visualization challenge. They become enthusiastic to develop good results and learn different creative skills on their learning journey.","Authors":"J. C. Roberts; P. D. Ritsos; J. R. Jackson; C. Headleand","DOI":"10.1109\/TVCG.2017.2745878","Keywords":"Explanatory visualization;Information Visualization;Teaching visualization;Learning Support","Title":"The Explanatory Visualization Framework: An Active Learning Framework for Teaching Creative Computing Using Explanatory Visualizations","Keywords_Processed":"teaching visualization;Learning Support;information visualization;explanatory visualization","Keyword_Vector":[0.1936282215,-0.1223195795,-0.0313142494,-0.081975062,0.0046979219,0.052259594,-0.1223113726,-0.0593446977,-0.040281383,0.0775220029,-0.0226776746,-0.0156011789,0.0723604096,-0.1073517484,0.0160799484,-0.0615447611,0.0306008757,-0.0723058018,0.0245671091,0.0061665987,-0.0459243401,-0.0188995079,-0.0620833342,-0.0019530954,-0.0296604968,0.0117482669,-0.0280339621,-0.0355599021,0.0011532086,0.0923516886,-0.0305323833,-0.0627924464,-0.0700463596,-0.0176830814,-0.0436061533,0.0208508401,-0.0292026143,0.0562914479,0.0005373342,-0.0251487618,0.0449019948,-0.0386324969,-0.002601755,-0.0561999906,0.0588099073,0.0001375772,-0.0328435675,-0.0388717261,0.0624763969,0.0046380067,-0.0053569112,0.0754188351],"Abstract_Vector":[0.1658689277,-0.0774527294,-0.01970067,0.0359445092,0.0269071045,-0.0121477613,0.0417917615,0.0020587624,-0.0028967392,0.0279606747,0.0077793231,0.0112459827,0.0767308514,-0.0085947373,-0.0575364814,-0.0427796817,0.0375969305,0.0143350339,-0.0283978389,0.0033907594,0.002055661,-0.0254682368,-0.0174991481,0.0390727379,-0.0278136668,-0.0283358821,-0.0105878358,0.0158233586,-0.0507732883,0.0245449525,0.0186376365,-0.0035004497,0.0170734733,0.0006794437,-0.0066324803,-0.0127728804,-0.0058734066,0.017400679,-0.0218471881,0.0231423178,0.0234060906,-0.0249919729,-0.017562027,0.0438012071,-0.0078531132,-0.0014701823,0.0386105947,0.0099797081,0.014496055,0.0078147022,0.0052008376,-0.0113104155,0.0069663468,0.012768035,-0.0083731071,-0.0120910169,-0.0211859183,-0.0016980458,-0.022137853,0.0078063648,-0.0028290743,-0.0215560042,0.0088787822,0.0054039615,-0.0158402497,-0.0304895544,0.0111798981,0.0155059742,-0.010658464,0.0055756241,0.0127131964,0.0054541807,-0.0600523089,-0.0136084144,-0.0133226819,0.0049543236,-0.0042909109,-0.0256489598,0.0060696355,0.0218699226,-0.0282092542,-0.0048913582,0.0045542468,-0.0210106287,0.0340237136,-0.007052726,-0.0412963039,0.0323099984,0.020920676,0.0241553573,0.0011858457,0.0193644387,-0.0089270271,-0.0243803083,-0.0050364089,-0.0158060824,0.0194726196,0.0057836109,-0.0066742643,0.0013650924,-0.0045631465,-0.0234085439,0.0264967878,-0.025117156,-0.0131197446,-0.0200786293,-0.0181476955,0.0067654923,-0.0163249463,0.0045386644,0.0141253151,0.0103913363,0.0138131183,-0.0037074309,-0.0220456466,-0.0004800886]},"280":{"Abstract":"In this paper we propose a novel method for the interactive exploration of protein tunnels. The basic principle of our approach is that we entirely abstract from the 3D\/4D space the simulated phenomenon is embedded in. A complex 3D structure and its curvature information is represented only by a straightened tunnel centerline and its width profile. This representation focuses on a key aspect of the studied geometry and frees up graphical estate to key chemical and physical properties represented by surrounding amino acids. The method shows the detailed tunnel profile and its temporal aggregation. The profile is interactively linked with a visual overview of all amino acids which are lining the tunnel over time. In this overview, each amino acid is represented by a set of colored lines depicting the spatial and temporal impact of the amino acid on the corresponding tunnel. This representation clearly shows the importance of amino acids with respect to selected criteria. It helps the biochemists to select the candidate amino acids for mutation which changes the protein function in a desired way. The AnimoAminoMiner was designed in close cooperation with domain experts. Its usefulness is documented by their feedback and a case study, which are included.","Authors":"J. By\u0161ka; M. Le Muzic; M. E. Gr\u00f6ller; I. Viola; B. Kozl\u00edkov\u00e1","DOI":"10.1109\/TVCG.2015.2467434","Keywords":"Protein;tunnel;molecular dynamics;aggregation;interaction;Protein;tunnel;molecular dynamics;aggregation;interaction","Title":"AnimoAminoMiner: Exploration of Protein Tunnels and their Properties in Molecular Dynamics","Keywords_Processed":"aggregation;interaction;molecular dynamic;protein;tunnel","Keyword_Vector":[0.1955455454,-0.1527556691,-0.1017023253,-0.0572461585,-0.0366819019,-0.0798099446,-0.0335746034,-0.0015279209,0.0375037984,0.0230706665,0.0506857784,-0.1247425882,-0.0109725285,-0.075425792,0.0317445717,-0.0277860185,0.0235857055,-0.031718933,0.0152906319,0.0347227394,-0.0990223763,0.0149469333,0.0340508396,0.0197320415,-0.0132789494,-0.0032134293,0.0377225898,-0.0126002377,0.0298502451,-0.0139569864,0.057825532,0.0816420462,0.014549016,-0.0946369124,0.0430019203,0.0119004939,0.0065520341,-0.1178203508,0.0384730451,0.0078998068,0.0184820342,0.159210834,-0.0629248257,-0.0097299082,0.0966404075,0.0692649521,0.0341432096,-0.0421723641,-0.1354574047,-0.150042582,-0.1703555623,-0.0514766105],"Abstract_Vector":[0.1303493271,-0.1053144906,0.0050761108,0.0050151884,-0.0330863203,-0.0282721866,0.0223932041,0.0053668684,-0.0297140355,0.0734952221,-0.0135189291,0.0063487635,-0.0496916012,0.0129673883,-0.0083793577,0.0057879749,-0.007216568,0.0000219956,-0.0259601145,0.0048567763,0.0130017234,-0.0038263718,-0.0073758495,0.0023498033,-0.0076156782,-0.0181089377,0.0101749718,-0.0146304753,-0.0161780026,0.0018016298,-0.0004054751,0.0288842326,-0.0117860367,0.0362925966,-0.0011706747,0.0228538566,-0.0429635289,0.0195303103,-0.0397260171,-0.0081183164,0.0156484538,0.0417114352,0.0400239186,-0.0393658316,0.0189197805,0.0129064382,0.0443164065,-0.0346431114,0.0260660462,-0.0216842218,-0.0481121391,0.011750637,0.0052528541,-0.0005635662,0.0005121047,0.0179076276,-0.0082676466,0.0102031363,0.0004070381,-0.0039646996,-0.0037304256,-0.018509968,0.0040503872,-0.0423422403,-0.0166812261,-0.073874224,0.0038423738,0.0157053851,-0.0388236896,-0.041550323,0.0113599561,-0.0443396347,-0.0052062435,0.0055132088,0.0301508905,-0.013978619,0.0235669393,-0.0103078254,0.0255219422,0.0216478351,0.043028727,-0.0249773727,-0.0297538191,-0.0217368791,-0.0035031388,-0.0118222323,-0.0094229473,0.018026592,0.0208229112,-0.0054841055,-0.0081149055,0.0063107829,0.0298749004,-0.0400493193,0.0203609702,-0.0071111181,0.0148580893,-0.0198037304,-0.0177951364,0.0045292054,0.0296060939,0.0068184872,0.0073172649,0.0293127013,0.0272580557,-0.0358038335,-0.0005367347,-0.0082554112,0.0464811905,-0.0086423533,-0.0008813998,0.0367520204,0.0096048608,-0.0430154956,-0.0198071945,-0.0035510178]},"281":{"Abstract":"We propose a visual marker embedding method for the pose estimation of a projection surface to correctly map projected images onto the surface. Assuming that the surface is fabricated by a full-color or multi-material three-dimensional (3D) printer, we propose to automatically embed visual markers on the surface with mechanical accuracy. The appearance of the marker is designed such that the marker is detected by infrared cameras even when printed on a non-planar surface while its appearance can be diminished by the projection to be as imperceptible as possible to human observers. The marker placement is optimized using a genetic algorithm to maximize the number of valid viewpoints from which the pose of the object can be estimated correctly using a stereo camera system. We also propose a radiometric compensation technique to quickly diminish the marker appearance. Experimental results confirm that the pose of projection objects are correctly estimated while the appearance of the markers was diminished to an imperceptible level. At the same time, we confirmed the limitations of the current method; only one object can be handled, and pose estimation is not performed at interactive frame rates. Finally, we demonstrate the proposed technique to show that it works successfully for various surface shapes and target textures.","Authors":"H. Asayama; D. Iwai; K. Sato","DOI":"10.1109\/TVCG.2017.2657634","Keywords":"Digital fabrication;spatial augmented reality;projection mapping;diminished reality;marker-based tracking","Title":"Fabricating Diminishable Visual Markers for Geometric Registration in Projection Mapping","Keywords_Processed":"spatial augmented reality;projection mapping;diminish reality;marker base tracking;digital fabrication","Keyword_Vector":[0.1053099199,-0.0507955137,0.007932152,0.0233665057,-0.0165831736,0.1088827465,-0.0218304174,-0.0319348361,-0.0155348171,-0.0108947406,0.0082167797,0.0051794824,0.0178182804,-0.0336460158,-0.0043685052,-0.0003211411,-0.0514779719,0.0027982122,-0.006496615,-0.0008860923,0.0083136004,-0.0084872818,-0.0053414438,-0.0124709315,-0.0037585553,-0.0117242029,-0.0187727173,0.008126698,0.015266051,-0.0200284689,0.004041046,0.0056369191,0.0203874981,-0.0148714116,0.0145710825,-0.0129895498,0.0123333182,0.0132897206,-0.0309129194,0.0110895945,0.0097095452,-0.0333099136,-0.0089530267,-0.0199062572,0.0040469147,0.0131608131,0.0139012924,-0.0263235337,0.024510419,0.0383569948,-0.0199213271,0.0111229304],"Abstract_Vector":[0.1657839494,-0.0044790195,0.0202873836,0.0109319211,-0.0674374654,-0.006125027,-0.0636360724,-0.0126958938,0.0389682771,-0.0245070657,0.016831224,0.0375944305,0.0209060042,0.0150352363,0.0571823741,-0.032808848,-0.0351965897,-0.0096244731,0.0022685776,-0.0325911361,0.0253399541,-0.0401477927,-0.0156681212,-0.0334590664,-0.0104013607,-0.0202373589,0.0233655632,0.0141627959,0.0013392436,-0.037331184,0.0551506297,-0.0133932291,0.0439953558,0.0615122787,-0.028795681,0.0070602786,0.0178953086,0.0302982926,-0.0289516368,-0.0169083472,0.0160118959,0.0198643896,0.0141365951,-0.0293476017,-0.0072059805,0.030359425,-0.0378099409,-0.0109500383,0.0519233116,0.0122166396,0.0023423285,-0.0014886267,0.0623473544,-0.0547153397,0.0012460079,-0.0280889498,0.0293392963,-0.0017559705,0.0183113971,0.005080968,0.0135824957,0.0234714503,-0.0108245614,0.0116171276,0.0384988178,-0.0118909969,-0.0030801013,0.0218020721,0.0155335484,0.0484111388,0.0092214822,-0.0043155274,0.0062575833,-0.0395887766,0.0146842867,0.0152894797,0.0146778752,-0.0107331617,-0.0119105937,-0.0389489341,0.0097992272,0.01232261,-0.001191695,0.0018610417,-0.0092097716,-0.0067652819,0.0093469356,-0.0140695361,-0.0148386757,-0.0037438628,-0.0106207551,0.002876698,-0.0103360862,-0.0038300776,-0.0072236705,-0.0408452193,0.0018109134,-0.0065549602,-0.0015021639,0.0448767077,0.0605956876,-0.0580624972,-0.0119441527,-0.0083344566,0.0046960792,-0.0387360199,0.0224474327,-0.0154437058,0.0083757159,-0.0231466312,0.0336230723,-0.0010523306,0.0185394449,0.0057649709,-0.0183329094,-0.0065555415]},"282":{"Abstract":"Light specification in three dimensional scenes is a complex problem and several approaches have been presented that aim to automate this process. However, there are many scenarios where a static light setup is insufficient, as the scene content and camera position may change. Simultaneous manual control over the camera and light position imposes a high cognitive load on the user. To address this challenge, we introduce a novel approach for automatic scene illumination with Fireflies. Fireflies are intelligent virtual light drones that illuminate the scene by traveling on a closed path. The Firefly path automatically adapts to changes in the scene based on an outcome-oriented energy function. To achieve interactive performance, we employ a parallel rendering pipeline for the light path evaluations. We provide a catalog of energy functions for various application scenarios and discuss the applicability of our method on several examples.","Authors":"S. Stoppel; M. P. Erga; S. Bruckner","DOI":"10.1109\/TVCG.2018.2864656","Keywords":"Dynamic lighting design;lighting drones","Title":"Firefly: Virtual Illumination Drones for Interactive Visualization","Keywords_Processed":"lighting drone;dynamic lighting design","Keyword_Vector":[0.0681199607,-0.0619140567,-0.0406516756,-0.0580724714,-0.0007265403,-0.0702515005,-0.0832381259,-0.0170201483,-0.0931157069,0.0974745575,-0.1105283311,0.0929669203,-0.0173147159,0.0469709375,-0.1386535084,-0.0121975138,-0.128106897,0.0185915826,-0.0916535546,0.0030847292,0.1675538301,0.0646324127,0.040379206,-0.036679853,-0.0875606353,0.0580591376,0.0210967216,0.0820214728,-0.0256303137,-0.0469674958,-0.1096073899,-0.0797178667,-0.0229569625,-0.025562487,-0.0068197152,-0.1119516983,0.1283462339,-0.1081224036,-0.0348937588,-0.0298824875,0.0383988275,0.0357450303,0.011849343,-0.0010314124,-0.0280616704,0.0075722943,0.0081363487,-0.0245461329,0.0520438308,0.0110299039,-0.0593688005,0.0129015732],"Abstract_Vector":[0.1716587451,-0.1495255047,0.000453356,-0.0106727622,-0.0599144148,-0.0076017119,0.0067149094,0.0327238553,-0.030525549,-0.0360920997,-0.0057520782,0.0074018438,0.0629512167,-0.0545257696,0.0734303342,-0.0433532538,-0.0873077866,0.0178632612,-0.0297552839,-0.0290201371,0.0244431409,-0.0568993738,0.0085690652,-0.1131204891,-0.0757360103,-0.0112125577,-0.0136395245,-0.0130462101,-0.0695865564,-0.0451134546,-0.0468839769,-0.0477315924,0.0730442065,0.0746565083,-0.0248028439,0.007292673,0.0097852482,-0.033995797,-0.096421864,0.047027208,0.0018326919,-0.0355328804,0.0817383383,-0.1502012048,-0.0348141616,-0.0409317498,0.0213268516,0.0153044356,-0.0059135099,0.0590048316,-0.069310426,0.0198128775,0.0791462137,0.0089430341,0.0772122502,-0.0109780835,0.0068337127,-0.057062471,-0.0177135657,-0.0654774107,-0.0274674276,-0.0433887396,-0.0047480449,0.0170108214,-0.0613845011,0.0591994698,-0.071189082,-0.0098532987,-0.0158812832,0.03905331,-0.0109207227,-0.0702736205,-0.0105837879,0.0160286337,-0.0893449765,-0.0128781307,0.0554762489,-0.0001442741,-0.0232186611,0.0718985659,0.0404378455,0.013211961,-0.01346418,-0.054268605,0.0894197372,0.0004663259,0.093383393,-0.0614130865,-0.0308439203,-0.001640888,0.139131694,0.0762507124,0.0454628269,0.0302921225,0.0491136443,0.0284324241,0.0530019276,0.0466691226,-0.0269864782,-0.0287796879,0.0125620066,0.0142538127,0.0172031736,0.0174520245,-0.0343137855,-0.0586109178,0.0414031007,0.0185038802,-0.0114546992,-0.0144476768,0.0173209495,0.0094830582,-0.0231087896,0.0097645068,0.0478737699,-0.0118072692]},"283":{"Abstract":"Efficient algorithms for 3D character control in continuous control setting remain an open problem in spite of the remarkable recent advances in the field. We present a sampling-based model-predictive controller that comes in the form of a Monte Carlo tree search (MCTS). The tree search utilizes information from multiple sources including two machine learning models. This allows rapid development of complex skills such as 3D humanoid locomotion with less than a million simulation steps, in less than a minute of computing on a modest personal computer. We demonstrate locomotion of 3D characters with varying topologies under disturbances such as heavy projectile hits and abruptly changing target direction. In this paper we also present a new way to combine information from the various sources such that minimal amount of information is lost. We furthermore extend the neural network, involved in the algorithm, to represent stochastic policies. Our approach yields a robust control algorithm that is easy to use. While learning, the algorithm runs in near real-time, and after learning the sampling budget can be reduced for real-time operation.","Authors":"J. Rajam\u00e4ki; P. H\u00e4m\u00e4l\u00e4inen","DOI":"10.1109\/TVCG.2018.2849386","Keywords":"Monte Carlo tree search;continuous control;reinforcement learning","Title":"Continuous Control Monte Carlo Tree Search Informed by Multiple Experts","Keywords_Processed":"continuous control;Monte Carlo tree search;reinforcement learning","Keyword_Vector":[0.0654208303,0.0347392673,0.0277586149,0.0114836148,-0.0190337981,-0.0486428453,0.0103747103,0.0062170067,-0.0361040565,-0.0168812522,0.0089750604,0.0209996408,0.0268241261,0.0212618638,0.0352368406,0.0058033852,-0.0029900044,-0.0931685008,0.0767072681,-0.0639611239,0.031038938,-0.0663651922,-0.0508386478,0.057388111,0.0233853619,-0.0760407529,0.1728468931,0.0438260076,-0.1277555902,0.0308430117,-0.0492211538,0.0474485792,0.0590687252,-0.0370473772,-0.0002037791,0.0936624091,-0.0926229846,-0.0667781858,-0.0581134253,0.0299643206,-0.0062985346,0.013169975,0.0219414971,-0.0341376611,0.0147319536,-0.0682249056,0.029833159,0.0252344487,0.0289288708,-0.0065715318,-0.0074148973,0.048190634],"Abstract_Vector":[0.2200735176,0.0802256687,-0.0164009838,0.0491226088,-0.0202759097,0.0235613764,-0.0328691613,0.0411658545,-0.0312109711,-0.1065180208,-0.0415875036,0.0074378,-0.1050453779,0.0102606442,-0.0551759628,-0.0656780498,0.0076767888,0.1103832761,-0.0187152907,-0.0715451882,0.0762059556,0.0366747964,0.0111278786,0.009510951,0.0139484126,-0.0457233285,0.0460428695,0.0246982321,-0.0122454087,0.0162145209,-0.0273502557,-0.0063528603,0.0065105056,0.0538335996,-0.0642898134,-0.0148955471,0.012449627,-0.0419299036,-0.0335582424,-0.0152472242,-0.0119655345,0.0219672289,-0.0145284013,-0.0364031022,-0.0390504135,-0.0300025086,0.0367383215,0.0263034359,0.0356857333,0.059060245,0.0090934251,0.0189504165,-0.0053470768,0.0328447688,-0.0175798174,0.0811379062,0.0531112803,0.001969388,0.0085052236,0.0430414836,0.0585323612,0.0373398392,-0.0181087768,0.0413879221,-0.0482920541,0.0637686553,0.0201315303,0.0196947664,-0.0184537103,-0.0046242326,-0.0153742424,0.0537417344,0.0106835695,-0.0179546469,0.0013620834,0.024958773,0.0243536792,-0.0132186251,-0.0198099943,-0.0165940707,-0.0368068862,0.0024550149,0.0466752759,0.0345731415,-0.0015718636,0.0321878123,0.0031879229,-0.0458605343,0.0034980486,-0.0059390032,-0.0194121561,0.0423491632,-0.0486688625,0.0175669053,0.0448661804,-0.001088769,0.0093046909,-0.0700113772,0.0108775804,0.0200688836,-0.0077074985,0.0118087658,-0.0045356257,0.0208393879,-0.0047021568,-0.0204684911,-0.0098263812,-0.0022175484,-0.0200718521,0.0428661104,-0.0072138313,0.066471028,0.0757136613,-0.0086426879,-0.021971819,0.0041642386]},"284":{"Abstract":"We present a family of three interactive Context-Aware Selection Techniques (CAST) for the analysis of large 3D particle datasets. For these datasets, spatial selection is an essential prerequisite to many other analysis tasks. Traditionally, such interactive target selection has been particularly challenging when the data subsets of interest were implicitly defined in the form of complicated structures of thousands of particles. Our new techniques SpaceCast, TraceCast, and PointCast improve usability and speed of spatial selection in point clouds through novel context-aware algorithms. They are able to infer a user's subtle selection intention from gestural input, can deal with complex situations such as partially occluded point clusters or multiple cluster layers, and can all be fine-tuned after the selection interaction has been completed. Together, they provide an effective and efficient tool set for the fast exploratory analysis of large datasets. In addition to presenting Cast, we report on a formal user study that compares our new techniques not only to each other but also to existing state-of-the-art selection methods. Our results show that Cast family members are virtually always faster than existing methods without tradeoffs in accuracy. In addition, qualitative feedback shows that PointCast and TraceCast were strongly favored by our participants for intuitiveness and efficiency.","Authors":"L. Yu; K. Efstathiou; P. Isenberg; T. Isenberg","DOI":"10.1109\/TVCG.2015.2467202","Keywords":"Selection;spatial selection;structure-aware selection;context-aware selection;exploratory data visualization and analysis;3D interaction;user interaction;Selection;spatial selection;structure-aware selection;context-aware selection;exploratory data visualization and analysis;3D interaction;user interaction","Title":"CAST: Effective and Efficient User Interaction for Context-Aware Selection in 3D Particle Clouds","Keywords_Processed":"user interaction;exploratory datum visualization and analysis;3d interaction;selection;structure aware selection;context aware selection;spatial selection","Keyword_Vector":[0.0339633535,-0.017989024,-0.0279108138,0.0299963332,-0.0173934265,0.0074565397,0.0047383535,-0.0002501349,-0.0186864314,-0.0031171807,0.0061110753,0.006703937,0.0047102202,0.0169367195,-0.0051109849,-0.010651526,0.0056091893,0.0170124257,0.0521652021,-0.0091136672,-0.0120728958,-0.0021108263,0.0061925038,0.0252694383,0.0408583544,-0.0138749446,-0.0053293813,0.0024015263,0.0193706474,-0.0038267976,0.0123114025,-0.034823838,0.0072607108,0.0173742626,0.0013375497,0.0638241486,0.0220590644,-0.0047533027,0.0121635022,0.0003436632,0.0644193129,0.0175539,0.0170385415,0.0133978801,-0.0444693949,-0.0189803792,0.0558469201,-0.0618258021,-0.0050680982,-0.0109568112,-0.0050863795,-0.0037932641],"Abstract_Vector":[0.2146100004,-0.1381412295,0.004132448,0.0114953684,-0.1073109186,0.067421296,0.0470467001,0.0367957007,-0.0833162356,-0.023744974,-0.022757639,0.0147525088,0.0461452878,-0.0187744175,0.0382735022,0.0154762609,0.0164778866,0.0539541433,0.0117921594,-0.0315840308,0.0515940079,-0.0090360137,-0.0063290023,0.017624927,-0.0252221813,0.0050361064,0.0214817386,-0.0015636218,-0.031746669,-0.0021150667,-0.0109333177,-0.0234717526,0.0113124424,-0.0090723614,-0.0179108527,0.0092324482,-0.0125255263,-0.0220826819,0.0108522727,-0.0374171732,-0.066259805,-0.0384629826,-0.0080692471,-0.0386944757,0.0049700049,0.0173252447,0.0358472446,0.0459246956,0.0072643773,-0.039488277,0.0343484954,0.0614939672,-0.0367258991,0.0172880306,-0.0501489672,0.0072660831,-0.0724865088,0.0173813894,-0.0138209187,0.0134918425,0.0151784386,0.0288585479,-0.0219853568,0.0246855417,0.0072201381,0.065349798,-0.0065445397,-0.0182240486,-0.0117153684,0.0029281158,-0.0194030299,0.0220531672,0.0307426289,0.0138965051,0.0002334896,-0.038561434,-0.021002407,-0.020873908,0.0122763151,-0.0222901038,0.0071183619,-0.0023216804,-0.0230246865,0.0227017188,-0.009160502,0.0566545849,-0.0006015064,-0.0489917552,0.0336568387,-0.0110948644,0.020610105,0.0193076262,-0.0004999266,0.0175727477,-0.0675893258,-0.033559416,0.0481301156,-0.042109053,0.0406481208,0.0145277021,-0.0251857405,0.0269595393,-0.0071051322,0.0109746053,-0.0081834346,-0.0640401265,0.0045054725,-0.0242785433,-0.0016269538,0.0096127231,-0.0203457183,0.0190692869,-0.0025223046,0.0144913925,-0.024938489,0.0316903268]},"285":{"Abstract":"Labeling is intrinsically important for exploring and understanding complex environments and models in a variety of domains. We present a method for interactive labeling of crowded 3D scenes containing very many instances of objects spanning multiple scales in size. In contrast to previous labeling methods, we target cases where many instances of dozens of types are present and where the hierarchical structure of the objects in the scene presents an opportunity to choose the most suitable level for each placed label. Our solution builds on and goes beyond labeling techniques in medical 3D visualization, cartography, and biological illustrations from books and prints. In contrast to these techniques, the main characteristics of our new technique are: 1) a novel way of labeling objects as part of a bigger structure when appropriate, 2) visual clutter reduction by labeling only representative instances for each type of an object, and a strategy of selecting those. The appropriate level of label is chosen by analyzing the scene's depth buffer and the scene objects' hierarchy tree. We address the topic of communicating the parent-children relationship between labels by employing visual hierarchy concepts adapted from graphic design. Selecting representative instances considers several criteria tailored to the character of the data and is combined with a greedy optimization approach. We demonstrate the usage of our method with models from mesoscale biology where these two characteristics-multi-scale and multi-instance-are abundant, along with the fact that these scenes are extraordinarily dense.","Authors":"D. Kou\u0159il; L. \u010cmol\u00edk; B. Kozl\u00edkov\u00e1; H. Wu; G. Johnson; D. S. Goodsell; A. Olson; M. E. Gr\u00f6ller; I. Viola","DOI":"10.1109\/TVCG.2018.2864491","Keywords":"labeling;multi-scale data;multi-instance data","Title":"Labels on Levels: Labeling of Multi-Scale Multi-Instance and Crowded 3D Biological Environments","Keywords_Processed":"multi instance datum;labeling;multi scale datum","Keyword_Vector":[0.0648887144,0.0335234919,-0.0169135287,0.0902785098,0.0462545974,0.0287614077,0.0082805167,0.0088563145,-0.0189832689,0.0238693468,-0.0202675747,-0.029477138,-0.0581120908,0.017038414,-0.0735608063,-0.1033657778,0.0388339216,-0.0151572058,0.0185116401,0.0958033133,0.0401191693,-0.0063394125,0.011754365,0.0751832695,0.0555285311,0.102102151,0.0817247227,0.0659804246,-0.0156628733,-0.0880354681,-0.0141723909,-0.0234817857,0.0458376519,0.014475952,-0.0392260205,0.0585768108,-0.0015877759,0.0422901594,-0.0223252231,0.0118587543,0.0219726367,-0.0490011131,0.1215218054,-0.0125376506,0.0330485702,-0.0151520587,0.0279385927,0.0289161497,-0.0171586224,-0.0110033793,0.0212889258,0.0078533756],"Abstract_Vector":[0.1479552673,0.1114662175,-0.0064742442,0.1026291464,-0.0006055393,-0.0278600503,-0.0436594515,0.0171131551,-0.0763275271,-0.0597015522,-0.0253149106,0.0221481365,-0.0446572811,0.0410225831,-0.0555589754,-0.0114885816,-0.0373183281,0.0116063743,-0.0267399595,0.0218851471,-0.0299360654,-0.0175518727,-0.0050101244,0.010265381,-0.072258155,0.0051062641,-0.0028737526,0.0119039314,0.0180959033,0.0216581419,-0.005561078,0.0169661127,0.0306756761,-0.0114635707,0.0063104513,0.0118422838,-0.0034067061,0.0170644749,0.0414718968,0.0547730501,0.0173218556,-0.0152080101,0.0078804797,-0.0303638061,-0.0272155852,-0.0209372087,-0.0054850016,-0.0342072391,-0.015336868,-0.023102224,0.0240870709,-0.039028985,-0.0259444855,-0.0470903267,0.010257756,0.029263727,0.0080625496,-0.0366548109,-0.0047866456,0.0013681441,0.0223728421,0.0113998214,0.0073325042,-0.0055416597,-0.0070598959,0.0020434787,-0.0284812811,-0.0045437509,0.01130693,0.0278898754,-0.0405672715,-0.0206197824,0.0075563673,0.0221031384,0.0065436221,-0.012583837,-0.014648067,0.0196456751,-0.0120291886,0.02288441,0.0108820093,0.0158788718,-0.0092205962,0.0224974638,-0.0023837008,0.0415993347,0.0075439529,-0.0209753412,0.0199211747,-0.0100238199,-0.0142027814,-0.0220993869,-0.024548081,-0.0371369227,-0.03494593,0.0066839588,0.0027489253,0.0337386579,-0.0051796151,0.0109128431,0.0034247712,-0.0133059686,-0.0063477954,0.0070662565,-0.0206082925,-0.014183652,-0.0032522163,0.0545875019,-0.0198909664,-0.0022000976,0.0010104733,-0.0429168093,-0.0300175476,-0.0182560989,0.0396828215,-0.0113930684]},"286":{"Abstract":"Redirected walking (RDW) promises to allow near-natural walking in an infinitely large virtual environment (VE) by subtle manipulations of the virtual camera. Previous experiments analyzed the human sensitivity to RDW manipulations by focusing on the worst-case scenario, in which users walk perfectly straight ahead in the VE, whereas they are redirected on a circular path in the real world. The results showed that a physical radius of at least 22 meters is required for undetectable RDW. However, users do not always walk exactly straight in a VE. So far, it has not been investigated how much a physical path can be bent in situations in which users walk a virtual curved path instead of a straight one. Such curved walking paths can be often observed, for example, when users walk on virtual trails, through bent corridors, or when circling around obstacles. In such situations the question is not, whether or not the physical path can be bent, but how much the bending of the physical path may vary from the bending of the virtual path. In this article, we analyze this question and present redirection by means of bending gains that describe the discrepancy between the bending of curved paths in the real and virtual environment. Furthermore, we report the psychophysical experiments in which we analyzed the human sensitivity to these gains. The results reveal encouragingly wider detection thresholds than for straightforward walking. Based on our findings, we discuss the potential of curved walking and present a first approach to leverage bent paths in a way that can provide undetectable RDW manipulations even in room-scale VR.","Authors":"E. Langbehn; P. Lubos; G. Bruder; F. Steinicke","DOI":"10.1109\/TVCG.2017.2657220","Keywords":"Virtual reality;redirected walking;room-scale;bending gains","Title":"Bending the Curve: Sensitivity to Bending of Curved Paths and Application in Room-Scale VR","Keywords_Processed":"bend gain;virtual reality;redirect walking;room scale","Keyword_Vector":[0.0838802685,-0.0052915717,-0.0102659057,-0.0467461834,0.0165686453,-0.024897638,-0.045428937,0.0078389044,-0.0703962252,-0.0257097241,0.0551740278,0.0640583609,-0.001235572,0.0091252225,0.0597267902,0.0678890282,0.0369398194,0.0610306928,0.206854714,-0.0181228147,-0.1048043342,0.0816202896,0.0986027184,0.0675003834,-0.0841488459,-0.0324308172,0.0205646291,-0.0109548781,0.1032661191,-0.1046879982,0.0021277056,-0.1128968479,0.0417921241,0.0357270041,-0.0377755998,-0.0046909513,-0.0162354765,-0.0058002668,-0.0398442707,-0.0395383619,-0.0159648905,0.0748311573,0.0821082327,0.0103244344,-0.0115071493,0.0145269358,0.0194891957,0.0307998044,-0.0054204087,0.0444227921,-0.0374678927,-0.0076717523],"Abstract_Vector":[0.166275111,-0.0349123835,-0.0283389697,-0.0132857898,-0.0250661452,0.0600450473,-0.0275547229,0.0003130065,0.0785301222,0.0269140136,-0.0705363013,0.0323118799,-0.0545054246,0.0239860494,0.0079308737,0.0696120928,0.0161602611,0.0119975045,0.0807468729,0.0123856717,-0.0598231245,-0.0518414324,-0.0126435295,0.0067257811,0.0206484291,-0.0195885448,-0.0089960796,0.0356193466,0.0001095391,0.0486405902,-0.0157441917,0.065572759,-0.0458436721,-0.0176381336,-0.0106091184,-0.047822357,0.0191960467,-0.0203234511,-0.0294384631,-0.0285874048,-0.0202911814,0.0654094357,0.0201913548,-0.0172427573,-0.0026640403,-0.0093359638,-0.0058679003,0.0252106317,-0.0035755039,0.0106952811,-0.0053087957,-0.047948019,0.0548655996,0.0057972595,-0.0422576863,-0.0049723514,0.020654515,0.0406330386,0.0078696788,-0.0456414417,0.0210210133,-0.0332617849,-0.0060114466,-0.0328973812,-0.0050771597,0.0102334317,0.0190307563,-0.0034986372,-0.0555329841,-0.0459057648,-0.021092423,0.0166139428,0.0142187945,0.0221011636,0.0156236886,-0.0159278768,0.0032026082,0.0093027917,0.000791282,-0.0311406912,-0.0161391207,-0.0048063197,0.0014400696,-0.0136123222,0.0337290133,0.0540067494,-0.0169951176,-0.0196680916,-0.01166764,0.0200643495,0.0678796915,0.0095395768,0.0003751154,0.00631543,-0.0003771315,-0.0132622455,-0.0255539405,-0.0164563201,-0.0014723778,0.0319919933,0.0061481875,0.0251805419,-0.0188197563,-0.0065530724,0.0242408994,-0.0032215353,0.0104828619,0.0232556113,-0.0338893365,-0.012802427,0.0129664015,-0.0045829351,0.0126926595,-0.0411338429,0.0116108053,0.022668093]},"287":{"Abstract":"Package managers provide ease of access to applications by removing the time-consuming and sometimes completely prohibitive barrier of successfully building, installing, and maintaining the software for a system. A package dependency contains dependencies between all packages required to build and run the target software. Package management system developers, package maintainers, and users may consult the dependency graph when a simple listing is insufficient for their analyses. However, users working in a remote command line environment must disrupt their workflow to visualize dependency graphs in graphical programs, possibly needing to move files between devices or incur forwarding lag. Such is the case for users of Spack, an open source package management system originally developed to ease the complex builds required by supercomputing environments. To preserve the command line workflow of Spack, we develop an interactive ASCII visualization for its dependency graphs. Through interviews with Spack maintainers, we identify user goals and corresponding visual tasks for dependency graphs. We evaluate the use of our visualization through a command line-centered study, comparing it to the system's two existing approaches. We observe that despite the limitations of the ASCII representation, our visualization is preferred by participants when approached from a command line interface workflow.","Authors":"K. E. Isaacs; T. Gamblin","DOI":"10.1109\/TVCG.2018.2859974","Keywords":"Software visualization;information visualization;command line interface","Title":"Preserving Command Line Workflow for a Package Management System Using ASCII DAG Visualization","Keywords_Processed":"command line interface;software visualization;information visualization","Keyword_Vector":[0.0318636286,0.0221624957,0.0538682724,0.0172314507,-0.0278846806,-0.0329362399,0.0212753159,0.0595660485,-0.1432061298,0.0247163619,0.1631717923,-0.0273701554,-0.0051206914,-0.0380651331,-0.0076885387,-0.0408053385,0.0079349222,-0.0052312514,-0.0531497331,-0.001357516,0.0576972809,-0.0326851704,-0.0205848092,-0.0066836801,-0.0251325305,0.048623314,0.0268758306,-0.0195687324,-0.0837257691,0.0172067157,0.0173928787,-0.0888514176,0.035159073,0.0025143815,0.0347372967,-0.0039574691,0.0081555738,0.0929727016,0.0549507556,-0.0265694922,-0.063107704,0.0609981969,0.0682919183,0.0337105042,0.020028488,-0.0369800454,-0.0354710318,0.0588086164,0.0618233193,0.0066095363,-0.0434609907,0.0107700017],"Abstract_Vector":[0.1821879011,0.0475596189,0.0134555674,-0.0004754494,0.0499352544,-0.0700623808,-0.0581689532,0.1252278314,0.0006411455,0.0589825463,0.0419905366,0.0041310784,-0.0251350101,-0.0033294263,0.0132182764,0.0705320949,-0.0238674794,0.0026568374,-0.0527492019,0.0192718503,-0.0089604044,-0.0054396098,-0.0322526494,-0.0069028125,-0.0091767065,-0.0575902366,0.0397298111,-0.0035430449,-0.0050489294,0.0834064338,0.0064918358,0.0291327335,0.0035716801,0.0178010269,-0.0866071672,-0.0340164625,0.0545779468,0.0294895293,0.0241289959,-0.0204677752,-0.0295605981,0.0356177566,-0.0133993913,-0.0611064847,-0.0494384759,0.0048305555,0.0208436588,0.0484395113,0.0135913031,-0.0046906178,-0.0195339173,0.0107069734,-0.0307246975,-0.037056037,-0.0065242525,0.0099399883,-0.012901736,0.0307244959,0.0220493278,-0.0441716047,0.0360886608,-0.010163311,-0.033588553,-0.0033253798,0.0508355796,0.0855443917,0.0285307203,0.0025154879,-0.0328862344,0.034412348,-0.0172020628,-0.024856779,0.0383450013,-0.0250909621,0.0260022345,-0.0582127586,-0.0066053282,-0.0582233941,-0.0504640031,-0.0009957944,0.0036125935,-0.012108256,-0.0004795897,-0.0059235975,0.0045974314,-0.0603731759,0.0348285376,0.0105624584,-0.0002645317,0.0384717723,0.0393072663,-0.0078798896,-0.0226377023,-0.0250164221,0.017528272,-0.0086701892,-0.0690893697,0.013346855,0.0093811453,0.0459247273,-0.0323429691,0.0248656048,0.0531803643,0.089511546,0.0020546275,0.0302333678,0.0144355507,0.0188792163,-0.0142976356,-0.070512228,-0.0118892856,-0.0390111565,-0.0373535867,-0.020776026,0.0373524839,0.0880096617]},"288":{"Abstract":"Understanding co-occurrence in urban human mobility (i.e. people from two regions visit an urban place during the same time span) is of great value in a variety of applications, such as urban planning, business intelligence, social behavior analysis, as well as containing contagious diseases. In recent years, the widespread use of mobile phones brings an unprecedented opportunity to capture large-scale and fine-grained data to study co-occurrence in human mobility. However, due to the lack of systematic and efficient methods, it is challenging for analysts to carry out in-depth analyses and extract valuable information. In this paper, we present TelCoVis, an interactive visual analytics system, which helps analysts leverage their domain knowledge to gain insight into the co-occurrence in urban human mobility based on telco data. Our system integrates visualization techniques with new designs and combines them in a novel way to enhance analysts' perception for a comprehensive exploration. In addition, we propose to study the correlations in co-occurrence (i.e. people from multiple regions visit different places during the same time span) by means of biclustering techniques that allow analysts to better explore coordinated relationships among different regions and identify interesting patterns. The case studies based on a real-world dataset and interviews with domain experts have demonstrated the effectiveness of our system in gaining insights into co-occurrence and facilitating various analytical tasks.","Authors":"W. Wu; J. Xu; H. Zeng; Y. Zheng; H. Qu; B. Ni; M. Yuan; L. M. Ni","DOI":"10.1109\/TVCG.2015.2467194","Keywords":"Co-occurrence;human mobility;telco data;bicluster;visual analytics;Co-occurrence;human mobility;telco data;bicluster;visual analytics","Title":"TelCoVis: Visual Exploration of Co-occurrence in Urban Human Mobility Based on Telco Data","Keywords_Processed":"human mobility;telco datum;co occurrence;visual analytic;bicluster","Keyword_Vector":[0.0681280716,-0.0034174547,-0.0209335078,-0.0000966703,0.0344144103,-0.0547281893,-0.057607085,-0.0534329536,-0.0886760585,0.033465379,-0.0290938688,0.0551285244,0.0198016462,0.0914672784,0.0304344393,-0.0337306976,-0.0009568173,0.0077387868,0.0459169435,-0.0143480606,0.0274574506,0.0895164496,-0.0632799352,0.1035347319,0.0404491519,-0.1164922528,-0.0057736455,-0.0311177291,-0.003584375,-0.0348996857,0.0432162455,-0.0114062458,-0.0328922656,-0.0117339753,0.0320978815,0.0973749085,0.0088601427,0.0068145033,0.016437412,0.0427521459,-0.0112128087,0.0067120677,-0.029026707,-0.0234249469,-0.0618203793,0.0271269957,0.0077512567,0.0678808944,0.0303315434,0.0061139361,0.0217424008,-0.0141017889],"Abstract_Vector":[0.1293863168,-0.0046096938,-0.0379654314,0.0095078939,-0.0241129787,0.0385714155,0.0119469073,0.0329360209,0.0673707049,-0.0238245408,-0.0379767364,-0.0346068876,-0.0836829588,-0.0132694981,0.0043117984,-0.002894609,0.0536529375,0.0165999316,-0.0144047275,-0.0209387769,0.0042664146,0.0010150732,0.0132653332,-0.0290555001,0.0165689667,-0.0174916458,-0.0073234173,-0.0415419219,-0.0192431427,-0.0151730682,-0.0598707606,0.0259826101,0.0236765862,-0.0343469873,-0.0023450538,0.0508103972,0.0165969677,0.009869405,-0.0173159858,0.0036210341,0.0338460136,0.075374298,-0.0027038741,0.032048703,0.0026957948,0.0078185789,0.0122150695,0.0168568752,0.029819659,-0.0090725848,0.0314691283,-0.0255289008,-0.041928276,0.0256366295,0.0173541373,0.0128477988,-0.033305495,0.0309250527,-0.0264191506,-0.0122266125,-0.0078439625,-0.0125909308,-0.0158679737,0.0037575249,-0.0189678816,0.0542160549,-0.0244014749,0.0118724516,0.0008458272,-0.0528706037,0.0224791608,0.0658515791,0.0045438899,0.0650821536,-0.0070717705,-0.0366313212,0.0163190632,0.0700995088,0.0042008499,0.041477631,-0.0161338911,-0.0021840116,0.0015606157,-0.0038692646,-0.0124929191,-0.0529009246,-0.0080788385,-0.0334318573,-0.0187780091,-0.0404716337,0.022513738,-0.0234765309,-0.0349787006,0.020735766,-0.0501720279,0.0262780816,0.0506092184,-0.0037351467,-0.0133368844,0.0360560512,0.0389815082,-0.0229983196,0.0534614527,0.0117825686,-0.0260966192,-0.0098416378,-0.0081413113,0.011665752,0.0191881851,-0.0015273682,-0.0141342225,0.0018089278,0.001293691,0.0201772485,0.0332455636,0.0346193201]},"289":{"Abstract":"Understanding hexahedral (hex-) mesh structures is important for a number of hex-mesh generation and optimization tasks. However, due to various configurations of the singularities in a valid pure hex-mesh, the structure (or base complex) of the mesh can be arbitrarily complex. In this work, we present a first and effective method to help meshing practitioners understand the possible configurations in a valid 3D base complex for the characterization of their complexity. In particular, we propose a strategy to decompose the complex hex-mesh structure into multi-level sub-structures so that they can be studied separately, from which we identify a small set of the sub-structures that can most efficiently represent the whole mesh structure. Furthermore, from this set of sub-structures, we attempt to define the first metric for the quantification of the complexity of hex-mesh structure. To aid the exploration of the extracted multi-level structure information, we devise a visual exploration system coupled with a matrix view to help alleviate the common challenge of 3D data exploration (e.g., clutter and occlusion). We have applied our tool and metric to a large number of hex-meshes generated with different approaches to reveal different characteristics of these methods in terms of the mesh structures they can produce. We also use our metric to assess the existing structure simplification techniques in terms of their effectiveness.","Authors":"K. Xu; G. Chen","DOI":"10.1109\/TVCG.2018.2864827","Keywords":"hexahedral mesh;base complex;sheet decomposition;complexity analysis","Title":"Hexahedral Mesh Structure Visualization and Evaluation","Keywords_Processed":"complexity analysis;sheet decomposition;hexahedral mesh;base complex","Keyword_Vector":[0.0418773242,0.0113523892,0.0319865932,0.0142861626,-0.0212233049,0.0042801547,-0.0087165383,0.0259073295,-0.0445800163,0.0160859902,-0.008899837,0.0146116105,0.0147001366,0.0068299382,0.0140931085,-0.0067218677,0.0339603492,-0.0192049398,-0.0158501925,0.0087243645,-0.0103362779,-0.0249137871,0.0175041401,0.0282863868,-0.0012101184,-0.0180309574,0.0712229993,0.0267399363,-0.0364265609,-0.023419657,0.0583593421,0.0173511714,0.0101634214,0.0345380993,0.0051951946,-0.0328465176,0.0960501579,0.0429237733,0.0161332479,0.0235187224,0.0330445267,0.0274193518,-0.0942573918,-0.0547009406,-0.0292705135,-0.0224262947,0.0440261349,0.0354754821,-0.0585545813,0.0253325705,0.0315281207,0.0616104871],"Abstract_Vector":[0.156561616,0.0260334203,-0.0315952217,-0.0195640962,-0.0378156852,0.0508049486,0.0211533168,0.0090339238,0.0668343745,0.0246500051,-0.0260536383,0.0093739769,-0.0308509081,-0.0100129366,0.0060594637,0.0265099592,0.0419355566,-0.0031806462,0.0413889095,-0.0479777089,0.003574225,-0.0309186674,0.0315096109,-0.0448047759,-0.027335483,0.0110788474,0.0052684081,-0.0129067227,-0.0371232981,0.0636670862,0.1212290041,-0.0151504858,-0.0175656627,-0.0723498589,0.022194808,0.011839002,-0.0101113535,0.0222004728,-0.0457922307,0.0458464452,0.0615203721,-0.0013055293,0.0172127435,0.011804594,0.0522367982,0.0193809491,0.0664207362,0.0287367774,0.0050362661,0.0224569132,-0.0263187468,-0.0045673953,-0.0266446044,-0.0619324432,0.0041941631,-0.0195593645,0.0106850373,0.0224688659,0.0139542453,-0.0476849772,-0.0104646934,0.0216638167,-0.0119854455,-0.0076479378,0.0234345223,0.0189412448,-0.0185115535,-0.0182780132,0.0041594208,-0.0273398046,0.0287615249,-0.0332854881,-0.0403848074,-0.0185563539,0.0160560767,-0.0116702644,-0.0199823093,-0.0132066261,-0.0285136197,0.0084274359,0.0036154366,-0.0075989887,0.0185918713,0.0154083645,0.0278892724,0.010319317,-0.0159377469,0.0071117328,0.0023443815,0.0008682129,0.0135987612,0.0107002138,-0.0082052244,0.0378786944,0.0060636809,-0.0178906913,0.0151087639,-0.0093205729,-0.0045174846,0.0117124987,0.0317783248,0.005260347,-0.0179654322,0.0117147315,-0.0025294692,-0.0056712482,-0.0377051388,0.0120396139,0.0202692262,0.0263876198,-0.0204941594,-0.0041536347,-0.0127073605,0.011298717,0.0314531416,0.0284932168]},"29":{"Abstract":"CoDDA (Copula-based Distribution Driven Analysis) is a flexible framework for large-scale multivariate datasets. A common strategy to deal with large-scale scientific simulation data is to partition the simulation domain and create statistical data summaries. Instead of storing the high-resolution raw data from the simulation, storing the compact statistical data summaries results in reduced storage overhead and alleviated I\/O bottleneck. Such summaries, often represented in the form of statistical probability distributions, can serve various post-hoc analysis and visualization tasks. However, for multivariate simulation data using standard multivariate distributions for creating data summaries is not feasible. They are either storage inefficient or are computationally expensive to be estimated in simulation time (in situ) for large number of variables. In this work, using copula functions, we propose a flexible multivariate distribution-based data modeling and analysis framework that offers significant data reduction and can be used in an in situ environment. The framework also facilitates in storing the associated spatial information along with the multivariate distributions in an efficient representation. Using the proposed multivariate data summaries, we perform various multivariate post-hoc analyses like query-driven visualization and sampling-based visualization. We evaluate our proposed method on multiple real-world multivariate scientific datasets. To demonstrate the efficacy of our framework in an in situ environment, we apply it on a large-scale flow simulation.","Authors":"S. Hazarika; S. Dutta; H. Shen; J. Chen","DOI":"10.1109\/TVCG.2018.2864801","Keywords":"In situ processing;Distribution-based;Multivariate;Query-driven;Copula","Title":"CoDDA: A Flexible Copula-based Distribution Driven Analysis Framework for Large-Scale Multivariate Data","Keywords_Processed":"distribution base;Multivariate;query drive;in situ processing;copula","Keyword_Vector":[0.1474253566,-0.0117977065,0.1376856161,-0.0389701102,-0.0591632427,-0.0232843867,0.0111465459,0.0426156931,0.0094248413,0.0267685692,-0.0294716854,-0.0238130771,0.0203078398,-0.0081366838,0.0124867505,0.0037051189,-0.0692949122,0.0658333123,0.0222952527,0.0084142384,-0.0637572456,0.0082654051,-0.041410436,-0.005654591,0.0348749974,0.027764994,0.0560582085,0.0184145261,0.0090306968,-0.0054104906,0.0697698639,0.0180922413,-0.0292286207,-0.0096444327,-0.0436828724,-0.013181915,0.0025618764,-0.0043794882,-0.0439157139,-0.0075264274,-0.0000727693,-0.0086128723,0.0154774119,0.008608417,0.0026011942,-0.0110616975,-0.0054062241,0.0173592019,-0.0122562693,-0.008288079,0.0180529929,0.0592317594],"Abstract_Vector":[0.1567963741,-0.0660833933,-0.0034423618,0.0298050062,-0.0047558685,-0.0256609061,-0.0338243382,-0.0260586633,0.0313317587,-0.0357969251,-0.006013282,0.0073267633,-0.0190192469,0.0045896962,-0.0191796166,0.0465656179,-0.0199172091,-0.0688290202,-0.0085566297,0.004797016,-0.0262489451,0.0241433404,-0.0186194497,0.0146989681,0.0024933508,0.0030347299,0.0312912019,0.0464717278,-0.007547533,0.1181927107,-0.0491532922,0.0827356356,-0.020905475,-0.0055882139,0.0068536846,-0.0023450941,0.0298288675,-0.0089544006,0.038170073,0.0307309122,0.0035563568,0.0278593296,-0.0073265169,0.0309752066,0.0069877063,-0.0368338159,0.0041038512,0.0226218021,0.0243975695,0.0177965426,-0.0416868478,0.0690986466,0.08921051,-0.0024721596,-0.005930437,-0.0142764442,0.0431106747,0.0270425694,0.0200011234,0.0480628526,-0.0069776232,-0.0405928779,-0.0255006759,-0.0170971134,-0.0736575589,0.0012134932,0.0250949753,0.0598659272,0.0068500891,0.0098700832,-0.0715286809,-0.0420661568,-0.0187768781,0.0033320476,-0.0639813079,0.092944056,0.0201165038,0.0165335578,-0.0629050475,-0.044885436,-0.0068773824,-0.0277260052,0.0765981343,-0.0369487106,-0.008166161,0.0821934787,-0.0043173284,0.0571707161,0.0330790793,0.0259827894,-0.0320955618,-0.0104133623,-0.0362273944,0.0382862859,-0.0752353925,0.0676256139,0.0367672208,0.0511241331,0.0113561024,-0.0000560168,0.0349284736,0.0066406771,0.0648883494,-0.0393809172,0.0728550401,0.0138283597,-0.0533574842,-0.0264019945,-0.0245825817,0.017072274,0.0036915889,-0.0000151743,-0.0075579409,0.0410380599,0.0141923431,-0.0057041154]},"290":{"Abstract":"We present a new motion tracking technique to robustly reconstruct non-rigid geometries and motions from a single view depth input recorded by a consumer depth sensor. The idea is based on the observation that most non-rigid motions (especially human-related motions) are intrinsically involved in articulate motion subspace. To take this advantage, we propose a novel L0 based motion regularizer with an iterative solver that implicitly constrains local deformations with articulate structures, leading to reduced solution space and physical plausible deformations. The L0 strategy is integrated into the available non-rigid motion tracking pipeline, and gradually extracts articulate joints information online with the tracking, which corrects the tracking errors in the results. The information of the articulate joints is used in the following tracking procedure to further improve the tracking accuracy and prevent tracking failures. Extensive experiments over complex human body motions with occlusions, facial and hand motions demonstrate that our approach substantially improves the robustness and accuracy in motion tracking.","Authors":"K. Guo; F. Xu; Y. Wang; Y. Liu; Q. Dai","DOI":"10.1109\/TVCG.2017.2688331","Keywords":"Performance capture, non-rigid, single-view, L0","Title":"Robust Non-Rigid Motion Tracking and Surface Reconstruction Using $L_0$ Regularization","Keywords_Processed":"performance capture non rigid single view l0","Keyword_Vector":[0.0310792589,-0.013647558,-0.0140312057,0.0009174194,0.0231495314,-0.0039517249,-0.0059748196,0.0085623533,-0.0238648265,-0.0094363682,-0.012672277,0.0283689004,-0.0027047675,0.0313379657,-0.0092745262,-0.0228127532,-0.0575189831,-0.026448033,0.0172823431,0.0454606827,0.0306754326,-0.0054468618,0.0489098892,-0.0067396342,-0.0120919024,-0.0056069238,0.0101400409,0.0368322845,-0.0455096869,0.0354956301,-0.0261792548,-0.0239103731,0.0037189567,0.0423837324,-0.0423141019,0.0287275023,-0.0681748519,-0.0208447158,0.0229717884,-0.049518457,-0.0240256839,0.0297437632,-0.0639577693,-0.060502715,-0.015781005,-0.0003998304,0.0811177199,0.0007591429,-0.0289730112,0.003893933,-0.006293188,0.1054798713],"Abstract_Vector":[0.2792527423,-0.1230933328,-0.0398612343,-0.0277697588,-0.0667013326,0.0227865856,-0.0298836444,-0.0031262164,-0.0666311631,-0.0605205856,-0.0505572934,0.0341661454,-0.0484265765,-0.0277690338,0.0225729921,-0.0098031653,-0.0862976887,0.0159574053,-0.0761411997,-0.0229466251,0.050877673,-0.0639582527,-0.0038692669,-0.0352743142,-0.0507524777,-0.0346703672,-0.015530612,-0.0745165363,-0.0391143686,-0.0464796755,-0.0017401447,-0.030503759,0.0201820142,-0.0002616445,-0.0625614534,-0.0258358185,0.0202107877,-0.0504427733,-0.0366213694,0.0351960331,0.0044467973,0.0128317722,0.0585300624,-0.0457733779,0.0039975084,0.0023646837,-0.0051002345,0.0075397008,-0.0363161134,0.0318059447,0.0031620219,0.0548777147,0.0686374307,-0.0148183061,0.0623642161,0.0025255757,0.0388158712,-0.0835226324,0.0394125758,-0.0383597935,0.0005559105,-0.0059649073,-0.0092668887,-0.0482302723,-0.0247151706,0.0090749584,-0.0008149194,-0.0214415959,-0.0074101446,0.0267361682,-0.0138202056,-0.0462553641,-0.002989127,-0.0043022805,-0.0592674738,-0.0144601102,0.0149312537,-0.0107685388,-0.0324163964,0.0773301229,-0.0116454574,0.0138171779,-0.0384027001,-0.0516902698,0.0857875602,0.0042657653,0.0281348144,-0.0631101638,-0.0371110658,-0.0538840178,0.0515922479,0.0037424443,0.0393571053,0.0334738993,0.0225031934,-0.0049876591,0.0061149806,0.009596886,0.0110766309,-0.0308813936,0.002201539,-0.0049397947,-0.0248887876,-0.0232718188,-0.0176245423,-0.0621662806,-0.001729656,0.0288777413,0.0003563053,-0.0267791918,-0.0192131952,0.001603159,0.0512160775,0.0568457017,0.0284111965,0.0060087961]},"291":{"Abstract":"In traditional design, shapes are first conceived, and then fabricated. While this decoupling simplifies the design process, it can result in unwanted material wastage, especially where off-cut pieces are hard to reuse. In absence of explicit feedback on material usage, the designer remains helpless to effectively adapt the design - even when design variabilities exist. We investigate waste minimizing furniture design wherein based on the current design, the user is presented with design variations that result in less wastage of materials. Technically, we dynamically analyze material space layout to determine which parts to change and how , while maintaining original design intent specified in the form of design constraints. We evaluate the approach on various design scenarios, and demonstrate effective material usage that is difficult, if not impossible, to achieve without computational support.","Authors":"B. Koo; J. Hergel; S. Lefebvre; N. J. Mitra","DOI":"10.1109\/TVCG.2016.2633519","Keywords":null,"Title":"Towards Zero-Waste Furniture Design","Keywords_Processed":"","Keyword_Vector":[0.0966472072,-0.0322627923,-0.0322915889,-0.1015225188,0.0690132123,-0.0596515937,-0.1141319708,0.0376617385,-0.0544380251,0.100945506,0.0199989026,0.2165556965,-0.0258300605,0.0563872409,-0.0118654343,-0.0052510349,-0.0553145577,0.0443146054,0.1537508917,-0.1469403554,0.1007644224,0.0197639326,0.2972928184,0.0717624775,-0.0249523433,0.0453530738,0.0340769312,-0.0631779874,-0.0552934401,-0.0391914544,0.042946503,0.0147416,-0.0328185756,0.0303190621,0.0076832325,-0.0276853499,-0.0507747734,-0.0523317707,0.0282090736,0.0306636942,-0.0476632707,0.0061381717,0.0157480882,-0.0946509379,-0.0190483815,-0.0385853834,0.0682835789,0.0775258205,-0.0208366493,0.0302389488,-0.0016439637,0.0802581574],"Abstract_Vector":[0.1551159219,-0.0117641838,0.0120419937,-0.0140474502,-0.0035382129,0.0497329831,-0.0090648843,0.0552740412,0.0841003008,-0.0044056049,-0.0750473249,0.024323679,0.0143421247,-0.0148883746,0.0733695361,-0.016981154,0.0159473125,0.0327638438,0.0457354401,0.0199553833,0.0134370265,-0.0221302904,-0.020763312,0.0020528461,0.0012143848,0.0063739539,0.0169582787,0.0234777983,-0.0129800852,-0.0354792567,0.0024791278,0.001595404,-0.0271022848,-0.0416761272,0.0026420404,0.0043453605,-0.0545188646,-0.0005955531,-0.01205006,-0.0120474204,0.0024255514,0.0185448324,0.0302884491,-0.0302675809,-0.0262501092,-0.003090452,-0.0266835634,-0.011299835,-0.0154758288,0.0252122753,0.0099611398,-0.0176269134,0.0081687438,0.0008536538,0.0332994506,-0.015168843,-0.0107666246,-0.003331883,-0.0070888138,-0.0120030131,-0.0165134326,0.0125074363,-0.026604693,-0.0268257046,-0.0176857331,0.0167387557,0.0003877527,-0.0412840428,-0.0376500357,0.0117199551,0.0278608088,-0.0440483791,-0.0003440233,-0.0116946474,-0.0289973866,-0.0088316039,0.0213181178,0.0094007403,0.0330174695,-0.043143622,0.0106444888,0.025784727,0.0202887981,-0.0319972345,-0.0269923291,0.0237081704,0.0168941496,-0.0179473714,0.0029466732,0.0058040666,-0.0310437121,-0.0108299744,-0.0030727549,-0.0090659227,0.0192416654,0.0005259529,0.0013432891,-0.0100923688,0.0015441072,-0.0088740465,-0.0121896085,-0.0188642111,-0.0126998146,-0.0198602575,0.0353559387,0.0188168039,-0.0056491179,0.0404090148,-0.002987599,0.0143527685,-0.0006556764,0.0311010522,0.0082830009,-0.0118618304,0.0097565638,-0.0143015807]},"292":{"Abstract":"Although there has been a great deal of interest in analyzing customer opinions and breaking news in microblogs, progress has been hampered by the lack of an effective mechanism to discover and retrieve data of interest from microblogs. To address this problem, we have developed an uncertainty-aware visual analytics approach to retrieve salient posts, users, and hashtags. We extend an existing ranking technique to compute a multifaceted retrieval result: the mutual reinforcement rank of a graph node, the uncertainty of each rank, and the propagation of uncertainty among different graph nodes. To illustrate the three facets, we have also designed a composite visualization with three visual components: a graph visualization, an uncertainty glyph, and a flow map. The graph visualization with glyphs, the flow map, and the uncertainty analysis together enable analysts to effectively find the most uncertain results and interactively refine them. We have applied our approach to several Twitter datasets. Qualitative evaluation and two real-world case studies demonstrate the promise of our approach for retrieving high-quality microblog data.","Authors":"M. Liu; S. Liu; X. Zhu; Q. Liao; F. Wei; S. Pan","DOI":"10.1109\/TVCG.2015.2467554","Keywords":"microblog data;mutual reinforcement model;uncertainty modeling;uncertainty visualization;uncertainty propagation;microblog data;mutual reinforcement model;uncertainty modeling;uncertainty visualization;uncertainty propagation","Title":"An Uncertainty-Aware Approach for Exploratory Microblog Retrieval","Keywords_Processed":"uncertainty propagation;uncertainty modeling;mutual reinforcement model;uncertainty visualization;microblog datum","Keyword_Vector":[0.0771265223,-0.0014292146,0.0582151176,0.0873349263,0.0379082515,-0.1333636523,0.0136007723,-0.0763422854,-0.0501166805,0.1079189226,-0.0094707746,0.0660805845,0.0302743154,0.0130541029,-0.1140965456,-0.0739925333,-0.0882613909,-0.0297287727,0.0466311804,0.0841718504,-0.0995834733,-0.0527428164,-0.1005905916,0.1393751647,0.0347362135,-0.0861449273,-0.1280825814,0.0158925839,0.0159307306,-0.0230295436,-0.0964204662,-0.0642982331,0.0545052067,-0.0219420354,0.1360005879,-0.0037385967,-0.1012276334,-0.0238347264,0.1224660336,0.0475676446,-0.0577689965,-0.0797230997,0.0104039581,-0.0495382684,-0.0085537724,-0.0206114084,0.0909051082,-0.0241304931,0.0363957074,-0.027709194,0.0099629168,-0.0284173069],"Abstract_Vector":[0.1857818309,-0.098838863,0.0074726012,0.0053720202,-0.0916214404,-0.0048284616,0.0183637493,0.0223353565,-0.0699084512,-0.011733902,0.0067393714,-0.0002656172,-0.0651947275,-0.0094983566,-0.0127047382,0.0225141593,0.0358198061,0.0235565513,-0.0189091924,-0.0638655162,0.0519811158,0.0345419488,0.0260835592,0.0319639645,-0.0304161465,0.0755033206,0.0568914926,-0.003953317,-0.0252526748,-0.0482230994,0.0000971186,-0.0439831855,-0.0186787971,0.0097532117,-0.0395243585,-0.0466505063,-0.0327995697,-0.0255013671,-0.0070376487,-0.0500806583,-0.0453922237,0.0196821776,0.0111272392,-0.0710846127,0.0110207212,0.0268617235,0.0470276132,-0.0200012504,-0.0029642895,-0.021962723,0.0164157689,0.0296606108,0.0144677786,0.0737001575,0.0306856709,-0.0464821152,0.0115251251,0.0134862927,-0.0138225328,0.0109871641,0.0141438666,-0.0143960529,-0.041070339,-0.0328071105,-0.0800773537,-0.0213740971,0.0055825114,0.0326306605,-0.0500103505,0.0381352531,-0.0327270184,-0.0255373449,-0.0165711417,0.0305123312,-0.0361370307,0.0623071045,-0.0590774207,0.0168707042,-0.0644988794,-0.000167799,0.0104500173,0.00767159,-0.0301948609,-0.0150228507,0.0055039408,-0.0373057932,0.008512038,0.0102704262,0.0076681849,0.0035684275,0.0182625988,-0.0096430723,-0.0455684294,-0.0085670054,0.0077089153,0.07406843,-0.0044524694,-0.070493777,-0.0386192597,0.0182407792,-0.045072182,-0.0434561166,-0.017333194,0.0185816185,-0.0233074068,-0.0229637468,-0.0285967038,-0.0620718913,0.0965915782,-0.0133273305,-0.0075215859,-0.0182201629,-0.0372041109,0.0647886393,-0.0497939633,-0.0429597835]},"293":{"Abstract":"Among the many types of deep models, deep generative models (DGMs) provide a solution to the important problem of unsupervised and semi-supervised learning. However, training DGMs requires more skill, experience, and know-how because their training is more complex than other types of deep models such as convolutional neural networks (CNNs). We develop a visual analytics approach for better understanding and diagnosing the training process of a DGM. To help experts understand the overall training process, we first extract a large amount of time series data that represents training dynamics (e.g., activation changes over time). A blue-noise polyline sampling scheme is then introduced to select time series samples, which can both preserve outliers and reduce visual clutter. To further investigate the root cause of a failed training process, we propose a credit assignment algorithm that indicates how other neurons contribute to the output of the neuron causing the training failure. Two case studies are conducted with machine learning experts to demonstrate how our approach helps understand and diagnose the training processes of DGMs. We also show how our approach can be directly used to analyze other types of deep models, such as CNNs.","Authors":"M. Liu; J. Shi; K. Cao; J. Zhu; S. Liu","DOI":"10.1109\/TVCG.2017.2744938","Keywords":"deep learning;deep generative models;blue noise sampling;credit assignment","Title":"Analyzing the Training Processes of Deep Generative Models","Keywords_Processed":"credit assignment;deep generative model;deep learning;blue noise sampling","Keyword_Vector":[0.0700290401,0.1110737654,-0.0386930954,-0.0017546912,-0.0114572416,-0.0111512185,-0.0058698447,-0.0280554217,-0.0371738356,0.0024885486,0.0263467701,0.00953803,0.0005940015,0.0031092961,0.0390260677,0.0221364021,-0.0147049832,-0.0257197049,0.0552743915,-0.0488998101,0.0120786097,0.0487294948,0.0119002092,-0.0241275058,-0.0264076979,-0.0249067834,0.0000846433,-0.0333895686,0.0444405254,-0.0095121673,-0.0636925049,-0.0466821087,0.0117450212,0.0328491183,-0.0031747764,-0.039177009,0.0115607202,0.0263035547,-0.004072409,-0.0039774614,-0.0193602853,0.0614613085,0.0193318812,-0.0265450721,-0.0161878839,0.0104467664,-0.0258088179,0.0121380589,-0.0436089588,0.0247881192,0.0329151129,-0.0142947173],"Abstract_Vector":[0.2015260055,0.0570908863,-0.0118236992,-0.0045839907,-0.0957545695,0.0883727613,-0.0138351709,-0.0065245971,0.0473635518,-0.0091058222,-0.0090622868,0.0254882071,-0.0610659349,0.0066870341,-0.0140829563,-0.014197533,0.068181968,0.0386015577,-0.0271597635,-0.1034885261,0.0182930515,0.0353885478,0.1035483327,-0.0322290333,-0.0223324671,-0.0188600177,0.0870881924,-0.0223536434,0.0186762907,-0.0021570247,-0.0063789253,0.0400597104,-0.0243696811,-0.0036292085,-0.0542808139,-0.0102452565,0.0469519072,-0.0026289308,-0.0159413634,0.0253810194,0.0577538389,0.0551731085,-0.0024413283,0.0452267179,-0.0399222818,-0.0058426972,0.0213103829,0.0857011207,-0.0065923176,-0.0038187927,0.0123252033,0.0316009051,0.0170862432,0.0466320539,-0.010763855,0.0214413763,-0.0067810413,0.0112734109,0.012563161,-0.0156606253,0.0107835668,0.000800244,-0.0394277095,-0.0079597011,0.0021041804,0.0411710496,-0.0435579238,0.0012670324,-0.0157498637,0.0443983376,0.0241753943,-0.0027424528,-0.0248962544,0.0070605581,0.0010655152,0.0282687159,0.0117716923,-0.0046958497,0.0130622445,0.0128084097,0.0349105407,0.0034321054,0.0227544283,-0.0015301118,0.0039091046,0.0523021509,0.0120839038,0.0084088393,-0.0104905969,0.0141459851,0.0083633625,-0.0126420006,0.0198271311,0.0564123422,0.0227801911,0.0065191113,0.0235766505,-0.0010584434,0.0121506478,0.0281513003,0.0537640647,0.012676147,-0.0296194836,-0.0159745151,-0.040755195,-0.0076968127,-0.0040444836,-0.0134586393,0.000161733,0.0019099454,-0.0150363521,-0.0275054058,0.0176159891,-0.0126395986,0.0121329071,-0.0393243578]},"294":{"Abstract":"This system paper presents the Topology ToolKit (TTK), a software platform designed for the topological analysis of scalar data in scientific visualization. While topological data analysis has gained in popularity over the last two decades, it has not yet been widely adopted as a standard data analysis tool for end users or developers. TTK aims at addressing this problem by providing a unified, generic, efficient, and robust implementation of key algorithms for the topological analysis of scalar data, including: critical points, integral lines, persistence diagrams, persistence curves, merge trees, contour trees, Morse-Smale complexes, fiber surfaces, continuous scatterplots, Jacobi sets, Reeb spaces, and more. TTK is easily accessible to end users due to a tight integration with ParaView. It is also easily accessible to developers through a variety of bindings (Python, VTK\/C++) for fast prototyping or through direct, dependency-free, C++, to ease integration into pre-existing complex systems. While developing TTK, we faced several algorithmic and software engineering challenges, which we document in this paper. In particular, we present an algorithm for the construction of a discrete gradient that complies to the critical points extracted in the piecewise-linear setting. This algorithm guarantees a combinatorial consistency across the topological abstractions supported by TTK, and importantly, a unified implementation of topological data simplification for multi-scale exploration and analysis. We also present a cached triangulation data structure, that supports time efficient and generic traversals, which self-adjusts its memory usage on demand for input simplicial meshes and which implicitly emulates a triangulation for regular grids with no memory overhead. Finally, we describe an original software architecture, which guarantees memory efficient and direct accesses to TTK features, while still allowing for researchers powerful and easy bindings and extensions. TTK is open source (BSD license) and its code. online documentation and video tutorials are available on TTK's website [108].","Authors":"J. Tierny; G. Favelier; J. A. Levine; C. Gueunet; M. Michaux","DOI":"10.1109\/TVCG.2017.2743938","Keywords":"Topological data analysis;scalar data;data segmentation;feature extraction;bivariate data;uncertain data","Title":"The Topology ToolKit","Keywords_Processed":"bivariate datum;feature extraction;datum segmentation;scalar datum;uncertain datum;topological datum analysis","Keyword_Vector":[0.0283788552,-0.0060315945,-0.0114399674,0.0079338622,0.0712440172,-0.0324686821,-0.0538196469,0.0549310569,-0.072257176,-0.0239790698,0.0143746915,0.0659350689,-0.0126162216,0.0156636668,0.0665805512,0.1302116222,-0.1335942004,-0.1264705018,0.3081694625,-0.0550274791,0.1018541784,-0.1963713465,0.2368803875,0.063973244,0.1493983556,0.2486368883,-0.1706592373,-0.1389121125,-0.0686967107,-0.0469990342,0.1344616055,0.0392365705,-0.0952107136,-0.048957553,-0.0560649429,-0.1183816372,-0.0394385216,-0.0266791548,0.0154280107,0.1289285267,0.0134597966,-0.0231203952,-0.0022990988,-0.0886412789,-0.0089756718,-0.0161306109,0.0502272545,0.1148103261,0.0586471309,-0.0160777384,-0.0643192817,0.0134821759],"Abstract_Vector":[0.1578758549,-0.0142664327,-0.0280059244,-0.0074533135,-0.0205012293,0.0658069359,-0.0146602414,0.0299108532,0.1834421135,-0.0020302167,-0.0784367932,0.0243142374,-0.0497929725,0.0261820711,0.0791946041,-0.0064983374,0.0284601865,0.0818363564,0.0632855879,0.0534510471,0.0025194058,-0.046747835,-0.049984801,0.0028884958,-0.016867186,-0.0180720099,0.0431246373,0.0409053559,-0.0532776282,-0.048234781,-0.0055256657,-0.0694781895,0.0163390166,-0.0541137505,0.0162656587,0.0321321524,0.0116223828,0.0393622241,-0.03309197,-0.0225180078,0.0235346294,0.0195677007,0.0109590516,-0.0134182515,-0.0372437653,0.0402167058,-0.0615892264,-0.0604294538,0.0204117679,-0.0187163543,0.0468895458,-0.0195103619,0.0369625965,-0.0035144265,0.0266039126,-0.0239924107,-0.039110069,-0.0247679035,-0.0138844027,0.0253396704,-0.0152208631,0.0090155339,-0.0138232429,0.0190593134,0.0085900833,-0.0034634881,0.0308815029,-0.0158359172,0.0063636402,-0.004040941,-0.0069425959,-0.0486732513,0.0152680366,-0.0044814059,-0.0269978396,0.0251293271,-0.001575407,-0.0334890843,0.0428826295,-0.05613662,0.0004464877,0.0609855682,0.0185404284,-0.0282203189,-0.0195972343,0.0106030985,0.0150562015,-0.027059073,0.044759913,0.0450076695,-0.059593981,0.0045679448,-0.0131424172,0.0012044061,0.0364466129,-0.0185286607,-0.0087478609,-0.0078024012,0.0100535917,-0.0021558938,-0.0478711931,0.0020248088,0.0256477541,0.0027620607,0.0324442349,-0.0259741723,0.0022200922,0.0058123045,0.015982574,-0.0000861837,0.0017149038,-0.0037774467,-0.0094021135,-0.0039108645,-0.0155155734,-0.0227422464]},"295":{"Abstract":"In meteorology, cluster analysis is frequently used to determine representative trends in ensemble weather predictions in a selected spatio-temporal region, e.g., to reduce a set of ensemble members to simplify and improve their analysis. Identified clusters (i.e., groups of similar members), however, can be very sensitive to small changes of the selected region, so that clustering results can be misleading and bias subsequent analyses. In this article, we - a team of visualization scientists and meteorologists-deliver visual analytics solutions to analyze the sensitivity of clustering results with respect to changes of a selected region. We propose an interactive visual interface that enables simultaneous visualization of a) the variation in composition of identified clusters (i.e., their robustness), b) the variability in cluster membership for individual ensemble members, and c) the uncertainty in the spatial locations of identified trends. We demonstrate that our solution shows meteorologists how representative a clustering result is, and with respect to which changes in the selected region it becomes unstable. Furthermore, our solution helps to identify those ensemble members which stably belong to a given cluster and can thus be considered similar. In a real-world application case we show how our approach is used to analyze the clustering behavior of different regions in a forecast of \u201cTropical Cyclone Karl\u201d, guiding the user towards the cluster robustness information required for subsequent ensemble analysis.","Authors":"A. Kumpf; B. Tost; M. Baumgart; M. Riemer; R. Westermann; M. Rautenhaus","DOI":"10.1109\/TVCG.2017.2745178","Keywords":"Uncertainty visualization;ensemble visualization;clustering;meteorology","Title":"Visualizing Confidence in Cluster-Based Ensemble Weather Forecast Analyses","Keywords_Processed":"meteorology;ensemble visualization;clustering;uncertainty visualization","Keyword_Vector":[0.1681023326,-0.100385809,-0.1093483826,0.0312065501,-0.0644717116,-0.0768974385,0.0316885116,0.017477218,-0.0289838135,0.0339363241,-0.0438670757,0.0815058916,-0.0071223343,0.0902472608,-0.0434258585,0.0017822793,0.0971484597,-0.0177627745,-0.0099153906,0.0082719969,-0.0368080974,-0.072675592,-0.0761310013,0.0161277452,0.0383973778,0.0375719279,0.0023399223,-0.0404973692,0.0401638477,0.0149746608,0.0485261385,-0.0209473857,-0.0051025122,-0.0141691784,-0.0404402495,0.0642463305,0.0820174957,0.025728796,-0.0172252093,0.0145053359,-0.0316147594,0.0107961838,0.0167771908,0.0151574167,-0.0522298549,0.0212440299,0.0266164777,-0.0284612646,-0.0199986393,-0.0533647717,-0.0268061397,0.1151794385],"Abstract_Vector":[0.2124115774,-0.0857082312,-0.0164654242,-0.0438766398,-0.0096585113,-0.0376490537,-0.0288363872,-0.0329851064,0.0107603038,-0.0183697104,-0.0043208052,0.0000140517,-0.0407248599,-0.028777816,-0.034778636,0.0948370241,0.0318930717,0.0510554344,-0.0024072089,-0.0379850675,0.0182175398,-0.034664871,0.0339208435,0.0199128562,0.0730197444,-0.0923443211,-0.0550340378,0.0484496941,0.04835316,0.0704380759,0.0266709603,0.0336561771,0.0115904151,0.0460468357,0.0750587393,-0.093147466,-0.0354619861,-0.0065580669,0.0008053322,-0.0545178827,-0.0129535351,0.0709576889,0.027596436,-0.0424199829,0.0036230042,-0.0715987707,-0.0191462575,-0.00313705,-0.0312509119,-0.0276898547,0.0386994223,0.0368854943,0.0197600376,0.0759351841,-0.0003074709,0.0247426014,-0.0265998113,-0.0777394713,0.0813295253,0.0182729493,0.013117951,-0.0236300569,-0.0137996713,-0.028577841,0.0448973144,0.0148853908,0.0364198034,-0.0139825153,0.068417759,-0.0155321207,-0.0463325732,-0.0135321555,-0.0061115788,0.0188566828,0.0516144362,-0.0502257965,0.0159079695,0.0010157452,-0.0340032133,0.0103489948,0.0286228865,0.0090717126,-0.0252422526,0.0286660338,0.0468894479,-0.0456865102,0.008865134,0.0349135439,0.0257665254,0.0357805737,-0.0289320934,0.0000225071,0.1106231035,-0.0796343423,0.0741942214,0.0273504032,-0.0215566311,0.068821741,-0.0002850948,0.0905106344,-0.0305988897,-0.0016284414,-0.0428762899,-0.0279473563,-0.0106513131,-0.005314996,-0.0224012471,0.0254714373,0.0298701271,-0.0525134793,-0.0130329166,0.010145263,-0.0045088973,0.0074901002,-0.0564774809,0.041255293]},"296":{"Abstract":"Whether and how does the structure of family trees differ by ancestral traits over generations? This is a fundamental question regarding the structural heterogeneity of family trees for the multi-generational transmission research. However, previous work mostly focuses on parent-child scenarios due to the lack of proper tools to handle the complexity of extending the research to multi-generational processes. Through an iterative design study with social scientists and historians, we develop TreeEvo that assists users to generate and test empirical hypotheses for multi-generational research. TreeEvo summarizes and organizes family trees by structural features in a dynamic manner based on a traditional Sankey diagram. A pixel-based technique is further proposed to compactly encode trees with complex structures in each Sankey Node. Detailed information of trees is accessible through a space-efficient visualization with semantic zooming. Moreover, TreeEvo embeds Multinomial Logit Model (MLM) to examine statistical associations between tree structure and ancestral traits. We demonstrate the effectiveness and usefulness of TreeEvo through an in-depth case-study with domain experts using a real-world dataset (containing 54,128 family trees of 126,196 individuals).","Authors":"S. Fu; H. Dong; W. Cui; J. Zhao; H. Qu","DOI":"10.1109\/TVCG.2017.2744080","Keywords":"Quantitative social science;Design study;Multiple tree visualization;Sankey diagram","Title":"How Do Ancestral Traits Shape Family Trees Over Generations?","Keywords_Processed":"sankey diagram;design study;quantitative social science;multiple tree visualization","Keyword_Vector":[0.1401509144,-0.0025913849,0.085167098,0.0657473702,-0.0596611199,0.0534279145,-0.0436346796,-0.0614900564,0.0025650112,-0.1547600158,-0.0073342251,0.0417398814,0.1546796221,-0.0376929697,0.0035962603,-0.0659098139,0.0482615245,-0.0269940212,-0.050001656,-0.0215890531,0.0626638465,-0.0682762429,0.0517202409,-0.0252541663,-0.0191231017,0.0318655568,-0.0301710032,0.065102748,0.0231496461,-0.0091696222,-0.0064807173,-0.1205269048,0.0136539057,-0.0244103939,0.0068795685,0.0571519959,-0.0200616311,-0.0287196857,0.0167869793,-0.0020271425,0.0349275435,-0.0048293064,-0.0286303666,-0.013511644,-0.0168677212,0.0173490015,-0.0039941851,-0.0002071828,0.0069549202,-0.0342321106,-0.1028448766,-0.0790970494],"Abstract_Vector":[0.145246408,-0.0165424491,0.0682738634,-0.0001977398,0.045344868,0.051438873,0.1112775837,-0.0524369437,0.0085112172,0.0209553837,0.0275839007,-0.0811066685,-0.0530638123,0.0857697225,0.0737077372,0.0009694605,-0.0071325391,-0.0110216499,-0.0025152212,-0.0005639735,0.006570025,0.0275126895,0.0424372526,0.0232753883,-0.029693588,-0.0469390735,0.0122070125,-0.0182606789,-0.023722653,0.0280060822,0.0295183413,0.0697734774,-0.0210577736,0.0103953175,0.051975915,-0.0143825325,0.0091035206,-0.0006884313,-0.0055905719,-0.0146644733,0.031383302,0.0149860388,0.024242887,0.0037049344,0.0401802368,0.0300464498,0.003783651,0.0122810363,0.0062865619,0.0411310026,-0.0146418448,0.0081200315,-0.0160720648,0.0087394261,0.0257719283,0.0165241906,0.0518517429,0.0343855285,0.02527555,0.0164558739,-0.0407251261,-0.0097018219,0.0250113084,-0.029882403,-0.0089679612,-0.0199500736,-0.005675619,-0.0206956699,-0.0008188438,0.0405440663,-0.0002146414,0.006466056,-0.000702996,0.0219314135,-0.0337081037,-0.050396576,0.0033484207,-0.0012456801,0.0202786192,0.0027439286,-0.0052814943,0.0019290635,-0.0425734652,0.0029830844,-0.0139607594,0.0224778319,-0.0403817507,-0.0185262897,-0.0026106926,0.0074095831,0.0022310829,0.0345437996,0.0335072495,-0.0088928628,0.0140540762,-0.0163790304,0.0087356893,-0.0259657558,-0.0291202284,-0.0086783005,-0.0233061224,-0.0144763548,0.0361657714,-0.0079717776,0.0272029534,0.0030240539,-0.0107538032,0.0111563125,-0.0427338876,-0.0150809938,-0.0032582384,0.0000245732,-0.0102171488,-0.0052147982,0.0073221815,-0.0417298295]},"297":{"Abstract":"Storyline visualization techniques have progressed significantly to generate illustrations of complex stories automatically. However, the visual layouts of storylines are not enhanced accordingly despite the improvement in the performance and extension of its application area. Existing methods attempt to achieve several shared optimization goals, such as reducing empty space and minimizing line crossings and wiggles. However, these goals do not always produce optimal results when compared to hand-drawn storylines. We conducted a preliminary study to learn how users translate a narrative into a hand-drawn storyline and check whether the visual elements in hand-drawn illustrations can be mapped back to appropriate narrative contexts. We also compared the hand-drawn storylines with storylines generated by the state-of-the-art methods and found they have significant differences. Our findings led to a design space that summarizes (1) how artists utilize narrative elements and (2) the sequence of actions artists follow to portray expressive and attractive storylines. We developed iStoryline, an authoring tool for integrating high-level user interactions into optimization algorithms and achieving a balance between hand-drawn storylines and automatic layouts. iStoryline allows users to create novel storyline visualizations easily according to their preferences by modifying the automatically generated layouts. The effectiveness and usability of iStoryline are studied with qualitative evaluations.","Authors":"T. Tang; S. Rubab; J. Lai; W. Cui; L. Yu; Y. Wu","DOI":"10.1109\/TVCG.2018.2864899","Keywords":"Hand-drawn illustrations;automatic layout;design space;interactions;optimization","Title":"iStoryline: Effective Convergence to Hand-drawn Storylines","Keywords_Processed":"design space;automatic layout;interaction;optimization;hand draw illustration","Keyword_Vector":[0.0275838411,-0.0182254786,-0.0240426874,0.0194579649,-0.0021592491,-0.0124509636,-0.000511505,0.0017422831,-0.0005209964,-0.0112081329,0.0077675639,-0.016489001,0.0069072392,0.049180343,0.0000848507,0.0017328737,0.0221749408,0.0072305337,0.0191865418,-0.0415287475,0.0127065982,0.0301358725,-0.0410212945,-0.029889508,0.0253849561,0.0172710316,0.0054315247,-0.012059412,0.0283456235,0.0081875062,-0.0024928675,0.0090852836,0.0036690822,0.0516304352,-0.0145288627,0.0245011913,0.02571807,0.0276581599,-0.0067404233,0.0343276408,0.0167830526,0.0212293689,0.0167660266,0.038163465,-0.0301619076,-0.0078649583,0.0224913557,0.0216453766,0.0089341498,0.029359059,0.0152124031,-0.0177360128],"Abstract_Vector":[0.2330028863,-0.1702754798,-0.0002407967,0.0151510486,-0.1200487495,0.048103344,0.0440178452,0.0448729459,-0.1127095212,0.0209196401,-0.0085381719,0.0107528459,-0.0571199151,-0.0000339886,0.0221262025,0.0389065303,0.0094335195,-0.0002594716,-0.0024354877,-0.0496524212,0.045368984,0.0026288497,-0.0063929034,0.0312511835,-0.039972312,0.0151033764,0.025994312,-0.0030761844,-0.0181594073,-0.0564869268,-0.0203587112,0.0146564063,-0.0389020434,0.0104169426,-0.0156838533,-0.0024190731,-0.0631880119,-0.0706245406,-0.0094171953,-0.0175247664,-0.0111959062,0.0056053201,-0.0347201293,-0.0107717756,-0.0150660549,0.0392479,-0.0156561789,-0.0275187048,0.0315651364,-0.0012130162,-0.0085205914,0.0191520402,-0.0220236619,0.0169145644,0.0443800056,-0.0056283878,0.0426161729,-0.0042713578,0.0135395699,-0.0155165615,0.0196713672,0.0098776985,-0.0088705872,-0.0312319107,-0.0533940409,0.0175971649,0.0397716551,0.0047762944,-0.0326804516,0.0056483244,-0.0256451551,-0.0346301198,0.0337798998,-0.0091525778,0.001096182,0.0142368235,-0.0008485533,0.0100931661,-0.0005970654,0.0297247167,0.0367859373,0.0282248643,-0.021550557,-0.0103879947,0.0696661079,-0.026235263,0.0220972394,0.0389050554,0.0298736586,-0.0125240947,-0.0035054674,-0.0011222834,-0.0101425845,-0.0434909175,-0.0107195061,-0.0352186026,0.0558181753,-0.0100456018,-0.0063538678,-0.0147770939,0.0216143253,0.0176201159,-0.0179468843,-0.0223620814,0.0022206817,-0.0103326331,-0.0273921856,0.0236982136,0.0372889913,0.011630123,0.0041933028,0.0345539475,-0.0323714265,0.0147077982,-0.0108182953,0.0095242243]},"298":{"Abstract":"We present a method for outdoor markerless motion capture with sparse handheld video cameras. In the simplest setting, it only involves two mobile phone cameras following the character. This setup can maximize the flexibilities of data capture and broaden the applications of motion capture. To solve the character pose under such challenge settings, we exploit the generative motion capture methods and propose a novel model-view consistency that considers both foreground and background in the tracking stage. The background is modeled as a deformable 2D grid, which allows us to compute the background-view consistency for sparse moving cameras. The 3D character pose is tracked with a global-local optimization through minimizing our consistency cost. A novel L1 motion regularizer is also proposed in the optimization to constrain the solution pose space. The whole process of the proposed method is simple as frame by frame video segmentation is not required. Our method outperforms several alternative methods on various examples demonstrated in the paper.","Authors":"Y. Wang; Y. Liu; X. Tong; Q. Dai; P. Tan","DOI":"10.1109\/TVCG.2017.2693151","Keywords":"Markerless motion capture;handheld video cameras;model-view consistency","Title":"Outdoor Markerless Motion Capture with Sparse Handheld Video Cameras","Keywords_Processed":"handheld video camera;markerless motion capture;model view consistency","Keyword_Vector":[0.1823326223,-0.1054753598,-0.0686562712,0.0408918208,-0.0546954234,-0.0476666424,0.028813596,0.0129811383,0.0421112717,-0.06748159,0.031097416,0.0275960347,-0.0196478293,-0.0300961022,-0.0344295924,0.0081476783,-0.0597982753,0.028963962,-0.0014673562,0.0495315007,0.0041294046,0.0909007902,-0.0284363873,-0.000287463,-0.0491702485,0.0689661433,0.0179464123,0.0176205815,0.0375175731,-0.0037057245,0.059425063,0.0125282176,-0.0902470229,0.0041277829,0.080718674,-0.0771447448,-0.0388834036,0.1234533195,0.0334963354,0.0586785462,0.0868713606,-0.0106790497,0.0567420033,0.1392756454,0.0460180101,0.0628775809,0.0057718194,0.0577986506,-0.0485573514,-0.007207836,0.0387277093,0.010462251],"Abstract_Vector":[0.187394461,-0.070398357,-0.0088870802,-0.0352451509,-0.0821521959,0.0673699804,0.0573728276,0.0346277291,-0.0286968312,-0.0033551846,-0.0407264963,0.0253070459,-0.0064503903,-0.0362038667,0.0003714082,-0.0246605583,-0.0329568032,0.0672625294,-0.0375404886,-0.089478101,0.0392695668,-0.0996815722,-0.0061178348,-0.045162056,0.0304692918,0.0296083597,0.0116974927,0.0086408949,-0.0013849953,-0.0803471272,0.0444532385,0.0067195797,0.0537992981,0.0340675202,-0.0279596041,0.0027409897,0.0366200094,0.0622143647,-0.0232046226,0.0425296633,0.1339404896,0.1277663669,0.0245340632,0.0133087583,-0.0335441104,-0.001367118,-0.074981875,0.0302220734,-0.0653353092,-0.0226521947,-0.0032308253,0.1334642804,-0.0130018368,0.0537374044,0.0018805435,0.004827485,-0.0715847904,-0.006426857,-0.0878797438,0.0062464494,-0.1015149448,-0.0196651088,0.0216870326,-0.0415523427,0.0252922041,0.0507477117,0.0220171437,0.0502669865,0.0165856049,-0.0250037403,0.0274370251,0.0213625509,-0.0077964277,0.0513460998,-0.0089494208,0.0224100268,0.0011217898,0.0392769696,0.0055548426,0.0722618311,0.0058023908,0.0498214072,0.0431149183,0.0250643693,0.0026604207,0.0431716111,-0.0189523911,-0.0145083571,0.0226571552,-0.0033751955,0.0313103898,0.0674786156,-0.0099527409,0.0187378783,-0.0066817784,-0.0143003945,0.068606787,-0.006192094,-0.0463424772,0.0162409483,-0.0165952146,-0.0211191499,-0.0102374546,0.0293402391,-0.003563986,0.026692701,0.0295306128,-0.0562120636,0.013842828,0.0229109669,-0.0349093247,-0.0363490335,0.0087760136,-0.0031944143,0.0127127307,0.0210893808]},"299":{"Abstract":"Analyzing depressions plays an important role in meteorology, especially in the study of cyclones. In particular, the study of the temporal evolution of cyclones requires a robust depression tracking framework. To cope with this demand we propose a pipeline for the exploration of cyclones and their temporal evolution. This entails a generic framework for their identification and tracking. The fact that depressions and cyclones are not well-defined objects and their shape and size characteristics change over time makes this task especially challenging. Our method combines the robustness of topological approaches and the detailed tracking information from optical flow analysis. At first cyclones are identified within each time step based on well-established topological concepts. Then candidate tracks are computed from an optical flow field. These tracks are clustered within a moving time window to distill dominant coherent cyclone movements, which are then forwarded to a final tracking step. In contrast to previous methods our method requires only a few intuitive parameters. An integration into an exploratory framework helps in the study of cyclone movement by identifying smooth, representative tracks. Multiple case studies demonstrate the effectiveness of the method in tracking cyclones, both in the northern and southern hemisphere.","Authors":"A. A. Valsangkar; J. M. Monteiro; V. Narayanan; I. Hotz; V. Natarajan","DOI":"10.1109\/TVCG.2018.2810068","Keywords":"Cyclone;scalar field;time-varying data;track graph;spatio-temporal clustering;tracking","Title":"An Exploratory Framework for Cyclone Identification and Tracking","Keywords_Processed":"scalar field;tracking;spatio temporal clustering;time vary datum;track graph;cyclone","Keyword_Vector":[0.0446299821,0.0293453415,0.056976366,0.0644401542,0.0296476529,-0.0384108238,-0.0066042367,-0.0462113156,0.0115684545,-0.0301089987,-0.0126088318,0.0081438663,-0.0431304605,-0.0129742351,-0.0475737863,0.0104447011,0.1086942474,-0.0755065044,0.0181434946,-0.0210965317,0.0396406623,0.0207869246,0.0126760427,0.020146299,-0.018159181,-0.0947644829,-0.011283392,0.0597704347,-0.0428251306,0.0315029821,0.0621654432,0.0555437498,0.0013805254,0.0407912572,-0.0644375788,-0.0646658152,0.0425981493,-0.0054171647,-0.0098731996,0.0106397706,-0.0018834995,-0.0042874855,0.0015286866,-0.0328233409,0.0499641968,-0.0029342498,0.0002352214,0.0144098675,0.0187698961,0.0028243321,0.0058306245,0.0155820167],"Abstract_Vector":[0.2465959514,0.0801530054,0.0106438746,0.0461747206,-0.056865491,-0.0024200058,-0.026907704,-0.0095511475,-0.0085522807,0.0176311027,-0.0276365649,-0.017283009,-0.0556960315,0.0655687406,-0.0368283051,0.0117999125,0.0761287165,0.0421472496,-0.0117583715,-0.0311304531,-0.0064789921,0.0059328381,-0.0104866566,-0.0402816826,-0.0758698545,0.0584555239,-0.0217056786,0.0016650213,-0.013777325,-0.0439835027,0.031967281,0.0460865265,-0.0396702126,0.0159693378,-0.0286467127,-0.0297596053,0.0678494529,0.0717093882,0.0030587047,0.0392526879,0.0229857814,-0.0047947525,0.0435249802,0.0210850016,-0.0044052921,0.0196294796,0.0541817277,0.0388265741,0.0320828341,-0.0197580227,0.0179596775,-0.0183260176,-0.0647715947,0.0167391381,0.0015399783,0.0062659425,0.0053992169,-0.0163643056,-0.0198551368,-0.0349171416,0.0276115527,0.0424147176,-0.0133895293,0.0040267956,0.0115456156,0.0004054815,-0.0200363795,0.0448667252,-0.0126973225,-0.0255805948,-0.0292477564,0.0092350298,0.0119106467,-0.0251852752,0.001868658,-0.0421786727,-0.0040294074,0.0015942671,-0.0576586271,-0.0176460101,0.0135192005,0.0044992354,0.011155101,0.0136794767,0.0196123486,-0.0057234621,-0.0114540298,-0.0369045081,-0.0454078918,-0.008895339,-0.0107531887,-0.0173892853,-0.0494712907,0.0245177963,-0.0351535282,-0.0218533592,0.0053796428,-0.0322940013,-0.0011747225,0.0146407349,0.0704105728,-0.0006613764,-0.0083995799,-0.015231264,-0.0464460271,-0.0079374773,-0.0147037932,0.0305860428,-0.0197143146,-0.0021192197,-0.0000702141,-0.0070285263,0.004922507,0.0355360065,0.0315466149,-0.0159859284]},"3":{"Abstract":"Advances in high-throughput imaging allow researchers to collect three-dimensional images of whole organ microvascular networks. These extremely large images contain networks that are highly complex, time consuming to segment, and difficult to visualize. In this paper, we present a framework for segmenting and visualizing vascular networks from terabyte-sized three-dimensional images collected using high-throughput microscopy. While these images require terabytes of storage, the volume devoted to the fiber network is $\\approx 4$  percent of the total volume size. While the networks themselves are sparse, they are tremendously complex, interconnected, and vary widely in diameter. We describe a parallel GPU-based predictor-corrector method for tracing filaments that is robust to noise and sampling errors common in these data sets. We also propose a number of visualization techniques designed to convey the complex statistical descriptions of fibers across large tissue sections\u2014including commonly studied microvascular characteristics, such as orientation and volume.","Authors":"P. A. Govyadinov; T. Womack; J. L. Eriksen; G. Chen; D. Mayerich","DOI":"10.1109\/TVCG.2018.2818701","Keywords":"Microvessel;network tracking;glyph visualization;predictor-corrector;segmantation;spherical harmonics;superquadrics;KESM","Title":"Robust Tracing and Visualization of Heterogeneous Microvascular Networks","Keywords_Processed":"superquadric;glyph visualization;segmantation;predictor corrector;spherical harmonic;network tracking;Microvessel;KESM","Keyword_Vector":[0.1308435083,-0.0375863141,0.0193650493,-0.033124059,0.0053105405,0.0153061293,-0.0147499855,0.0303582235,-0.0152364571,-0.0447923766,-0.0143324008,0.0009088138,-0.0188188742,-0.0146480049,-0.0186320157,-0.0031087577,-0.018206437,0.0761381016,-0.0016740102,0.0103839965,0.0119229741,-0.0318415269,-0.0573792234,-0.0156845946,0.0065334667,0.0145632797,0.0238070626,-0.0334335741,-0.0157426834,-0.0138070682,0.011876752,0.0114968659,-0.0326469473,0.0458075397,-0.0271958449,-0.0453928659,0.02779847,-0.0408613466,0.0298443629,0.0090804704,-0.0405738154,-0.0352368627,0.140667715,-0.0130835285,0.0021298778,-0.0599297517,0.0388315021,0.0735563007,-0.0391671342,-0.0258027594,-0.0130627158,0.0183769931],"Abstract_Vector":[0.2704042866,-0.1038478968,-0.0570505825,0.0381248516,-0.0135891978,-0.0446185187,0.0745839611,0.0470490286,-0.0160801309,0.0403276357,-0.0217563845,0.0036534356,-0.0268363338,-0.0016931782,0.0115680109,0.0514934929,-0.0367782478,-0.062391513,-0.0253617948,0.0108906731,0.0178791361,-0.0159463473,-0.0281119258,0.0256252939,-0.0301465524,-0.0190971496,-0.0711728759,0.0049552445,0.0093234431,-0.0384736796,-0.0012381079,0.0102277932,-0.0326832709,-0.0166539619,-0.0071821555,-0.0139967829,0.0140979016,0.0188848621,0.0170805112,-0.0341967444,-0.0061779166,0.0630670132,-0.0348331312,-0.0281104981,-0.0473399679,0.0956711955,0.0134186176,0.0542293486,-0.0280130142,0.027874528,-0.0370966427,-0.0006415363,-0.0757345687,0.032654817,0.0011930256,0.0244955746,0.0605238738,-0.0157985785,0.0137315327,-0.0104277541,0.0163657195,0.0291097697,0.019764716,0.0039833482,-0.003516952,0.0595857606,-0.0330302189,0.0152940613,0.0253862422,-0.0055563096,0.0648226699,-0.048767895,-0.0238189576,0.0244037813,-0.0191811851,0.0838675951,-0.03226336,-0.0162562274,0.01334707,0.0144421802,-0.008837669,-0.0453111461,-0.020076369,0.0126039313,0.0552700455,-0.0124656477,0.0010540014,0.040467961,-0.0462916123,0.0449389837,0.0270822238,-0.0240768032,0.0730579093,0.0205148098,-0.0255863065,-0.0752262564,-0.0460675626,-0.0342012139,0.0020562439,-0.0456993074,0.0011887949,-0.0099537566,0.0457928742,0.0301630797,-0.0084504334,0.0576262199,-0.0152984419,0.0328242364,-0.0118798976,-0.010144681,-0.0377898803,0.0091316868,0.0244495094,-0.0086172807,-0.0067858718,0.0227088585]},"30":{"Abstract":"We propose a novel massively parallel construction algorithm for Bounding Volume Hierarchies (BVHs) based on locally-ordered agglomerative clustering. Our method builds the BVH iteratively from bottom to top by merging a batch of cluster pairs in each iteration. To efficiently find the neighboring clusters, we keep the clusters ordered along the Morton curve. This ordering allows us to identify approximate nearest neighbors very efficiently and in parallel. We implemented our algorithm in CUDA and evaluated it in the context of GPU ray tracing. For complex scenes, our method achieves up to a twofold reduction of build times while providing up to 17 percent faster trace times compared with the state-of-the-art methods.","Authors":"D. Meister; J. Bittner","DOI":"10.1109\/TVCG.2017.2669983","Keywords":"Ray tracing;object hierarchies;three-dimensional graphics and realism","Title":"Parallel Locally-Ordered Clustering for Bounding Volume Hierarchy Construction","Keywords_Processed":"object hierarchy;three dimensional graphic and realism;ray tracing","Keyword_Vector":[0.0569053722,0.0306495155,-0.0306130364,-0.0311856251,0.0207102895,-0.0621057927,-0.0786489126,-0.0072602208,-0.1672797947,-0.0436213911,0.0699461632,0.1299902581,0.0056680163,0.0549013728,0.112337185,0.1273046965,0.0672581817,0.0940705742,0.3178355217,-0.0191455952,-0.1918200459,0.1026181032,0.2236046997,0.099217457,-0.1944830479,-0.0739496588,-0.0119299847,0.0057407391,0.1412762329,-0.2005549972,-0.0289220415,-0.1987390416,0.1203605074,0.0870240489,-0.0929584377,0.011208856,-0.1135354899,-0.0106585454,-0.0150024823,-0.1500795809,-0.0473536782,0.1184422124,-0.0066135008,0.0100726147,-0.001524501,-0.0335965504,0.103963681,0.0534304944,-0.0622547689,0.0932269539,-0.0328863442,0.0693975709],"Abstract_Vector":[0.0929274542,-0.0175875035,-0.0198470426,-0.008995254,-0.0368177127,0.0599502546,0.0153256065,0.0345595862,0.0259307344,0.0067153997,-0.0190103419,0.0313812174,-0.0095419779,0.0010830548,0.0059178793,0.0132094287,-0.0063478297,-0.0069133883,0.0137591024,0.0012981145,-0.0034515011,-0.0165129175,0.0411974735,-0.0158144272,-0.0021429294,0.0211156789,0.00854904,0.0115182153,0.0073855522,0.0083429608,0.0107528627,-0.0023291899,-0.0131830648,-0.0212169245,0.0189092139,-0.0170610097,-0.0113028794,0.0071179907,-0.0365930042,-0.031199505,0.0091647561,0.0437323301,0.0000621176,-0.0016782209,0.0116558438,0.014521376,0.0005819722,-0.0192564333,0.0049983963,-0.0332374531,0.0111403916,-0.0186034611,0.0108351755,-0.0236038533,0.0169694279,-0.0165687695,0.0017793013,0.0213068593,-0.0069903846,-0.0131872316,-0.000956643,0.0063326848,-0.0052232949,0.0232306817,-0.0029632848,-0.0473086737,-0.0122159464,-0.0260472347,-0.0351925697,0.0422098613,-0.0149712476,0.0472638047,0.0093644772,0.026433093,-0.0015228128,-0.0679739344,0.0567984361,0.0177736557,0.0140507909,-0.0331259062,0.0113028032,-0.0245065525,-0.021291481,0.0548197679,0.0115628857,0.00704799,-0.0090596368,-0.0155913441,-0.0448164339,-0.0150515107,0.0134102237,-0.0084646633,-0.0165759776,0.0310515289,0.026808558,-0.0331143894,0.0134484865,0.0094988665,0.0013358811,-0.0511251272,0.0115515037,-0.0380110983,-0.0144364168,0.0129812189,0.0216971781,0.0423039274,0.0175455683,0.0334906236,-0.0104031786,-0.0264393058,0.005796847,0.0316940769,0.0244155947,-0.0042885698,0.0071094052,0.0140072397]},"300":{"Abstract":"The emerging prosperity of cryptocurrencies, such as Bitcoin, has come into the spotlight during the past few years. Cryptocurrency exchanges, which act as the gateway to this world, now play a dominant role in the circulation of Bitcoin. Thus, delving into the analysis of the transaction patterns of exchanges can shed light on the evolution and trends in the Bitcoin market, and participants can gain hints for identifying credible exchanges as well. Not only Bitcoin practitioners but also researchers in the financial domains are interested in the business intelligence behind the curtain. However, the task of multiple exchanges exploration and comparisons has been limited owing to the lack of efficient tools. Previous methods of visualizing Bitcoin data have mainly concentrated on tracking suspicious transaction logs, but it is cumbersome to analyze exchanges and their relationships with existing tools and methods. In this paper, we present BitExTract, an interactive visual analytics system, which, to the best of our knowledge, is the first attempt to explore the evolutionary transaction patterns of Bitcoin exchanges from two perspectives, namely, exchange versus exchange and exchange versus client. In particular, BitExTract summarizes the evolution of the Bitcoin market by observing the transactions between exchanges over time via a massive sequence view. A node-link diagram with ego-centered views depicts the trading network of exchanges and their temporal transaction distribution. Moreover, BitExTract embeds multiple parallel bars on a timeline to examine and compare the evolution patterns of transactions between different exchanges. Three case studies with novel insights demonstrate the effectiveness and usability of our system.","Authors":"X. Yue; X. Shu; X. Zhu; X. Du; Z. Yu; D. Papadopoulos; S. Liu","DOI":"10.1109\/TVCG.2018.2864814","Keywords":"Bitcoin exchange;transaction data;comparative analysis;visual analytics;FinTech","Title":"BitExTract: Interactive Visualization for Extracting Bitcoin Exchange Intelligence","Keywords_Processed":"Bitcoin exchange;comparative analysis;transaction datum;visual analytic;FinTech","Keyword_Vector":[0.148399815,-0.116367385,-0.0278794565,0.1196408962,-0.0421724986,0.2580059335,0.002177221,-0.0719567489,-0.0806850202,0.0131374065,0.0323922537,-0.0086717498,0.0826732916,-0.093929164,-0.0160605379,-0.0787441653,0.036443901,-0.0778605175,-0.0294512077,-0.0397991753,-0.0845838806,-0.0207146587,0.0614172433,0.0798943362,0.0084867014,-0.0082389257,-0.0000900934,0.0677008948,0.0371362072,-0.079945085,0.0230792128,0.115875672,-0.1482169652,0.0405176615,0.1785733553,-0.1303850896,-0.0647205201,-0.0932103094,-0.0102385205,-0.0046711803,-0.0225229042,0.2151957951,-0.0546544265,-0.0178432218,-0.0192731501,0.0017232646,-0.0161369617,-0.0102788422,-0.0057871296,-0.0747452817,0.0107330512,-0.0046634023],"Abstract_Vector":[0.2203595868,-0.1132224847,0.0522568825,0.0000214019,-0.0180021792,-0.013226929,0.0172593696,0.0176739625,-0.0057065281,0.0636010622,-0.0301962076,0.0246022559,-0.1039053351,0.0104702943,-0.000095694,-0.0391316103,0.0017173012,-0.0135767094,0.0006595128,0.0148253622,0.0114117383,-0.0596308503,-0.009218957,0.059875011,0.0222920401,0.0250509794,-0.0105843809,0.0082279555,-0.0093121346,-0.0604622674,0.098601241,-0.0135473219,-0.0102754852,-0.0189829996,-0.0378766102,-0.0372984574,-0.0238102983,-0.0238683685,0.0041464113,0.0488337415,-0.0288237456,0.0150807585,-0.0745228508,-0.0077158126,0.0407009107,-0.010698526,0.0129271204,0.0228504772,0.0036935864,0.0079581777,-0.0442790169,0.0121624011,0.0071757756,-0.0236021409,0.0016766683,-0.0316557517,0.0129039865,-0.0049916897,-0.0075649635,0.0157136686,-0.026243346,0.0347154237,-0.0116154988,-0.0149342147,0.002529751,0.0468954961,-0.0905839402,0.046265826,-0.0402874999,0.0006862643,-0.0042167322,-0.0114623606,-0.0113617339,0.0087011937,-0.0232284348,0.0258381846,-0.0356638748,0.0123836332,-0.0016814818,0.0253150669,0.0004491313,0.0417027561,0.0328177074,-0.0498911553,0.0125219462,0.0210600983,0.0211014629,-0.0109836238,-0.0140761751,-0.0210099228,0.0180295962,-0.0034837575,0.0395174386,-0.017811808,0.0274199482,-0.0354748975,0.0149117306,-0.0116241817,0.0084907521,-0.0123449002,-0.0147138575,0.0189143406,-0.0148255451,-0.0152008,-0.0024433691,-0.0142606084,0.0259819167,-0.0267140291,-0.0077637913,0.0054667233,0.0075844144,0.0292038108,0.0145400795,-0.0118581361,-0.015252416,-0.0084450727]},"301":{"Abstract":"In this paper, we present a method for reconstructing the drawing process of Chinese brush paintings. We demonstrate the possibility of computing an artistically reasonable drawing order from a static brush painting that is consistent with the rules of art. We map the key principles of drawing composition to our computational framework, which first organizes the strokes in three stages and then optimizes stroke ordering with natural evolution strategies. Our system produces reasonable animated constructions of Chinese brush paintings with minimal or no user intervention. We test our algorithm on a range of input paintings with varying degrees of complexity and structure and then evaluate the results via a user study. We discuss the applications of the proposed system to painting instruction, painting animation, and image stylization, especially in the context of art teaching.","Authors":"F. Tang; W. Dong; Y. Meng; X. Mei; F. Huang; X. Zhang; O. Deussen","DOI":"10.1109\/TVCG.2017.2774292","Keywords":"Chinese brush painting;animation;drawing analysis;art teaching","Title":"Animated Construction of Chinese Brush Paintings","Keywords_Processed":"draw analysis;animation;art teaching;chinese brush painting","Keyword_Vector":[0.1492523663,0.1743901228,-0.0692492504,-0.0056030116,0.0173088645,-0.0597309391,-0.031287565,0.0415489504,-0.0627097968,0.0062718673,-0.0343211477,0.0749710499,0.0568264654,0.0461782423,0.0990874431,0.0845164456,-0.1388535672,-0.1710586914,0.1474349686,-0.0440040168,0.0059549069,-0.1503083836,0.1734463504,-0.0117891243,0.0316101942,0.145199253,-0.0183115113,-0.0167517389,-0.0967283435,0.0201383288,0.1153950454,0.0142964556,-0.0545745299,0.0543540303,-0.0720326701,-0.009893243,-0.0652422697,0.0598066325,0.0483722095,-0.0029293749,-0.0002168654,0.097695614,-0.1621616894,-0.1958347727,-0.0709551576,0.0430063993,0.1164752458,0.1317475845,-0.1196534213,0.0405074634,-0.0089762253,0.2168867901],"Abstract_Vector":[0.2540657966,0.0582835225,0.0219589629,-0.00210033,-0.0474274923,0.0598847511,-0.0163525542,0.0289180627,0.1032149964,0.0020112749,-0.055021984,0.0320655916,-0.0698053337,0.0001398158,0.0097549162,-0.0653016758,0.0003047502,0.0527177618,-0.0127570884,-0.0106310154,0.0129790363,-0.0021038481,-0.0539554475,0.0246789811,0.0030909385,-0.0254114715,0.0362045385,-0.0142636443,-0.0056880495,-0.0155206409,0.0011475479,-0.0329051355,-0.0465539611,-0.0075698307,-0.0065190121,-0.0519447282,-0.0352928265,-0.0136933258,-0.003098607,0.011971889,0.0507746747,0.052743479,-0.0125541922,0.0125236513,-0.0463987713,-0.0014822292,0.0201743481,0.0212031402,-0.0084007694,-0.0121518058,-0.005096449,0.0275769986,-0.0013553484,0.0380303805,0.0346214576,0.0132741237,-0.0010378927,0.012281401,-0.0307473866,-0.0426451001,-0.0210068056,-0.0155444704,-0.0227936401,-0.0013702958,-0.0211258859,0.0237312343,0.005519758,-0.023863875,0.0381413079,0.0062192496,0.0295992571,-0.0003992824,0.00681647,-0.042333441,0.0286505429,0.0133801801,-0.0308194647,0.018036116,0.0038692565,-0.0359867937,0.0055327869,0.0177764154,0.0494862714,-0.000989516,0.0046875836,0.0183094681,0.0091359341,-0.0129377498,-0.0298304038,0.0450931874,0.0126116917,0.0116675112,0.0154882563,0.0109565873,0.0157114573,-0.0172712828,-0.038116924,-0.0037702068,-0.0522193721,0.0186688637,-0.0404258661,-0.0114779508,-0.0083222211,-0.0136042183,0.034832683,0.001780301,0.0015032792,0.0290105719,-0.0438552527,0.0269501925,-0.0355222501,-0.050404775,0.0119615787,-0.0149529541,-0.0088236053,0.0021620085]},"302":{"Abstract":"For neurodegenerative conditions like Parkinson's disease, early and accurate diagnosis is still a difficult task. Evaluations can be time consuming, patients must often travel to metropolitan areas or different cities to see experts, and misdiagnosis can result in improper treatment. To date, only a handful of assistive or remote methods exist to help physicians evaluate patients with suspected neurological disease in a convenient and consistent way. In this paper, we present a low-cost VR interface designed to support evaluation and diagnosis of neurodegenerative disease and test its use in a clinical setting. Using a commercially available VR display with an infrared camera integrated into the lens, we have constructed a 3D virtual environment designed to emulate common tasks used to evaluate patients, such as fixating on a point, conducting smooth pursuit of an object, or executing saccades. These virtual tasks are designed to elicit eye movements commonly associated with neurodegenerative disease, such as abnormal saccades, square wave jerks, and ocular tremor. Next, we conducted experiments with 9 patients with a diagnosis of Parkinson's disease and 7 healthy controls to test the system's potential to emulate tasks for clinical diagnosis. We then applied eye tracking algorithms and image enhancement to the eye recordings taken during the experiment and conducted a short follow-up study with two physicians for evaluation. Results showed that our VR interface was able to elicit five common types of movements usable for evaluation, physicians were able to confirm three out of four abnormalities, and visualizations were rated as potentially useful for diagnosis.","Authors":"J. Orlosky; Y. Itoh; M. Ranchet; K. Kiyokawa; J. Morgan; H. Devos","DOI":"10.1109\/TVCG.2017.2657018","Keywords":"Virtual reality;eye tracking;diagnosis;visualization","Title":"Emulation of Physician Tasks in Eye-Tracked Virtual Reality for Remote Diagnosis of Neurodegenerative Disease","Keywords_Processed":"diagnosis;eye tracking;visualization;virtual reality","Keyword_Vector":[0.126409353,0.1221431468,0.0794561306,0.0237828731,-0.0554906408,-0.0511596706,-0.0034998691,-0.1043829101,0.0161393279,-0.0853524416,-0.0223208596,0.0487041895,-0.0907918105,0.0388432565,0.0619230448,-0.128997104,-0.0056137064,-0.0415582468,-0.0361044728,-0.004718718,-0.0362759372,0.0235688012,0.011390212,-0.0455679387,-0.0115157858,0.0597454525,-0.0748747181,-0.0915135692,0.0097308243,-0.0134597322,-0.0412848701,0.0115181846,-0.0185166189,0.0227189707,0.0114831171,0.041249452,0.017201273,-0.0197211766,-0.0112335644,-0.0231388469,0.0048763484,-0.0329295622,0.0030337898,0.0384594191,0.0182290504,-0.0201089284,0.0844336623,-0.0070221761,0.0320300429,-0.0277369624,-0.0777647248,0.008162745],"Abstract_Vector":[0.2103816402,0.1149891121,0.2046457189,0.0206045484,-0.0621057507,0.0663013637,0.1693145427,-0.1096984605,0.017498118,-0.0322474762,0.0593948926,-0.0164595371,0.0016891133,0.0537842152,-0.028152231,0.1195442528,-0.0046118007,0.078201105,-0.0196247153,0.0086655799,0.056685071,-0.0235216238,0.0293770639,-0.0242306624,-0.0209775848,0.0015406883,0.0391742113,-0.0443645715,-0.0246040587,-0.0159039451,0.0136673071,-0.0054026994,0.0359400254,0.0247761914,-0.025489141,-0.0454405829,-0.0185698598,0.0297770869,-0.0178656101,-0.0606693888,0.0337587714,0.0026468984,0.0292452555,0.0080241834,0.0162106041,0.0032834958,-0.0055851374,0.0134384632,0.0024850966,-0.0105066602,0.0072748184,0.0494641186,-0.028142958,0.0161466729,-0.0313716063,-0.0271344734,0.0258285283,-0.0162210574,0.0162732424,0.0186146162,-0.041942982,0.0209412332,0.0176087551,0.0230127634,0.0323500638,-0.0031079093,-0.0326621792,-0.0115618071,-0.001676184,-0.0176249258,-0.0095741703,0.0026746461,0.0080060428,0.0050572643,-0.0109283953,-0.0163239037,0.0299190757,0.0004287622,-0.0168373456,-0.0375560237,0.0165910687,0.0171461481,-0.0156969357,0.0117556309,-0.0192980463,0.011775712,-0.0270806937,0.01632541,0.0040336413,0.0110333235,0.0149201712,-0.0130340553,-0.044881729,0.0084523065,-0.0234087366,0.0077846286,0.0115947403,-0.0176757607,0.017463577,0.0104016102,0.0217460168,-0.0196944196,0.0370026229,0.0088479859,-0.012494938,-0.0009455663,0.006273209,-0.007909945,-0.0342117238,-0.0108026778,-0.012628035,-0.0204598459,0.0049363834,-0.0111098019,0.0082515559,-0.0081679158]},"303":{"Abstract":"Designing volume visualizations showing various structures of interest is critical to the exploratory analysis of volumetric data. The last few years have witnessed dramatic advances in the use of convolutional neural networks for identification of objects in large image collections. Whereas such machine learning methods have shown superior performance in a number of applications, their direct use in volume visualization has not yet been explored. In this paper, we present a deep-learning-assisted volume visualization to depict complex structures, which are otherwise challenging for conventional approaches. A significant challenge in designing volume visualizations based on the high-dimensional deep features lies in efficiently handling the immense amount of information that deep-learning methods provide. In this paper, we present a new technique that uses spectral methods to facilitate user interactions with high-dimensional features. We also present a new deep-learning-assisted technique for hierarchically exploring a volumetric dataset. We have validated our approach on two electron microscopy volumes and one magnetic resonance imaging dataset.","Authors":"H. Cheng; A. Cardone; S. Jain; E. Krokos; K. Narayan; S. Subramaniam; A. Varshney","DOI":"10.1109\/TVCG.2018.2796085","Keywords":"Volume visualization;convolutional neural networks","Title":"Deep-Learning-Assisted Volume Visualization","Keywords_Processed":"convolutional neural network;volume visualization","Keyword_Vector":[0.0993789879,-0.0627089597,-0.0652277516,-0.0270464081,0.028875893,-0.0705391446,-0.1108288646,-0.0477945871,-0.1268247798,0.1410421484,-0.1120015503,0.1988830664,0.033247696,0.0080437463,-0.0678650807,-0.0795416095,-0.0168613201,-0.0508007495,-0.0171132448,0.1346685756,0.0001152007,0.0257457054,-0.0647589557,0.0761581122,0.0801050944,-0.1957429422,-0.0096046259,-0.0925287677,-0.0503191132,-0.0086088707,-0.0267803125,-0.041888242,-0.0920435873,-0.0030250183,0.00288209,0.0599659542,-0.0077684403,0.029679709,0.0889724704,0.0780984937,-0.0940961528,-0.0500520624,0.0437257329,-0.022368366,-0.0998648221,0.0433015535,-0.0473714616,0.0116657667,0.0866263643,0.018650587,-0.0105407223,0.0165105774],"Abstract_Vector":[0.194078064,-0.0873348166,-0.0093280464,-0.0115837669,0.0026933895,-0.0901781165,-0.0742971419,-0.0316744633,0.0390860675,-0.05094068,0.0414468791,-0.0626819941,0.0216972321,-0.006766269,-0.0015176152,0.0307676044,-0.0092149957,-0.0168647405,0.003351425,-0.011634585,0.017607375,-0.0281805607,-0.0003717656,-0.0322434559,-0.0355752579,-0.0788051021,0.0054864415,0.0241533088,0.0490019315,0.0364467462,-0.0095169414,-0.0295149021,0.0493353921,-0.0099255124,-0.0176427338,-0.019269932,0.0354301241,-0.0463009701,0.033928993,0.0316635371,0.0328819981,-0.0231677221,0.021751053,0.0231921074,0.0030412827,-0.0466161301,0.0044293167,-0.0714177093,0.0207751869,0.0168758725,-0.0344152744,-0.0064948017,0.0404448133,0.0418606034,0.0851888566,-0.0676466634,0.0256761132,-0.0065921332,-0.0144004873,0.0074176187,-0.0093107631,-0.0303470443,0.0690997351,0.0396143944,-0.0745950414,0.0102229024,-0.0477276666,0.0164062568,0.0050803665,0.0644031057,-0.0392759627,-0.017572091,0.012790215,0.0049234841,-0.0012879914,0.0301502154,0.0454098573,-0.0412752221,-0.0040739814,-0.0132302572,-0.0337412581,-0.0008628453,0.0187468678,0.0340500827,0.0328580457,-0.001253546,-0.0047022709,-0.0022538847,-0.0040401301,-0.0436222814,0.0381157673,-0.0066387326,0.0421025203,0.0149576032,-0.0079975939,-0.0187901829,0.0028447787,0.0129499879,-0.0163830332,0.0771180904,0.0178729392,0.0181835295,0.0702601155,0.006199201,0.0109086635,-0.0047763499,-0.0364647264,-0.0371218587,0.0021820082,-0.0069277568,-0.0498615011,0.0051671741,-0.0022520903,0.0305975213,-0.0023539999,-0.0351313085]},"304":{"Abstract":"Models of human perception - including perceptual \u201claws\u201d - can be valuable tools for deriving visualization design recommendations. However, it is important to assess the explanatory power of such models when using them to inform design. We present a secondary analysis of data previously used to rank the effectiveness of bivariate visualizations for assessing correlation (measured with Pearson's r) according to the well-known Weber-Fechner Law. Beginning with the model of Harrison et al. [1], we present a sequence of refinements including incorporation of individual differences, log transformation, censored regression, and adoption of Bayesian statistics. Our model incorporates all observations dropped from the original analysis, including data near ceilings caused by the data collection process and entire visualizations dropped due to large numbers of observations worse than chance. This model deviates from Weber's Law, but provides improved predictive accuracy and generalization. Using Bayesian credibility intervals, we derive a partial ranking that groups visualizations with similar performance, and we give precise estimates of the difference in performance between these groups. We find that compared to other visualizations, scatterplots are unique in combining low variance between individuals and high precision on both positively- and negatively correlated data. We conclude with a discussion of the value of data sharing and replication, and share implications for modeling similar experimental data.","Authors":"M. Kay; J. Heer","DOI":"10.1109\/TVCG.2015.2467671","Keywords":"Weber\u2019s law;perception of correlation;log transformation;censored regression;Bayesian methods;Weber's law;perception of correlation;log transformation;censored regression;Bayesian methods","Title":"Beyond Weber's Law: A Second Look at Ranking Visualizations of Correlation","Keywords_Processed":"bayesian method;perception of correlation;log transformation;Weber law;censor regression","Keyword_Vector":[0.2076834755,-0.1087799104,0.0516359558,0.1164065234,-0.0650462393,0.2330048153,-0.0562137382,-0.0709699934,-0.0078903427,0.0611659868,0.0642925451,-0.0067809392,0.0429900282,-0.021269811,0.0266582235,0.1777157424,0.0629641288,-0.0570703793,-0.1255852441,-0.0165789458,-0.0326569364,0.0343588045,0.0614021483,0.0877652286,-0.0368678406,-0.0032431032,0.004799863,0.0112323001,-0.018174857,-0.0608570154,0.0619364878,0.1408707299,-0.0851042554,-0.0357909076,0.1455547237,-0.0752581297,-0.0347245643,-0.0549730516,0.0102340581,0.0201206172,-0.0285114316,0.2189062415,0.0119468178,-0.040089347,-0.0376009305,-0.0391444943,0.0760222311,0.011615428,-0.0393122499,-0.0832709348,0.0479062902,0.0183079322],"Abstract_Vector":[0.1991264436,-0.039387005,-0.0048455368,-0.0300795998,-0.0081521,0.0030962619,-0.0163169881,-0.0109493428,0.1137829569,0.0059548126,-0.0191581335,-0.0080593289,-0.047288769,-0.0006844746,-0.0530186222,-0.1045884237,0.0164096304,0.0141059145,0.0059885402,-0.0137030309,-0.0700938235,0.0075431435,-0.000378128,-0.0186896114,0.0064483195,-0.0286150592,0.0037096857,-0.028356246,-0.0249796973,-0.0296780072,-0.0081006727,-0.001913114,0.0022303034,-0.0233590712,-0.026222185,-0.0012636686,-0.0148782742,0.0003336783,0.0403794368,-0.0731994879,0.0032924841,-0.0123236523,-0.0121759921,-0.0074075307,-0.0370466582,-0.0194278844,-0.004287015,-0.0121375121,0.0019562967,0.0171283922,-0.0466384146,-0.0011156316,-0.0176885598,0.0000960838,0.0021658558,0.0085927264,0.0242167149,0.0073251755,0.0148423934,0.015321873,-0.0280877022,-0.0092810645,-0.0218048901,-0.0279377704,-0.0109643785,0.0313561362,-0.0500036618,-0.0267600446,0.011400745,0.018413351,0.0269487525,-0.0200026072,-0.0183732273,0.0308955048,-0.0041566346,-0.0079361057,-0.0214198039,-0.0162917309,0.0332331848,-0.0283125457,-0.023018174,-0.0088929931,-0.0242778689,-0.0203938759,-0.0256109232,0.0213367058,0.0132048528,-0.0276274027,0.0185657755,0.0402495774,-0.0500439226,-0.0305861817,-0.0074785339,0.0061355843,0.0278504478,-0.0074396457,0.0220520457,-0.0019349329,-0.0126548932,0.0053970067,0.0073595759,-0.0186716719,0.0017018457,0.0510075924,0.0309804077,-0.0129598386,-0.0306311236,-0.0031739361,-0.0140065192,0.0114283011,0.0144188818,0.0223648343,-0.0155011522,0.0003260625,0.022108473,0.0123754438]},"305":{"Abstract":"This design study focuses on the analysis of a time sequence of categorical sequences. Such data is relevant for the geoscientific research field of landscape and climate development. It results from microscopic analysis of lake sediment cores. The goal is to gain hypotheses about landscape evolution and climate conditions in the past. To this end, geoscientists identify which categorical sequences are similar in the sense that they indicate similar conditions. Categorical sequences are similar if they have similar meaning (semantic similarity) and appear in similar time periods (temporal similarity). For data sets with many different categorical sequences, the task to identify similar sequences becomes a challenge. Our contribution is a tailored visual analysis concept that effectively supports the analytical process. Our visual interface comprises coupled visualizations of semantics and temporal context for the exploration and assessment of the similarity of categorical sequences. Integrated automatic methods reduce the analytical effort substantially. They (1) extract unique sequences in the data and (2) rank sequences by a similarity measure during the search for similar sequences. We evaluated our concept by demonstrations of our prototype to a larger audience and hands-on analysis sessions for two different lakes. According to geoscientists, our approach fills an important methodological gap in the application domain.","Authors":"A. Unger; N. Dr\u00e4ger; M. Sips; D. J. Lehmann","DOI":"10.1109\/TVCG.2017.2744686","Keywords":"Visualization in Earth Science;Time Series Data;Categorical Data;Design Study","Title":"Understanding a Sequence of Sequences: Visual Exploration of Categorical States in Lake Sediment Cores","Keywords_Processed":"Categorical Data;Design Study;Time Series Data;visualization in Earth Science","Keyword_Vector":[0.1352295016,0.0894506444,0.1003025198,0.2115609838,0.267454306,-0.1013122427,-0.0293380916,-0.0966767667,0.0042119287,-0.0108179075,0.0470845175,-0.0030336997,-0.0743466309,-0.0353763978,-0.0054975879,-0.0845452574,0.0078888497,0.0269221653,-0.0802085009,0.0075912153,-0.0732850438,0.0164357003,0.0331512521,0.0009946384,0.0030011126,0.0192795652,0.0188070524,-0.0373340268,-0.0752562026,-0.0356988654,0.0204526517,0.0989923058,0.0160662459,-0.0073361185,-0.0440213271,-0.0167140142,0.0514102818,-0.0106620571,-0.094710884,-0.0509393517,-0.0758581888,-0.0915142026,-0.1064961852,-0.0122079916,-0.030503227,-0.051955999,0.0265468876,-0.0013488902,-0.0817477605,0.0189253707,-0.0267396017,-0.0579822403],"Abstract_Vector":[0.2355244595,0.2281814383,0.0329176457,0.1919867335,-0.1009456063,-0.1143784365,0.0487516291,0.0385860743,-0.0875421464,-0.040983137,-0.0307847572,-0.0323508996,0.0127022237,0.1081950121,-0.0108684815,0.0626628453,-0.0429707898,0.0048162489,0.0245496041,0.0386460303,-0.0837962721,0.0082402359,-0.0182495819,-0.0634512405,0.0254749439,0.0155800872,-0.0171364261,0.0248961182,-0.0037498654,-0.0132225773,0.022130963,-0.0013394238,-0.0397488359,0.0985342909,-0.0104735176,-0.0606593108,-0.0056798579,0.0144221273,-0.0401260017,0.0106481238,0.0062335626,-0.0676384186,-0.0550154824,0.0140582868,-0.0161336376,-0.0307283477,0.0156672364,0.0027274701,-0.0328514424,-0.0191669356,-0.0196434631,-0.0067072991,0.0347221478,0.0456557583,-0.043792908,0.064896202,0.0160535472,0.0388865673,-0.024388451,-0.0172163958,-0.0296297312,0.0241632776,-0.0183055003,0.007940204,0.001943646,-0.0035591681,-0.0214086688,-0.0359478741,-0.0117211822,0.0049919096,0.0185075723,0.0052785901,-0.0330865457,0.0128154992,0.0078531288,0.0171136242,0.0511100421,0.003726725,0.0270374729,0.0214850152,-0.012729896,0.0207878136,0.0202085481,0.0163404054,-0.0059987044,0.0639215334,0.0229804979,-0.0275888421,-0.0130025837,0.0006513045,-0.0030598812,0.0037534053,-0.0048287416,-0.0130563848,0.0048353198,0.0363334874,-0.0137323363,0.0184280478,-0.0298215633,0.0446031635,-0.0173334569,0.005425866,-0.0218583261,0.0279492805,-0.0365615369,-0.0285973414,-0.0055515582,0.0305206158,-0.0043741506,-0.0027397131,-0.0194546248,0.0437520011,0.0203128018,0.0117429635,0.0175308011,0.0156892176]},"306":{"Abstract":"Data analysis novices often encounter barriers in executing low-level operations for pairwise comparisons. They may also run into barriers in interpreting the artifacts (e.g., visualizations) created as a result of the operations. We developed Duet, a visual analysis system designed to help data analysis novices conduct pairwise comparisons by addressing execution and interpretation barriers. To reduce the barriers in executing low-level operations during pairwise comparison, Duet employs minimal specification: when one object group (i.e. a group of records in a data table) is specified, Duet recommends object groups that are similar to or different from the specified one; when two object groups are specified, Duet recommends similar and different attributes between them. To lower the barriers in interpreting its recommendations, Duet explains the recommended groups and attributes using both visualizations and textual descriptions. We conducted a qualitative evaluation with eight participants to understand the effectiveness of Duet. The results suggest that minimal specification is easy to use and Duet's explanations are helpful for interpreting the recommendations despite some usability issues.","Authors":"P. Law; R. C. Basole; Y. Wu","DOI":"10.1109\/TVCG.2018.2864526","Keywords":"Pairwise comparison;novices;data analysis;automatic insight generation","Title":"Duet: Helping Data Analysis Novices Conduct Pairwise Comparisons by Minimal Specification","Keywords_Processed":"datum analysis;pairwise comparison;novice;automatic insight generation","Keyword_Vector":[0.1037073134,-0.0333083275,-0.031784811,-0.0612185235,0.011001043,-0.0160756107,0.0138363453,-0.0005111152,-0.0418849967,0.0363283814,-0.0626138251,0.0986407168,0.0089533357,-0.0515486552,0.00381882,0.0062626369,0.0896392991,-0.0066280626,-0.0333271267,-0.0087703817,-0.0589601266,-0.0061680688,-0.0654195626,-0.0444568913,0.151049318,0.0286795216,0.0108288437,-0.0173905923,-0.0247414505,-0.0459722759,0.0201130328,-0.0137232767,0.017225567,-0.0299604697,-0.0747431474,0.0311379691,-0.0063161465,0.0412949621,0.0808365062,-0.0029218977,-0.0314445058,0.0053794191,0.0104564432,0.0367431805,0.0011389514,0.019113924,0.0400145397,-0.0026122458,0.0025472868,-0.0366317268,-0.0245361923,0.0240726425],"Abstract_Vector":[0.1768413111,-0.1294816684,-0.0145357111,-0.0073241446,-0.064809934,0.0710339298,0.0120846376,0.0172957818,-0.0760308982,0.0116976862,-0.0219097145,-0.0183504656,0.0805734946,-0.0098074297,0.0000566433,0.0611378157,-0.024473906,0.0163106054,0.0331746336,-0.0030054062,-0.0001241366,0.0021893597,-0.034739661,0.0312841003,0.0236926118,-0.0743228585,-0.0421979686,-0.0604843084,0.0402690958,0.0686203934,-0.0102145355,-0.0209496956,0.0027715851,0.0158196714,0.0527585404,-0.0405520986,-0.011906691,0.0100527701,-0.0301727026,-0.0606661481,0.0295681766,0.0150650639,0.0079021779,-0.0350482796,0.0236781757,-0.0331563689,0.0780513947,-0.0290141564,-0.0386414348,-0.0790718416,-0.0041407008,0.0067287482,0.0034122907,-0.0405942821,0.0205534489,0.0355597505,0.027623792,0.0256358563,-0.0313596267,0.046890331,0.0265080074,-0.0050790373,0.02227818,0.0912568934,0.0090438685,-0.0540529368,0.003143365,0.0206111379,-0.0498615905,0.0172681775,0.0229799741,0.0481676652,-0.0240176255,-0.0202467019,0.0169183347,-0.0456680914,0.0272884368,-0.0510039326,-0.005749896,0.0232084127,-0.0056081689,-0.0387646574,0.0339341248,0.0029856707,-0.0140252716,0.0573351315,-0.0529829077,-0.0020627878,0.0019793708,0.0295305101,-0.0529158395,-0.0039651567,-0.0188162079,-0.0562402327,0.0105340607,-0.0333120631,0.0211825946,0.0070252439,0.0100054113,-0.0028926047,0.0018745651,-0.0138064536,-0.0456065656,-0.0271488447,-0.0095146412,-0.0088941487,0.0363333493,-0.0259090553,0.0151828307,0.0253924947,-0.0633826733,0.0220638086,-0.0196737528,0.0616164317,0.0233885251,0.0123598358]},"307":{"Abstract":"Fish Tank Virtual Reality (FTVR) displays create a compelling 3D spatial effect by rendering to the perspective of the viewer with head-tracking. Combining FTVR with a spherical display enhances the 3D experience with unique properties of the spherical screen such as the enclosing shape, consistent curved surface, and borderless views from all angles around the display. The ability to generate a strong 3D effect on a spherical display with head-tracked rendering is promising for increasing user's performance in 3D tasks. An unanswered question is whether these natural affordances of spherical FTVR displays can improve spatial perception in comparison to traditional flat FTVR displays. To investigate this question, we conducted an experiment to see whether users can perceive the depth and size of virtual objects better on a spherical FTVR display compared to a flat FTVR display on two tasks. Using the spherical display, we found significantly that users had 1cm depth accuracy compared to 6.5cm accuracy using the flat display on a depth-ranking task. Likewise, their performance on a size-matching task was also significantly better with the size error of 2.3mm on the spherical display compared to 3.1mm on the flat display. Furthermore, the perception of size-constancy is stronger on the spherical display than the flat display. This study indicates that the natural affordances provided by the spherical form factor improve depth and size perception in 3D compared to a flat display. We believe that spherical FTVR displays have potential as a 3D virtual environment to provide better task performance for various 3D applications such as 3D designs, scientific visualizations, and virtual surgery.","Authors":"Q. Zhou; G. Hagemann; D. Fafard; I. Stavness; S. Fels","DOI":"10.1109\/TVCG.2019.2898742","Keywords":"Fish tank virtual reality;spherical display;depth cues;3D perception;size constancy;3D visualization","Title":"An Evaluation of Depth and Size Perception on a Spherical Fish Tank Virtual Reality Display","Keywords_Processed":"3d visualization;3d perception;depth cue;spherical display;fish tank virtual reality;size constancy","Keyword_Vector":[0.0367050564,0.0267295952,-0.0115072704,0.0224576346,0.0326222228,-0.0288706756,-0.0088469766,0.0058771993,-0.0405770932,-0.0293233082,0.0259057704,0.0160070599,-0.0141046193,0.0393616748,0.0452443022,0.0218904712,-0.04835064,-0.0941443598,0.0951515882,-0.0458243231,-0.0006846159,-0.0702950575,-0.0437219431,0.0868778252,0.008603624,-0.0146727576,0.1568264779,0.0536051435,-0.0674103493,0.0144063519,-0.0303286839,0.0258034207,-0.0237733735,-0.0800775922,0.0363265256,0.0867071384,-0.0562038914,-0.0326022951,-0.0506338092,-0.0242442052,0.046497825,-0.0183261631,0.0115296543,0.0144346882,0.0010074312,-0.0423509809,-0.00123508,0.021490137,0.0549412911,0.0057044484,-0.054402551,0.0081098986],"Abstract_Vector":[0.2306632684,0.1171635255,-0.0373429581,0.0380757902,-0.1160947221,0.1120103934,-0.0242938487,0.0160930676,0.0986752328,-0.0431421608,-0.0310710534,-0.0333913432,-0.0762146707,-0.0160091345,-0.060502919,0.000537837,0.1023787363,0.0957083983,0.0040731084,-0.0746963358,0.01332977,0.0977734213,0.0038147318,-0.0588436158,-0.0225249027,-0.0090281935,0.0057755526,-0.0172625666,-0.0594882341,0.0341165999,0.0599983816,-0.0420475322,-0.0163190681,-0.0304205431,-0.0006505557,0.000997104,0.0686164138,0.0004652048,-0.0404213732,-0.040365001,0.0660079054,-0.0048722161,0.0245078748,-0.0492872573,0.0612234223,-0.0122084362,0.0950367812,0.0583651698,-0.0236984191,-0.0053767674,0.0236965472,0.0152593412,-0.0105678551,0.0052194285,-0.0191295477,-0.0116150828,0.0532793399,-0.0019400295,0.0349791367,-0.009485606,-0.0024732941,0.0058454212,-0.0250809698,-0.0040811252,-0.0129040861,0.0405170404,-0.0092555057,0.0103425299,0.075160363,0.03240399,0.0008723189,-0.0435596302,-0.0313621269,-0.0043244761,0.0177746599,0.0182202008,-0.0196037456,-0.017553762,0.0145755049,-0.0158526963,-0.0116214938,0.0236279601,0.0060358967,0.0215437763,0.0233749808,0.0013339844,0.0012867156,0.0469499891,-0.0159919926,-0.0292075315,-0.0365350775,0.0102297923,-0.032506769,-0.0030814259,0.0282870968,0.0029422031,-0.0162101152,0.018619115,-0.0174865777,-0.0018958628,0.0057523537,-0.0142547657,0.0023503818,-0.0258390659,0.0139441092,-0.0119652168,-0.0034745737,-0.0284335195,-0.0059411358,-0.0146551092,0.0020472461,-0.0054832002,-0.0079326308,0.0155340575,0.0421148168,0.0008548336]},"308":{"Abstract":"Modern virtual reality simulations require a constant high-frame rate from the rendering engine. They may also require very low latency and stereo images. Previous rendering engines for virtual reality applications have exploited spatial and temporal coherence by using image-warping to re-use previous frames or to render a stereo pair at lower cost than running the full render pipeline twice. However these previous approaches have shown artifacts or have not scaled well with image size. We present a new image-warping algorithm that has several novel contributions: an adaptive grid generation algorithm for proxy geometry for image warping; a low-pass hole-filling algorithm to address un-occlusion; and support for transparent surfaces by efficiently ray casting transparent fragments stored in per-pixel linked lists of an A-Buffer. We evaluate our algorithm with a variety of challenging test cases. The results show that it achieves better quality image-warping than state-of-the-art techniques and that it can support transparent surfaces effectively. Finally, we show that our algorithm can achieve image warping at rates suitable for practical use in a variety of applications on modern virtual reality equipment.","Authors":"A. Schollmeyer; S. Schneegans; S. Beck; A. Steed; B. Froehlich","DOI":"10.1109\/TVCG.2017.2657078","Keywords":"Image warping;stereoscopic rendering;transparency warping;A-buffer ray casting;image warping strategies;surface estimation quadtree","Title":"Efficient Hybrid Image Warping for High Frame-Rate Stereoscopic Rendering","Keywords_Processed":"buffer ray cast;image warping strategy;stereoscopic render;surface estimation quadtree;image warp;transparency warp","Keyword_Vector":[0.2285276858,0.0197559239,0.3749794587,-0.0498513755,-0.1947150247,-0.1162703672,-0.0626250853,-0.0012953883,0.1830324838,-0.3701821005,-0.1280911344,0.0741448988,0.1255352894,-0.0509784925,-0.1207449982,-0.1101369424,0.0797408227,-0.1967372442,-0.0680630523,-0.0695836896,0.047130723,-0.0240369105,0.0632422562,0.0059532509,-0.0800356668,0.0127058954,-0.0759527087,-0.2287110592,0.0352899623,-0.1464120039,-0.0732183621,-0.0758825774,0.0147394225,-0.0282861702,0.0461209304,0.0416492371,-0.0490830938,0.0526867629,0.0085857299,-0.1225862135,0.0244919436,-0.015233779,0.0558280789,0.0439390193,-0.0536432994,0.0003763598,0.0409261472,-0.0639639615,-0.0040757671,-0.0063209602,-0.0718293514,-0.0869887673],"Abstract_Vector":[0.1972502828,0.1602226707,0.2610472101,0.0146629981,-0.0530896315,0.005947121,0.0377535941,-0.1467559189,0.0141391634,0.0453562237,0.0005514997,-0.0308219172,0.0024993355,-0.0497278968,0.0111872717,0.0330891651,-0.0260928668,-0.0338802863,0.0036901111,0.0592588764,0.0097337591,-0.0148131282,0.0292193452,-0.0149277336,-0.0457226964,-0.0553503232,-0.0353511439,0.0004571934,-0.0209090515,0.0066297707,0.0074383561,-0.023475632,0.0330748188,0.0285087538,-0.0017054463,0.0006697681,0.0101070575,-0.008357772,0.0051096528,-0.0487830795,-0.000456958,0.0211147502,0.01877749,-0.0071495705,0.037457892,0.0352228828,-0.0193381162,0.0133801478,0.0195046265,0.0138798648,0.0334515543,0.0205604113,0.0156008172,0.0239370565,-0.017747243,-0.0184421537,0.0036423893,0.0107446603,0.0296349883,-0.0224445199,-0.0089326871,-0.001378981,-0.0101076341,-0.0034653017,0.0201560628,-0.0061434167,-0.0089620137,0.0388140151,-0.0124077852,-0.028054949,-0.0297836884,-0.0160477538,0.0253230858,-0.0238078129,-0.0048729848,-0.0154128695,0.0132328814,0.0131626114,0.0041547267,-0.0048320154,0.0187806099,0.020846143,-0.0040953326,-0.0207163705,-0.0066087805,-0.0067162886,-0.022622849,0.0160098009,0.0086319115,-0.002456565,-0.0205029518,-0.0048807459,-0.006308469,0.029208607,0.0042042457,0.0068659808,0.0033827438,-0.0227802997,-0.0154091569,-0.0049853647,0.0135970387,-0.0321123401,-0.0166906301,-0.0240029579,0.020338099,-0.0054180484,0.0073187614,0.0073434121,0.0001922529,-0.0015341064,0.0070035667,-0.0027338508,0.01713151,0.0199020094,-0.011391226,0.0123121685]},"309":{"Abstract":"Shading is a tedious process for artists involved in 2D cartoon and manga production given the volume of contents that the artists have to prepare regularly over tight schedule. While we can automate shading production with the presence of geometry, it is impractical for artists to model the geometry for every single drawing. In this work, we aim to automate shading generation by analyzing the local shapes, connections, and spatial arrangement of wrinkle strokes in a clean line drawing. By this, artists can focus more on the design rather than the tedious manual editing work, and experiment with different shading effects under different conditions. To achieve this, we have made three key technical contributions. First, we model five perceptual cues by exploring relevant psychological principles to estimate the local depth profile around strokes. Second, we formulate stroke interpretation as a global optimization model that simultaneously balances different interpretations suggested by the perceptual cues and minimizes the interpretation discrepancy. Lastly, we develop a wrinkle-aware inflation method to generate a height field for the surface to support the shading region computation. In particular, we enable the generation of two commonly-used shading styles: 3D-like soft shading and manga-style flat shading.","Authors":"P. K. Jayaraman; C. Fu; J. Zheng; X. Liu; T. Wong","DOI":"10.1109\/TVCG.2017.2705182","Keywords":"Shading;perception;inflation;manga;cartoon","Title":"Globally Consistent Wrinkle-Aware Shading of Line Drawings","Keywords_Processed":"inflation;cartoon;shade;manga;perception","Keyword_Vector":[0.0871781727,-0.1171094894,-0.1019295055,0.0586955623,-0.0558437382,-0.0976385756,0.0344760923,-0.0144890883,0.0124818323,0.0129736736,-0.0041027472,-0.0092671209,0.0012933577,-0.0564681719,0.0224354306,-0.0138055991,0.0027369159,-0.002862044,-0.012370216,-0.046505879,0.0291554657,0.0585090269,0.0598526109,0.0293657178,0.0152586056,-0.024466437,0.0455027102,-0.0039360287,-0.01728365,-0.0399592246,0.021160658,0.0353381666,0.0148756326,-0.0402033543,0.0202296024,0.001916479,0.001292814,-0.039496358,0.0307200447,0.003536154,-0.0220011731,0.0050887814,0.0106203508,-0.0293270218,0.0035473032,-0.0028676815,0.0174643235,0.0039523146,0.0142461929,-0.0191984367,0.0353927637,0.0161831523],"Abstract_Vector":[0.2471761837,-0.1626507763,0.0207619572,-0.0040444842,-0.0592087244,-0.0246640773,0.0189758976,0.0119377065,-0.099550833,0.0051200378,-0.0295305874,-0.0021618221,-0.0562448924,-0.0034208543,-0.0198641968,-0.0081593806,0.0108194754,-0.0207087381,-0.0301751055,0.0526810927,-0.0029821521,-0.0334545947,0.0236643303,0.0210017658,-0.0441457615,-0.0040278539,-0.0365015871,0.0258817442,-0.0539096538,-0.0731111871,0.0570017891,0.0105933241,-0.0571862613,0.0100655265,0.0013012426,-0.0418976933,-0.0408791415,-0.0103790023,0.0321858353,-0.012398486,-0.061067517,0.0115545021,0.0016409558,-0.032896085,0.0344599904,-0.0184280104,0.0248006098,0.050472011,-0.0223716493,0.0215622449,-0.0253094221,0.0173406614,-0.0187744123,0.080513466,-0.0096313003,0.0128498458,0.0671607454,0.0102229082,0.0448443844,-0.0251603657,0.0169719413,0.0225329893,0.0216620744,-0.0373528774,-0.0474523571,0.0111102259,-0.0196125805,0.0556493656,0.0599456298,-0.0208758551,0.0073717615,0.0719217693,0.05284706,0.0033462588,0.0317026536,0.0192009153,-0.0333937927,0.0088273892,0.0091304752,-0.0090487886,-0.0481835193,0.0296957932,0.0388953334,0.0387361627,0.0830974122,-0.0278888816,-0.012581017,0.0121753983,0.0022834069,0.0174534182,0.0429389453,-0.0298739414,-0.0276468289,0.0468207902,-0.0412023115,-0.015504317,0.0297814408,0.0131995997,-0.0125494959,0.0184414013,0.0461519155,-0.0311239143,-0.0478205963,0.0062404642,-0.0148792076,-0.0083624702,0.008461925,-0.0324306943,0.051726185,0.0278289047,0.0149180261,-0.015271934,-0.0657266041,-0.0445590915,0.0127670936,0.0369982132]},"31":{"Abstract":"Comparing multiple variables to select those that effectively characterize complex entities is important in a wide variety of domains - geodemographics for example. Identifying variables that correlate is a common practice to remove redundancy, but correlation varies across space, with scale and over time, and the frequently used global statistics hide potentially important differentiating local variation. For more comprehensive and robust insights into multivariate relations, these local correlations need to be assessed through various means of defining locality. We explore the geography of this issue, and use novel interactive visualization to identify interdependencies in multivariate data sets to support geographically informed multivariate analysis. We offer terminology for considering scale and locality, visual techniques for establishing the effects of scale on correlation and a theoretical framework through which variation in geographic correlation with scale and locality are addressed explicitly. Prototype software demonstrates how these contributions act together. These techniques enable multiple variables and their geographic characteristics to be considered concurrently as we extend visual parameter space analysis (vPSA) to the spatial domain. We find variable correlations to be sensitive to scale and geography to varying degrees in the context of energy-based geodemographics. This sensitivity depends upon the calculation of locality as well as the geographical and statistical structure of the variable.","Authors":"S. Goodwin; J. Dykes; A. Slingsby; C. Turkay","DOI":"10.1109\/TVCG.2015.2467199","Keywords":"Scale;Geography;Multivariate;Sensitivity Analysis;Variable Selection;Local Statistics;Geodemographics;Energy;Scale;Geography;Multivariate;Sensitivity Analysis;Variable Selection;Local Statistics;Geodemographics;Energy","Title":"Visualizing Multiple Variables Across Scale and Geography","Keywords_Processed":"Local Statistics;Geography;Multivariate;variable selection;geodemographic;scale;energy;Sensitivity Analysis","Keyword_Vector":[0.0748728058,-0.0477713448,-0.0479235768,-0.0186497696,-0.0185373296,-0.1218092849,-0.0597223415,-0.05233516,-0.0840665783,0.0564740844,-0.0814398172,0.0936884247,-0.0375315178,0.0285251471,-0.1120342913,-0.0131261906,-0.124051796,0.0606167792,-0.0942721022,-0.0669387794,0.100208103,0.0887584288,0.0267409929,-0.0003331071,-0.1085636449,0.0979212349,0.0333929226,0.0882194217,-0.0317240395,-0.011179891,-0.0664514394,-0.036693065,-0.1116880153,-0.0410913941,0.0260359594,-0.1575464407,0.0475794653,0.0179367116,-0.0587127266,0.0983952193,0.0861023522,-0.0113688551,0.0550396515,0.144521997,0.0106398084,0.0011046708,0.0542649878,-0.0476533438,0.0246893934,0.0420865899,-0.0578712489,-0.0036110698],"Abstract_Vector":[0.1466532498,-0.0515687865,0.0100484752,-0.0099271484,-0.05997759,-0.0230699929,0.01221434,0.0124271885,-0.0673967492,-0.0611202056,0.0086893996,0.017026848,-0.0138992885,-0.0076500092,0.0080036733,-0.0230191697,0.0176454683,0.0317647799,-0.0232242711,-0.0245884645,0.0122378285,0.0156351733,-0.0002560221,0.017177814,-0.0365414891,0.014788087,0.0232243669,-0.0194924642,-0.0277826753,-0.0572383578,-0.0016139412,-0.0100599555,-0.0144778325,-0.0189494278,-0.0058953774,-0.0127167219,0.0183459692,0.0040815258,-0.0175835798,0.0204501016,-0.035575832,-0.0108292262,0.0579466328,-0.0387595822,0.0268526723,0.0344270014,-0.0216372682,-0.0190291644,-0.0008246249,0.0034251429,0.0199901442,0.0262433353,-0.0017869336,-0.0122447349,0.0017471548,0.0077108543,-0.0268778415,0.0003905158,-0.0058617581,0.0144552587,0.0016682524,-0.0249211975,0.0097802375,0.0188147494,0.0102492306,-0.007920538,0.0029326867,0.0183390128,-0.0326585574,0.0179233961,0.0152147438,-0.0177646145,0.0309712589,0.0230474937,0.0332380628,0.0261475302,-0.0206380684,0.0304732035,-0.0173745034,0.0525645437,0.0017476686,-0.0284945324,0.0113681965,-0.0562277904,-0.0119332869,-0.0305437185,-0.0195618477,0.0400340544,0.0032802387,0.0224294764,0.0256787041,-0.0204371849,-0.0195915129,0.0207032665,0.0343725698,-0.0131000129,0.0634326542,-0.004914519,-0.0050761563,0.0007756548,0.0014818462,-0.0278185189,-0.0020607147,0.0044178992,0.0158435398,0.0031272956,0.0201463033,-0.0308977436,0.0046093575,0.0246838489,0.0572569386,0.0306430992,0.0090477736,-0.0246519915,-0.0123817343,-0.0026540379]},"310":{"Abstract":"General visualization tools typically require manual specification of views: analysts must select data variables and then choose which transformations and visual encodings to apply. These decisions often involve both domain and visualization design expertise, and may impose a tedious specification process that impedes exploration. In this paper, we seek to complement manual chart construction with interactive navigation of a gallery of automatically-generated visualizations. We contribute Voyager, a mixed-initiative system that supports faceted browsing of recommended charts chosen according to statistical and perceptual measures. We describe Voyager's architecture, motivating design principles, and methods for generating and interacting with visualization recommendations. In a study comparing Voyager to a manual visualization specification tool, we find that Voyager facilitates exploration of previously unseen data and leads to increased data variable coverage. We then distill design implications for visualization tools, in particular the need to balance rapid exploration and targeted question-answering.","Authors":"K. Wongsuphasawat; D. Moritz; A. Anand; J. Mackinlay; B. Howe; J. Heer","DOI":"10.1109\/TVCG.2015.2467191","Keywords":"User interfaces;information visualization;exploratory analysis;visualization recommendation;mixed-initiative systems;User interfaces;information visualization;exploratory analysis;visualization recommendation;mixed-initiative systems","Title":"Voyager: Exploratory Analysis via Faceted Browsing of Visualization Recommendations","Keywords_Processed":"visualization recommendation;exploratory analysis;mixed initiative system;user interface;information visualization","Keyword_Vector":[0.2121225903,-0.1512590196,-0.0385102865,0.2513351816,-0.1099214454,0.3749358495,-0.0261419697,-0.1125285051,-0.1049718662,0.0702285573,0.0440932785,-0.0254843952,0.1048065664,0.0462170675,-0.0748854757,-0.0964196301,-0.0412940268,0.0295270938,0.030815895,-0.0968318291,-0.0702305256,-0.0790516001,0.0444351516,-0.0549147378,-0.0093358118,-0.0504605253,-0.0895625674,0.0488630255,0.033583729,0.067400137,-0.0368694327,-0.051284978,0.0459190611,-0.0429781932,0.0605938653,-0.0361627436,-0.0276117855,-0.0381915883,-0.062877046,-0.0268605989,0.0275283563,-0.0391398993,0.0115025023,-0.0148093104,0.032922022,0.0300910098,0.0232348383,-0.0126584967,0.0264037995,0.0134155598,-0.0192297797,-0.0379783365],"Abstract_Vector":[0.202661487,0.0028498709,0.0718946005,0.0274048731,0.0528604966,-0.0571163029,-0.0820334958,-0.0564534165,-0.0176723865,-0.0512593513,-0.1002234878,-0.0068534562,-0.0786127609,-0.1164014475,0.0599226617,0.0007482655,0.0511798062,-0.0173627429,0.0276430277,-0.0468312969,-0.0612515241,-0.051677618,0.0441386146,0.0032849542,-0.0241021603,0.0816766603,-0.0551547165,-0.0258392404,-0.0012337645,-0.0272553243,-0.0399239959,0.0729255796,0.0587892447,0.015198472,-0.0276945378,-0.0084051006,-0.0670081257,0.1006827914,0.0259578385,-0.0151027133,-0.0150573414,-0.029271344,-0.0267551777,0.015816801,0.0007949574,-0.0087769955,0.0709781905,-0.001572601,0.0076340986,-0.0026099876,-0.0640769745,0.075965725,-0.0953727531,-0.0030276803,-0.0194026202,0.0402170083,0.0066115884,-0.0222784752,-0.002394217,0.0261551596,-0.0232782585,-0.0157664474,0.0164255868,-0.0225646492,0.0230738588,-0.0108789624,-0.0150738378,-0.0086995654,-0.02691739,0.0248376394,0.0329447441,-0.0103148641,-0.0031358833,0.0018746421,0.0390408643,0.0161085385,-0.0400259013,0.0419531658,-0.016124708,-0.011088792,0.0124769319,-0.0167199109,-0.0376331641,0.0487827892,-0.0228977003,-0.0167560102,-0.0091844401,0.0138345528,0.0033491255,-0.0121201447,0.0009517305,-0.0310840357,0.0211914447,-0.0269329421,0.0461405167,0.0297958323,-0.0278791691,0.0276320636,0.0015565604,0.0815589234,-0.0263733737,-0.0227476995,-0.0313167171,-0.024124955,0.0340155675,0.0234624271,-0.0080453383,0.0083792443,-0.0017918054,-0.0332442959,-0.0153038408,0.0009304376,-0.0243395208,-0.0194003595,-0.0197296271,0.0536272057]},"311":{"Abstract":"We report on the design and results of an experiment investigating factors influencing Slater's Plausibility Illusion (Psi) in virtual environments (VEs). Slater proposed Psi and Place Illusion (PI) as orthogonal components of virtual experience which contribute to realistic response in a VE. PI corresponds to the traditional conception of presence as \u201cbeing there,\u201d so there exists a substantial body of previous research relating to PI, but very little relating to Psi. We developed this experiment to investigate the components of plausibility illusion using subjective matching techniques similar to those used in color science. Twenty-one participants each experienced a scenario with the highest level of coherence (the extent to which a scenario matches user expectations and is internally consistent), then in eight different trials chose transitions from lower-coherence to higher-coherence scenarios with the goal of matching the level of Psi they felt in the highest-coherence scenario. At each transition, participants could change one of the following coherence characteristics: the behavior of the other virtual humans in the environment, the behavior of their own body, the physical behavior of objects, or the appearance of the environment. Participants tended to choose improvements to the virtual body before any other improvements. This indicates that having an accurate and well-behaved representation of oneself in the virtual environment is the most important contributing factor to Psi. This study is the first to our knowledge to focus specifically on coherence factors in virtual environments.","Authors":"R. Skarbez; S. Neyret; F. P. Brooks; M. Slater; M. C. Whitton","DOI":"10.1109\/TVCG.2017.2657158","Keywords":"Virtual reality;virtual environments;presence;place illusion;plausibility illusion;immersion;coherence;psychophysics;user studies","Title":"A Psychophysical Experiment Regarding Components of the Plausibility Illusion","Keywords_Processed":"user study;virtual reality;presence;coherence;place illusion;virtual environment;psychophysic;immersion;plausibility illusion","Keyword_Vector":[0.0627039168,-0.0107740101,-0.0035784145,-0.0105306135,0.0204481854,-0.0587907568,-0.0665272371,-0.0607212959,-0.0954373844,0.0440939308,-0.0434036075,0.0456854596,0.05343488,0.045161198,-0.0207534196,-0.0783351006,-0.0378723335,0.029572445,-0.0695319804,-0.0698284649,0.0797186229,0.0378301947,0.1224269683,0.018613769,0.0119103898,-0.0077786164,0.1439313493,0.0349704285,-0.0037553711,-0.0138058888,-0.0177700765,-0.0088012838,-0.0433276357,-0.0166992445,0.0748752045,0.0458257944,0.0741454492,-0.0350820248,-0.0078215202,0.0502770066,0.0036470475,-0.0584071936,-0.0400934104,0.0049889986,-0.0131146941,-0.0392539537,-0.0290223721,0.0522430587,0.0098992932,0.0037991356,0.0342323996,0.0393345839],"Abstract_Vector":[0.2110229369,0.0334829678,0.0669469173,0.0626790472,-0.1009879088,0.0336355588,-0.1369320548,-0.0419020515,0.0131532237,-0.0524992929,-0.0780497977,-0.0034180194,-0.0993277426,-0.0452438859,0.0425890855,0.027241166,0.0081206059,-0.0351803361,0.0551293325,-0.0944291129,-0.0126471874,-0.0450833826,0.0790140174,-0.0228657731,-0.0125058287,0.0431934515,-0.0081870955,0.0220599259,-0.0512001692,0.0317663875,0.0076609488,0.0289493247,0.092437806,0.056970721,-0.0387387825,-0.0010935426,0.0154063208,0.0430347942,0.0370869073,0.0604153453,0.0492982702,0.0759307109,-0.0285934832,0.0018494246,-0.0372174197,-0.0322440518,0.0452380898,0.0841616039,0.0084512223,-0.0334857724,-0.0596885731,0.020616233,-0.0021933558,-0.0164199232,0.0303913398,-0.0140470066,0.0464071792,0.015832182,-0.0132050155,0.0811455827,-0.0016736634,0.0409116829,0.0344120355,-0.0684345538,-0.0008571253,-0.0360364326,0.0392164492,-0.0673782776,0.0730867402,0.1454086717,-0.0138215578,0.0252688553,0.032812215,0.0353376905,0.0480360203,0.0299148612,-0.0927345186,0.018142598,0.0264190435,0.0335859459,-0.035000975,-0.115267137,0.0063316389,0.056482274,0.0414568312,-0.0487676375,0.0008576819,-0.0135396906,-0.0235493639,-0.0295045201,-0.011719076,-0.0250076114,0.022535031,0.0182998633,0.0300483221,-0.0581547302,-0.0288213495,0.0515290208,0.0018142077,0.0288950639,0.008714134,0.0221740474,0.0664941089,0.0277664834,0.0417489775,-0.0374946005,-0.0000649575,0.0116885693,0.0007746571,-0.0499853678,-0.0303230704,0.0193990808,-0.0621665,-0.021695025,-0.0243300897,0.0215142564]},"312":{"Abstract":"Learning more about people mobility is an important task for official decision makers and urban planners. Mobility data sets characterize the variation of the presence of people in different places over time as well as movements (or flows) of people between the places. The analysis of mobility data is challenging due to the need to analyze and compare spatial situations (i.e., presence and flows of people at certain time moments) and to gain an understanding of the spatio-temporal changes (variations of situations over time). Traditional flow visualizations usually fail due to massive clutter. Modern approaches offer limited support for investigating the complex variation of the movements over longer time periods. We propose a visual analytics methodology that solves these issues by combined spatial and temporal simplifications. We have developed a graph-based method, called MobilityGraphs, which reveals movement patterns that were occluded in flow maps. Our method enables the visual representation of the spatio-temporal variation of movements for long time series of spatial situations originally containing a large number of intersecting flows. The interactive system supports data exploration from various perspectives and at various levels of detail by interactive setting of clustering parameters. The feasibility our approach was tested on aggregated mobility data derived from a set of geolocated Twitter posts within the Greater London city area and mobile phone call data records in Abidjan, Ivory Coast. We could show that MobilityGraphs support the identification of regular daily and weekly movement patterns of resident population.","Authors":"T. von Landesberger; F. Brodkorb; P. Roskosch; N. Andrienko; G. Andrienko; A. Kerren","DOI":"10.1109\/TVCG.2015.2468111","Keywords":"Visual analytics;movement data;networks;graphs;temporal aggregation;spatial aggregation;flows;clustering;Visual analytics;movement data;networks;graphs;temporal aggregation;spatial aggregation;flows;clustering","Title":"MobilityGraphs: Visual Analysis of Mass Mobility Dynamics via Spatio-Temporal Graphs and Clustering","Keywords_Processed":"spatial aggregation;flow;clustering;graph;movement datum;network;temporal aggregation;visual analytic","Keyword_Vector":[0.06148767,0.0103162691,-0.0176891305,0.0431365766,0.0367806487,-0.0420934866,-0.0195431683,-0.0405081764,-0.0316670395,-0.0161790687,-0.0009853646,0.0340191842,-0.0315726769,0.0882477524,-0.0181193677,-0.0152747125,-0.0431902243,-0.0023013366,0.0360686109,0.019692731,0.0391571228,0.043370316,-0.1568033428,0.0523035173,0.0412564152,-0.0658494457,-0.1124277143,-0.0323581825,0.0346571195,-0.00950142,0.0388045168,-0.0513214557,-0.043333082,0.0729029914,0.0052820329,0.1178982776,0.0064259483,-0.03145789,0.0070295524,0.0433736927,0.0172666642,0.0527707933,0.01019005,-0.0441223825,-0.034885847,-0.0300493277,0.0492712397,0.0398143001,0.0503378681,0.0194872896,-0.0023530962,-0.0438007217],"Abstract_Vector":[0.1423260351,0.0264379452,0.0083006346,0.0111192786,-0.0340137085,0.0035517099,0.0015373669,0.0073528973,-0.0108997915,0.004407475,-0.0045972049,-0.0055559635,-0.0162847679,-0.0190033009,-0.0047894074,0.0405922168,0.0362226322,0.0514066029,0.0321978634,-0.061073632,0.0199850666,-0.0083779165,0.0168262856,0.0153995892,0.0247582285,-0.0020936742,0.026681899,0.0100097049,0.0116111066,0.0049077092,0.0116754167,0.0212619699,-0.0183430737,0.0185792561,0.0262037442,-0.0237871569,-0.0202597634,0.0077563453,-0.0423078474,-0.0077093194,0.0077054868,-0.0048450799,0.011163182,-0.0082844393,-0.007622642,-0.0250092031,0.0468578054,0.0500977792,0.0030919505,-0.0007826539,0.0113487091,-0.0346282216,0.039575558,-0.0099958839,0.0116834293,0.0366999761,0.0081568686,-0.0006085581,0.0207767167,0.0032320362,0.0516514823,-0.0353234106,0.020521444,-0.0094704687,0.0177477554,-0.0039310316,0.0139941437,-0.0022867438,0.0168380276,0.0018364607,-0.0080523576,0.040613513,-0.0179587425,-0.0058986357,0.0197528304,0.0053455646,-0.0070769312,-0.00079208,-0.0022684388,0.0101699862,-0.0076742446,0.0572069014,0.0227918456,0.0104568929,-0.0256263247,0.0054190459,0.0025163288,0.0017941633,0.0030761815,0.0025291396,0.021281899,0.0040010778,0.0228695568,-0.0146619148,-0.0118227103,-0.0138712482,0.0210021543,0.0088003094,0.0219263591,-0.0092554937,-0.0111505748,0.0119624194,0.0025559515,-0.0247000008,0.0256033233,-0.0352109462,-0.0161262135,0.0175774783,0.0371720096,0.0406323777,-0.0024166954,-0.0096229032,-0.0040237373,-0.0294176682,-0.0128279129,-0.0103520836]},"313":{"Abstract":"In the social psychology literature, crowds are classified as audiences and mobs. Audiences are passive crowds, whereas mobs are active crowds with emotional, irrational and seemingly homogeneous behavior. In this study, we aim to create a system that enables the specification of different crowd types ranging from audiences to mobs. In order to achieve this goal we parametrize the common properties of mobs to create collective misbehavior. Because mobs are characterized by emotionality, we describe a framework that associates psychological components with individual agents comprising a crowd and yields emergent behaviors in the crowd as a whole. To explore the effectiveness of our framework we demonstrate two scenarios simulating the behavior of distinct mob types.","Authors":"F. Durup\u0131nar; U. G\u00fcd\u00fckbay; A. Aman; N. I. Badler","DOI":"10.1109\/TVCG.2015.2501801","Keywords":"Crowd simulation;autonomous agents;simulation of affect;crowd taxonomy;mob behavior;OCEAN personality model;OCC model;PAD model","Title":"Psychological Parameters for Crowd Simulation: From Audiences to Mobs","Keywords_Processed":"crowd simulation;OCC model;pad model;ocean personality model;crowd taxonomy;autonomous agent;mob behavior;simulation of affect","Keyword_Vector":[0.0150237451,-0.0088784655,-0.0101852886,-0.011165978,0.0120383535,-0.0208772533,0.0056066844,-0.0008949984,-0.001417171,0.0104970097,0.0002603594,0.018467539,0.0052278334,0.0145817003,-0.0056157313,-0.0056149797,-0.0084382519,-0.0095557921,0.0146506181,0.0263988875,0.0313076836,0.028708361,0.0093296288,0.0087024241,0.0002864247,-0.006394195,0.0012607387,-0.0081001665,-0.0061969916,-0.0127284326,0.0030832138,-0.016720472,-0.0263887927,-0.0159953204,-0.0069954115,-0.0227027658,0.0232939294,-0.0328086655,-0.0021491045,-0.0000281889,-0.0092963763,-0.0164246373,0.0056832715,-0.0101329038,-0.0148714845,-0.0240974731,0.0245607173,0.0020424844,0.011395414,0.0101530626,-0.026311588,0.0556466199],"Abstract_Vector":[0.2028231958,0.0742059662,0.0051440993,0.0109850522,-0.0202183405,0.0704956598,-0.0281007633,0.0376794431,0.0395682648,-0.052929123,-0.0226628485,-0.0100103931,-0.1012729097,-0.0494697367,-0.0493426872,-0.0949572313,-0.0505702847,0.0414390429,0.0059222946,-0.0331209853,-0.0508820247,0.0907595376,-0.0421505294,0.0208748153,0.0389828188,-0.0139388835,0.0111559581,0.0153902192,0.0005815564,-0.0388220267,0.0028748768,-0.0202218251,-0.0218501924,0.0069965176,-0.0209960048,-0.0203404085,-0.0158602771,-0.035203363,0.0304725591,-0.0014081686,0.0177479666,0.0171653463,-0.0032958128,-0.0148469134,-0.0132699286,-0.0195754556,0.0408492547,0.0286330864,-0.0055441962,-0.0030132717,-0.0116986262,-0.0223634232,-0.0019164697,-0.0321422866,-0.0236798624,-0.034521732,-0.019406579,-0.0311434556,-0.0009555436,-0.0346892135,0.0337123725,-0.0157328518,0.0093473275,0.016705505,0.0011057467,-0.003127951,0.0055600532,0.0046064834,-0.0070168318,0.0038047817,-0.0063871217,-0.014977291,-0.0251322585,0.0511817748,0.0367009769,-0.003063307,0.0138970538,0.0119652924,-0.0075653289,0.0144630108,0.0418497492,-0.0103730486,0.0034244467,-0.0147265904,0.0201738039,-0.0106984724,-0.0010986484,0.0423507151,0.0264547024,-0.055284987,0.0219786495,-0.0231726101,-0.0092820535,0.0179711396,0.0201831741,-0.0287755341,-0.0004845806,0.0280136579,0.0054312948,-0.014052841,-0.0266352036,0.0189399447,0.0066699471,0.0017527216,0.0077734771,0.0145405386,0.0028819951,-0.0168631222,0.0268204624,-0.0344369283,-0.0172754373,0.0200196964,0.0237179778,0.0183525151,0.0081686391,-0.0204495211]},"314":{"Abstract":"Financial institutions are interested in ensuring security and quality for their customers. Banks, for instance, need to identify and stop harmful transactions in a timely manner. In order to detect fraudulent operations, data mining techniques and customer profile analysis are commonly used. However, these approaches are not supported by Visual Analytics techniques yet. Visual Analytics techniques have potential to considerably enhance the knowledge discovery process and increase the detection and prediction accuracy of financial fraud detection systems. Thus, we propose EVA, a Visual Analytics approach for supporting fraud investigation, fine-tuning fraud detection algorithms, and thus, reducing false positive alarms.","Authors":"R. A. Leite; T. Gschwandtner; S. Miksch; S. Kriglstein; M. Pohl; E. Gstrein; J. Kuntner","DOI":"10.1109\/TVCG.2017.2744758","Keywords":"Visual Knowledge Discovery;Time Series Data;Business and Finance Visualization;Financial Fraud Detection","Title":"EVA: Visual Analytics to Identify Fraudulent Events","Keywords_Processed":"business and Finance visualization;financial Fraud Detection;visual Knowledge Discovery;Time Series Data","Keyword_Vector":[0.0420496099,-0.0236531355,0.0156153898,-0.0085817951,-0.0207470679,-0.0322865065,-0.0441762864,-0.0146710741,-0.0177883169,0.008580813,-0.0391646542,0.1133658552,0.0109183137,-0.038844501,-0.0024528158,-0.0003406774,0.0877708728,-0.0618783603,0.0090357309,0.0301743295,-0.0217687146,-0.0573241129,-0.0353070156,0.0056165233,0.0303304204,0.0061132906,-0.0199806989,-0.0352458838,-0.0227584714,0.0197994482,-0.0037567766,0.017980402,0.0460687739,-0.0186291812,-0.0444355799,-0.0259252804,0.0101587588,-0.004382819,0.019768045,0.0514473581,-0.0199427319,0.017209831,-0.0066727683,0.0278048962,-0.0357858269,-0.0037230032,-0.0097186153,0.006561688,0.0423672368,0.0512107848,0.0161978674,0.0441194942],"Abstract_Vector":[0.1295530124,0.0843292312,0.0912682681,0.0027525495,0.0113679442,-0.0170610412,0.1165761878,-0.0159183225,-0.0048990303,-0.0264567154,-0.0053750105,-0.0094134492,-0.0291876558,-0.027138319,-0.0293424773,0.0408178646,-0.0827549378,0.0730492674,0.0162632516,-0.0417434086,0.0163781521,-0.1057707309,0.0261453398,-0.0074313451,0.0961372249,0.0511380115,-0.0127279897,0.0004624717,0.0359202531,-0.011863777,0.0148394702,0.046711376,-0.0908789851,0.1186530611,0.0869289561,0.0585719087,0.1313664581,-0.0430437669,-0.0488549548,-0.0393439563,-0.0378723503,-0.0437702471,0.0005568665,0.0309246456,-0.0458814244,0.0502932114,0.1129938628,-0.0260619387,0.0045519451,0.0088580692,0.0171046446,-0.0539868584,0.1001437602,0.0158545041,-0.014942293,0.0843560695,-0.0076279237,-0.0190415023,0.0259484584,0.0796667164,-0.0539659657,0.0039437484,-0.0302302222,0.0724324646,-0.0429847103,0.0162280968,0.0430330194,-0.0214641177,0.0110041946,0.016267922,0.0953074195,0.0234638175,0.0525910056,-0.0213838826,0.032026878,0.0279764416,-0.0186014947,0.0421261153,0.0016393922,0.0094355899,-0.0178684307,-0.0051034947,-0.0157407238,-0.0152063727,0.0270436032,0.002314426,0.0463402321,-0.0104372057,0.0214814591,0.0126526067,0.0139087252,0.0276947612,-0.0001073395,-0.0015349218,0.0252400487,-0.0203187101,0.0154444412,0.0052621761,0.0196689147,0.0314439382,0.0139613806,0.0573765146,0.0283492473,0.0378802089,0.032762496,0.0026121192,-0.0136342617,-0.0091533383,-0.0081630572,0.0090746915,0.0158333425,-0.0180935722,-0.0030492898,0.0155287486,0.0115305185,0.0132595809]},"315":{"Abstract":"This paper presents a novel immersive system called MR360 that provides interactive mixed reality (MR) experiences using a conventional low dynamic range (LDR) 360\u00b0 panoramic video (360-video) shown in head mounted displays (HMDs). MR360 seamlessly composites 3D virtual objects into a live 360-video using the input panoramic video as the lighting source to illuminate the virtual objects. Image based lighting (IBL) is perceptually optimized to provide fast and believable results using the LDR 360-video as the lighting source. Regions of most salient lights in the input panoramic video are detected to optimize the number of lights used to cast perceptible shadows. Then, the areas of the detected lights adjust the penumbra of the shadow to provide realistic soft shadows. Finally, our real-time differential rendering synthesizes illumination of the virtual 3D objects into the 360-video. MR360 provides the illusion of interacting with objects in a video, which are actually 3D virtual objects seamlessly composited into the background of the 360-video. MR360 was implemented in a commercial game engine and tested using various 360-videos. Since our MR360 pipeline does not require any pre-computation, it can synthesize an interactive MR scene using a live 360-video stream while providing realistic high performance rendering suitable for HMDs.","Authors":"T. Rhee; L. Petikam; B. Allen; A. Chalmers","DOI":"10.1109\/TVCG.2017.2657178","Keywords":"Mixed reality rendering;image based lighting;image based shadowing;360\u00b0 panoramic video","Title":"MR360: Mixed Reality Rendering for 360\u00b0 Panoramic Videos","Keywords_Processed":"image base shadowing;image base lighting;mixed reality render;360 panoramic video","Keyword_Vector":[0.0291216226,0.0221424006,0.0437652045,0.0459586426,0.0427065807,-0.0488970693,-0.0145780265,0.0135661452,-0.0155958539,0.000803907,0.0321614792,0.0018496585,0.0444894066,0.0171330946,0.018923506,0.0014579634,-0.0002084582,-0.0187942669,-0.047541716,-0.0050057727,0.0222630529,-0.0127782875,0.0533922361,-0.0521902116,0.0021130125,-0.0097980027,-0.0309206037,0.0104152116,-0.0114360646,0.0250240014,-0.0289466477,-0.0253045336,-0.0138338647,0.0045546752,-0.0284062497,0.0419841563,-0.0183832821,-0.0058421097,0.0034801029,0.0469515359,-0.0291729259,0.0413111978,0.0669262549,-0.0045569129,0.0539101308,0.0358820668,-0.0114237669,0.0265283968,0.0402117417,-0.0637912933,-0.0310286238,0.0076461893],"Abstract_Vector":[0.1915367273,-0.0069760882,0.0428233761,0.0172756298,0.0215134592,-0.0492497355,-0.062535449,-0.0129361524,0.0375557515,-0.0458201892,0.0049768662,0.0308362647,-0.0086498374,-0.016626018,0.0229933853,0.004993945,-0.063349411,-0.0617291879,-0.0383216525,-0.0886482826,-0.0642761277,0.0195914886,-0.027720196,0.0700870993,-0.0086230562,-0.0061433316,0.0737414014,0.038444086,0.0299336175,-0.1012171827,0.0306126166,-0.0420081068,-0.0595326535,0.0306872895,-0.0489170422,0.0613096261,0.0317663623,-0.1149657435,0.0142760857,0.0085163554,0.00717555,0.0606517066,0.0291342261,-0.0178612556,-0.018409159,-0.040012731,0.0528033375,-0.0068685076,-0.01266371,-0.025193576,0.0465915212,0.0233728977,-0.0201366679,-0.0190216099,0.040297571,-0.0083592237,-0.0502751354,-0.0191929953,0.0664880991,-0.0335046896,0.011035224,0.0421488943,0.0161031145,-0.0323306856,0.0734257361,-0.0334665545,-0.0068434967,-0.1120693384,0.0495411823,0.0100875355,0.0588081503,0.0139165147,-0.0329435453,0.056709814,0.0011633734,0.0251500608,0.0329867161,-0.0733444723,-0.0970285454,-0.0075300819,-0.0028048902,-0.0147123608,-0.023350942,0.0124512898,0.0304973091,0.0239155727,-0.0311435081,-0.0369207155,-0.0245964903,-0.044852928,-0.0026595749,0.0335971349,-0.0175996018,0.0200690764,-0.0286294331,-0.0200724721,0.0048979509,0.0098226642,0.0067195679,-0.0105917706,-0.0072387219,-0.0447495237,0.0597652291,-0.0136049825,-0.0043851002,-0.0945027682,-0.0099652392,-0.0438335381,-0.0196661564,-0.0138967053,0.0254122281,-0.0311940685,0.0727133018,0.0856092931,0.0109018614,0.015052852]},"316":{"Abstract":"Despite the widely recognized importance of symmetric second order tensor fields in medicine and engineering, the visualization of data uncertainty in tensor fields is still in its infancy. A recently proposed tensorial normal distribution, involving a fourth order covariance tensor, provides a mathematical description of how different aspects of the tensor field, such as trace, anisotropy, or orientation, vary and covary at each point. However, this wealth of information is far too rich for a human analyst to take in at a single glance, and no suitable visualization tools are available. We propose a novel approach that facilitates visual analysis of tensor covariance at multiple levels of detail. We start with a visual abstraction that uses slice views and direct volume rendering to indicate large-scale changes in the covariance structure, and locations with high overall variance. We then provide tools for interactive exploration, making it possible to drill down into different types of variability, such as in shape or orientation. Finally, we allow the analyst to focus on specific locations of the field, and provide tensor glyph animations and overlays that intuitively depict confidence intervals at those points. Our system is demonstrated by investigating the effects of measurement noise on diffusion tensor MRI, and by analyzing two ensembles of stress tensor fields from solid mechanics.","Authors":"A. Abbasloo; V. Wiens; M. Hermann; T. Schultz","DOI":"10.1109\/TVCG.2015.2467031","Keywords":"Uncertainty visualization;tensor visualization;direct volume rendering;interaction;glyph based visualization;Uncertainty visualization;tensor visualization;direct volume rendering;interaction;glyph based visualization","Title":"Visualizing Tensor Normal Distributions at Multiple Levels of Detail","Keywords_Processed":"glyph base visualization;tensor visualization;interaction;uncertainty visualization;direct volume render","Keyword_Vector":[0.1635510292,-0.0513440957,0.0573621709,-0.0421217448,0.0071600767,-0.0530094881,-0.0134744515,-0.0202296882,-0.0666633269,0.0178156314,-0.0358071481,0.1000035282,0.0172751283,-0.095477734,-0.0481740577,-0.0520339679,0.0793531065,-0.0199172813,0.1030072191,0.0427469194,-0.0951022455,-0.0550438351,-0.1373509785,-0.0020563507,0.0601764609,0.0130769002,-0.0222568087,-0.0129553026,0.0042618875,0.1053765161,-0.0898418129,-0.0814588201,-0.0040342588,0.0900755262,0.2028486641,0.0007882653,-0.0523164198,-0.0162974207,0.016792633,0.0347436286,0.0584382902,-0.0562592363,-0.0632331313,0.0224390927,0.0388436619,-0.0599579375,-0.0538578108,0.0240233244,0.1752248154,-0.0653568614,0.0324632683,0.1030763606],"Abstract_Vector":[0.1691856078,-0.0059293546,-0.0563340761,-0.0334361068,-0.0057533242,0.0396340777,-0.0108160454,-0.0192828097,-0.0518180981,0.0378070751,0.0034330462,-0.000572247,0.011473597,-0.0051541661,-0.0266194474,0.0564398289,0.0250127615,0.0132431675,0.0540311421,0.0406382329,-0.0217755344,0.0100258288,-0.0242510036,0.0138529995,-0.0262155493,0.0172646726,-0.0109598506,-0.0114757184,0.0376871502,-0.0320073973,-0.008050705,0.0182279844,-0.0444522927,0.0115293579,-0.0091611994,-0.0180210332,-0.004945911,0.0007642746,-0.0212834296,-0.0157888254,-0.0425006486,0.0059688321,0.0195103392,-0.0354738495,0.0274007845,-0.0048438957,-0.0007655461,-0.0261614008,0.0094016111,-0.0178594099,0.004556469,0.0448856617,-0.0240487027,0.0356031458,-0.0051291148,0.0492207532,-0.0115387115,0.0296783183,0.01530658,-0.0429691772,0.0117492056,0.005733379,-0.0075678862,-0.007646339,0.0021556357,-0.0013910139,0.0543796275,0.0148699714,0.0212805362,0.0615569812,0.0232721587,0.0102253601,-0.0257753414,-0.0385774855,-0.0162524041,-0.007624681,0.0205768524,0.0392796562,0.0143310867,0.0124268028,0.0360927461,-0.016576723,-0.0119379373,-0.0080976985,-0.0235198138,0.0443436353,0.0200763378,-0.0330722512,-0.002637421,0.0418541514,-0.0104873274,0.010165903,-0.0031230939,0.018388111,-0.0201184325,-0.0254459515,0.048059438,0.0199348319,0.0019590533,0.0179701413,-0.0026089038,0.0066623106,0.0077283461,0.021498668,-0.0397969161,-0.0042307715,-0.0227471608,0.0250923266,0.0425104068,-0.0192014077,-0.025624946,-0.020747854,-0.0034708811,-0.0048731718,0.0173202529,-0.007248106]},"317":{"Abstract":"Multi-view deep neural network is perhaps the most successful approach in 3D shape classification. However, the fusion of multi-view features based on max or average pooling lacks a view selection mechanism, limiting its application in, e.g., multi-view active object recognition by a robot. This paper presents VERAM, a view-enhanced recurrent attention model capable of actively selecting a sequence of views for highly accurate 3D shape classification. VERAM addresses an important issue commonly found in existing attention-based models, i.e., the unbalanced training of the subnetworks corresponding to next view estimation and shape classification. The classification subnetwork is easily overfitted while the view estimation one is usually poorly trained, leading to a suboptimal classification performance. This is surmounted by three essential view-enhancement strategies: 1) enhancing the information flow of gradient backpropagation for the view estimation subnetwork, 2) devising a highly informative reward function for the reinforcement training of view estimation and 3) formulating a novel loss function that explicitly circumvents view duplication. Taking grayscale image as input and AlexNet as CNN architecture, VERAM with 9 views achieves instance-level and class-level accuracy of 95.5 and 95.3 percent on ModelNet10, 93.7 and 92.1 percent on ModelNet40, both are the state-of-the-art performance under the same number of views.","Authors":"S. Chen; L. Zheng; Y. Zhang; Z. Sun; K. Xu","DOI":"10.1109\/TVCG.2018.2866793","Keywords":"3D shape classification;multi-view 3D shape recognition;visual attention model;recurrent neural network;reinforcement learning;convolutional neural network","Title":"VERAM: View-Enhanced Recurrent Attention Model for 3D Shape Classification","Keywords_Processed":"visual attention model;recurrent neural network;3d shape classification;convolutional neural network;reinforcement learning;multi view 3d shape recognition","Keyword_Vector":[0.0497168865,0.0372002437,0.0199290969,0.0498509571,0.0387081977,-0.0338557742,-0.0114132817,-0.0866602458,-0.0331193179,-0.0262964877,0.0275055755,0.0283045516,-0.0824573525,0.0628896568,0.0947435989,-0.0863343262,-0.0457171442,-0.0528659335,0.0138853744,-0.029013706,-0.0338585596,-0.0342561338,-0.0178151509,0.0189513938,0.0275057265,-0.006255405,0.1147618076,0.0171917031,-0.0565334949,0.0014621224,-0.0451099013,0.0245180252,-0.0310277464,-0.0657912615,0.0111207466,0.043288016,-0.0165361789,-0.013439755,-0.0724880577,-0.0179606295,0.0406905774,0.0086201143,-0.0454565601,0.0219961397,0.0292730461,-0.0753509356,-0.0006219041,0.030127617,-0.0042465499,-0.020695398,-0.0126071024,-0.0313395386],"Abstract_Vector":[0.1642803611,0.0507313382,-0.0160146196,-0.0257982882,-0.0476489507,0.0704937224,-0.0064971194,0.0300042992,0.0809106175,0.0045809299,-0.0522899089,-0.0149903041,-0.1009343422,-0.0113984136,-0.0426008139,-0.0586082594,0.0562332376,0.0839635014,0.0010027481,-0.0105747019,-0.0321085865,0.0619721878,-0.003256467,-0.0199267281,-0.0403567947,-0.0056012162,-0.0250407251,0.0025564551,-0.0077633056,-0.0347355204,0.0232137948,-0.0284199552,-0.0267394587,0.022114111,-0.033235446,-0.0058560368,-0.0222200632,-0.0314763233,0.018093464,-0.0076497986,0.0225973863,-0.0275031135,0.013146291,-0.0167314691,-0.0279474245,-0.0247974582,0.0309274681,0.0166765326,0.0002067491,0.0241623881,0.0323616972,-0.0141120924,-0.0372400422,0.0118295809,0.0011844649,0.0320930455,0.0279931641,0.0409211082,-0.0193555729,0.0154399906,0.0147858515,-0.0220699843,0.0116785196,-0.0268895305,0.0015888224,0.0030294123,0.016762444,0.0302598161,-0.0305828919,0.0230339313,0.0209684682,0.027213708,0.0015120921,-0.0020688895,0.0379206563,-0.0275681481,0.0385642478,0.0492385541,-0.0309579686,-0.0324393532,0.0003852224,0.0095701147,0.0400725553,0.0205732306,-0.0285311834,0.0322641832,-0.0298252782,-0.0097276185,0.0033290792,-0.0331451658,0.0402652447,0.0004941142,-0.0251416846,-0.0096445403,-0.0007791499,-0.0252110084,0.0448076144,0.0203093785,0.0086053644,0.0342547799,-0.0080314579,-0.0279758539,0.0021944738,-0.0247620003,-0.0330690609,-0.0160037558,-0.0206258183,0.0250073882,-0.0345859351,0.0023703756,-0.0192740091,-0.0069305965,-0.0408980179,-0.0085714465,0.0207352171,-0.015816942]},"318":{"Abstract":"Quality assessment of image retargeting results is useful when comparing different methods. However, performing the necessary user studies is a long, cumbersome process. In this paper, we propose a simple yet efficient objective quality assessment method based on five key factors: i) preservation of salient regions; ii) analysis of the influence of artifacts; iii) preservation of the global structure of the image; iv) compliance with well-established aesthetics rules; and v) preservation of symmetry. Experiments on the RetargetMe benchmark, as well as a comprehensive additional user study, demonstrate that our proposed objective quality assessment method outperforms other existing metrics, while correlating better with human judgements. This makes our metric a good predictor of subjective preference.","Authors":"Y. Liang; Y. Liu; D. Gutierrez","DOI":"10.1109\/TVCG.2016.2517641","Keywords":"Image retargeting;quality assessment;similarity and aesthetic measure;symmetry","Title":"Objective Quality Prediction of Image Retargeting Algorithms","Keywords_Processed":"quality assessment;similarity and aesthetic measure;symmetry;image retargete","Keyword_Vector":[0.073837936,0.1063402251,-0.0658742075,-0.0026319027,-0.0172933124,0.004953957,0.0143113677,-0.0007576546,0.0077665715,0.0107757772,-0.0010502282,-0.011822314,0.0141699825,-0.0229645551,-0.0118943742,-0.001944682,-0.008397691,0.005156233,-0.0042142753,0.0193878886,-0.0008193656,0.0072774001,-0.0037919841,0.0067006774,0.0057980846,0.0015324257,0.0052837116,-0.0377920036,-0.006764984,0.000475847,0.0037415662,0.0193488643,-0.0135058595,0.0355460632,-0.0007649825,-0.0485878796,0.0109898257,-0.0049580213,0.0538608662,-0.0098664434,0.0908312568,-0.0341650785,0.0089020873,0.056524988,-0.0150314396,0.0263329493,0.0207849699,0.0896038222,-0.0099488892,-0.0058679342,0.0309028282,-0.0252056828],"Abstract_Vector":[0.2870253521,0.0979272964,-0.0462076269,-0.0874474031,0.0236434533,0.0162380486,-0.1096561715,-0.0464748947,-0.0862959482,-0.0250427519,-0.0513455719,0.0012060819,-0.077939102,0.0039815044,-0.0317215838,0.0266522067,-0.0197172892,-0.0228422614,-0.0097671157,-0.0121279377,-0.0480036259,-0.008257412,-0.0161757842,0.0699346022,0.0315050116,0.0206516975,-0.0004387011,0.0224336387,0.1125296682,-0.0510567842,-0.0060626037,-0.0148177125,-0.0566014934,0.0178562138,0.014227812,-0.0195399921,0.0054812834,-0.0050740907,0.0057827322,0.0588600816,-0.0056757967,-0.0147024318,-0.0013581556,-0.0842046852,-0.0494442851,0.0229609758,0.0563848817,0.0722706193,0.0343456753,-0.0545856607,0.0048592122,-0.0462927401,-0.0079862035,-0.0023923052,0.0286423511,-0.0494464954,0.0140045388,-0.0537160547,-0.0216863143,-0.0144628347,0.0259421579,0.0154475617,0.027891776,-0.0072433651,-0.0089516859,-0.0117684596,0.0005087097,0.0501313297,-0.0043637797,-0.0210735787,-0.0725082244,0.0037509994,-0.0077771821,0.0544639743,-0.0477846434,-0.0281486424,-0.0181992975,-0.0055615849,-0.0494522003,-0.018833865,0.0167761904,0.0137138065,-0.0265018621,0.0405151468,0.0452782086,0.0105979244,-0.0404559332,-0.0139675274,0.0164348682,-0.0230887451,-0.0048715394,0.0640512401,-0.0140828751,-0.0155569167,-0.0294421813,-0.0224025833,-0.0206371813,-0.0079255176,-0.0219525586,0.0181312108,0.0383014905,0.0169224159,-0.0054082388,-0.0461334727,-0.0225898806,0.0045588079,0.0221564144,0.0076414776,-0.037882186,-0.0263285509,-0.0297686755,-0.0304676411,0.0218464506,-0.013582855,0.0347251838,0.0069550913]},"319":{"Abstract":"We present a novel method to optimize the attenuation of light for the single scattering model in direct volume rendering. A common problem of single scattering is the high dynamic range between lit and shadowed regions due to the exponential attenuation of light along a ray. Moreover, light is often attenuated too strong between a sample point and the camera, hampering the visibility of important features. Our algorithm employs an importance function to selectively illuminate important structures and make them visible from the camera. With the importance function, more light can be transmitted to the features of interest, while contextual structures cast shadows which provide visual cues for perception of depth. At the same time, more scattered light is transmitted from the sample point to the camera to improve the primary visibility of important features. We formulate a minimization problem that automatically determines the extinction along a view or shadow ray to obtain a good balance between sufficient transmittance and attenuation. In contrast to previous approaches, we do not require a computationally expensive solution of a global optimization, but instead provide a closed-form solution for each sampled extinction value along a view or shadow ray and thus achieve interactive performance.","Authors":"M. Ament; T. Zirr; C. Dachsbacher","DOI":"10.1109\/TVCG.2016.2569080","Keywords":"Direct volume rendering;volume illumination;extinction optimization","Title":"Extinction-Optimized Volume Illumination","Keywords_Processed":"extinction optimization;direct volume render;volume illumination","Keyword_Vector":[0.1129371049,-0.0093338689,0.0157829499,-0.0041835804,0.0055705636,-0.0073673359,-0.008466099,0.0065590501,-0.0066851251,-0.0352354887,-0.0024542718,0.0136647256,0.0202255054,0.0252301342,0.0158358322,-0.0054433453,0.0095823467,0.0250355067,0.0398246917,-0.0297522165,-0.0062623662,0.0542227683,-0.0944798314,-0.0321756789,0.0579026897,0.027499318,0.0415508793,-0.0097886951,0.0201726219,0.0340044018,0.0874578239,-0.002816932,0.003020974,0.0751103317,0.005744386,0.0768349046,0.1111194446,0.0603088514,0.0160669049,-0.0040152567,0.0345063724,0.0815522118,-0.0244278949,-0.0213222645,-0.0209324193,-0.002682992,0.0665543076,0.0217493283,-0.0606980388,0.0306527334,0.0013240103,0.0004713411],"Abstract_Vector":[0.2243334496,-0.0709416398,-0.0150629372,-0.0314033108,-0.0096731138,0.0102854917,-0.01983487,0.0124766177,-0.0413551605,-0.0054013797,-0.0193346032,-0.0537134022,-0.0107613013,0.0186634443,-0.0153397712,0.00831431,-0.0143408292,0.0232812158,0.0873791683,0.0078302561,0.0037375384,-0.0072483069,0.0379522146,0.0192705225,-0.0066284116,0.0159483898,-0.0689748436,0.0201403681,-0.0440853592,-0.0181193872,0.0049843184,-0.0370057431,0.0344347909,0.0101529969,-0.0372249263,0.0365386984,-0.00280011,0.0444829767,0.0212192519,0.016070006,-0.0673056757,-0.0108753428,-0.0034757542,0.0129814001,0.0173607657,-0.0174794128,-0.0224299364,-0.0354161378,-0.0322713679,0.0503088925,0.0392033692,0.0106662731,-0.0037992869,0.0033483133,-0.0155630398,0.032327123,-0.0184143654,0.0239195839,0.0204278276,0.0209504137,-0.0139931127,0.0391290962,0.0118909905,-0.0140575386,0.0227644418,-0.0174612595,0.0022661583,-0.0065078057,0.0316641878,0.0037214483,0.0009991463,-0.0039009916,-0.0085409149,-0.0128839753,-0.0124218828,0.029584418,0.0266444688,0.0299062712,-0.001778594,-0.0024805586,0.0082714338,-0.0146889966,-0.012106349,-0.0354324276,0.0154830137,-0.0374815263,0.0110824821,0.0239526863,-0.0240293232,-0.0114664609,-0.0192037694,-0.0033743723,0.0070396283,-0.0076339777,-0.0236806534,0.0308916432,0.0150806511,0.0015407864,-0.0312049436,-0.0051500632,-0.0120222703,0.0146740604,-0.0029565566,-0.0038942164,0.0280938367,0.0111817926,0.0135465922,-0.0025703587,-0.0071046868,-0.01518135,-0.0187239667,-0.014491035,0.0025732549,-0.018842542,0.0112047116,0.0214208339]},"32":{"Abstract":"We show how mouse interaction log classification can help visualization toolsmiths understand how their tools are used \u201cin the wild\u201d through an evaluation of MAGI - a cancer genomics visualization tool. Our primary contribution is an evaluation of twelve visual analysis task classifiers, which compares predictions to task inferences made by pairs of genomics and visualization experts. Our evaluation uses common classifiers that are accessible to most visualization evaluators: k-nearest neighbors, linear support vector machines, and random forests. By comparing classifier predictions to visual analysis task inferences made by experts, we show that simple automated task classification can have up to 73 percent accuracy and can separate meaningful logs from \u201cjunk\u201d logs with up to 91 percent accuracy. Our second contribution is an exploration of common MAGI interaction trends using classification predictions, which expands current knowledge about ecological cancer genomics visualization tasks. Our third contribution is a discussion of how automated task classification can inform iterative tool design. These contributions suggest that mouse interaction log analysis is a viable method for (1) evaluating task requirements of client-side-focused tools, (2) allowing researchers to study experts on larger scales than is typically possible with in-lab observation, and (3) highlighting potential tool evaluation bias.","Authors":"C. C. Gramazio; J. Huang; D. H. Laidlaw","DOI":"10.1109\/TVCG.2017.2734659","Keywords":"Classification;task analysis;visual analysis;biology visualization;visualization;cancer genomics","Title":"An Analysis of Automated Visual Analysis Classification: Interactive Visualization Task Inference of Cancer Genomics Domain Experts","Keywords_Processed":"classification;visualization;task analysis;biology visualization;visual analysis;cancer genomic","Keyword_Vector":[0.1591398867,-0.0280409219,0.1854328605,0.0298099084,-0.1061764655,0.023584682,-0.0001001355,-0.0746657057,0.0974247784,-0.0745344813,-0.047492349,0.0221466849,-0.0337458042,0.0079337431,-0.0139959013,0.1497643677,0.1474067333,-0.1044839699,-0.0574990539,-0.0141009489,0.0495444196,0.0177545466,0.0636669208,0.081407807,-0.0533506619,-0.0432117515,-0.0389001435,-0.0672981109,-0.1042606212,-0.0392854315,-0.0339678762,0.0287921206,0.0659309394,0.0439394169,-0.0056309874,0.0352204004,0.0156890988,0.0271486415,0.052056505,0.0254302508,0.0188853447,0.0190127399,0.005377295,-0.0347260231,0.0309590124,-0.0470228406,0.0461850615,-0.0002687034,0.0432645167,0.0264696884,-0.014463671,0.0694674653],"Abstract_Vector":[0.1839505151,-0.0511806327,0.0363871901,-0.0402376051,-0.0007839786,-0.0303962001,0.0686663709,0.0053978015,-0.0125782702,0.0146898499,0.0247336042,-0.0127438875,-0.0337906456,-0.0115091106,-0.0121831258,0.0185135509,-0.0177042884,0.0306760418,-0.0095197329,0.0222232623,-0.0024993246,-0.0206566189,-0.0199546659,0.014108636,-0.0214364717,0.0130586306,-0.0083008898,0.0154368691,0.0040412078,-0.0342899078,0.0263082963,0.0537537691,-0.0172665085,-0.0057711321,-0.033388234,-0.0234583393,-0.0001347112,0.014566591,0.0172284508,-0.0090343101,-0.0236026649,-0.0085655751,-0.0052463839,0.0236858578,0.0017751214,-0.0231644653,-0.0204364177,0.0131353941,-0.0210740402,-0.0087889417,0.0124202718,0.0000751842,-0.0033448981,-0.0188147893,0.0024680829,-0.0184682961,-0.0047497204,0.0310218632,-0.0154106625,-0.0090955322,0.0246360546,-0.0283963187,-0.0204597352,-0.0156820689,0.028161951,0.0159201624,0.0008992911,0.0054895608,-0.0024419762,0.0105997246,-0.0157758778,0.0007470635,-0.0002160978,-0.0485633568,0.0257217145,0.0133035343,-0.0032945304,0.0252119782,-0.0306181386,0.0210818782,-0.0025528193,-0.0102431038,0.0417892431,-0.0159228181,0.0098045373,0.0020952457,0.0151382982,0.0011119481,-0.0181187785,0.0004177858,0.004436163,-0.028907025,-0.0203743978,0.0087828117,0.0423560589,-0.0015574462,0.0010436724,-0.0091868401,0.0004815511,0.0150105393,-0.0240651577,0.0121656041,0.0063903627,0.0300175254,0.0129929954,-0.0120853024,0.0315542215,-0.0049808991,0.0003837028,-0.0066152874,0.0072964888,-0.001798758,0.016184999,-0.0269700056,-0.0103278804,0.0250235951]},"320":{"Abstract":"Type 1 diabetes is a chronic, incurable autoimmune disease affecting millions of Americans in which the body stops producing insulin and blood glucose levels rise. The goal of intensive diabetes management is to lower average blood glucose through frequent adjustments to insulin protocol, diet, and behavior. Manual logs and medical device data are collected by patients, but these multiple sources are presented in disparate visualization designs to the clinician-making temporal inference difficult. We conducted a design study over 18 months with clinicians performing intensive diabetes management. We present a data abstraction and novel hierarchical task abstraction for this domain. We also contribute IDMVis: a visualization tool for temporal event sequences with multidimensional, interrelated data. IDMVis includes a novel technique for folding and aligning records by dual sentinel events and scaling the intermediate timeline. We validate our design decisions based on our domain abstractions, best practices, and through a qualitative evaluation with six clinicians. The results of this study indicate that IDMVis accurately reflects the workflow of clinicians. Using IDMVis, clinicians are able to identify issues of data quality such as missing or conflicting data, reconstruct patient records when data is missing, differentiate between days with different patterns, and promote educational interventions after identifying discrepancies.","Authors":"Y. Zhang; K. Chanana; C. Dunne","DOI":"10.1109\/TVCG.2018.2865076","Keywords":"Design study;task analysis;event sequence visualization;time series data;qualitative evaluation;health applications","Title":"IDMVis: Temporal Event Sequence Visualization for Type 1 Diabetes Treatment Decision Support","Keywords_Processed":"qualitative evaluation;task analysis;design study;event sequence visualization;health application;time series datum","Keyword_Vector":[0.1174047003,-0.0260740856,0.0743990162,-0.0949463056,-0.0079608448,-0.0784302027,-0.0809307025,-0.0252573996,-0.0159276601,-0.0620661101,0.0137796135,-0.0087253996,0.0760749008,0.0123634652,0.0691054458,-0.0126282952,0.1114740596,-0.074364532,0.1658318419,-0.027623483,-0.0972466076,0.0353456131,0.1281094006,0.0671115019,-0.1171342606,-0.0328792182,-0.0672334042,-0.0915327437,0.1091599143,-0.13044769,-0.0814021742,-0.1649556857,0.0719881007,0.0426944711,-0.0283179896,-0.0346953392,-0.040504401,0.0245622013,-0.00746241,-0.0752509123,0.0402427986,0.0336517054,0.0076609954,0.069071702,-0.0263194877,-0.0014986993,-0.0056783591,0.0188874742,0.0040984349,0.0311343576,-0.0289354225,0.0036786862],"Abstract_Vector":[0.2088003677,0.0050000294,0.1953682589,-0.0706924443,0.0445436399,0.1114002919,0.1182309886,-0.1337524011,0.117750678,-0.0682296066,-0.0204106496,-0.0072147695,-0.0475214127,0.0640487122,0.013616806,0.1089677393,-0.1020210326,-0.012477241,0.0670533721,0.0610166539,0.0012487065,0.0007148877,0.0546173087,-0.0405025461,-0.0797835842,-0.0386153635,0.0494100723,0.0316752907,-0.097640798,0.036595596,0.0338685455,0.0302820579,0.041647814,0.0399415787,-0.0042815377,-0.0134022581,-0.0312165065,-0.0341016389,-0.0172023472,-0.0378678862,0.0004917482,0.0590592927,-0.0020233373,-0.0478203142,0.0027228252,0.0307740317,-0.0520191877,0.0238166069,0.0639124111,0.0172160924,-0.0285258548,0.0291120754,-0.0314085871,-0.0509629594,-0.0393196802,-0.0673147477,0.0010309989,0.0229867409,-0.0042472446,0.0142247719,0.0059659483,-0.0019551748,0.0117691746,0.0209882897,0.0118788954,-0.0167856633,-0.0377459635,-0.008163024,-0.0244261447,-0.000833846,0.0009809477,-0.014575262,-0.0394156307,-0.0116887034,-0.0292112176,-0.0207530156,0.0132490646,0.0492726086,0.0193696272,0.0002055811,-0.0261745538,0.0359765176,-0.0122215948,0.0363083281,-0.0063581523,-0.0069292775,0.0162645326,0.0393793604,-0.0414137599,-0.0071960444,-0.0481977811,0.0285104961,0.0226968454,0.040944405,-0.0049070775,-0.0094952043,0.0184599321,-0.002583692,0.0170549452,-0.0074839718,0.0307389253,-0.0322620333,0.0039756216,-0.0354134343,-0.0090422571,0.0034391111,-0.0087415639,-0.0058717317,0.0422385486,0.007784849,0.0635537331,-0.0181210307,0.0275140054,0.0057172432,-0.0344223141,-0.0121085184]},"321":{"Abstract":"Evaluating the effectiveness of data visualizations is a challenging undertaking and often relies on one-off studies that test a visualization in the context of one specific task. Researchers across the fields of data science, visualization, and human-computer interaction are calling for foundational tools and principles that could be applied to assessing the effectiveness of data visualizations in a more rapid and generalizable manner. One possibility for such a tool is a model of visual saliency for data visualizations. Visual saliency models are typically based on the properties of the human visual cortex and predict which areas of a scene have visual features (e.g. color, luminance, edges) that are likely to draw a viewer's attention. While these models can accurately predict where viewers will look in a natural scene, they typically do not perform well for abstract data visualizations. In this paper, we discuss the reasons for the poor performance of existing saliency models when applied to data visualizations. We introduce the Data Visualization Saliency (DVS) model, a saliency model tailored to address some of these weaknesses, and we test the performance of the DVS model and existing saliency models by comparing the saliency maps produced by the models to eye tracking data obtained from human viewers. Finally, we describe how modified saliency models could be used as general tools for assessing the effectiveness of visualizations, including the strengths and weaknesses of this approach.","Authors":"L. E. Matzen; M. J. Haass; K. M. Divis; Z. Wang; A. T. Wilson","DOI":"10.1109\/TVCG.2017.2743939","Keywords":"Visual saliency;evaluation;eye tracking","Title":"Data Visualization Saliency Model: A Tool for Evaluating Abstract Data Visualizations","Keywords_Processed":"eye tracking;evaluation;visual saliency","Keyword_Vector":[0.0704957917,0.0018226484,0.0049442269,-0.0046798676,0.0462454823,-0.0142266563,0.0265612225,-0.0518558758,-0.054453659,0.002514702,-0.0122486511,0.061610696,0.0050049778,0.0626958711,0.0325357618,0.0136684145,-0.0574437654,-0.0521810857,0.0250674391,0.0108584514,0.0652691942,-0.0215860607,-0.0948732809,0.0677078276,-0.0166412065,-0.0939563958,0.0245790041,0.0170570217,-0.0081740262,-0.0735026999,0.0344466906,-0.0728535387,-0.0299221526,-0.003326887,0.0339091677,0.0285832509,0.0110992641,0.0185526516,0.0039136734,0.0511376803,-0.003249492,-0.0308350087,-0.0519649269,-0.0722752443,-0.0254076265,0.0495821464,0.0183573014,-0.0277313006,-0.0441831485,-0.0257361852,0.0080304055,0.0059812134],"Abstract_Vector":[0.1528360204,-0.0219979656,-0.0189595882,0.1286854708,0.09092491,0.0743651583,-0.0597553961,-0.0069793802,0.0631833186,-0.021896958,-0.0151016111,0.0909500172,-0.0133546629,-0.0981062042,0.0581178704,0.1190648712,-0.0887764577,0.0049117964,-0.093719089,-0.0883190188,-0.0599891459,0.043343941,0.0182343084,0.0693070357,0.0175211382,-0.0649189272,0.0172044642,0.016722691,0.0289871494,-0.0982332902,0.0699691555,-0.0213767655,-0.0231428334,0.0402223087,0.0331046144,0.0282259217,-0.0185526505,0.1041761981,-0.0427105884,0.017685188,0.0481021639,-0.0590464665,0.0004028248,0.0220003,0.0181024094,0.0483828238,0.0889363955,0.0347157785,-0.083733894,0.1063044533,-0.0887670669,0.0442271283,-0.0531285629,0.0500903508,0.0138657119,0.0227674494,0.0490551606,-0.0258579251,-0.0287612059,-0.0138018247,0.0602108195,-0.0716183293,-0.0050053171,0.020127816,-0.0211835505,0.0511821414,-0.0219240149,-0.0015093516,0.0023274444,-0.0705195553,-0.0358217696,0.0101012768,0.0007336528,0.0108418931,-0.0030208551,0.0112078209,-0.022767065,0.0175254092,0.0548862331,-0.0300491041,0.035307402,-0.031110124,0.0013324346,0.0203329366,0.0124547319,-0.0213047957,0.0541959055,0.0185433828,0.0079175014,-0.0434283332,-0.0193634925,0.0011919082,-0.0093358862,0.0103317293,0.0216374405,0.0337700789,0.0007006976,-0.0141944648,-0.0060518692,0.0042300884,0.0043708759,0.0509886384,-0.0394698138,0.0046153892,0.0190168665,-0.0171193701,-0.0027189053,0.0023602275,0.0291198303,0.0254536663,-0.0076744576,-0.0124212477,0.046054965,0.0344804936,0.0052949913,0.0111508638]},"322":{"Abstract":"Recent visualization research efforts have incorporated experimental techniques and perceptual models from the vision science community. Perceptual laws such as Weber's law, for example, have been used to model the perception of correlation in scatterplots. While this thread of research has progressively refined the modeling of the perception of correlation in scatterplots, it remains unclear as to why such perception can be modeled using relatively simple functions, e.g., linear and log-linear. In this paper, we investigate a longstanding hypothesis that people use visual features in a chart as a proxy for statistical measures like correlation. For a given scatterplot, we extract 49 candidate visual features and evaluate which best align with existing models and participant judgments. The results support the hypothesis that people attend to a small number of visual features when discriminating correlation in scatterplots. We discuss how this result may account for prior conflicting findings, and how visual features provide a baseline for future model-based approaches in visualization evaluation and design.","Authors":"F. Yang; L. T. Harrison; R. A. Rensink; S. L. Franconeri; R. Chang","DOI":"10.1109\/TVCG.2018.2810918","Keywords":"Information visualization;perception and psychophysics;evaluation\/methodology;Weber's law;power law","Title":"Correlation Judgment and Visualization Features: A Comparative Study","Keywords_Processed":"perception and psychophysic;power law;evaluation methodology;Weber law;information visualization","Keyword_Vector":[0.1552436141,-0.0728701827,0.0115900027,0.0313732743,-0.0150830437,0.1627319038,-0.0410831584,0.00420172,-0.025859229,-0.0314095302,0.0219799907,0.0165389849,0.0095650615,-0.0557114308,-0.0188232076,-0.0304123878,-0.0708639224,-0.0067964573,0.001966587,0.0474161011,0.0346008343,0.0386191573,0.0058597581,-0.0178597824,0.0009793123,-0.0131948851,0.0243867891,0.0063598162,0.0159917798,-0.0113795184,-0.0194339683,-0.0455755154,0.0215295229,-0.0255337403,-0.0383049747,-0.0228359279,-0.0093043048,0.0195684257,-0.0258526083,-0.0025166214,-0.011240925,0.0085217684,-0.0497001156,-0.0231571438,-0.0140997202,0.0463092048,0.0464047211,-0.0025198693,-0.0078952053,0.014122636,-0.0340462914,0.069023243],"Abstract_Vector":[0.3004089627,-0.088660831,0.0007136269,-0.039816375,0.0334528978,-0.1501229519,-0.1134803477,-0.0298766378,0.0467681329,-0.138118971,0.0965943285,-0.1178466216,-0.0661586638,-0.0473714522,-0.0004519529,-0.0570231113,-0.0717372971,-0.0246924741,0.046035358,-0.0024822059,-0.0072974969,-0.0310848434,0.0297293037,-0.0192626039,-0.001465993,-0.0794059932,0.0315551344,0.0037830854,0.024394212,-0.0534551667,-0.0124009255,-0.0416298643,0.0835125212,-0.0287212875,-0.0901466176,0.0070933138,-0.0001643653,-0.0075003806,-0.0919549855,0.0655509807,0.0145930144,-0.0593153751,0.0327533314,-0.0129985159,-0.0191578521,0.0460930077,0.001505988,-0.0336381826,-0.0408915362,-0.0311632835,-0.0670557795,-0.0616274914,0.0847741102,0.0234293581,0.0496745542,-0.0966821445,0.1387767863,-0.0302166086,-0.0385670633,-0.0055296434,0.0106243672,0.0148332836,0.0011111068,0.0066816712,0.0008768091,0.0630270572,-0.0612883732,0.015647362,-0.0132762634,0.0363531191,0.0245363159,0.0035067507,0.0433013849,-0.059687881,-0.0470119501,-0.0660489419,0.0669022353,0.0274586129,-0.0467434688,0.0521912404,0.0324363402,0.0337934285,0.0297767636,-0.0745935242,0.0402440398,0.0385416104,0.0025833979,-0.0225315101,-0.0366506309,-0.0134082353,0.1297924803,0.0455345041,0.0220860615,-0.0345805173,0.0360587041,0.0341313741,0.0442179654,0.0134450992,-0.0294781564,0.0185956526,0.0039141431,0.0128536422,0.014232782,0.0353861727,0.0045850865,0.0091988088,0.0475674785,-0.0278283326,0.0476148084,-0.0699511686,0.0521826422,0.0173888265,-0.0376701824,-0.0080475227,0.0102276816,-0.02499266]},"323":{"Abstract":"We present the first visualization tool that combines pathlines from blood flow and wall thickness information. Our method uses illustrative techniques to provide occlusion-free visualization of the flow. We thus offer medical researchers an effective visual analysis tool for aneurysm treatment risk assessment. Such aneurysms bear a high risk of rupture and significant treatment-related risks. Therefore, to get a fully informed decision it is essential to both investigate the vessel morphology and the hemodynamic data. Ongoing research emphasizes the importance of analyzing the wall thickness in risk assessment. Our combination of blood flow visualization and wall thickness representation is a significant improvement for the exploration and analysis of aneurysms. As all presented information is spatially intertwined, occlusion problems occur. We solve these occlusion problems by dynamic cutaway surfaces. We combine this approach with a glyph-based blood flow representation and a visual mapping of wall thickness onto the vessel surface. We developed a GPU-based implementation of our visualizations which facilitates wall thickness analysis through real-time rendering and flexible interactive data exploration mechanisms. We designed our techniques in collaboration with domain experts, and we provide details about the evaluation of the technique and tool.","Authors":"K. Lawonn; S. Gla\u00dfer; A. Vilanova; B. Preim; T. Isenberg","DOI":"10.1109\/TVCG.2015.2467961","Keywords":"Medical visualization;aneurysms;blood flow;wall thickness;illustrative visualization;Medical visualization;aneurysms;blood flow;wall thickness;illustrative visualization","Title":"Occlusion-free Blood Flow Animation with Wall Thickness Visualization","Keywords_Processed":"medical visualization;aneurysm;blood flow;illustrative visualization;wall thickness","Keyword_Vector":[0.1099557392,-0.0720673142,0.0058020985,-0.0882057904,0.0550116616,-0.007899897,-0.0310502085,0.0290483367,0.0106034114,0.011758189,-0.0039553122,-0.1251734792,-0.0102425156,0.0161861325,0.0353132616,-0.0468971964,0.001227451,-0.0180456891,-0.0076042523,0.0270624122,0.0072167069,-0.0459118351,-0.0155935713,0.0045765319,-0.0393046794,-0.0000584906,0.0053685551,-0.0209047355,0.001946235,0.0017405626,-0.0074521767,-0.0274270185,0.0183923365,0.0126600874,-0.0385811802,-0.0299690906,-0.0127399637,0.0141878995,0.01377747,-0.0029354225,-0.0182964703,0.0060212254,-0.0173496774,0.0207951211,-0.0122784917,0.0353845101,0.0092750494,0.0029229723,0.0050661,0.0344302105,-0.0553935566,0.0267735891],"Abstract_Vector":[0.2578560902,-0.0490054294,-0.0403830795,-0.0117430847,-0.0075683457,-0.0735396152,-0.0608155457,-0.0207851099,0.0735372117,-0.0903090959,0.0507251164,-0.0251261656,-0.0186536902,-0.0573653657,-0.0129567833,0.0393199208,-0.0645591071,0.0105938624,0.0219577085,-0.0160726339,0.0121892525,0.0363824236,0.045097401,-0.0212658675,-0.0414056104,-0.0285548696,0.040660331,0.0845484286,0.0229173474,-0.0022060651,0.0007063541,-0.0102898395,-0.0397406459,-0.0234983432,0.0650665578,-0.0487359421,-0.0279557596,-0.0565717189,0.0257735827,0.0669257281,-0.0039620436,0.0014774238,-0.0148058317,0.0178843059,-0.0149494902,-0.0024540639,-0.032756429,0.0026796135,-0.0378906399,0.0063469406,-0.0188164804,-0.0385758322,-0.0154454018,-0.0370413557,-0.0090342013,-0.0281792155,-0.0528728759,-0.0648673342,-0.0593194174,0.034055286,-0.0178344815,-0.0453029662,-0.0002914524,0.1038319491,0.02483031,-0.0301990767,0.1160215421,0.074505878,0.0455568022,-0.0157071149,-0.0170403144,-0.0174283766,-0.0243820654,0.0506980387,0.0254615142,0.0282689867,-0.0144944563,-0.0906134593,-0.019018552,-0.0639734317,-0.0065583202,0.0157105094,-0.0796908297,-0.0588551026,0.0002602486,-0.0211463258,-0.0070724866,0.0209284065,-0.0992728052,-0.0475603623,0.052264392,-0.0004798642,-0.0079199608,-0.0500652988,0.04734404,-0.0502788068,0.0189866913,-0.011261854,-0.0107628079,-0.0358565404,0.0485637558,-0.0766603251,0.0486092232,-0.0491556439,0.0392459501,0.0104688426,0.0158299577,-0.0309968492,-0.028583196,0.0451538044,0.0265858316,0.045720788,0.033478844,0.0172457308,-0.0086448719,-0.0407096411]},"324":{"Abstract":"Many algorithms for scientific visualization and image analysis are rooted in the world of continuous scalar, vector, and tensor fields, but are programmed in low-level languages and libraries that obscure their mathematical foundations. Diderot is a parallel domain-specific language that is designed to bridge this semantic gap by providing the programmer with a high-level, mathematical programming notation that allows direct expression of mathematical concepts in code. Furthermore, Diderot provides parallel performance that takes advantage of modern multicore processors and GPUs. The high-level notation allows a concise and natural expression of the algorithms and the parallelism allows efficient execution on real-world datasets.","Authors":"G. Kindlmann; C. Chiw; N. Seltzer; L. Samuels; J. Reppy","DOI":"10.1109\/TVCG.2015.2467449","Keywords":"Domain specific language;portable parallel programming;scientific visualization;tensor fields;Domain specific language;portable parallel programming;scientific visualization;tensor fields","Title":"Diderot: a Domain-Specific Language for Portable Parallel Scientific Visualization and Image Analysis","Keywords_Processed":"domain specific language;tensor field;scientific visualization;portable parallel programming","Keyword_Vector":[0.1012799591,-0.0519316308,0.0068014772,0.0554874355,-0.0390185308,0.1239510432,-0.0549787364,-0.0544237191,-0.0335341732,0.0156735616,0.0072099336,0.0266026007,0.0189630114,-0.0317928306,0.034171483,0.091775262,0.0073204286,0.009434747,0.0048960187,-0.040339667,-0.0143410394,0.0517515473,0.0228847503,0.056059842,-0.0969161381,0.0294340887,-0.0317765647,-0.0094854236,-0.0131955599,0.0234339491,-0.0617305519,-0.0879945436,-0.0497118491,0.0265573956,-0.0004044926,0.0309255545,-0.0400353437,0.0466283606,-0.0170766305,-0.0138465655,0.0200003252,0.004561947,-0.0035191033,-0.0287401688,0.0373162306,-0.0571809389,-0.0231727296,-0.0075183042,0.0304846702,-0.0192265615,0.0467440378,0.0162709569],"Abstract_Vector":[0.1120555132,-0.0737450187,-0.0004134165,0.0017415189,-0.0430671141,-0.0180199567,0.0226902309,0.000803175,-0.036395954,0.0494300036,-0.0390918059,0.025892395,-0.0634240557,0.0086147871,-0.0300822108,-0.0026986162,0.0036795708,-0.0499725954,-0.0390305186,0.0391287597,0.0168338088,0.0433355139,0.0204506328,0.0151623718,0.0277366344,0.0902868018,0.044985503,0.0805917879,-0.0120892689,0.0088103642,-0.0056529058,-0.0530924915,0.0555753515,-0.0108703222,0.032479194,-0.0016819374,0.0176636208,0.0338231838,-0.0033277922,-0.0631631448,0.0193520781,-0.0014593173,0.0465683755,-0.0131314437,0.0123461696,0.0254381824,0.0180878606,-0.0508446113,-0.007253791,-0.0001192602,-0.0128718722,0.011606746,-0.0264210963,0.0160741541,-0.0079962702,0.011903406,-0.0181583479,0.0307437371,0.0023831678,0.0000462267,-0.029169416,0.0072638463,0.0221180982,0.0028313278,-0.0320323329,0.0039791295,0.0303676087,-0.0041544514,-0.0268987529,-0.0249801111,0.0175788324,-0.0068118484,-0.00870763,0.0362474955,0.0249994258,-0.0250108215,0.0043613374,-0.0062415007,0.0261089096,0.0262178487,-0.0120051805,-0.0170202792,-0.0018430112,-0.0122337223,0.0322089589,0.0057898384,-0.0047234091,-0.019709564,0.0242338677,0.0343407125,0.0351737912,0.0071275136,0.0123622868,-0.0177523031,0.0184212154,-0.0188860136,0.0410837062,-0.0105667183,0.0183239463,0.0088488191,-0.0266302562,-0.0240906691,-0.0005152714,-0.0136513802,0.0101246156,-0.0203045248,0.0160698775,0.0029241593,-0.0039875315,0.0136911052,0.0078409815,0.0109494988,0.01150714,-0.0184843892,-0.0318345673,-0.0171034169]},"325":{"Abstract":"We propose a novel parallel connected component labeling (CCL) algorithm along with efficient out-of-core data management to detect and track feature regions of large time-varying imagery datasets. Our approach contributes to the big data field with parallel algorithms tailored for GPU architectures. We remove the data dependency between frames and achieve pixel-level parallelism. Due to the large size, the entire dataset cannot fit into cached memory. Frames have to be streamed through the memory hierarchy (disk to CPU main memory and then to GPU memory), partitioned, and processed as batches, where each batch is small enough to fit into the GPU. To reconnect the feature regions that are separated due to data partitioning, we present a novel batch merging algorithm to extract the region connection information across multiple batches in a parallel fashion. The information is organized in a memory-efficient structure and supports fast indexing on the GPU. Our experiment uses a commodity workstation equipped with a single GPU. The results show that our approach can efficiently process a weather dataset composed of terabytes of time-varying radar images. The advantages of our approach are demonstrated by comparing to the performance of an efficient CPU cluster implementation which is being used by the weather scientists.","Authors":"C. Peng; S. Sahani; J. Rushing","DOI":"10.1109\/TVCG.2016.2637904","Keywords":"Connected component labeling;time-varying imagery feature detection;GPGPU","Title":"A GPU-Accelerated Approach for Feature Tracking in Time-Varying Imagery Datasets","Keywords_Processed":"connected component labeling;time vary imagery feature detection;gpgpu","Keyword_Vector":[0.2392809689,-0.1474913995,0.0367528581,0.1075674022,-0.0192481863,0.1089163873,0.1804462332,-0.1159130483,0.0180638431,0.0467788868,0.0085101811,0.048548061,0.0223006445,-0.0527418258,0.0383764813,0.1618714152,-0.0070851256,0.0269278775,-0.101540279,0.0101635956,-0.017666645,0.0113182439,0.0625184907,0.1364774987,-0.0711441973,0.0386230022,-0.0180970815,-0.0268938287,0.0030783607,0.0376574067,-0.0588282864,0.0096650575,0.0635788034,0.0214232902,-0.0081918295,0.0465243957,-0.0056851313,-0.0422582301,0.0202297499,0.0589857801,-0.0092235025,-0.0488589503,-0.0090793739,-0.0486039809,0.0397030271,-0.0337079361,0.0808666364,-0.0188016252,-0.0251362084,0.0684622972,0.0194398142,0.0047422082],"Abstract_Vector":[0.334941996,-0.1368508962,-0.033259844,0.0451620058,0.130348352,-0.1204326009,-0.0110006748,-0.0872263073,0.0345779714,0.1126213875,0.0588054261,0.0144735523,0.030865725,0.0683578784,-0.1177306853,-0.0491480143,0.1124903057,-0.0086360827,-0.0477098123,-0.0354312928,-0.0038587003,-0.0278076743,-0.0598781732,0.0254015622,-0.0693667685,0.0221798477,0.002947612,0.001294245,-0.0957742948,-0.0211128934,-0.0079620113,0.0111719122,-0.0253351173,0.0179095168,0.014369079,-0.0844682768,0.0372357303,-0.0109842741,-0.0312975302,0.0121453853,-0.001597606,-0.0303184148,-0.0447567238,0.076986391,-0.0299258679,-0.0245934117,0.0478909047,0.0418715682,-0.0168624918,-0.020541567,0.0354679354,-0.0310503867,0.0271112222,-0.014286747,-0.0031292401,-0.014183371,-0.0145383818,-0.0923295819,-0.0242917797,0.0025929378,-0.0033896036,-0.0119380821,-0.009188907,-0.0112571898,0.0528760181,-0.0695764688,0.0328071564,0.0307199696,-0.0132276354,0.0108080235,0.0239060812,0.0229828415,-0.0088288035,-0.0238357721,-0.012502511,0.0637437141,0.0037014814,-0.0287111388,-0.0024146023,-0.0038385763,-0.0252519566,0.0027373403,-0.0304914578,0.0232084927,0.0022114124,0.0010898879,-0.0587206487,0.0339890442,-0.0029646107,0.0160156889,0.0014219698,0.0444792597,-0.0025161847,0.0424311942,0.0126927468,-0.0008049484,-0.0115339913,0.0367372728,0.0339220372,-0.0480440176,-0.0552717516,0.0024845484,0.0633979061,0.0267184166,0.0149062731,0.0008071576,0.0161576322,0.0650762398,0.0196910569,0.0043962335,0.0118955653,0.0153160619,-0.0580561799,0.0167362375,0.0088888868,0.020434365]},"326":{"Abstract":"In this paper, we present a novel method on surface partition from the perspective of approximation theory. Different from previous shape proxies, the ellipsoidal variance proxy is proposed to penalize the partition results falling into disconnected parts. On its support, the Principle Component Analysis (PCA) based energy is developed for asymptotic cluster aspect ratio and size control. We provide the theoretical explanation on how the minimization of the PCA-based energy leads to the optimal asymptotic behavior for approximation. Moreover, we show the partitions on densely sampled triangular meshes converge to the theoretic expectations. To evaluate the effectiveness of surface approximation, polygonal\/triangular surface remeshing results are generated. The experimental results demonstrate the high approximation quality of our method.","Authors":"Y. Cai; X. Guo; Y. Liu; W. Wang; W. Mao; Z. Zhong","DOI":"10.1109\/TVCG.2016.2623779","Keywords":"Surface approximation;approximation theory;principal component analysis;optimal asymptotic behavior;surface remeshing","Title":"Surface Approximation via Asymptotic Optimal Geometric Partition","Keywords_Processed":"principal component analysis;surface remeshe;surface approximation;optimal asymptotic behavior;approximation theory","Keyword_Vector":[0.1381751257,-0.0714788711,0.0216670971,-0.1162851554,0.0297328548,-0.0113704073,-0.0391984206,-0.0091119479,0.0297496997,0.0118678298,0.0103322905,-0.0888393352,-0.0129617056,-0.0166318863,0.0214004249,0.0070247182,-0.0146405547,-0.0151334761,-0.0020325242,0.0208012849,-0.0196505808,-0.0211751773,-0.0192297006,-0.0105504841,0.016749394,-0.0018557299,-0.0035363453,0.002111657,0.0073835757,-0.0081762125,-0.0023049719,0.0021096696,0.0200444529,-0.0033387496,-0.0166878915,-0.0047925688,0.0031140257,0.00823616,0.0071308469,0.0032789506,-0.0037039008,-0.0156080743,0.0099561438,0.0047432096,-0.0030139068,0.0143594617,0.0219829183,-0.0027141284,0.0065534348,0.022529527,-0.0207887422,-0.0003019731],"Abstract_Vector":[0.178240084,-0.1099053347,0.0059706391,0.0237885722,-0.0075343226,0.0518681481,0.0068260621,0.0060964292,0.0018777087,-0.0185852038,0.0045178947,0.0401176052,0.0676573148,-0.0019798359,-0.0096336417,-0.0340328377,-0.0490453195,-0.0410229791,0.0481006632,-0.0350022116,0.0401041571,0.0154436417,0.0156826342,-0.0506513222,0.0332718413,0.0252213749,-0.0121810591,-0.0243971295,0.0215782603,0.0080918559,-0.0089930273,-0.032406849,0.0142686544,-0.0181219371,-0.0139726992,-0.0341448805,0.0126218581,0.0060171935,0.0450820783,-0.0408331947,0.010446564,-0.0104468921,-0.0332430658,0.0062175897,0.0009112998,-0.0236190232,-0.0058888783,0.0067391019,-0.0151734253,-0.0195070615,-0.0055397261,-0.0032953248,0.0084709527,-0.0047734504,-0.0296014973,-0.0121911249,0.0191644822,0.0098238434,0.0138540396,0.002997717,-0.0123461266,-0.0187128027,-0.020063892,0.0412049307,0.0276863828,-0.0020757927,0.0413403849,0.0093734309,-0.0077056595,-0.0343702833,-0.007485469,-0.0172779247,0.022709282,0.0008341918,0.0037856167,-0.0043936735,0.0268438427,0.0209982108,-0.0098593703,-0.0110287578,-0.0044100718,-0.0023405162,0.0153974336,0.0064792027,0.0230698123,-0.0032274315,-0.0145747747,-0.0188898594,-0.0074889925,-0.001790634,-0.0184723217,0.0016524171,-0.0376417716,-0.0094737783,0.0020403334,-0.0221835107,-0.0012334282,0.0042434632,0.0093025878,0.0116709359,-0.0055122125,-0.0377124781,-0.0094895612,-0.0107410694,0.002241713,0.0002895261,0.0282108492,0.0010659286,-0.0102647831,0.0054136565,0.0164770695,-0.0091200124,-0.0027076871,-0.0095940552,0.0047652906,-0.0035138775]},"327":{"Abstract":"In movement data analysis, there exists a problem of comparing multiple trajectories of moving objects to common or distinct reference trajectories. We introduce a general conceptual framework for comparative analysis of trajectories and an analytical procedure, which consists of (1) finding corresponding points in pairs of trajectories, (2) computation of pairwise difference measures, and (3) interactive visual analysis of the distributions of the differences with respect to space, time, set of moving objects, trajectory structures, and spatio-temporal context. We propose a combination of visualisation, interaction, and data transformation techniques supporting the analysis and demonstrate the use of our approach for solving a challenging problem from the aviation domain.","Authors":"N. Andrienko; G. Andrienko; J. M. C. Garcia; D. Scarlatti","DOI":"10.1109\/TVCG.2018.2864811","Keywords":"Visual analytics;movement data;flight trajectories","Title":"Analysis of Flight Variability: a Systematic Approach","Keywords_Processed":"flight trajectory;movement datum;visual analytic","Keyword_Vector":[0.1170839308,-0.0677170296,-0.0453099284,-0.0335030997,-0.01453967,-0.083644123,-0.0771705316,-0.0453560365,-0.0966935541,0.0661510808,-0.0779867957,0.076496973,-0.0103855351,0.1676500059,-0.1659954512,0.0139124613,-0.1025973605,0.0402127415,-0.0813113287,-0.0142300368,0.0911105408,0.0112716542,-0.0378045313,0.0096399474,-0.0697689277,0.0027973208,-0.0415424703,0.0500116648,-0.0436413287,-0.0698344275,-0.0478186194,-0.0647709676,-0.0636003992,-0.0430180696,0.0129627645,-0.0639008342,0.0751336655,-0.0763404161,-0.0303812482,-0.0328981396,0.0126611162,0.0081694774,-0.0023153356,-0.0160828773,0.001707489,-0.0083084166,0.0079740082,-0.0055173925,0.0362556908,-0.0016229009,-0.0182176726,-0.0530911595],"Abstract_Vector":[0.2644497674,-0.1501837727,-0.0026495583,0.0111543521,-0.1600138925,0.0475395948,0.0605374245,0.0565884576,-0.0965415897,-0.0687317391,-0.0376149095,0.0461564628,0.073346408,-0.0256952328,0.0271574464,0.0299013907,0.0472137114,0.0929545544,-0.0090924834,-0.1447071281,0.0919090283,-0.0083611498,0.0251838807,-0.0040694035,-0.051150431,-0.0101587223,0.0675789996,-0.0684080335,-0.0541121969,-0.0215333569,-0.1091233642,-0.0220741553,0.0209524267,0.0019834946,-0.0169587532,-0.0607019615,0.0372181482,-0.0769641872,-0.0031575587,0.0380965576,-0.0721292221,-0.02256969,0.0328664908,-0.1151219859,0.0431747213,0.016602957,0.0383683577,0.0570997515,0.0209553528,0.0294928228,0.0102459059,0.0685404794,0.0169721076,0.0420458976,-0.0217052986,-0.0298112723,-0.0031526062,-0.0349534286,-0.0042749681,0.0197874646,0.0290527129,0.0223635382,-0.0382632507,-0.057472219,-0.0285425491,0.0376530641,-0.0021711565,0.0057762105,-0.0422698845,0.003799935,-0.0165607785,-0.0440444146,0.0219650669,0.0252597616,0.0003058208,0.0072029172,-0.040600914,0.0008120784,0.0265298807,-0.0183896332,0.0083900353,-0.0249675883,0.0150998501,0.0362682809,0.0257133215,0.0058609849,0.0074766892,-0.0076796032,-0.010797687,-0.0224878391,0.0039816963,0.0355479107,-0.0205392822,-0.0286071361,-0.0089719711,0.0472260469,0.042837447,-0.0199086929,0.0042045982,0.0538640049,0.0167747178,-0.0048497603,0.0272300474,-0.022738266,0.0015651765,-0.0027446629,-0.0058469769,-0.0031060332,0.0308171399,-0.0203044672,-0.0324300549,-0.0060068694,-0.0010155475,-0.0345469178,0.0166806813,-0.0034949197]},"328":{"Abstract":"With the broad range of motion capture devices available on the market, it is now commonplace to directly control the limb movement of an avatar during immersion in a virtual environment. Here, we study how the subjective experience of embodying a full-body controlled avatar is influenced by motor alteration and self-contact mismatches. Self-contact is in particular a strong source of passive haptic feedback and we assume it to bring a clear benefit in terms of embodiment. For evaluating this hypothesis, we experimentally manipulate self-contacts and the virtual hand displacement relatively to the body. We introduce these body posture transformations to experimentally reproduce the imperfect or incorrect mapping between real and virtual bodies, with the goal of quantifying the limits of acceptance for distorted mapping on the reported body ownership and agency. We first describe how we exploit egocentric coordinate representations to perform a motion capture ensuring that real and virtual hands coincide whenever the real hand is in contact with the body. Then, we present a pilot study that focuses on quantifying our sensitivity to visuo-tactile mismatches. The results are then used to design our main study with two factors, offset (for self-contact) and amplitude (for movement amplification). Our main result shows that subjects' embodiment remains important, even when an artificially amplified movement of the hand was performed, but provided that correct self-contacts are ensured.","Authors":"S. Bovet; H. G. Debarba; B. Herbelin; E. Molla; R. Boulic","DOI":"10.1109\/TVCG.2018.2794658","Keywords":"Virtual Reality;Avatar;Embodiment;Agency;Body Ownership;Self-contact","Title":"The Critical Role of Self-Contact for Embodiment in Virtual Reality","Keywords_Processed":"virtual reality;self contact;embodiment;body ownership;avatar;agency","Keyword_Vector":[0.0376036762,-0.0328826798,0.0086032327,-0.0490701389,0.0888968501,0.0384082493,0.2159714692,-0.0076602318,-0.0662197607,-0.0239621314,-0.0305806173,0.0496522554,0.0226464835,0.0195366901,0.0183407395,-0.0192547087,0.016834754,-0.0025001707,-0.0107062898,0.0238381534,0.0232020589,-0.0008739473,0.0150582054,-0.0045154265,-0.0850755278,-0.0006190702,-0.0156461362,-0.0407653506,-0.0050176237,0.0356956589,0.0198805776,-0.0131602628,0.0300785481,-0.0527789488,-0.0451943262,-0.0058148395,-0.0115591658,0.0595929812,-0.0148827783,0.0136952992,-0.0093676234,-0.0158075838,-0.0088076429,0.0069253614,-0.0198036734,0.0156984495,0.0100766593,-0.0158785081,-0.0143841826,-0.0324677812,0.0271567578,-0.00731948],"Abstract_Vector":[0.1935329618,0.0239809212,-0.0132943929,0.146735345,0.1235609978,0.0539058471,-0.0527839775,0.0160123274,0.0083229982,-0.0063416692,-0.0033918991,0.0781911931,0.0076864362,-0.0199513699,-0.0463080033,-0.0066473184,0.0170677915,0.0622844264,-0.1024228931,0.012560242,0.0143214462,-0.0491401219,-0.0156260532,0.0491894108,-0.0423524284,-0.0208028599,-0.008041773,0.0344985265,-0.0672123411,0.0440029899,-0.0011618697,-0.0096628129,-0.0430085758,-0.0283914312,0.0167272343,-0.0275353783,0.0150117946,0.0659054442,-0.0576768485,0.0241008838,-0.012181099,-0.0091213796,-0.0306969969,0.0171769185,-0.0493354325,0.0056855314,0.0093774106,-0.0022115795,0.0114555031,0.031985227,-0.0364645589,-0.0104107938,-0.0010330746,0.0105589287,0.0487377574,-0.0544551218,-0.0234875508,0.0074969192,-0.0104640706,0.0114542987,0.0031708061,0.008557462,0.0035173139,0.0225781469,0.0359145766,-0.0011500956,0.0065566958,0.0251069937,0.0400502346,0.0123031309,-0.032808652,-0.0107920291,-0.0076614115,-0.0565223378,0.0305707871,-0.0425571389,-0.0063224322,-0.0107866498,-0.0403822613,0.0099580848,-0.0193650717,0.0168594897,-0.012316451,-0.0220643001,0.0206346223,0.0172416118,-0.0296272894,-0.0029254519,-0.0090897673,-0.029086341,0.0061030847,0.0279282353,-0.0383199631,0.0332884331,0.015756437,-0.0221808437,0.0317806957,0.016297747,-0.0342542674,0.0080193744,-0.0524723347,0.0314610761,-0.0111282278,-0.0350309521,-0.0324869675,0.0014309838,-0.0109542943,0.000383559,-0.0136205912,0.0054531673,-0.0048055122,0.0312707681,0.0030091075,0.0096125923,-0.0367334042,0.0142255629]},"329":{"Abstract":"The rapid development of information technology paved the way for the recording of fine-grained data, such as stroke techniques and stroke placements, during a table tennis match. This data recording creates opportunities to analyze and evaluate matches from new perspectives. Nevertheless, the increasingly complex data poses a significant challenge to make sense of and gain insights into. Analysts usually employ tedious and cumbersome methods which are limited to watching videos and reading statistical tables. However, existing sports visualization methods cannot be applied to visualizing table tennis competitions due to different competition rules and particular data attributes. In this work, we collaborate with data analysts to understand and characterize the sophisticated domain problem of analysis of table tennis data. We propose iTTVis, a novel interactive table tennis visualization system, which to our knowledge, is the first visual analysis system for analyzing and exploring table tennis data. iTTVis provides a holistic visualization of an entire match from three main perspectives, namely, time-oriented, statistical, and tactical analyses. The proposed system with several well-coordinated views not only supports correlation identification through statistics and pattern detection of tactics with a score timeline but also allows cross analysis to gain insights. Data analysts have obtained several new insights by using iTTVis. The effectiveness and usability of the proposed system are demonstrated with four case studies.","Authors":"Y. Wu; J. Lan; X. Shu; C. Ji; K. Zhao; J. Wang; H. Zhang","DOI":"10.1109\/TVCG.2017.2744218","Keywords":"Sports visualization;visual knowledge discovery;sports analytics;visual knowledge representation","Title":"iTTVis: Interactive Visualization of Table Tennis Data","Keywords_Processed":"sport analytic;sport visualization;visual knowledge discovery;visual knowledge representation","Keyword_Vector":[0.039132729,-0.0178593487,0.0408832864,-0.0235251398,0.0315778781,0.0061640271,0.1308107406,-0.0312360651,-0.0280915905,-0.0399531664,-0.0250493335,0.0363191676,0.0062664331,0.0081544401,-0.0207858705,-0.0136355017,0.0457390849,-0.043823418,0.0207080085,0.0325909794,0.0172023232,0.0198424983,0.0377378464,0.0360593811,-0.0282392763,-0.0337045446,-0.0176888326,0.0564747331,-0.0271218004,0.0291698046,-0.0075039116,0.0628391458,0.0289864874,0.01370555,-0.0354798475,-0.0405139547,0.0370592421,0.021017801,-0.0038958118,0.0381363274,-0.0075428672,-0.0133663748,-0.0230725749,-0.032154001,0.0319152604,0.0199641123,0.0198820755,0.0043051189,0.0292311458,0.0056340523,0.0315026132,-0.0071867936],"Abstract_Vector":[0.1729432489,-0.0987112261,-0.0123556866,0.0322994762,0.0204655382,0.0186746649,-0.0112581243,-0.0175793027,-0.0351977379,0.0370464235,0.0153561492,-0.028390273,-0.0334219057,0.0170015678,0.0056348191,-0.0095180166,-0.0016246342,-0.0079061467,0.0080228602,0.009090084,-0.0053673167,-0.0041513118,0.0003261417,0.0047209855,-0.0246596815,-0.0030351246,-0.0102733453,-0.0159827421,0.0106861642,0.0010465344,0.0363605513,0.0209188275,-0.0358784094,-0.0157298147,0.0207675963,-0.0412664569,-0.0131237025,-0.0131535672,-0.054762695,-0.0309648723,0.0077525357,0.0311731807,0.0176607767,0.014727999,-0.0006648053,0.0104544281,0.0265491865,-0.0203506675,-0.0251667125,-0.0194045458,0.0107810418,0.0312735569,0.0248441866,0.0375860255,0.0055386786,0.0110995431,-0.0037424694,0.0035427328,0.0047900381,0.0259961176,0.0097400975,-0.0160268028,-0.0271557291,-0.0159340943,-0.0150918103,-0.0388000429,0.0097944173,-0.0163047572,-0.0316892254,-0.0070367297,-0.0332502281,0.0394571162,-0.0186566499,0.0068772984,0.0109849621,0.0349987577,-0.0201674664,0.0049738144,-0.0045491483,-0.0002012118,0.0064390046,0.0153901474,0.0085614929,0.0094130716,-0.0367393721,-0.0153512012,0.0001338798,-0.013686991,0.0026842265,0.0299996961,0.017057582,0.014424689,0.0057950236,0.0174453589,0.0375625446,-0.0048788843,-0.0002971033,0.0227838386,-0.0368156605,-0.0040201394,0.0119379984,0.0175293657,0.0105556475,0.0456356636,-0.0171413437,-0.0193317301,-0.0057139292,0.0443431629,-0.018610721,-0.0213311312,-0.001250471,0.0124139114,-0.0233376553,-0.0193009211,0.0021644369,0.013064049]},"33":{"Abstract":"Virtual Environments (VEs) provide the opportunity to simulate a wide range of applications, from training to entertainment, in a safe and controlled manner. For applications which require realistic representations of real world environments, the VEs need to provide multiple, physically accurate sensory stimuli. However, simulating all the senses that comprise the human sensory system (HSS) is a task that requires significant computational resources. Since it is intractable to deliver all senses at the highest quality, we propose a resource distribution scheme in order to achieve an optimal perceptual experience within the given computational budgets. This paper investigates resource balancing for multi-modal scenarios composed of aural, visual and olfactory stimuli. Three experimental studies were conducted. The first experiment identified perceptual boundaries for olfactory computation. In the second experiment, participants ($N=25$) were asked, across a fixed number of budgets ($M=5$), to identify what they perceived to be the best visual, acoustic and olfactory stimulus quality for a given computational budget. Results demonstrate that participants tend to prioritize visual quality compared to other sensory stimuli. However, as the budget size is increased, users prefer a balanced distribution of resources with an increased preference for having smell impulses in the VE. Based on the collected data, a quality prediction model is proposed and its accuracy is validated against previously unused budgets and an untested scenario in a third and final experiment.","Authors":"E. Doukakis; K. Debattista; T. Bashford-Rogers; A. Dhokia; A. Asadipour; A. Chalmers; C. Harvey","DOI":"10.1109\/TVCG.2019.2898823","Keywords":"Multi-Modal;Cross-Modal;Tri-Modal;Sound;Graphics;Olfactory","Title":"Audio-Visual-Olfactory Resource Allocation for Tri-modal Virtual Environments","Keywords_Processed":"olfactory;Cross Modal;Multi Modal;graphic;sound;tri Modal","Keyword_Vector":[0.1383572282,-0.0384564173,0.0578177233,-0.0775916752,-0.0012691354,0.0059719407,-0.0335639084,0.0138238038,0.0214443868,-0.0544364868,0.0083635337,-0.0114668577,-0.0358451945,0.0002982182,-0.0366584852,0.0553781428,-0.0403203355,0.0599234024,-0.0140103099,0.0349642355,-0.0263053049,-0.0102107817,-0.0230989746,-0.0016869708,0.0406565692,0.0147946204,0.0376447961,-0.0032974588,0.0303544981,0.0169016154,0.0105999753,0.0140443373,-0.0041634171,-0.0094963973,-0.0020047405,-0.0001896024,-0.0083932486,0.0089718912,-0.0169971785,-0.007520133,-0.0418337997,0.0062391487,0.0186527161,-0.0018184835,-0.0011838191,0.0019580287,0.0171551391,0.0071070934,0.0082540448,-0.0034837976,0.0041581596,0.0002215814],"Abstract_Vector":[0.1959919982,-0.0740605325,0.0370581234,-0.0154074506,0.0002968287,-0.0383912504,0.0396281843,-0.0122636124,0.04331742,0.0193332027,0.0218075333,0.0622286697,-0.0377132055,0.0291802681,0.003754306,-0.0355480462,0.0592649362,0.0372125599,-0.014181394,-0.0098552742,0.0277840065,-0.0464851374,0.0160017161,0.0340597381,0.0927730511,0.0457852255,-0.0536196453,0.0423964414,0.0234282414,-0.0164372335,0.0131643568,-0.0069528944,0.0060428006,-0.0411922463,0.0026608834,-0.0284125829,-0.0005550333,0.0336951717,-0.0609503087,0.0248733468,0.0156383285,0.0345193524,0.0100266229,-0.0220673402,0.0581904385,-0.0602906047,-0.053434083,0.0098692712,-0.0238575959,-0.0517429491,0.0023556766,0.0462943949,-0.034330533,-0.0058112021,-0.0716104891,0.0393988365,0.0173695161,-0.0164001015,0.0406022221,0.0926963868,0.0175162257,-0.0009744027,-0.0269814869,-0.026489123,0.0101036124,0.000328693,0.0090626099,-0.017614801,0.0854966862,0.0194770147,-0.0113258911,-0.0222899598,-0.0376153651,-0.0220714057,0.0741775138,0.038392616,-0.0121273128,0.0067436665,-0.0624346553,-0.0210683823,0.0454645362,0.0462956799,0.0189422294,0.0456728087,0.0416216362,-0.0090141795,-0.0135088177,0.0147558862,0.0024615064,0.0206759136,-0.0252350437,-0.0704427732,0.1556015475,-0.0858228755,0.1576980094,0.0457512522,-0.0346174931,0.1355580255,0.019548777,0.1785874971,-0.0268139816,-0.0601640398,-0.0436911824,-0.0317965018,0.0234722929,-0.0404688353,-0.0621388343,0.0864386353,0.050495602,-0.0925305106,-0.064002406,0.0358219732,-0.0346587648,-0.0297674035,0.0005190525,0.0895446472]},"330":{"Abstract":"The ubiquity of smart mobile devices, such as phones and tablets, enables users to casually capture 360\u00b0 panoramas with a single camera sweep to share and relive experiences. However, panoramas lack motion parallax as they do not provide different views for different viewpoints. The motion parallax induced by translational head motion is a crucial depth cue in daily life. Alternatives, such as omnidirectional stereo panoramas, provide different views for each eye (binocular disparity), but they also lack motion parallax as the left and right eye panoramas are stitched statically. Methods based on explicit scene geometry reconstruct textured 3D geometry, which provides motion parallax, but suffers from visible reconstruction artefacts. The core of our method is a novel multi-perspective panorama representation, which can be casually captured and rendered with motion parallax for each eye on the fly. This provides a more realistic perception of panoramic environments which is particularly useful for virtual reality applications. Our approach uses a single consumer video camera to acquire 200-400 views of a real 360\u00b0 environment with a single sweep. By using novel-view synthesis with flow-based blending, we show how to turn these input views into an enriched 360\u00b0 panoramic experience that can be explored in real time, without relying on potentially unreliable reconstruction of scene geometry. We compare our results with existing omnidirectional stereo and image-based rendering methods to demonstrate the benefit of our approach, which is the first to enable casual consumers to capture and view high-quality 360\u00b0 panoramas with motion parallax.","Authors":"T. Bertel; N. D. F. Campbell; C. Richardt","DOI":"10.1109\/TVCG.2019.2898799","Keywords":"Casual 360\u00b0 scene capture;plenoptic modeling;image-based rendering;novel-view synthesis;virtual reality","Title":"MegaParallax: Casual 360\u00b0 Panoramas with Motion Parallax","Keywords_Processed":"image base render;virtual reality;plenoptic modeling;Casual 360 scene capture;novel view synthesis","Keyword_Vector":[0.1439963131,-0.0063930271,-0.0089869519,-0.0694734044,0.082301364,-0.0334690828,0.0872166928,0.0148400832,-0.0550935983,-0.0791846666,0.0065335423,0.0409641351,-0.005863723,0.0587796896,0.0703534085,0.0467645851,-0.0639348994,-0.1481036219,0.1076295562,-0.0723175427,0.041793909,-0.0957758434,-0.0786578005,0.1178187034,0.0073719339,-0.1167356667,0.3383998091,0.1033266456,-0.1632920047,0.0492343729,-0.0515300634,0.0054772416,-0.0449773812,-0.1456724426,0.1013961405,0.1571420122,-0.1245555462,-0.0929622111,-0.1299795383,-0.0338090183,0.0856801893,-0.0219942752,0.1349095877,0.008186183,0.00795519,-0.110659466,0.094433841,-0.0463899653,0.0772263592,-0.0265268423,-0.046241635,-0.0031281152],"Abstract_Vector":[0.2408867875,-0.1011823637,-0.0448316649,-0.0017714731,-0.0258298342,0.1657202161,-0.01452561,0.0217044905,0.0387371937,-0.0240358034,-0.0670090443,-0.026326337,-0.1400613381,-0.0396583173,-0.0800891999,-0.0580596984,0.0075914925,0.0786940422,-0.0778060482,0.0565266663,-0.0607341327,0.1511254783,-0.0667735859,-0.0430397651,0.0784045186,-0.1296557176,-0.0905794315,-0.0077730559,-0.0024267522,-0.0453175744,-0.0102177963,0.0083061913,0.0492950964,0.0797946202,0.0339310701,-0.0317933358,0.029758327,-0.0367570918,-0.0036352583,-0.0563468912,-0.0023110438,-0.0458309079,-0.0577748497,0.012877732,0.0100178638,0.0069894305,0.0103331209,0.0205398946,-0.0067861551,0.0589568766,0.0066157578,-0.0165841208,-0.0461688281,0.0246755117,-0.0190249395,-0.0000286131,0.0650791611,0.0018787311,-0.015411375,-0.019220345,0.0125351387,0.0028849759,-0.0029220168,0.0199898695,-0.0191619526,0.0185841831,-0.0332068009,0.0094666748,0.0445962329,-0.025389059,0.0436178349,-0.0232885413,0.0359971216,-0.027954115,0.0161623919,-0.0063210568,-0.0389908792,-0.0115541405,0.0128525638,-0.0467811579,0.0099249421,0.0165179162,-0.0034877107,0.0417747678,-0.0038737538,-0.0088920544,0.0250614799,-0.0029980723,0.0312602194,-0.0433931249,0.0234725987,0.0005001656,-0.0297119645,-0.023369895,-0.0231245387,0.0591160332,0.0128230526,0.0017775672,-0.0536893669,0.0191094986,-0.0222081318,-0.0049530983,-0.0350996053,0.0105190174,0.0282648403,0.0135815762,-0.0097778561,-0.0356893386,0.0224794167,0.0272767768,0.0190618835,-0.0049514547,0.0080996833,-0.0321286474,-0.0089935243,0.0124268234]},"331":{"Abstract":"Between the recent popularity of virtual reality (VR) and the development of 3D, immersion has become an integral part of entertainment concepts. Head-mounted Display (HMD) devices are often used to afford users a feeling of immersion in the environment. Another technique is to project additional material surrounding the viewer, as is achieved using cave systems. As a continuation of this technique, it could be interesting to extend surrounding projection to current television or cinema screens. The idea would be to entirely fill the viewer's field of vision, thus providing them with a more complete feeling of being in the scene and part of the story. The appropriate content can be captured using large field of view (FoV) technology, using a rig of cameras for 1100 to 3600 capture, or created using computer-generated images. The FoV is, however, rather limited in its use for existing (legacy) content, achieving between 36 to 90 degrees (0) field, depending on the distance from the screen. This paper seeks to improve this FoV limitation by proposing computer vision techniques to extend such legacy content to the peripheral (extrafoveal) vision without changing the original creative intent or damaging the viewer's experience. A new methodology is also proposed for performing user tests in order to evaluate the quality of the experience and confirm that the sense of immersion has been increased. This paper thus presents: i) an algorithm to spatially extend the video based on human vision characteristics, ii) its subjective results compared to state-of-the-art techniques, iii) the protocol required to evaluate the quality of the experience (QoE), and iv) the results of the user tests.","Authors":"L. Turban; F. Urban; P. Guillotel","DOI":"10.1109\/TVCG.2016.2527649","Keywords":"Augmented video;large field of view;human vision;immersion","Title":"Extrafoveal Video Extension for an Immersive Viewing Experience","Keywords_Processed":"large field of view;human vision;immersion;augmented video","Keyword_Vector":[0.0265605072,0.0011478862,-0.0073838756,-0.009196567,0.0119789071,-0.0425375223,-0.0257352764,0.0159045121,-0.0411937411,-0.0166701805,-0.0117574447,0.0413189804,-0.0155706464,0.0303496877,-0.0037141579,0.0226372984,-0.0621353371,-0.058840366,0.0093670121,-0.0176426375,0.0271705803,-0.0805048808,0.0688636308,-0.0036502215,-0.0090701638,0.0763280201,-0.0152123665,0.0605495271,-0.0844280951,0.0600679876,0.0122602745,-0.0222573918,-0.0242560956,0.0761966608,0.0201943444,0.0893476556,-0.1661257897,-0.0348234018,0.0661134391,-0.0852765981,-0.0145758074,0.0081301082,-0.1086451918,-0.0467497632,0.0265480448,0.032628698,0.1664535421,-0.0368593848,-0.1262586313,0.0591274453,0.064308022,0.106612074],"Abstract_Vector":[0.1965287888,0.0253143366,0.0425084864,-0.0263021546,0.0006433019,0.0152893566,-0.0862456136,-0.0092258887,-0.0374409139,-0.0501639938,-0.0824514204,0.0483309846,-0.050206634,0.0342251179,0.0455511148,-0.0283950448,0.0006503797,0.0328387274,-0.0498729464,-0.0662421719,0.0346883334,-0.0267250669,-0.0085296002,-0.0045153272,-0.019652164,0.0419559118,0.004819642,-0.0065247118,0.0459869428,-0.0087216496,0.0608449474,0.0324444707,0.0199334458,0.0306006323,-0.0340398358,-0.0072524061,-0.0165896205,-0.0185800979,0.0190282166,-0.020920094,-0.0845677668,-0.0748021036,0.0352537868,-0.0241958398,-0.050770449,0.0112545461,0.0110462829,0.0795876254,0.0034545461,-0.0539112721,0.0225666701,-0.0623151589,0.0014466072,0.0023526443,0.0586481174,0.0373161757,-0.0051331504,-0.0192010696,-0.0357348923,0.0034765923,-0.0019753084,0.0277139193,-0.0228974317,-0.0267745975,0.0110599748,0.0029110044,-0.0098893851,0.0137377536,-0.0082764165,-0.0355310593,-0.0460469618,0.0203602056,-0.0014798487,-0.0180689212,-0.0090815643,-0.0589190185,0.0047993807,-0.0110612867,0.0138391763,-0.0417525304,0.0148179429,-0.0229226776,-0.0064778639,0.0263260846,0.0335617003,0.0070402415,-0.0026632167,-0.0087905192,0.0038552073,-0.0202541467,0.00526882,0.0176629485,-0.0022580422,0.0069217677,-0.022617763,-0.0094767274,-0.0011958082,-0.0125589671,0.0310514035,0.0170176566,0.0144828258,-0.0236262136,-0.0043180859,0.0076424982,-0.0034852705,0.0015628653,-0.0225890387,-0.0159737488,-0.0224740388,0.0166516875,-0.0049504571,0.0048670486,-0.0077333911,0.0086912693,-0.016675358,-0.0266155274]},"332":{"Abstract":"Many foundational visualization techniques including isosurfacing, direct volume rendering and texture mapping rely on piecewise multilinear interpolation over the cells of a mesh. However, there has not been much focus within the visualization community on techniques that efficiently generate and encode globally continuous functions defined by the union of multilinear cells. Wavelets provide a rich context for analyzing and processing complicated datasets. In this paper, we exploit adaptive regular refinement as a means of representing and evaluating functions described by a subset of their nonzero wavelet coefficients. We analyze the dependencies involved in the wavelet transform and describe how to generate and represent the coarsest adaptive mesh with nodal function values such that the inverse wavelet transform is exactly reproduced via simple interpolation (subdivision) over the mesh elements. This allows for an adaptive, sparse representation of the function with on-demand evaluation at any point in the domain. We focus on the popular wavelets formed by tensor products of linear B-splines, resulting in an adaptive, nonconforming but crack-free quadtree (2D) or octree (3D) mesh that allows reproducing globally continuous functions via multilinear interpolation over its cells.","Authors":"K. Weiss; P. Lindstrom","DOI":"10.1109\/TVCG.2015.2467412","Keywords":"Multilinear interpolation;adaptive wavelets;multiresolution models;octrees;continuous reconstruction;Multilinear interpolation;adaptive wavelets;multiresolution models;octrees;continuous reconstruction","Title":"Adaptive Multilinear Tensor Product Wavelets","Keywords_Processed":"continuous reconstruction;multiresolution model;multilinear interpolation;adaptive wavelet;octree","Keyword_Vector":[0.0486793837,0.0001402842,-0.0191518927,-0.0405365857,0.0160901258,-0.0589891491,-0.0746808078,-0.0162897207,-0.1376755838,-0.0145344239,0.0179755816,0.1125401125,0.0091052699,0.0393921736,0.0459632,0.0849516076,0.0268682259,0.0989138501,0.1693732397,-0.0292280196,-0.0653207264,0.0922633366,0.1676701749,0.0505036128,-0.123425459,-0.0204125972,0.0198274169,0.0041732385,0.0798014553,-0.1205549999,-0.0193593289,-0.0963767124,0.0818166396,0.0633450341,-0.0298536798,-0.0181854337,-0.0572936945,-0.029995807,-0.0172892156,-0.0805454875,-0.035754612,0.0661751806,0.0329440224,-0.0008791773,-0.0057120375,-0.0359758666,0.0824526951,0.065222317,-0.0434857908,0.0574318284,-0.0033666767,0.0491225306],"Abstract_Vector":[0.0887651959,-0.0181036931,-0.0167752171,-0.00595525,-0.057567274,0.0870440613,0.003853541,0.0318756875,0.0621044723,0.0050078378,-0.0215474141,0.0340830011,-0.0047612414,0.0004423982,0.0172234809,0.0326295845,0.0189206416,0.0126135763,0.0479818787,-0.0399021218,0.0192373303,-0.048976518,0.0233955075,-0.0339945446,-0.024500375,0.0121520569,0.0179287108,-0.0115628677,-0.0097532902,0.0268209524,0.0848935729,0.0056111577,-0.0159161214,-0.0215462255,0.0392284868,-0.0072035054,-0.0219177391,-0.000791023,-0.0471645135,0.0140208622,0.0640780153,0.0432109022,0.0219732298,-0.011578687,0.0262096192,0.0188679718,0.0244362719,0.0046467756,0.0113097247,-0.0083235926,-0.0094100794,-0.0429034081,0.0078964325,-0.0364195025,0.0224161086,0.0021128976,0.0021221542,0.0536494599,-0.0065259677,-0.0429772684,0.0109076942,0.0108723902,-0.0235819335,0.0162496285,-0.0204517593,-0.0345848874,-0.021689329,-0.053428252,-0.0368349822,0.0466354555,0.0140889711,0.0244041189,-0.0058674726,0.0386888918,0.0146594631,-0.0756063417,0.0493440682,-0.0072408887,0.0141774508,-0.0460708175,-0.0067918717,-0.0112560939,-0.0057191704,0.0532539195,0.0285246874,0.0037749434,-0.0068140627,0.0181335525,-0.0502192033,-0.0298795005,0.0113937235,0.0008141567,-0.0056821631,0.0190589441,0.0175036287,-0.0459280495,0.0232815209,-0.0171094903,0.0011429924,-0.0272063672,0.002084467,-0.000193677,-0.0476751005,0.0076807238,0.0288322423,0.036811036,0.0013010602,0.0215122243,0.0033806982,-0.0336076269,-0.0016829767,0.0260469542,0.0431879219,-0.0011411108,-0.0046550202,0.0188012167]},"333":{"Abstract":"With the growing adoption of machine learning techniques, there is a surge of research interest towards making machine learning systems more transparent and interpretable. Various visualizations have been developed to help model developers understand, diagnose, and refine machine learning models. However, a large number of potential but neglected users are the domain experts with little knowledge of machine learning but are expected to work with machine learning systems. In this paper, we present an interactive visualization technique to help users with little expertise in machine learning to understand, explore and validate predictive models. By viewing the model as a black box, we extract a standardized rule-based knowledge representation from its input-output behavior. Then, we design RuleMatrix, a matrix-based visualization of rules to help users navigate and verify the rules and the black-box model. We evaluate the effectiveness of RuleMatrix via two use cases and a usability study.","Authors":"Y. Ming; H. Qu; E. Bertini","DOI":"10.1109\/TVCG.2018.2864812","Keywords":"explainable machine learning;rule visualization;visual analytics","Title":"RuleMatrix: Visualizing and Understanding Classifiers with Rules","Keywords_Processed":"rule visualization;visual analytic;explainable machine learning","Keyword_Vector":[0.0247573012,-0.0026401682,-0.0072800789,-0.0051917769,0.0023010672,0.0043821464,0.0029639631,-0.0088448702,-0.0223301891,-0.0109680885,-0.0102625332,0.0083149895,0.0053014824,0.0065217344,-0.0103368345,-0.0039951279,0.0153021034,-0.0046725504,-0.0092099882,0.0111531816,0.0121033969,0.0107108224,-0.0016850677,0.0191232678,-0.0029105012,0.0337728485,-0.0019037296,0.0115369034,-0.0074701485,0.0247044921,-0.0137565614,-0.0015020059,-0.0175619571,0.017018809,-0.0093318077,-0.0107712948,0.0004037595,0.0062962681,0.0062455509,-0.0181026031,-0.0000801364,0.0476113119,-0.0185103548,-0.0140153518,-0.0269090378,0.0084263107,-0.0210586872,0.0186153844,0.0027571413,-0.0350742374,-0.0010740522,-0.0011328092],"Abstract_Vector":[0.1409154431,-0.0148217789,-0.0057542274,0.0181036669,-0.0189585753,0.0207126112,-0.0032175388,0.0035270668,0.0258739755,-0.0034308775,-0.0300128982,0.0308783412,-0.0510711536,0.0153206737,0.0089828341,0.0050663782,-0.0352913379,-0.0209911617,0.0008822084,-0.0198125519,0.0170769833,0.0001989706,0.0274557284,0.009575302,0.0061398552,0.0119146345,0.018732627,0.023134437,-0.0154626491,0.0201378071,0.0286548954,0.0291319935,-0.0495449845,-0.0042188192,0.0158232444,-0.0208777134,-0.0185347104,0.0077798027,0.0054568054,0.0019752325,0.0154806127,0.0199197114,-0.0121593401,-0.007557108,-0.0117111505,0.0149560415,0.0264162918,0.0320012286,-0.0247210617,0.0213055495,-0.0329055657,-0.0079932097,-0.0146911859,0.0085255247,0.0413457214,-0.011762237,0.0227573814,-0.0025691906,0.0270788312,0.0007657127,0.025717174,0.0141508212,0.0355994767,0.0074416193,0.0190042622,-0.0043291363,0.0088659379,-0.02363918,-0.0099399296,0.0258495297,-0.0109891608,0.019546422,0.0096342241,0.008314138,0.0084013604,-0.012361683,0.0123516177,-0.0106164661,0.0234838788,-0.0138382973,0.0075689522,0.0488703766,0.0442662831,0.0091497382,-0.0243291571,0.0024190937,-0.0072733299,0.0259492776,0.0204202289,-0.0054430824,0.0034487734,0.0120192528,-0.0053166625,0.0394519645,-0.0051774876,-0.0019372825,-0.0009466501,0.0012509147,0.0185055153,0.0036315568,-0.0209687541,0.014564102,-0.0261210715,-0.0207816361,-0.0154033503,-0.0224374835,-0.0041385827,-0.0002330234,-0.0004058751,0.0053528616,-0.012000749,0.0204738468,0.0017776156,0.0060667241,0.0026406742,-0.0038337519]},"334":{"Abstract":"Event sequences analysis plays an important role in many application domains such as customer behavior analysis, electronic health record analysis and vehicle fault diagnosis. Real-world event sequence data is often noisy and complex with high event cardinality, making it a challenging task to construct concise yet comprehensive overviews for such data. In this paper, we propose a novel visualization technique based on the minimum description length (MDL) principle to construct a coarse-level overview of event sequence data while balancing the information loss in it. The method addresses a fundamental trade-off in visualization design: reducing visual clutter vs. increasing the information content in a visualization. The method enables simultaneous sequence clustering and pattern extraction and is highly tolerant to noises such as missing or additional events in the data. Based on this approach we propose a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. We demonstrate the usability and effectiveness of our approach through case studies with two real-world datasets. One dataset showcases a new application domain for event sequence visualization, i.e., fault development path analysis in vehicles for predictive maintenance. We also discuss the strengths and limitations of the proposed method based on user feedback.","Authors":"Y. Chen; P. Xu; L. Ren","DOI":"10.1109\/TVCG.2017.2745083","Keywords":"Time Series Data;Data Transformation and Representation;Visual Knowledge Representation;Visual Analytics","Title":"Sequence Synopsis: Optimize Visual Summary of Temporal Event Data","Keywords_Processed":"Data Transformation and Representation;Time Series Data;visual Knowledge Representation;visual analytic","Keyword_Vector":[0.0465501343,-0.0402255623,-0.0272147707,0.0343770655,0.0237415933,0.0134318212,0.0540286588,0.0132719373,-0.0375806808,-0.0220819431,-0.0122089171,-0.0164258242,0.0126265634,0.1208247001,-0.0267615758,-0.0361265827,0.0136019728,0.0253107798,0.0167112051,-0.0285395408,0.0133683784,-0.0584045999,-0.0051327572,0.0052399124,-0.0282776546,0.0110278352,0.0432908215,-0.041041252,-0.0431333838,0.0007390948,-0.0176113439,-0.0246671906,-0.003170963,0.0228850698,-0.0073195865,-0.0358163023,-0.0226554929,-0.0069079596,-0.0163783451,0.0140131581,-0.0298182351,-0.017628225,-0.0000826074,0.0035142949,-0.0108973717,0.0234731837,0.0028396553,-0.0312770298,0.0000350351,-0.0131240071,-0.0101520521,0.0483795438],"Abstract_Vector":[0.2713420367,-0.1750717018,0.0058928744,0.0130230409,-0.0236114071,-0.0461599336,-0.046399758,-0.032698388,0.0136117253,-0.0426672236,0.017526403,-0.0119229809,-0.03301344,-0.029367058,-0.0002078085,0.044985945,-0.0558870513,-0.0699995811,-0.0166115901,-0.0168550481,0.0076782767,-0.0013413771,-0.0078212021,-0.0274894746,-0.0658262037,-0.0567720948,0.0334172878,0.0296442992,-0.0356120008,-0.0783442043,0.0226670631,-0.0078339381,-0.0371092516,0.0138903508,-0.0108894915,0.0111687125,-0.0287568735,-0.0458371183,0.031767188,0.0100222665,0.0290235985,0.0056786535,-0.0111612793,0.0212069192,-0.0612676213,-0.0279599096,-0.0351249292,0.0043741083,-0.0528198271,-0.0000746386,-0.0336472663,0.0264401081,-0.0342131098,-0.0198471855,0.0383300515,-0.0468512191,-0.0030080911,-0.0342469628,0.0258866816,0.0125249417,-0.0240098573,-0.007587369,-0.0229671699,-0.0228993236,0.0300956232,-0.0265866178,-0.0402450213,-0.0095485335,0.0523252239,-0.0362620399,0.0703942667,-0.002042858,0.0246649204,-0.0214015764,-0.0054575703,0.0173214136,0.0064353816,-0.0132024384,-0.0246496362,0.0009598668,0.011981582,-0.0199506923,-0.0848834354,-0.0742957664,0.0195128028,-0.0464651606,0.024763653,0.0169657163,-0.0462016581,-0.0249982515,0.0436863809,0.0118658068,0.0103331967,0.0085572455,0.055001738,-0.0011326552,0.0167114957,0.0152847119,-0.0130106691,-0.0221844316,-0.0407207308,-0.0160690274,0.0094845386,0.0215953425,-0.0088509801,-0.0083658764,-0.0482830954,-0.0351641536,-0.0145561869,-0.0091186095,-0.015068923,-0.0055630506,-0.0235620936,0.001090239,0.0119185266,0.020500269]},"335":{"Abstract":"Recently, an approach for determining the value of a visualization was proposed, one moving beyond simple measurements of task accuracy and speed. The value equation contains components for the time savings a visualization provides, the insights and insightful questions it spurs, the overall essence of the data it conveys, and the confidence about the data and its domain it inspires. This articulation of value is purely descriptive, however, providing no actionable method of assessing a visualization's value. In this work, we create a heuristic-based evaluation methodology to accompany the value equation for assessing interactive visualizations. We refer to the methodology colloquially as ICE-T, based on an anagram of the four value components. Our approach breaks the four components down into guidelines, each of which is made up of a small set of low-level heuristics. Evaluators who have knowledge of visualization design principles then assess the visualization with respect to the heuristics. We conducted an initial trial of the methodology on three interactive visualizations of the same data set, each evaluated by 15 visualization experts. We found that the methodology showed promise, obtaining consistent ratings across the three visualizations and mirroring judgments of the utility of the visualizations by instructors of the course in which they were developed.","Authors":"E. Wall; M. Agnihotri; L. Matzen; K. Divis; M. Haass; A. Endert; J. Stasko","DOI":"10.1109\/TVCG.2018.2865146","Keywords":"Visualization evaluation;heuristics;value of visualization","Title":"A Heuristic Approach to Value-Driven Evaluation of Visualizations","Keywords_Processed":"visualization evaluation;value of visualization;heuristic","Keyword_Vector":[0.0966253711,0.0237856619,-0.0200294987,-0.0333514601,0.0246843453,-0.0519220442,-0.0684346056,-0.0806808459,-0.099063279,-0.0486477837,-0.0010054,0.0327672189,0.0069676304,0.0056873445,0.073555791,0.0387672803,0.0616940522,0.0280176166,0.0947769577,-0.0159603779,0.0956584918,0.047501282,-0.1706524905,0.0813102409,0.0082758555,0.081249351,-0.0414971623,0.0377220499,0.0419482671,0.1109425136,-0.0884927842,-0.0103434817,-0.0191133567,0.0187724752,-0.023143291,-0.024387064,-0.0676633487,0.0807935127,-0.0589070875,-0.0149472322,0.0147215913,0.0029041384,-0.1102610439,-0.0749965486,0.048309932,0.0697737567,-0.0607220142,-0.0846371458,0.0202893972,-0.0432363162,0.0144473105,-0.0726399854],"Abstract_Vector":[0.1944987915,0.0782425131,-0.0025514845,0.1145506507,-0.0656498836,-0.0388444597,0.1043892148,0.0781836777,-0.0157677195,-0.0367587352,-0.0706166381,-0.0041043379,-0.0088386799,0.1291909711,-0.0049233452,0.0850953679,0.0136050945,0.0864686996,-0.0015251834,-0.0539499567,-0.060019239,-0.0807753755,0.0067095556,0.0497487554,0.0858268115,-0.0150184931,-0.0240275446,0.083644815,0.0452604697,0.011461252,0.0479950268,-0.040838153,0.0445084352,0.0272655137,0.032890363,-0.0879079185,-0.0387623983,-0.0201943323,-0.0072287307,0.009411334,0.0403490176,-0.0073767465,0.0528744296,0.0086915995,0.0174849182,-0.0677396239,0.0051302624,0.0375145284,-0.0027237686,-0.0230856556,0.0108076299,-0.0041404638,0.0391516148,-0.0115044304,-0.0148418632,-0.0264567711,-0.0090099211,0.008938352,-0.000688486,-0.0330262689,-0.007820204,-0.0332392132,0.0566406916,-0.0671115781,0.0002578009,-0.0350690086,-0.0100488652,-0.0226168874,0.0485706547,0.0192013186,-0.0226866514,0.0303619762,0.0077281571,0.0120329199,0.0015601588,-0.0322571696,0.0071367969,0.0139335799,-0.0057495557,0.0249988852,-0.0247057383,0.0649514892,0.0396219813,0.0209753982,0.0154843751,-0.0148280994,0.0116313498,0.0029745926,0.0390461767,-0.0266822837,-0.0103718248,-0.0500597113,-0.0049254358,-0.0238361126,-0.0170732857,-0.0290838922,-0.0219404031,0.0528004027,0.05637437,0.0493377813,-0.0370679354,-0.0258255203,-0.0272929001,0.0069802788,-0.0165963768,-0.0182265836,0.0032835247,0.0041974102,-0.0037215268,0.0497959693,0.0088830705,0.0479493396,-0.0200803472,0.0418306909,0.0279557371,0.0246983918]},"336":{"Abstract":"While deep learning models have achieved state-of-the-art accuracies for many prediction tasks, understanding these models remains a challenge. Despite the recent interest in developing visual tools to help users interpret deep learning models, the complexity and wide variety of models deployed in industry, and the large-scale datasets that they used, pose unique design challenges that are inadequately addressed by existing work. Through participatory design sessions with over 15 researchers and engineers at Facebook, we have developed, deployed, and iteratively improved ActiVis, an interactive visualization system for interpreting large-scale deep learning models and results. By tightly integrating multiple coordinated views, such as a computation graph overview of the model architecture, and a neuron activation view for pattern discovery and comparison, users can explore complex deep neural network models at both the instance-and subset-level. ActiVis has been deployed on Facebook's machine learning platform. We present case studies with Facebook researchers and engineers, and usage scenarios of how ActiVis may work with different models.","Authors":"M. Kahng; P. Y. Andrews; A. Kalro; D. H. Chau","DOI":"10.1109\/TVCG.2017.2744718","Keywords":"Visual analytics;deep learning;machine learning;information visualization","Title":"ActiVis: Visual Exploration of Industry-Scale Deep Neural Network Models","Keywords_Processed":"deep learning;visual analytic;machine learning;information visualization","Keyword_Vector":[0.0550459006,-0.006068808,-0.0297564535,-0.0562830582,0.0155856509,-0.04789911,-0.0956663779,-0.0253754447,-0.1575464797,0.0395547689,0.0042869361,0.0969591165,0.0329007781,0.0163602439,0.0504536762,0.0353057701,0.0789623922,0.098719492,0.2017849401,-0.0728186487,-0.0778191477,0.1794312639,0.2162753588,0.0888489766,-0.0881677143,-0.1007998351,0.0544233529,-0.0703176382,0.0956708119,-0.142997596,-0.0230912641,-0.0883069054,0.0538105071,0.0326707378,-0.0026823653,0.0085879243,-0.0032635166,0.0092852582,0.0028014042,-0.0305404531,-0.0624727038,0.0086211345,0.067948416,0.023288019,-0.0270073586,-0.0101426864,0.0182773023,0.0517658793,-0.0070510211,0.0415979078,-0.0215971413,0.030984561],"Abstract_Vector":[0.1451600304,-0.0392088559,0.0103279479,0.0176962103,-0.0879995367,0.0922967062,-0.0108698768,0.0057718598,0.0632762183,0.0295418555,-0.0269971304,0.0445253814,-0.0118586166,-0.0192918664,0.0208111589,0.0512531506,0.020109516,-0.0039053478,0.0523494144,-0.0355902531,-0.0043338158,-0.0707943565,0.0487080267,-0.0105886408,0.0085388044,-0.0228743769,0.0023374294,-0.0031383175,-0.004978162,0.0260758219,0.094137499,0.0195526798,-0.0381214652,-0.0115780283,0.0854630491,0.0227384411,-0.0323228147,-0.0307523981,-0.0407171747,0.004426867,0.02604355,-0.0012243835,0.0150250445,-0.0134691854,0.0422208984,0.0233908937,0.0104282247,0.0081467812,0.0154259831,0.0109274331,-0.0161420205,-0.0193851293,-0.0060614128,-0.0413385139,-0.0134263567,-0.0204456566,0.0164652109,0.0668684272,0.0052518196,-0.0451136211,0.0156321359,-0.0241666903,-0.0286158492,0.0082358135,-0.0139173754,0.0083533029,-0.0185516789,-0.0516339239,-0.0583325315,0.0223751085,-0.0075442522,-0.0034368665,0.0093809305,0.0190418909,0.0267166631,-0.0715707169,0.036929418,0.0068582998,0.0003892476,-0.0529266469,0.0246241202,-0.0033960504,-0.0491009083,0.0804850333,0.0226520309,0.020753634,0.0194295922,-0.0038617658,-0.0232352485,0.0073167968,-0.0086027173,0.0459007715,0.0469906453,0.0222530951,0.0094069192,-0.0505372076,0.0213293807,-0.0313725392,-0.069123267,0.0041537776,0.0054111286,-0.0223944989,-0.0227223499,0.0501301575,0.0437372908,0.0364847479,0.0348072214,0.0559073254,-0.0074689664,-0.0211961271,0.014574736,0.0138327451,0.037947652,0.001174644,0.0133993717,0.0031775362]},"337":{"Abstract":"A narrative collage is an interesting image editing method for summarizing the main theme or storyline behind an image collection. We present a novel method to generate narrative images with plausible semantic scene structures. To achieve this goal, we introduce a layer graph and a scene graph to represent the relative depth order and semantic relationship between image objects, respectively. We first cluster the input image collection to select representative images, and then we extract a group of semantic salient objects from each representative image. Both layer graphs and scene graphs are constructed and combined according to our specific rules for reorganizing the extracted objects in every image. We design an energy model to appropriately locate every object on the final canvas. The experimental results show that our method can produce competitive narrative collage results and that it performs well on a wide range of image collections.","Authors":"F. Fang; M. Yi; H. Feng; S. Hu; C. Xiao","DOI":"10.1109\/TVCG.2017.2759265","Keywords":"Narrative collage;image collections;image segmentation;scene graphs;image synthesis","Title":"Narrative Collage of Image Collections by Scene Graph Recombination","Keywords_Processed":"narrative collage;image segmentation;image synthesis;image collection;scene graph","Keyword_Vector":[0.1089096644,-0.0032040277,0.0030289113,-0.0102958112,-0.0137442754,0.0011343705,-0.0631653395,-0.0342844684,-0.0812217658,-0.0422802096,0.0486664236,0.0359118906,-0.0006261692,0.023257538,0.0544312211,0.1327845142,0.0719285012,0.0543489429,0.1721107398,-0.0629117641,-0.0593634407,0.1493323899,-0.0057170052,0.067604482,-0.1102531976,0.0583983586,-0.0194926338,-0.0231218892,0.0582268959,0.0423009616,-0.0773491256,-0.1279926056,-0.0358576605,0.0924419688,-0.0194074333,0.1384121641,-0.0289364335,0.0627026832,-0.0213651908,-0.0468078157,0.0559942582,0.0720685454,0.0275118235,-0.0571792292,0.0879936555,-0.1067571778,0.0103564087,0.0043201593,0.0490494018,-0.0396031805,0.0654285011,0.0034894195],"Abstract_Vector":[0.1626871001,-0.0498321744,0.0036954053,0.0156603722,-0.017086868,0.0233169245,0.0780370176,0.0220420749,-0.0091010437,0.0182779452,-0.0009526805,0.0021453656,0.0151713869,-0.0199668382,-0.0202954744,0.0176085036,-0.01755551,0.0239131599,-0.0072869135,0.0133834009,0.003111149,-0.0178618765,0.0037995041,0.0389878482,-0.0013063818,-0.0115618683,-0.009444246,0.0057010958,0.0171342971,0.0218615249,0.0023788905,0.0199010754,0.0313217718,0.0111281049,-0.0292705153,-0.0139252153,-0.0111822603,0.0224136764,-0.0082856228,-0.0155844246,0.0246086878,0.0099773343,-0.005862706,0.0226817538,0.0173322732,0.0114407666,0.0257760904,0.002879308,-0.0011712722,-0.0275764323,-0.0405613322,-0.0039674124,-0.0011478836,-0.0208334811,0.0291982774,0.0000443373,-0.0096618549,-0.0071391053,0.022091257,-0.0350932993,0.0163019182,0.0226147907,0.0003544464,-0.0011825801,-0.0276166381,-0.0600108907,-0.0136048259,-0.0339830339,-0.0241958501,-0.0234832545,0.0324666401,-0.0013729941,-0.0099415879,0.0009078839,0.027323382,-0.0239723335,0.0322618287,-0.0079278526,0.0342073419,-0.0114660343,0.0051579847,-0.0091238504,0.0273696302,0.0066158748,-0.0017171127,-0.0097150086,-0.0327593728,0.0270742737,-0.0194783296,-0.0190882382,0.0178961872,0.0007696965,-0.0152450886,-0.0131153734,-0.0510178106,-0.0082102246,0.0020880349,0.0214713007,-0.0173090557,-0.0196729594,0.0039068602,0.003640381,-0.0392644762,-0.0225238531,-0.0231105947,0.0220379154,0.0282938994,-0.0141868257,0.0227456369,-0.0117081192,0.0012392291,0.005025498,0.0238568337,0.0144625964,0.0172877363,0.0398522898]},"338":{"Abstract":"Graphic design tools provide powerful controls for expert-level design creation, but the options can often be overwhelming for novices. This paper proposes Context-Aware Asset Search tools that take the current state of the user's design into account, thereby providing search and selections that are compatible with the current design and better fit the user's needs. In particular, we focus on image search and color selection, two tasks that are central to design. We learn a model for compatibility of images and colors within a design, using crowdsourced data. We then use the learned model to rank image search results or color suggestions during design. We found counterintuitive behavior using conventional training with pairwise comparisons for image search, where models with and without compatibility performed similarly. We describe a data collection procedure that alleviates this problem. We show that our method outperforms baseline approaches in quantitative evaluation, and we also evaluate a prototype interactive design tool.","Authors":"B. Kovacs; P. O'Donovan; K. Bala; A. Hertzmann","DOI":"10.1109\/TVCG.2018.2842734","Keywords":"Graphic design;machine learning;AB testing;image search;user interfaces;color","Title":"Context-Aware Asset Search for Graphic Design","Keywords_Processed":"image search;user interface;color;machine learning;Graphic design;AB testing","Keyword_Vector":[0.0683526153,0.0392918158,0.0019776414,0.0326786576,0.0390422704,-0.0367161043,-0.004328263,0.0136313007,-0.037725904,-0.0638622131,0.0129852154,0.0404744615,0.0472728726,0.0433847422,0.0983674149,0.0308621048,0.0133058466,-0.0676081168,0.0305470005,-0.0143383609,0.087619218,-0.1597617392,0.0212116589,0.0708625252,0.006482681,-0.0485114189,0.1087899255,0.1166499393,0.0107504095,0.0152251815,-0.0292570974,-0.0345411016,-0.0421224143,-0.0565984577,0.0708308975,0.1731191692,0.1045184704,-0.0605089524,-0.0289785313,0.0104840605,0.0987831884,0.0136080344,0.0613836197,0.0294760901,-0.0371275282,-0.0412806987,-0.0692486438,-0.0155990172,-0.0588128088,-0.030897085,0.002639481,0.081560545],"Abstract_Vector":[0.1813789653,0.081436599,0.0152078633,0.0253451222,0.0094565386,0.0533011169,-0.0212828133,0.0354991117,0.0518757087,-0.0360698993,-0.0021941307,-0.0375310589,-0.0385661965,0.0741049749,-0.0163062766,-0.0654803045,-0.0022015499,0.0016393896,0.0088667803,-0.0144609071,0.0556265247,0.0925397621,0.0001393674,-0.0051105849,0.0290069115,0.0158169027,-0.0239426434,0.0190203908,0.051911148,-0.0832670973,-0.0061397879,0.0840899066,-0.0054691736,0.0810647731,0.0215051789,0.0273741187,-0.043707444,0.0210747792,0.0075265837,0.0868731446,-0.0218509781,0.0546400674,-0.0397090781,-0.0528502796,-0.0404460768,0.0315812748,0.0776992356,-0.0332611792,0.0366867331,0.0148093932,0.0179044406,-0.0386023842,0.0063136777,0.0280459834,0.001987055,0.0293051052,0.0385261785,-0.0504371327,-0.0004668411,-0.0189381698,0.0506342029,0.0169134409,0.0266087454,0.0396530442,0.0192498524,-0.0084976374,-0.0198360301,-0.0326781875,-0.0269887057,-0.0427060053,0.0193753891,0.0586566566,0.0570374997,-0.0024712801,-0.0188292607,-0.0127392935,-0.0236322564,-0.0483043991,-0.030677551,-0.011183398,0.000585362,-0.0183761456,-0.013096466,0.0167251651,-0.0085527839,0.036757318,0.0481283058,0.0114611475,0.0387882425,0.017103722,-0.0200346519,0.0293924184,-0.0077630661,-0.0135007241,0.0347844487,0.0156881239,0.0389344823,0.0362516581,-0.0080791158,0.0531401262,0.0006758813,-0.00505886,0.0043106109,0.0308929987,0.0218502189,-0.0027391141,-0.0202342258,-0.016238632,0.0017043425,-0.0023002889,-0.0491073737,0.0087394349,0.0065426098,0.0151307869,-0.0138848651,0.002498875]},"339":{"Abstract":"Due to the perceptual characteristics of the head, vibrotactile Head-mounted Displays are built with low actuator density. Therefore, vibrotactile guidance is mostly assessed by pointing towards objects in the azimuthal plane. When it comes to multisensory interaction in 3D environments, it is also important to convey information about objects in the elevation plane. In this paper, we design and assess a haptic guidance technique for 3D environments. First, we explore the modulation of vibration frequency to indicate the position of objects in the elevation plane. Then, we assessed a vibrotactile HMD made to render the position of objects in a 3D space around the subject by varying both stimulus loci and vibration frequency. Results have shown that frequencies modulated with a quadratic growth function allowed a more accurate, precise, and faster target localization in an active head pointing task. The technique presented high usability and a strong learning effect for a haptic search across different scenarios in an immersive VR setup.","Authors":"V. A. de Jesus Oliveira; L. Brayda; L. Nedel; A. Maciel","DOI":"10.1109\/TVCG.2017.2657238","Keywords":"Vibrotactile head-mounted display;haptic interaction;spatial awereness;3D environments","Title":"Designing a Vibrotactile Head-Mounted Display for Spatial Awareness in 3D Spaces","Keywords_Processed":"spatial awereness;3d environment;Vibrotactile head mount display;haptic interaction","Keyword_Vector":[0.1690024944,-0.1360448755,-0.1117764248,0.254002637,-0.0059683038,0.1273317066,0.0292547953,0.1938609992,-0.0845478701,-0.0372813291,-0.1589397025,-0.0041323112,-0.0631833003,-0.0748500714,0.120164265,-0.2024468343,0.0119243693,0.0744569703,0.0059768183,-0.063248955,0.0346940147,-0.0771277334,-0.0809908075,-0.0111886559,-0.1176251119,-0.0208565296,0.057057335,-0.0948107204,0.0122189021,-0.0229082656,0.0357055686,-0.051672686,0.0193842801,0.102373505,-0.056565843,-0.0237976491,-0.0133848653,-0.0589185108,0.0851605174,0.0433605822,0.0881344987,-0.0244505335,0.0529711694,-0.0433869934,-0.0025176432,-0.0875951851,0.0686175174,-0.0063162348,-0.0262064987,0.0689306414,0.0288451137,0.0299263499],"Abstract_Vector":[0.2359608892,-0.0151758975,-0.0013730271,0.0214523812,0.0031987537,-0.0462179733,-0.1051070841,-0.0391795753,0.0140071418,-0.0557292255,-0.034276984,-0.0928082721,-0.0729178101,-0.0477747974,-0.0266287957,0.046897662,0.0269149509,0.0655496853,0.0218907076,-0.036747704,0.0442072862,-0.0321405328,0.0355193277,-0.0479223819,-0.0386512396,0.029583179,-0.0715785848,0.019264336,0.0159612453,-0.0248134049,-0.0630981767,0.0177926373,-0.0000085493,0.0278272483,0.0337596156,0.016866156,0.0011324317,0.0106975032,-0.0158066138,0.129208301,-0.0711466329,0.0335750424,0.0243591651,0.0103428202,-0.0170134478,-0.023856091,0.0233264194,-0.0383540952,0.00522087,0.0218708537,0.000528577,0.0688025028,-0.0297691491,-0.027587856,0.0273196087,-0.0061170496,-0.0383515634,-0.0428798604,-0.0427331574,-0.0240112982,0.0144476938,-0.0378370591,0.0369289844,0.0626668851,-0.0016074013,0.0009998946,0.0746061049,0.013841233,-0.0351870768,0.0059837855,-0.0113512324,0.0382215503,0.0155531285,-0.0363831247,0.0119738744,0.0133408682,-0.0096412408,-0.0127348423,-0.0340764993,-0.0603714784,-0.0231928258,-0.0141168577,-0.0113735273,0.0055278658,0.0480379316,0.019721324,-0.030504921,0.0231438466,0.034370938,-0.0110627172,-0.0375287984,-0.0431468864,-0.0003551258,0.015035783,-0.0164660869,0.0225206747,0.0073561567,0.0178175638,0.0279056268,0.013091435,-0.0036939026,0.0032124581,-0.0027335866,0.0465829782,-0.0162381174,-0.0379804286,0.010201207,0.0018633511,0.0026107061,-0.0173127111,-0.0459459244,0.0612783591,0.0048868243,0.0108509384,-0.0387246991,-0.0055671923]},"34":{"Abstract":"Dimensionality reduction (DR) is a common strategy for visual analysis of labeled high-dimensional data. Low-dimensional representations of the data help, for instance, to explore the class separability and the spatial distribution of the data. Widely-used unsupervised DR methods like PCA do not aim to maximize the class separation, while supervised DR methods like LDA often assume certain spatial distributions and do not take perceptual capabilities of humans into account. These issues make them ineffective for complicated class structures. Towards filling this gap, we present a perception-driven linear dimensionality reduction approach that maximizes the perceived class separation in projections. Our approach builds on recent developments in perception-based separation measures that have achieved good results in imitating human perception. We extend these measures to be density-aware and incorporate them into a customized simulated annealing algorithm, which can rapidly generate a near optimal DR projection. We demonstrate the effectiveness of our approach by comparing it to state-of-the-art DR methods on 93 datasets, using both quantitative measure and human judgments. We also provide case studies with class-imbalanced and unlabeled data.","Authors":"Y. Wang; K. Feng; X. Chu; J. Zhang; C. Fu; M. Sedlmair; X. Yu; B. Chen","DOI":"10.1109\/TVCG.2017.2701829","Keywords":"Dimensionality reduction;supervised;visual class separation;high-dimensional data","Title":"A Perception-Driven Approach to Supervised Dimensionality Reduction for Visualization","Keywords_Processed":"dimensionality reduction;high dimensional datum;supervise;visual class separation","Keyword_Vector":[0.1531838444,0.0102543091,0.0288048796,-0.0104964161,0.0281982007,0.0603375687,-0.0564377141,-0.0459647018,0.0431213467,-0.0265297288,0.1037712559,0.0913188068,-0.1364621334,-0.0234406237,0.0813694896,0.0899750414,-0.1165211454,-0.0442338489,0.0537246024,-0.1889961533,0.2229638811,0.0555126647,-0.2301849168,0.0080761096,-0.0225795246,-0.0171143488,0.1122322056,0.0435580163,0.2055146489,-0.0649806505,-0.0289109318,-0.0078139236,0.096359225,-0.0565627168,0.0013151844,-0.0841099953,-0.0052972022,0.0928426964,0.0465508064,0.0085403104,-0.116925255,-0.090118892,-0.1150307262,-0.0104717683,0.0075832582,0.2505556644,-0.0307011095,-0.0829158385,-0.0554964018,-0.1133962449,-0.0026212214,0.0628814174],"Abstract_Vector":[0.2590993774,0.0583956119,-0.0072666771,-0.0316580481,-0.005675103,-0.028752713,0.0481211383,-0.0017675259,0.0686279271,-0.0361905031,0.0571394185,-0.0407347792,-0.0298777648,-0.056737644,-0.0035853422,-0.0308886916,-0.0833152953,0.0411972077,0.0003615525,-0.0544776203,-0.018966291,-0.1456814825,0.0437318607,0.0146552921,0.1708955703,0.008657211,-0.0110519123,0.0389457555,0.0137328181,-0.0397391697,-0.0327989235,0.0944210441,-0.0317566959,0.0604601624,0.1168125437,0.0801081809,0.1157200403,0.041340185,-0.0234854435,-0.0226142858,-0.0143318576,-0.0021601729,0.0219600666,0.022305327,-0.0101108503,0.0917607634,-0.0271039301,0.0141819897,-0.0017295274,-0.0618797775,0.0241662302,-0.0466396833,0.0218224519,0.0099612348,0.0297963669,-0.0479463033,-0.033937931,-0.0120754648,0.0762464219,0.0354340658,0.0234273215,-0.0318386793,-0.0644143614,-0.0052414264,0.0152774166,-0.0091807431,0.0043628618,0.051016598,0.010614474,-0.0047035526,0.0084504366,-0.0075559937,0.0250271409,0.0320792954,0.0016043121,0.0148605151,-0.0185499321,-0.0184055839,0.0019348104,0.0537597802,0.0347097274,-0.0346598412,-0.0164279939,-0.0237746318,-0.0451897426,0.016195055,0.0269349575,0.0242475255,-0.0116556214,-0.0050667482,-0.0018668669,-0.007138698,-0.0088319926,-0.0027942628,-0.0404637512,0.0128874673,0.0322833955,-0.0208342243,-0.0258037766,0.0045455156,0.009644078,-0.0109669419,0.0137522292,-0.024738437,-0.0189968072,-0.0132390555,0.0366393613,-0.0024463405,0.0212069207,-0.0179344612,-0.0224731801,0.0039902914,0.0505059176,-0.0047631504,-0.0198583086,-0.0009875549]},"340":{"Abstract":"We propose a system to facilitate biology communication by developing a pipeline to support the instructional visualization of heterogeneous biological data on heterogeneous user-devices. Discoveries and concepts in biology are typically summarized with illustrations assembled manually from the interpretation and application of heterogenous data. The creation of such illustrations is time consuming, which makes it incompatible with frequent updates to the measured data as new discoveries are made. Illustrations are typically non-interactive, and when an illustration is updated, it still has to reach the user. Our system is designed to overcome these three obstacles. It supports the integration of heterogeneous datasets, reflecting the knowledge that is gained from different data sources in biology. After pre-processing the datasets, the system transforms them into visual representations as inspired by scientific illustrations. As opposed to traditional scientific illustration these representations are generated in real-time - they are interactive. The code generating the visualizations can be embedded in various software environments. To demonstrate this, we implemented both a desktop application and a remote-rendering server in which the pipeline is embedded. The remote-rendering server supports multi-threaded rendering and it is able to handle multiple users simultaneously. This scalability to different hardware environments, including multi-GPU setups, makes our system useful for efficient public dissemination of biological discoveries.","Authors":"P. Mindek; D. Kou\u0159il; J. Sorger; D. Toloudis; B. Lyons; G. Johnson; M. E. Gr\u00f6ller; I. Viola","DOI":"10.1109\/TVCG.2017.2744518","Keywords":"Biological visualization;remote rendering;public dissemination","Title":"Visualization Multi-Pipeline for Communicating Biology","Keywords_Processed":"biological visualization;public dissemination;remote render","Keyword_Vector":[0.0603000246,-0.0133662843,0.03477025,0.0277224358,-0.0223618496,-0.046601228,-0.0031400516,-0.0130020248,-0.0366816065,-0.0250113314,-0.0363994844,0.0370775105,-0.0312991425,0.0227432406,-0.0450594288,0.008922373,0.0285958733,-0.0750288412,0.0090250303,0.037256425,0.0110901848,0.0339746276,-0.0247419559,0.1360725473,0.0003567616,-0.1643333084,-0.0553702591,0.054567664,-0.1085166625,-0.0507896078,0.0380693439,0.0330095924,0.0383900303,0.039479082,-0.017004194,-0.0493240659,0.0448068359,-0.0332849957,0.0649477847,0.130827042,-0.0066686681,0.0215115654,-0.0531000596,-0.0582951432,0.0652202318,-0.0193188733,0.0233769574,0.0113049098,0.0453128763,0.0443040487,0.0646206361,0.0254348568],"Abstract_Vector":[0.2127398623,-0.0806175378,0.0039138142,0.0423782134,-0.0968260635,0.0083254423,0.0112616207,0.014718957,-0.0717695763,0.0091962035,-0.0386575493,0.0491960491,-0.1509374458,-0.0491133141,-0.052802882,0.0852593497,0.0495422565,-0.0273029599,-0.0682846015,-0.0143836919,0.1090923997,0.0809127568,0.0139070023,0.0340496747,0.0102575694,0.1541481049,0.0957918018,0.0793854295,0.0283413313,0.061419711,-0.068294804,-0.1096176023,0.0624108723,-0.024237572,0.0480827418,-0.0996515353,0.057584124,0.014950842,0.0062890377,-0.0857513359,-0.0526009684,-0.0061775911,0.0659085115,-0.0231759876,-0.018386437,0.0034896804,0.0419568644,0.0155901959,-0.020287051,-0.0178805771,0.0822060473,0.0030874772,0.0248978275,0.0521743373,0.0324955213,-0.0333575965,0.026801439,0.0012219198,0.0471665839,-0.0224917866,0.0472803635,0.0042210613,-0.0138969421,-0.0323362912,-0.0311940133,-0.0365506153,-0.0091784841,-0.0160438626,-0.0326477042,0.0186589731,-0.0139886483,0.0021808468,-0.0127299678,-0.0062245162,-0.051453442,0.0326328303,-0.0122627631,0.0350969361,-0.0322005841,0.0163793921,0.0078492544,0.0083933351,-0.0006415354,0.0162258425,-0.0381750338,-0.0840603805,0.0100897933,-0.0060549524,-0.000193641,0.0340335089,-0.0114426117,-0.0534729147,0.0031576329,-0.0067489332,0.0197545919,-0.0016405749,0.0073421623,-0.1057919155,-0.0855291894,0.0602619699,-0.0404950785,-0.0534399162,-0.0126378116,0.044269138,-0.0792397291,-0.0213497029,-0.0063572584,-0.0876258322,0.0772691251,-0.0389838498,0.0025739206,-0.0520871838,-0.0734055773,0.0341447834,-0.0679689906,-0.0431579503]},"341":{"Abstract":"Virtual reality often uses motion tracking to incorporate physical hand movements into interaction techniques for selection and manipulation of virtual objects. To increase realism and allow direct hand interaction, real-world physical objects can be aligned with virtual objects to provide tactile feedback and physical grasping. However, unless a physical space is custom configured to match a specific virtual reality experience, the ability to perfectly match the physical and virtual objects is limited. Our research addresses this challenge by studying methods that allow one physical object to be mapped to multiple virtual objects that can exist at different virtual locations in an egocentric reference frame. We study two such techniques: one that introduces a static translational offset between the virtual and physical hand before a reaching action, and one that dynamically interpolates the position of the virtual hand during a reaching motion. We conducted two experiments to assess how the two methods affect reaching effectiveness, comfort, and ability to adapt to the remapping techniques when reaching for objects with different types of mismatches between physical and virtual locations. We also present a case study to demonstrate how the hand remapping techniques could be used in an immersive game application to support realistic hand interaction while optimizing usability. Overall, the translational technique performed better than the interpolated reach technique and was more robust for situations with larger mismatches between virtual and physical objects.","Authors":"D. T. Han; M. Suhail; E. D. Ragan","DOI":"10.1109\/TVCG.2018.2794659","Keywords":"Virtual reality;3D interaction;passive haptics;hand interaction;remapped reach;3D object selection","Title":"Evaluating Remapped Physical Reach for Hand Interactions with Passive Haptics in Virtual Reality","Keywords_Processed":"virtual reality;3d interaction;3d object selection;remappe reach;passive haptic;hand interaction","Keyword_Vector":[0.1258919109,-0.155264061,-0.1334209096,0.1470257875,-0.1342433323,-0.1159549075,0.0744581375,-0.0056112332,0.0177072783,-0.0441852429,0.0335697418,-0.0238138091,0.0278427297,0.1773471009,-0.0596140152,0.019699917,0.0792229464,0.0163728004,0.0338351891,-0.0943447835,-0.0199879897,-0.0305403263,-0.012019784,0.0025647229,0.011131825,-0.0131310879,-0.0057735806,-0.0159072471,-0.0524483007,0.0314614314,-0.0221266686,0.0200011311,0.0928190141,0.0287928797,-0.0217038416,0.0020969359,0.0307282866,-0.0278458444,0.041244281,0.024277885,-0.0227353304,0.0120067435,-0.0807733615,-0.0597305007,0.0894408897,0.0290080249,0.0403910289,0.0267898253,-0.0375273564,0.051630051,0.0039237146,0.0676253614],"Abstract_Vector":[0.2489548098,-0.1231244678,0.0103695734,-0.0270163013,-0.0301013552,-0.0610595084,0.0054135401,0.0352528352,-0.0174359514,0.0328534145,0.0350644799,-0.0055766256,0.0141850324,-0.0049985967,-0.0246531648,0.0259741572,-0.0050437724,-0.0004117091,-0.0428809687,0.0307525905,-0.0279324468,-0.0225391232,0.0022583212,0.0191094553,-0.0420209632,-0.0339375723,0.0162117963,0.0077461046,-0.0235712467,-0.0123063114,-0.002391209,0.0637037054,0.0113793186,0.0110921935,-0.0001904897,0.0165781973,-0.0089907462,-0.0157247956,-0.0086269578,0.0109312883,0.0036316199,0.0084886283,0.0389631383,0.0058724524,0.0339925223,0.0034889159,0.0196871521,0.0165494784,-0.0095733802,-0.012032727,-0.0126840069,-0.032338764,0.0110596308,0.0281627092,0.008286815,-0.0169569359,0.0226133695,-0.0001709634,-0.0187962684,0.0275201516,-0.0094440157,0.0161649164,0.0002805323,-0.0502579972,0.0156573224,-0.0474823228,-0.0102353402,-0.013669189,-0.0302896479,-0.0030696987,0.0143097715,-0.0518933913,-0.0012135788,0.0354942778,0.0188184157,0.0253661314,-0.0194930242,0.0160807595,-0.0018081686,0.045541962,0.0006613121,-0.0286241826,-0.0187823087,-0.0316307124,-0.0273749023,-0.0287938768,0.0034697419,-0.0050200169,0.03225368,-0.016400875,-0.0180092922,0.0097548198,-0.0050902022,-0.0066999563,-0.016383698,0.0216564417,0.0429509046,-0.0158271085,-0.0494026021,-0.0114562426,0.020435065,0.0149477331,-0.0031890089,0.0148884703,-0.040168492,-0.0030713203,-0.0313949581,-0.0018906301,-0.0235811688,0.0157733107,-0.0184247336,-0.0160231995,0.0093077895,-0.0568596843,-0.0149181037,0.016455094]},"342":{"Abstract":"This paper introduces dynamic composite physicalizations, a new class of physical visualizations that use collections of self-propelled objects to represent data. Dynamic composite physicalizations can be used both to give physical form to well-known interactive visualization techniques, and to explore new visualizations and interaction paradigms. We first propose a design space characterizing composite physicalizations based on previous work in the fields of Information Visualization and Human Computer Interaction. We illustrate dynamic composite physicalizations in two scenarios demonstrating potential benefits for collaboration and decision making, as well as new opportunities for physical interaction. We then describe our implementation using wheeled micro-robots capable of locating themselves and sensing user input, before discussing limitations and opportunities for future work.","Authors":"M. Le Goc; C. Perin; S. Follmer; J. Fekete; P. Dragicevic","DOI":"10.1109\/TVCG.2018.2865159","Keywords":"information visualization;data physicalization;tangible user interfaces","Title":"Dynamic Composite Data Physicalization Using Wheeled Micro-Robots","Keywords_Processed":"tangible user interface;datum physicalization;information visualization","Keyword_Vector":[0.1786718222,-0.127822651,-0.0713493949,0.059829792,0.0623746486,0.0490434055,-0.1213371719,0.0274145102,-0.1024409867,0.1227753626,-0.0491722303,0.1144406432,0.0916992681,0.0978271578,-0.0320888503,-0.040255696,0.054338523,-0.0643299317,0.1192885935,0.0468799243,0.075067094,0.1048670739,0.1634410234,-0.0663350872,0.0811890642,0.0056427653,0.0487133469,-0.1219706001,-0.0308879118,-0.0627449824,0.0211304344,0.0956859404,-0.0753976446,-0.0662460026,0.072136627,0.0046952821,-0.0087030918,0.0593439666,0.0732893965,0.0940935606,-0.0384780586,-0.0552490552,0.0288272723,-0.0432055629,-0.0350390502,0.0040163509,-0.0562402527,-0.017360696,0.0486733604,-0.0115033996,-0.0153081683,0.0658610479],"Abstract_Vector":[0.2803761872,-0.1903593847,-0.029379032,-0.0134911996,-0.0761017527,-0.0142366973,-0.0087760016,0.023618655,0.062948939,-0.0198996425,-0.0106581388,-0.0286919054,0.0416647619,-0.0278179866,0.0479818484,0.0253597166,-0.0063407304,0.0150683991,0.0407200572,0.0906993108,-0.0137207843,-0.0752181686,-0.0397007518,0.0105320831,-0.0204745025,-0.0226250826,0.0336597992,-0.0239527447,-0.0293948883,-0.0570436072,0.0157093666,-0.010174586,-0.0123480305,0.0041297633,0.0230565531,0.0175539878,-0.0060853864,-0.0054545742,-0.0264330184,-0.0345091613,0.0381290415,0.0242436493,0.0045497013,-0.0333623746,-0.0042636677,0.0442019881,-0.0003289682,-0.0054127943,-0.0414450112,-0.0111032434,0.0006423965,-0.0342072183,0.0412840441,-0.011039314,0.0631896314,-0.0439101195,0.0047814863,-0.0237332281,0.0023502476,0.0510190898,0.0523380594,-0.0281837795,-0.0053986314,0.0063632985,0.0037337355,0.0004875241,-0.0072428399,-0.0218430058,0.0080499293,0.0072075079,0.0478355322,-0.0146642749,0.0308101597,-0.0034534595,-0.041283919,-0.0004152981,-0.0088827229,-0.0213807518,0.0034004278,-0.0147324677,-0.0457497966,0.047787001,-0.0097105129,-0.0166859722,0.0120161688,0.0401836998,-0.0114487117,-0.008200659,0.0391896581,0.0552997367,-0.0283862543,0.0573983444,-0.043456582,0.0031165831,0.039959611,0.0083573629,0.0181449837,-0.0304093452,0.0244123406,0.0073193294,-0.0781857337,-0.0578584054,0.0443565041,0.02166198,0.0197639323,-0.016068809,-0.0106614946,-0.0045960701,0.0138803735,0.0061899953,-0.0259532136,-0.0040179757,-0.0157461445,0.013086971,0.0128037992,-0.0299017821]},"343":{"Abstract":"Local histograms (i.e., point-wise histograms computed from local regions of mesh vertices) have been used in many data analysis and visualization applications. Previous methods for computing local histograms mainly work for regular or rectilinear grids only. In this paper, we develop theory and novel algorithms for computing local histograms in tetrahedral meshes and curvilinear grids. Our algorithms are theoretically sound and efficient, and work effectively and fast in practice. Our main focus is on scalar fields, but the algorithms also work for vector fields as a by-product with small, easy modifications. Our methods can benefit information theoretic and other distribution-driven analysis. The experiments demonstrate the efficacy of our new techniques, including a utility case study on tetrahedral vector field visualization.","Authors":"B. Zhou; Y. Chiang; C. Wang","DOI":"10.1109\/TVCG.2018.2796555","Keywords":"Tetrahedral meshes and curvilinear grids;scalar field data;vector field data;geometry-based techniques;mathematical foundations for visualization","Title":"Efficient Local Statistical Analysis via Point-Wise Histograms in Tetrahedral Meshes and Curvilinear Grids","Keywords_Processed":"scalar field datum;vector field datum;geometry base technique;mathematical foundation for visualization;tetrahedral mesh and curvilinear grid","Keyword_Vector":[0.050492406,0.0386564707,-0.0044832064,-0.0022081742,0.0107560702,-0.0251237309,-0.022909821,-0.03243571,-0.0442100163,-0.006194371,0.0383621465,0.0499724326,-0.0311290166,0.0207521593,0.0387523615,0.0105973436,-0.003631477,-0.0128747696,0.0689361665,-0.0225006937,0.0205970587,0.0147021921,0.0238405386,0.0148572068,0.0024541665,0.0030261743,-0.0046745963,-0.0134134925,0.0217922843,-0.0478571418,-0.0329574231,-0.0549796511,-0.0190093151,0.0186953167,0.0037877549,-0.0149485659,-0.0080687092,0.0040166659,-0.0326482254,-0.0535918786,0.0121024842,0.0167028088,0.0061760353,-0.0068168309,0.0191836274,-0.0054602346,0.0391790888,0.1037426158,-0.0777985769,-0.0237307866,0.0515268358,-0.0441634681],"Abstract_Vector":[0.1949746729,0.0641793089,-0.0535531961,-0.0316391569,-0.0289980489,0.0984796028,0.0229493487,0.0446270054,0.0693270282,-0.0221633672,-0.0191870893,0.011521175,-0.0412624557,-0.0240060045,0.0024462986,-0.040860337,-0.007753478,0.0209959606,0.01085249,-0.0134346783,-0.0090305562,-0.0370903866,-0.0027026345,0.0226900604,0.0490033593,0.0010325366,0.0328237698,0.0070110843,0.0465884538,0.0371499423,0.023170203,0.0456329828,-0.0058390226,-0.0115719915,0.0131772861,0.0447658922,-0.09193531,0.0023971086,-0.0014084573,-0.0144266218,0.0438252439,-0.0096215102,0.0121062582,0.0586580367,-0.0260952683,0.0109707679,0.0312737541,0.0481332105,-0.0002972648,-0.0574589848,0.0230224421,0.0453865594,-0.0253892922,-0.0176475972,0.060848041,0.0004032422,-0.0071927381,0.0284711288,-0.0508653855,0.0010022922,-0.0445048891,0.0188184808,0.023330239,-0.0252739524,-0.004113399,0.045807901,-0.0237868861,-0.0116529693,-0.0561985425,0.0136893618,0.0096283292,-0.0002816203,-0.0047007177,0.0139763971,-0.0241943895,-0.0441795043,0.0494268278,0.0271049358,-0.0391062372,-0.0251652592,0.040996037,0.0403705672,0.0006050063,0.0351754373,0.0160391218,0.0368097904,0.0167689712,-0.0030358748,-0.014140611,-0.0188316519,0.0143303644,-0.0279932657,-0.0010499327,-0.0181884035,-0.0030637304,-0.0481144415,0.0301678718,-0.0133757613,0.0019099476,-0.0244028875,0.0181489709,-0.015223977,-0.0420420806,0.0266117306,-0.0377554105,-0.0262212982,-0.0082247351,0.0203921583,0.0058857237,-0.012397653,-0.0144453781,0.0122388608,-0.0252733042,0.0005813967,0.0087124521,0.0098381633]},"344":{"Abstract":"Bipartite graphs model the key relations in many large scale real-world data: customers purchasing items, legislators voting for bills, people's affiliation with different social groups, faults occurring in vehicles, etc. However, it is challenging to visualize large scale bipartite graphs with tens of thousands or even more nodes or edges. In this paper, we propose a novel visual summarization technique for bipartite graphs based on the minimum description length (MDL) principle. The method simultaneously groups the two different set of nodes and constructs aggregated bipartite relations with balanced granularity and precision. It addresses the key trade-off that often occurs for visualizing large scale and noisy data: acquiring a clear and uncluttered overview while maximizing the information content in it. We formulate the visual summarization task as a co-clustering problem and propose an efficient algorithm based on locality sensitive hashing (LSH) that can easily scale to large graphs under reasonable interactive time constraints that previous related methods cannot satisfy. The method leads to the opportunity of introducing a visual analytics framework with multiple levels-of-detail to facilitate interactive data exploration. In the framework, we also introduce a compact visual design inspired by adjacency list representation of graphs as the building block for a small multiples display to compare the bipartite relations for different subsets of data. We showcase the applicability and effectiveness of our approach by applying it on synthetic data with ground truth and performing case studies on real-world datasets from two application domains including roll-call vote record analysis and vehicle fault pattern analysis. Interviews with experts in the political science community and the automotive industry further highlight the benefits of our approach.","Authors":"G. Y. Chan; P. Xu; Z. Dai; L. Ren","DOI":"10.1109\/TVCG.2018.2864826","Keywords":"Bipartite Graph;Visual Summarization;Minimum Description Length;Information Theory","Title":"ViBr: Visualizing Bipartite Relations at Scale with the Minimum Description Length Principle","Keywords_Processed":"information theory;Minimum Description length;visual summarization;Bipartite Graph","Keyword_Vector":[0.1289086526,-0.0600555491,-0.0075778275,-0.1065879462,0.009935009,-0.0590153406,-0.1121401751,-0.0215104322,-0.0996233,0.0875616785,-0.1084294585,0.0800738559,0.001232129,0.0070342242,-0.0993240838,-0.0067289741,-0.03518724,0.0508253086,-0.0585074559,-0.0585921204,0.0866562961,0.0962515627,0.0405218184,0.0079868472,-0.0206481971,0.0270032902,0.043743385,0.0124301488,-0.0011386678,-0.0170333676,-0.0416818565,-0.0069613106,-0.057964742,-0.0562711074,0.0279778887,-0.0375748065,0.0662690493,-0.0480810209,-0.0073580624,-0.0201831975,0.0049159435,-0.0233148548,0.0251317518,-0.0387159747,-0.0142669695,0.0138834146,0.033042247,0.0050916902,0.0206291906,-0.0024333532,-0.0068371695,-0.013068614],"Abstract_Vector":[0.1535595572,-0.0817038203,0.0050558365,-0.0016897214,-0.0618743017,0.0409369167,0.0137408583,0.0352101121,-0.0260409971,-0.0091035525,-0.050438419,0.0530750787,0.0379443956,-0.0150626324,0.0334709423,0.0029106631,-0.0522160266,0.0111223098,0.0044235997,-0.0108555988,-0.0025838817,-0.0301688386,-0.0121032651,0.0063447472,0.0050311808,0.001098923,-0.0094768425,-0.0079262026,-0.015846874,-0.0138174089,-0.0296364033,0.0064586045,0.0218138437,-0.0011248347,0.0025787328,-0.0109707134,-0.0207540589,-0.0144648917,-0.0534738221,0.0363361687,0.0061209796,0.0231919391,0.0180334086,-0.0720852341,-0.002091957,-0.010141642,0.0120378172,-0.0020625468,0.0080507174,0.0186871839,-0.0184066518,0.0134776788,0.0336434897,-0.0182515013,0.0319353477,-0.0176897743,-0.0140258707,-0.0259425599,-0.0144870741,-0.0258818905,-0.0023256641,-0.0047209481,0.0114236364,-0.0147669875,-0.0245635429,0.0004189865,0.0077326831,-0.0033772848,-0.0174902438,-0.0016323746,-0.0170645326,-0.0190277409,-0.0035115884,0.0237228019,0.0020258352,0.0001385407,-0.0225323869,0.0022931552,0.0057967576,-0.0071540185,-0.0095408678,-0.0044520841,0.006174291,0.021375794,0.0010365658,-0.0058584949,-0.002495207,-0.0173486376,-0.0070527342,-0.0074238677,0.0029820407,0.011104455,-0.0037265202,-0.0018509356,-0.0369174765,0.0029785801,0.0052440566,0.0097093353,-0.0160606143,-0.0147506141,0.0029082219,0.0116467413,0.0035178517,-0.005055817,-0.016681515,0.0131528305,0.0133557443,0.0078633793,-0.0011763697,-0.0071152887,-0.0210717122,0.0118539691,0.0091457601,0.0007281598,0.0068089115,0.0141567426]},"345":{"Abstract":"Most graphics hardware features memory to store textures and vertex data for rendering. However, because of the irreversible trend of increasing complexity of scenes, rendering a scene can easily reach the limit of memory resources. Thus, vertex data are preferably compressed, with a requirement that they can be decompressed during rendering. In this paper, we present a novel method to exploit existing hardware texture compression circuits to facilitate the decompression of vertex data in graphics processing unit (GPUs). This built-in hardware allows real-time, random-order decoding of data. However, vertex data must be packed into textures, and careless packing arrangements can easily disrupt data coherence. Hence, we propose an optimization approach for the best vertex data permutation that minimizes compression error. All of these result in fast and high-quality vertex data decompression for real-time rendering. To further improve the visual quality, we introduce vertex clustering to reduce the dynamic range of data during quantization. Our experiments demonstrate the effectiveness of our method for various vertex data of 3D models during rendering with the advantages of a minimized memory footprint and high frame rate.","Authors":"K. C. Kwan; X. Xu; L. Wan; T. Wong; W. Pang","DOI":"10.1109\/TVCG.2017.2695182","Keywords":"Vertex data compression;real-time rendering;hardware texture compression;permutation;GPU acceleration","Title":"Packing Vertex Data into Hardware-Decompressible Textures","Keywords_Processed":"real time render;gpu acceleration;vertex datum compression;hardware texture compression;permutation","Keyword_Vector":[0.1373108592,-0.0215707178,0.024372293,-0.0153684205,0.0910875905,-0.1063659274,-0.0883329219,-0.0497663341,-0.0403307901,-0.0154313261,-0.0505356153,0.0957538984,-0.0086206944,0.0255440627,-0.0787786077,0.0066357817,-0.0502069191,-0.0046062618,-0.050644031,0.0473177161,0.0413773608,0.0665485384,-0.1482690866,0.1033921086,-0.0603428952,-0.0390163338,-0.1785099456,0.1190564569,-0.0606124038,0.0211439789,-0.0041836584,-0.0932598796,0.0914848133,-0.0803697965,0.0126080451,-0.0520112663,-0.0476621292,-0.2157221076,0.0160918274,0.0849934971,-0.0127626483,0.0453120892,-0.0620750185,0.079189098,-0.1134570901,-0.1157415229,0.0030887647,0.0703257207,-0.0093370675,0.007851028,0.056863838,0.0267238815],"Abstract_Vector":[0.2804600725,-0.0973751047,0.0084671206,-0.0309592594,-0.0308745464,-0.0330360134,-0.0073789598,-0.0184983365,-0.0268660574,0.0090834512,-0.0280129327,-0.0080414086,0.0057212304,0.0292047233,-0.0014052488,0.054040899,0.0697721995,0.045635484,0.0152733891,-0.0780182792,-0.0061871516,-0.0441042617,-0.0043974807,-0.0082988245,0.0202168121,0.0158791792,0.0303307728,-0.0046925202,0.0005714592,-0.0312845327,-0.0325632553,0.0554895873,-0.0130984614,0.0164622224,0.0073600441,-0.0828575842,0.0451837404,0.031856182,0.0169992941,-0.0203100843,-0.0798955757,0.0220194595,0.0452425745,-0.0185135588,-0.0155105421,-0.0134264846,0.0281520926,0.0132434723,0.0316514606,0.019066169,-0.0138317695,-0.0224881022,0.0066102979,0.0282361254,0.0277679932,-0.0071480453,0.0265233359,0.0011207607,-0.0108066106,0.0080672144,-0.051087534,0.0417542351,-0.0220706991,-0.0027504217,-0.0035614637,-0.0164162928,-0.0454049756,0.086419122,-0.0437790119,-0.0282885947,-0.0416672629,-0.044879761,0.0000306469,0.0059813143,-0.0356801037,-0.0171524486,-0.0318632451,0.0073222481,-0.0234468062,0.0222392553,-0.0158317605,-0.0053795573,0.0362393511,0.0325216333,0.0017136792,-0.0139148464,0.0085520953,0.010581779,0.008060958,-0.0997691039,0.0026902209,-0.06097559,-0.0150641552,0.003094669,0.0085095935,0.0626745271,0.0208381529,0.0259630371,-0.0267426794,-0.0021003606,0.0102959624,-0.0368123135,0.0252840037,0.0068354548,0.0079187619,0.0292352649,-0.0001784856,-0.0093516515,-0.0006429584,-0.0150392343,0.0104919341,0.0281478824,0.0007727855,-0.0056704736,-0.0011681122,0.015797383]},"346":{"Abstract":"Consider a multi-dimensional spatio-temporal (ST) dataset where each entry is a numerical measure defined by the corresponding temporal, spatial and other domain-specific dimensions. A typical approach to explore such data utilizes interactive visualizations with multiple coordinated views. Each view displays the aggregated measures along one or two dimensions. By brushing on the views, analysts can obtain detailed information. However, this approach often cannot provide sufficient guidance for analysts to identify patterns hidden within subsets of data. Without a priori hypotheses, analysts need to manually select and iterate through different slices to search for patterns, which can be a tedious and lengthy process. In this work, we model multidimensional ST data as tensors and propose a novel piecewise rank-one tensor decomposition algorithm which supports automatically slicing the data into homogeneous partitions and extracting the latent patterns in each partition for comparison and visual summarization. The algorithm optimizes a quantitative measure about how faithfully the extracted patterns visually represent the original data. Based on the algorithm we further propose a visual analytics framework that supports a top-down, progressive partitioning workflow for level-of-detail multidimensional ST data exploration. We demonstrate the general applicability and effectiveness of our technique on three datasets from different application domains: regional sales trend analysis, customer traffic analysis in department stores, and taxi trip analysis with origin-destination (OD) data. We further interview domain experts to verify the usability of the prototype.","Authors":"D. Liu; P. Xu; L. Ren","DOI":"10.1109\/TVCG.2018.2865018","Keywords":"Spatio-temporal data;tensor decomposition;interactive exploration;automatic pattern discoveries","Title":"TPFlow: Progressive Partition and Multidimensional Pattern Extraction for Large-Scale Spatio-Temporal Data Analysis","Keywords_Processed":"interactive exploration;automatic pattern discoverie;spatio temporal datum;tensor decomposition","Keyword_Vector":[0.200183627,-0.0933961079,0.0020364459,0.0878097881,0.0005770645,0.2318095286,-0.0657057912,0.0604504451,0.0339442241,-0.0456976492,0.0328163012,0.0095848545,-0.0067040601,-0.0122744297,-0.0414274482,-0.0948731141,-0.1202082612,-0.0435705796,0.1805914823,0.277130535,0.0982388268,0.0979371073,0.1024087085,-0.0255393256,0.1405777935,0.040807456,-0.0127921967,0.1096711329,-0.1083207811,0.0178938089,-0.1308751495,-0.018912378,0.0293315528,0.0824586635,-0.0504877789,-0.019535643,0.1012226534,0.0310659181,-0.062326711,-0.0740597667,-0.077153666,-0.070802263,0.0584133738,0.0381028364,-0.0626825162,0.0595408858,0.0220057813,-0.0319319023,0.0027596731,-0.0710153877,0.0670476044,-0.0259186003],"Abstract_Vector":[0.2301657958,-0.0663496029,-0.0046404311,-0.0197893236,0.0253613367,-0.0993032105,-0.0770005234,-0.0489583873,0.0338483636,-0.0278929936,0.0247983892,-0.0309349738,-0.0101120023,-0.000887671,0.0053096761,-0.0155612803,-0.0006457064,-0.0155200132,-0.0344167347,-0.0154775395,0.0070044333,-0.0503133004,-0.0252211709,-0.0362174965,-0.02141857,0.0185620202,0.021173205,-0.0062838691,0.0486077815,0.0071408693,-0.0209086153,-0.003572953,0.0196169587,0.0163303937,-0.0592125434,0.0191364301,0.0314545313,-0.0277955421,-0.034037099,-0.0213201202,0.0118146958,0.0220166225,0.0135567169,0.0056953591,0.0279779185,0.0116487978,0.0415196813,-0.0258841091,-0.0310376868,-0.0046413768,-0.0083920972,-0.0335583757,-0.0167805145,0.0105231618,0.0058975014,-0.0086755124,0.0214065026,-0.0133775264,-0.0041914527,-0.0375293587,-0.0253799029,0.0038931921,0.0348025273,-0.0477546013,0.0503887214,0.0149325091,0.0161848152,-0.0151342712,-0.0126451267,-0.0009798256,-0.0164014643,0.0038306356,-0.0134516945,0.061218979,0.0260178046,0.0128403871,-0.0575981479,-0.0143961207,-0.0215508957,-0.0405733062,0.010086986,-0.0177981867,-0.0484267165,0.0453769654,-0.0316387627,-0.0076402363,-0.0172676739,0.01175672,0.0215488801,-0.0044775691,0.0291203682,-0.0045601859,-0.009015156,0.0373174809,-0.0354260591,-0.0014913572,-0.0000809889,0.0171698603,0.0372324282,-0.0317710184,-0.01679188,-0.0105865145,0.0009957672,0.0138357109,-0.0139422523,-0.0065924298,-0.0129794403,0.0075639384,0.005250947,0.0123229729,-0.0407883329,-0.0268591374,-0.0114978212,0.0362035357,-0.0030241484,-0.01058304]},"347":{"Abstract":"Visualization of the trajectories of moving objects leads to dense and cluttered images, which hinders exploration and understanding. It also hinders adding additional visual information, such as direction, and makes it difficult to interactively extract traffic flows, i.e., subsets of trajectories. In this paper we present our approach to visualize traffic flows and provide interaction tools to support their exploration. We show an overview of the traffic using a density map. The directions of traffic flows are visualized using a particle system on top of the density map. The user can extract traffic flows using a novel selection widget that allows for the intuitive selection of an area, and filtering on a range of directions and any additional attributes. Using simple, visual set expressions, the user can construct more complicated selections. The dynamic behaviors of selected flows may then be shown in annotation windows in which they can be interactively explored and compared. We validate our approach through use cases where we explore and analyze the temporal behavior of aircraft and vessel trajectories, e.g., landing and takeoff sequences, or the evolution of flight route density. The aircraft use cases have been developed and validated in collaboration with domain experts.","Authors":"R. Scheepens; C. Hurter; H. Van De Wetering; J. J. Van Wijk","DOI":"10.1109\/TVCG.2015.2467112","Keywords":"Moving Object Visualization;traffic flows;interaction;Moving Object Visualization;traffic flows;interaction","Title":"Visualization, Selection, and Analysis of Traffic Flows","Keywords_Processed":"move Object visualization;traffic flow;interaction","Keyword_Vector":[0.0746509239,-0.0050395327,0.1281026731,-0.0048539593,-0.0656431393,-0.0231590961,0.032354288,-0.0016234152,0.0257478613,-0.120351402,-0.0253088277,0.022424089,0.0471654823,-0.0188536499,-0.0663326886,-0.0519713552,0.1028464097,-0.1040624734,0.0150441724,-0.0421288771,0.0181604661,0.0334119515,-0.0005126516,0.1215304179,-0.063100875,0.0102701001,-0.0530277353,-0.0155669832,0.014763168,-0.0275902235,-0.0403546564,0.0623881537,0.0103570787,0.0139628621,-0.0343996788,-0.0361994246,-0.0094534675,-0.08737135,0.1067028069,0.005904738,-0.02479245,0.0110712641,-0.0420896824,0.0951483654,-0.0309844977,-0.0153524509,-0.0245406889,-0.0025588154,0.058426447,0.0248940116,0.0034631107,0.0150283242],"Abstract_Vector":[0.1429773041,-0.0033549488,0.2298470755,-0.0590610901,0.1006447976,0.0549000909,0.200488401,-0.1036287397,-0.0151978444,-0.0451242359,-0.0023901267,-0.015896298,-0.030886664,0.0188568841,-0.0838301748,0.1024118431,-0.073850385,0.0597528596,-0.0247942689,0.0103158275,0.0085312329,-0.0227678045,-0.0063193911,-0.0531092355,-0.0656579982,0.0802119624,0.034814563,-0.0612290059,0.047559645,-0.0183890074,0.04742128,-0.0306377629,-0.0586500012,-0.0427906259,-0.0373610366,0.051973748,0.0391682961,-0.0414122986,0.031707073,-0.0999413939,-0.0389385943,0.050331697,0.0382516218,0.055634324,-0.0339886996,0.0559280294,0.0829834414,-0.0384427717,0.0394116119,0.0736519866,-0.0775431078,-0.0278802985,0.1453601352,0.0022145925,-0.0150703489,0.0644694172,-0.0281380222,-0.0528957045,-0.1145953891,0.099099297,-0.0482747242,0.0076620736,-0.0393565319,0.0314344543,-0.0095032739,0.0489144717,0.0575450874,-0.0911886144,0.0688629501,-0.0177266786,0.073421469,-0.0043088636,0.0129594523,0.0373491731,0.0283649304,0.0073121576,-0.0026904636,0.0307215175,-0.0197858192,0.0040998934,-0.0238893618,-0.0160488477,0.0239284321,0.0079885546,0.030494309,-0.0411651816,0.0360529811,0.0126257151,0.0209174857,-0.0000783189,0.0061538706,0.0485359527,0.003714008,-0.0190468574,-0.013455788,0.0064797875,0.0016259299,-0.0136891621,0.0025335068,0.0135056322,0.0290024265,-0.0030333874,-0.0324381524,0.0315444237,-0.0172670243,0.0565827485,-0.031368831,-0.0184421743,0.004576076,-0.0322971564,0.0211855742,0.0081915439,-0.0565975842,-0.0213586353,0.010985441,0.0136536215]},"348":{"Abstract":"Numerical Weather Prediction (NWP) ensembles are commonly used to assess the uncertainty and confidence in weather forecasts. Spaghetti plots are conventional tools for meteorologists to directly examine the uncertainty exhibited by ensembles, where they simultaneously visualize isocontours of all ensemble members. To avoid visual clutter in practical usages, one needs to select a small number of informative isovalues for visual analysis. Moreover, due to the complex topology and variation of ensemble isocontours, it is often a challenging task to interpret the spaghetti plot for even a single isovalue in large ensembles. In this paper, we propose an interactive framework for uncertainty visualization of weather forecast ensembles that significantly improves and expands the utility of spaghetti plots in ensemble analysis. Complementary to state-of-the-art methods, our approach provides a complete framework for visual exploration of ensemble isocontours, including isovalue selection, interactive isocontour variability exploration, and interactive sub-region selection and re-analysis. Our framework is built upon the high-density clustering paradigm, where the mode structure of the density function is represented as a hierarchy of nested subsets of the data. We generalize the high-density clustering for isocontours and propose a bandwidth selection method for estimating the density function of ensemble isocontours. We present novel visualizations based on high-density clustering results, called the mode plot and the simplified spaghetti plot. The proposed mode plot visually encodes the structure provided by the high-density clustering result and summarizes the distribution of ensemble isocontours. It also enables the selection of subsets of interesting isocontours, which are interactively highlighted in a linked spaghetti plot for providing spatial context. To provide an interpretable overview of the positional variability of isocontours, our system allows for selection of informative isovalues from the simplified spaghetti plot. Due to the spatial variability of ensemble isocontours, the system allows for interactive selection and focus on sub-regions for local uncertainty and clustering re-analysis. We examine a number of ensemble datasets to establish the utility of our approach and discuss its advantages over state-of-the-art visual analysis tools for ensemble data.","Authors":"B. Ma; A. Entezari","DOI":"10.1109\/TVCG.2018.2864815","Keywords":"Spaghetti plots;ensemble visualization;uncertainty visualization;high-density clustering;ensemble forecasting","Title":"An Interactive Framework for Visualization of Weather Forecast Ensembles","Keywords_Processed":"ensemble visualization;ensemble forecasting;high density clustering;uncertainty visualization;Spaghetti plot","Keyword_Vector":[0.2786843533,-0.0344142887,0.2205402603,-0.1015834518,-0.0816285659,-0.0688821625,-0.0780894541,0.0056948052,0.0328560127,-0.200938843,-0.0053928824,0.0287953438,-0.0397204881,0.0157806287,-0.1277465856,0.0369100505,-0.0886085266,0.0193013665,-0.0630149379,0.0625206605,-0.0055066424,-0.0136746406,-0.012942675,0.0639802652,0.1090708072,-0.0299300103,0.0752192702,-0.1693167099,0.1277726702,0.0430800862,0.0418668715,-0.0495015971,0.0471697501,-0.0497621874,-0.0428954054,-0.0822860306,-0.068720861,0.0095040895,-0.1016453639,-0.0341702615,0.0510245871,0.0620343568,-0.0025158255,-0.0001960997,-0.0488523218,0.0077841888,-0.0084605346,-0.0328611919,0.017536613,-0.0114427563,-0.0032102716,0.0492847705],"Abstract_Vector":[0.1844936662,0.0401144595,0.1087225668,-0.0227648102,0.0528303647,-0.0277253228,-0.0156946344,-0.050604644,0.0193260313,-0.0826386192,-0.0539562543,-0.0102521418,-0.1059010261,-0.0732356791,0.0181425096,0.0136375643,0.0055096256,0.0287573852,-0.0060114078,-0.0462502711,0.0197040045,0.0101355695,0.0336784558,0.0580529664,-0.014248482,0.0073654906,0.0004288486,0.01988893,0.0234352971,0.0535314881,-0.0643903759,0.0158885339,-0.0065895021,-0.0208330905,-0.0015353576,-0.0314042697,0.0154202897,0.0260223474,-0.014237575,0.0544190218,-0.0189197246,-0.0743982253,0.0414439967,-0.0346355353,0.028180095,0.052440854,-0.095014504,0.0189970688,-0.0077576495,-0.0631424778,0.0233950373,0.0496620447,0.0961501295,-0.0772155453,-0.0323399546,0.0728803198,0.0078103474,-0.0508273106,0.0786620926,0.0068561147,0.0216221076,0.0626070518,-0.0873029279,-0.0307979857,-0.0045218686,-0.0046313763,0.040195927,-0.0192783927,0.0240398864,-0.0302454066,-0.0436102216,0.0844483807,-0.0469981034,-0.0024285357,0.1168448522,0.0125414782,-0.0353705532,-0.030702925,-0.0356397648,-0.0880465525,0.012170021,0.0051164861,-0.0127654059,0.0253566281,-0.0157384775,-0.0250418747,0.0396529551,0.0020576121,0.0518141459,0.0272259921,0.0323020182,0.0762752243,0.0453712395,-0.0041363556,-0.0022378587,0.0298596712,0.0322661328,0.0422817415,-0.077546469,-0.0208131713,-0.0644266524,-0.0355045417,-0.0581364226,-0.0049031448,-0.054911532,-0.0326272345,0.004870699,0.0287109581,-0.0350323387,0.0083585538,-0.0282929843,0.0283254478,-0.0219343699,-0.0248860344,-0.05849553,0.0362015742]},"349":{"Abstract":"Isosurfaces are fundamental geometrical objects for the analysis and visualization of volumetric scalar fields. Recent work has generalized them to bivariate volumetric fields with fiber surfaces, the pre-image of polygons in range space. However, the existing algorithm for their computation is approximate, and is limited to closed polygons. Moreover, its runtime performance does not allow instantaneous updates of the fiber surfaces upon user edits of the polygons. Overall, these limitations prevent a reliable and interactive exploration of the space of fiber surfaces. This paper introduces the first algorithm for the exact computation of fiber surfaces in tetrahedral meshes. It assumes no restriction on the topology of the input polygon, handles degenerate cases and better captures sharp features induced by polygon bends. The algorithm also allows visualization of individual fibers on the output surface, better illustrating their relationship with data features in range space. To enable truly interactive exploration sessions, we further improve the runtime performance of this algorithm. In particular, we show that it is trivially parallelizable and that it scales nearly linearly with the number of cores. Further, we study acceleration data-structures both in geometrical domain and range space and we show how to generalize interval trees used in isosurface extraction to fiber surface extraction. Experiments demonstrate the superiority of our algorithm over previous work, both in terms of accuracy and running time, with up to two orders of magnitude speedups. This improvement enables interactive edits of range polygons with instantaneous updates of the fiber surface for exploration purpose. A VTK-based reference implementation is provided as additional material to reproduce our results.","Authors":"P. Klacansky; J. Tierny; H. Carr; Z. Geng","DOI":"10.1109\/TVCG.2016.2570215","Keywords":"Bivariate data;data segmentation;data analysis;isosurfaces;continuous scatterplot","Title":"Fast and Exact Fiber Surfaces for Tetrahedral Meshes","Keywords_Processed":"isosurface;bivariate datum;datum segmentation;datum analysis;continuous scatterplot","Keyword_Vector":[0.0072533357,0.0017879773,-0.0007465341,0.0006390663,0.0034430775,-0.0034729711,0.0067706927,0.0021919397,-0.0060925658,-0.0065410452,0.0084982165,0.0062560269,0.003027989,-0.0030562777,0.0006408616,-0.011180403,0.0075136791,-0.0151169715,0.0004299834,-0.0029645394,0.0019738835,0.0026795642,-0.011654017,0.0047445562,0.0066664139,0.0083706439,-0.0013012061,0.003153546,0.0090786944,0.0014609512,-0.0170083864,-0.0131951433,-0.0140915008,0.0185324378,0.0047093447,0.0001436404,0.0019387891,0.0050336172,0.0130951683,0.0058845555,0.0024084442,0.0112306633,-0.0226071752,0.0093485212,-0.0009267556,-0.0131773518,-0.0241231292,-0.0026661785,0.007762801,-0.0131673994,0.0184455692,-0.003679443],"Abstract_Vector":[0.0687356503,0.0242568552,-0.0163905934,-0.0067426323,-0.0086542674,0.0168603572,0.0005565836,0.0018634885,0.0054296161,-0.0077845867,-0.0163798971,0.0051042963,-0.0251341252,-0.0071434995,0.0184482468,-0.0006407228,-0.0088181013,-0.0178922319,-0.0039781614,-0.0164919475,0.0053582018,-0.0058646491,0.0125721432,-0.0079697725,0.0014169789,0.0050114363,0.0002675997,-0.0087094299,-0.0264925868,0.0275490998,0.0146390402,0.0181031425,-0.006621671,-0.0027163056,0.0108348804,0.008707375,-0.0119951954,0.0021983467,0.0008724759,0.0050526693,0.0122472025,0.0001767115,0.0027364137,0.0038496697,0.0039685926,-0.0176984475,0.0353707498,-0.0002321054,0.0177646331,0.0016151452,-0.0228979772,-0.0071189787,0.0049581683,-0.0091487091,0.0062067223,0.0020228209,-0.0022210437,0.0138263725,0.003468097,0.0013208193,-0.0072203089,0.0095013082,-0.0045942603,0.0044134345,0.0085513454,-0.007654951,-0.004963675,-0.0073882275,-0.0029613048,-0.0122722694,-0.0206240176,-0.0138951817,0.002825946,-0.0054695777,0.0081008581,0.0058117296,-0.0031378537,0.0153499306,0.0102743853,0.015879314,0.0185893651,0.0042418083,0.0062223352,-0.00545912,0.012219873,-0.000484474,-0.014560495,-0.0090364629,-0.0116002554,-0.0001009303,-0.0063954205,0.0020167764,0.013495196,-0.0021221178,0.0138383246,-0.0067962562,0.0024141242,-0.0036298458,0.0010516632,0.0083087216,-0.0024083419,0.0033915429,0.0006377455,-0.0005965037,-0.0008091245,-0.008197269,-0.0149367222,0.0168533847,-0.0101799463,-0.0019970623,-0.0077570669,0.0081300896,-0.0036669374,0.0238821794,0.004897793,-0.0245860518]},"35":{"Abstract":"Specularities are often problematic in computer vision since they impact the dynamic range of the image intensity. A natural approach would be to predict and discard them using computer graphics models. However, these models depend on parameters which are difficult to estimate (light sources, objects' material properties and camera). We present a geometric model called JOLIMAS: JOint LIght-MAterial Specularity, which predicts the shape of specularities. JOLIMAS is reconstructed from images of specularities observed on a planar surface. It implicitly includes light and material properties, which are intrinsic to specularities. This model was motivated by the observation that specularities have a conic shape on planar surfaces. The conic shape is obtained by projecting a fixed quadric on the planar surface. JOLIMAS thus predicts the specularity using a simple geometric approach with static parameters (object material and light source shape). It is adapted to indoor light sources such as light bulbs and fluorescent lamps. The prediction has been tested on synthetic and real sequences. It works in a multi-light context by reconstructing a quadric for each light source with special cases such as lights being switched on or off. We also used specularity prediction for dynamic retexturing and obtained convincing rendering results. Further results are presented as supplementary video material, which can be found on the Computer Society Digital Library at http:\/\/doi.ieeecomputersociety.org\/10.1109\/TVCG.2017.2677445.","Authors":"A. Morgand; M. Tamaazousti; A. Bartoli","DOI":"10.1109\/TVCG.2017.2677445","Keywords":"JOLIMAS;specular reflection;multiple light sources;phong;blinn-phong;specularity;prediction;retexturing;quadric;dual space;conic;real time","Title":"A Geometric Model for Specularity Prediction on Planar Surfaces with Multiple Light Sources","Keywords_Processed":"quadric;prediction;dual space;conic;blinn phong;phong;specularity;retexture;real time;jolimas;specular reflection;multiple light source","Keyword_Vector":[0.0427639808,-0.0013894503,0.0790149875,0.0234523439,-0.041793781,-0.024596738,0.0015511421,-0.0112238547,-0.0002762668,-0.0409819384,-0.0615518179,0.0163321001,-0.027572412,-0.0100683141,-0.0208581889,0.0290604547,0.1029709918,-0.0671199052,0.0397993978,-0.0281805811,-0.0008183097,0.0372383738,0.0211441836,0.0733504461,-0.0035660368,-0.0903571709,-0.0150876904,0.0838330087,-0.0920055157,-0.0126537722,0.0056299475,0.075260279,0.0985295766,0.0347351593,-0.0518058937,-0.0702430587,0.0720075692,0.0051371859,0.0170870983,0.0916425606,0.012669309,0.0481218724,-0.0660923548,-0.0822410785,0.1165037728,-0.0022200931,0.0145676053,0.0200416997,0.0316152774,0.0550836634,0.0660002465,0.0270630282],"Abstract_Vector":[0.193240687,0.0332838047,0.0675062616,-0.0144496922,-0.0250510035,-0.0363549106,-0.0409227715,-0.0303458941,-0.011861108,0.0086728095,0.0139290358,-0.0151161033,-0.0004929214,-0.0112569737,0.0098372164,-0.0084345781,0.0048373068,0.0580288584,-0.0151093882,-0.022248477,0.0501728257,-0.045272422,0.0126642031,-0.0241268766,0.0080449014,0.0070647587,-0.0029813139,0.0351107788,0.0621403315,-0.0144489888,0.0057739012,-0.0221496084,-0.0167074782,0.0122316951,-0.0073357161,-0.0193218067,-0.0280489596,-0.0043248462,0.0168028315,0.0430251865,-0.0167632871,0.0156725701,0.0107814183,0.0312697025,0.0093929577,-0.040183184,-0.0007105612,-0.0222022045,0.0354022113,0.0417427807,0.0033756357,0.0040226508,-0.0000613992,0.0122455596,-0.027543956,0.0297992657,-0.0320996833,0.0160642015,-0.0064583012,0.0076672885,0.0235979135,0.0061134882,-0.0349083688,0.0086801253,-0.0228510629,-0.029435926,-0.0195402497,0.0215780529,0.0115226014,-0.0097104904,-0.001081054,-0.0077792519,-0.0334200115,0.0013109524,0.0123511784,0.0277394312,-0.0072594472,0.0136613743,-0.0162780868,-0.0175072704,-0.0064779342,0.0154983934,0.0144282312,0.0789375964,-0.0095970473,0.0406380687,-0.001728337,-0.0143329376,0.0105280267,-0.0046451052,-0.020460351,-0.0281587789,0.0179591331,0.0113984502,0.0379369774,-0.0112763643,0.0105391589,-0.0176846218,-0.004002018,0.00917592,-0.0004728665,-0.0053186106,-0.00398382,0.0082222504,-0.0062063561,-0.0094062288,0.0344571156,-0.0107885054,-0.0114618302,0.0252115351,-0.0041860332,0.0068933845,-0.0116861201,-0.0279990557,0.0150969738,0.003267791]},"350":{"Abstract":"Many researchers across diverse disciplines aim to analyze the behavior of cohorts whose behaviors are recorded in large event databases. However, extracting cohorts from databases is a difficult yet important step, often overlooked in many analytical solutions. This is especially true when researchers wish to restrict their cohorts to exhibit a particular temporal pattern of interest. In order to fill this gap, we designed COQUITO, a visual interface that assists users defining cohorts with temporal constraints. COQUITO was designed to be comprehensible to domain experts with no preknowledge of database queries and also to encourage exploration. We then demonstrate the utility of COQUITO via two case studies, involving medical and social media researchers.","Authors":"J. Krause; A. Perer; H. Stavropoulos","DOI":"10.1109\/TVCG.2015.2467622","Keywords":"Visual temporal queries;cohort definition;electronic medical records;information visualization;Visual temporal queries;cohort definition;electronic medical records;information visualization","Title":"Supporting Iterative Cohort Construction with Visual Temporal Queries","Keywords_Processed":"cohort definition;electronic medical record;visual temporal query;information visualization","Keyword_Vector":[0.0821900019,0.0621730686,0.1284746038,0.0354701928,-0.0587069136,-0.0558540673,0.0080874852,0.0114102876,-0.0031330361,0.0431759155,0.0065567351,-0.0042738543,0.1339498245,0.0537113284,0.1065946941,-0.0073135312,-0.0036180188,0.0589185581,-0.031316653,0.0420195925,0.041632341,-0.0826694056,0.0585775132,-0.0002313655,-0.006333387,-0.0033402911,0.0541486286,0.1187542346,0.0581468066,0.0180016017,0.0435223355,-0.087412006,-0.0105393811,-0.0318868507,0.010310572,0.0000881013,-0.006859155,-0.0514746196,0.0637372008,0.0085639513,0.1007756459,-0.0366136023,0.0158544025,0.0196193208,0.0249110859,0.0794517774,-0.0263173725,0.1241644763,-0.0371529003,0.0003031036,0.0384697612,-0.0705316585],"Abstract_Vector":[0.1497685285,0.0344499026,0.0092363878,-0.0013996167,-0.0182983247,0.0345757744,0.0040944374,0.0109933246,0.026790681,-0.009350365,-0.0173121801,-0.0280561671,-0.0108119702,0.0435571515,0.0142719668,-0.0041329948,0.0406777752,-0.006064287,-0.0149114088,-0.0267410538,0.0726049612,0.0758907426,0.0106129467,-0.0219265573,-0.0029421838,0.0486929247,0.0067812051,0.0166634766,0.0390073446,-0.0302279531,-0.0034436623,0.0445701203,0.0001621087,0.0771654928,0.0040256978,-0.0007768547,-0.0009549626,0.0198442334,0.0146511582,0.0413080416,-0.0075654549,0.0343913238,0.0051744571,-0.0029374997,-0.0187548348,0.0277933951,0.066053847,-0.0229964193,-0.0104548729,0.0216087747,0.0182596494,0.023191401,0.0122569192,0.058743572,-0.0134496838,0.013494208,0.016469338,0.0177217915,0.0026863377,-0.0500844567,0.0189754998,-0.0111498192,0.0114893096,0.0003525884,-0.0142604071,-0.0127347938,0.0063720784,-0.0103361654,-0.0201822113,-0.0148089929,0.033722461,0.0145858802,0.0201025653,-0.0109644792,-0.0191114709,0.0156182003,-0.01792836,-0.0182043079,-0.0286434293,0.0014144384,-0.0265274895,-0.0170839347,0.0264061904,0.0033435377,-0.014107924,0.0259829392,0.0087438692,0.0127835185,0.0113810267,0.0117691386,-0.0150741957,0.0117267991,0.0020763063,-0.0159695692,0.0050222014,-0.0148272258,0.0145646028,0.0507304756,-0.0186729919,0.0074076162,0.0103555509,0.0129937486,-0.0002830722,0.0048854957,0.0247755246,-0.0020033007,0.0062051017,0.019724744,-0.0028540312,-0.0035541982,-0.012953363,0.0292448127,0.016927844,0.0172583505,-0.0077286947,0.0082783416]},"351":{"Abstract":"Physical visualizations, or data physicalizations, encode data in attributes of physical shapes. Despite a considerable body of work on visual variables, \u201cphysical variables\u201d remain poorly understood. One of them is physical size. A difficulty for solid elements is that \u201csize\u201d is ambiguous - it can refer to either length\/diameter, surface, or volume. Thus, it is unclear for designers of physicalizations how to effectively encode quantities in physical size. To investigate, we ran an experiment where participants estimated ratios between quantities represented by solid bars and spheres. Our results suggest that solid bars are compared based on their length, consistent with previous findings for 2D and 3D bars on flat media. But for spheres, participants' estimates are rather proportional to their surface. Depending on the estimation method used, judgments are rather consistent across participants, thus the use of perceptually-optimized size scales seems possible. We conclude by discussing implications for the design of data physicalizations and the need for more empirical studies on physical variables.","Authors":"Y. Jansen; K. Hornb\u00e6k","DOI":"10.1109\/TVCG.2015.2467951","Keywords":"Data physicalization;Physical visualization;Psychophysics;Experiment;Data physicalization;physical visualization;psychophysics;experiment;physical variable","Title":"A Psychophysical Investigation of Size as a Physical Variable","Keywords_Processed":"physical visualization;experiment;physical variable;psychophysic;datum physicalization","Keyword_Vector":[0.1079265194,0.0182931307,0.0034721343,-0.0028014063,0.0764459849,-0.0479060609,-0.097949687,0.0671933751,-0.0944447303,-0.0220986071,-0.0641642739,0.0259553867,0.0528028165,-0.0171505184,0.0510630183,0.0594396419,-0.0455822136,0.0076690197,0.1961390464,-0.0853106893,0.1060865418,-0.0348814924,0.1201115149,0.0487413286,0.0519058183,0.1369350847,-0.089907013,-0.086344188,-0.0586469986,0.0620424149,0.0290810211,0.0117039771,-0.176367117,-0.0152239844,0.0187732855,0.0189686272,-0.0283418412,0.1046346745,0.0277747546,0.0705504819,0.0183315994,-0.0201270251,-0.0701930844,-0.131057526,0.1087461793,0.0113788662,-0.0563943729,0.0188598004,0.0404598146,-0.0604648422,0.0033668392,0.1118206937],"Abstract_Vector":[0.1450364233,0.0498693012,-0.0173689264,-0.0005865722,-0.0675576575,0.0714537566,-0.0127850609,0.027829318,0.1418574351,0.0016979802,-0.0807961073,0.024162388,-0.0158520928,0.0054808239,0.0841707967,-0.087896056,0.0247507167,0.1090502713,-0.0131923463,0.1714782264,-0.0546732912,-0.0750605461,0.0980400773,0.0156198915,-0.1036121087,0.002414049,0.1408487411,-0.0108616531,0.1363536446,0.0612760068,0.042239994,-0.0581117868,-0.054682407,0.0580260098,0.0154030396,0.0099113618,-0.0568960253,-0.0059445856,0.0679873544,0.0118754588,0.0416064926,-0.1110003268,-0.0014488635,0.0178993161,-0.0689511459,-0.0053601909,-0.0145458039,0.0606312699,-0.0356452542,-0.0360017534,0.0890204247,0.0543724267,0.003603596,0.0630705463,0.0271493749,0.0212508519,0.0294907474,-0.0206437375,-0.0303277562,0.03108104,-0.0104263373,-0.0133436698,-0.0116519121,0.0218966529,0.052654819,0.0000002838,0.0472230696,0.054473539,-0.0307749083,0.0267339498,0.0034922162,-0.005068435,-0.0138362745,0.0056701514,-0.0013155165,0.0254863348,-0.0263481359,-0.0086662104,-0.0477250464,0.0248861714,0.0047512789,-0.0551369153,-0.0367816339,-0.0067456831,0.0122996682,-0.0101187686,-0.0246812295,0.0762066429,-0.0197916802,-0.0038503883,0.003162527,0.0182562742,0.0120455089,-0.0313800064,-0.018288519,-0.0059615484,-0.0285154778,0.0092659208,0.0042474258,-0.0327793371,0.0587397264,0.0027169024,0.0246983922,0.0182557548,-0.0248592514,0.0086426162,0.0082499898,-0.018114738,0.0040230884,-0.0086312515,-0.0081903017,0.010862759,0.0080643849,-0.0051250084,-0.0152032114,0.0245980931]},"352":{"Abstract":"Traditional scatterplots fail to scale as the complexity and amount of data increases. In response, there exist many design options that modify or expand the traditional scatterplot design to meet these larger scales. This breadth of design options creates challenges for designers and practitioners who must select appropriate designs for particular analysis goals. In this paper, we help designers in making design choices for scatterplot visualizations. We survey the literature to catalog scatterplot-specific analysis tasks. We look at how data characteristics influence design decisions. We then survey scatterplot-like designs to understand the range of design options. Building upon these three organizations, we connect data characteristics, analysis tasks, and design choices in order to generate challenges, open questions, and example best practices for the effective design of scatterplots.","Authors":"A. Sarikaya; M. Gleicher","DOI":"10.1109\/TVCG.2017.2744184","Keywords":"Scatterplots;task taxonomies;study of designs","Title":"Scatterplots: Tasks, Data, and Designs","Keywords_Processed":"study of design;scatterplot;task taxonomy","Keyword_Vector":[0.081262828,-0.0199667155,-0.0025830997,-0.0141064636,0.0003028341,0.0114191747,-0.0043094798,0.0094146629,0.0081636453,-0.0275376422,0.009465212,-0.001216666,-0.0023828129,-0.0142548841,0.0083602546,0.0079926226,-0.026268189,-0.0186489557,0.0461512128,-0.003327962,0.0061984476,0.0122260945,-0.0386392471,-0.0105931585,0.045866425,0.0167792069,0.0155242251,0.0210326033,0.0231459358,0.0208297675,0.0326056065,-0.0078559854,0.0066118631,0.0066870951,-0.0058859662,0.0655213367,0.0396644898,-0.0284434894,0.0182273101,0.0180540917,0.0387445707,0.0469241311,0.041636397,0.0021352166,-0.045944167,-0.0162782707,0.0599129238,-0.0400972525,0.0044190848,0.0167582486,-0.0101822629,-0.0186713536],"Abstract_Vector":[0.1958392909,-0.0000239161,-0.0430463624,-0.0257343672,-0.0201071916,0.089364461,0.0071059057,0.0145700099,0.1147875217,0.0290825806,-0.0733089777,0.0039155924,-0.0695335997,0.0062530525,-0.0004792804,0.0338208988,0.0245103829,0.0138900219,0.0498019096,-0.0086289074,-0.0406524484,-0.0350213435,0.0166402775,-0.0000609681,0.0197132957,-0.0362428514,-0.0303174183,0.0408870633,-0.0046596825,0.037120385,-0.0151794071,0.005266569,-0.0192570603,-0.0236558467,0.0105127456,-0.0071340076,-0.0120578473,-0.0503392212,0.0063761154,-0.0175354145,0.0479031681,0.083138657,0.0031641629,0.0174742982,-0.0062528595,0.007268468,0.0081881293,0.02340109,0.0218730793,-0.0198481721,0.0191986044,-0.0568961316,0.0131006425,-0.0057944679,-0.0271649993,0.0003050453,-0.0147260484,-0.0106430646,-0.0409019682,-0.0460622651,0.0283806143,-0.0466128669,0.0228072424,-0.0669130459,0.017348222,-0.0077568153,-0.0143010893,-0.0048321702,-0.060172114,-0.0857229973,-0.0158696177,0.0396860954,0.0112779994,0.0245262446,-0.0089420363,-0.0280445372,0.0262869731,0.0467141571,-0.0024433171,-0.0221474002,0.0209613469,-0.0134246145,0.0071969398,-0.0231861617,0.0214914274,-0.0013258524,0.0002949796,-0.0117731758,-0.0230302005,0.0084536542,0.0643347162,0.0109246771,-0.0082783496,-0.0011787287,0.0004039485,-0.0402383697,0.0249142952,-0.0352213593,0.0299591597,0.0480874585,0.0297546279,-0.0026687191,-0.0076770187,-0.0069228903,-0.0062765629,0.0117358623,-0.0216291357,0.0271315047,-0.0097218441,-0.0043487772,-0.0124526439,0.0098486328,-0.0136047314,-0.0408192409,0.0445713281,0.0258746749]},"353":{"Abstract":"In this paper, we present a novel grid encoding model for content-aware image retargeting. In contrast to previous approaches such as vertex-based and axis-aligned grid encoding models, our approach takes each horizontal\/vertical distance between two adjacent vertices as an optimization variable. Upon this difference-based encoding scheme, every vertex position of a target grid is subsequently determined after optimizing the one-dimensional values. Our quad edge-based grid model has two major advantages for image retargeting. First, the model enables a grid optimization problem to be developed in a simple quadratic program while ensuring the global convexity of objective functions. Second, due to the independency of variables, spatial regularizations can be applied in a locally adaptive manner to preserve structural components. Based on this model, we propose three quadratic objective functions. Note that, in our work, their linear combination guides a grid deformation process to obtain a visually comfortable retargeting result by preserving salient regions and structural components of an input image. Comparative evaluations have been conducted with ten existing state-of-the-art image retargeting methods, and the results show that our method built upon the quad edge-based model consistently outperforms other previous methods both on qualitative and quantitative perspectives.","Authors":"Y. Kim; H. Eun; C. Jung; C. Kim","DOI":"10.1109\/TVCG.2018.2866106","Keywords":"Image retargeting;2D grid deformation;saliency detection;line segment detection;image quality assessment","Title":"A Quad Edge-Based Grid Encoding Model for Content-Aware Image Retargeting","Keywords_Processed":"saliency detection;line segment detection;image quality assessment;image retargete;2d grid deformation","Keyword_Vector":[0.1090269897,-0.0824264479,-0.0704561202,-0.0900198106,0.0092658619,-0.0862677301,-0.1668962802,-0.0577872061,-0.1771550246,0.1729747614,-0.1315944397,0.1636222587,0.0841564421,-0.0708785193,0.0011322887,-0.0672416243,0.1753643306,0.0036096382,0.026761938,-0.0645443023,-0.0116239738,0.1093310756,0.0632762981,0.0680070602,0.0792624451,-0.0546783629,0.1042641513,-0.1131603746,-0.0315315081,0.095486405,-0.0635884656,0.080859771,-0.0304116176,-0.0289383696,0.0260978827,0.0789216021,-0.0404254017,0.0964092543,0.0873061598,0.0411577674,-0.0821745636,-0.1045309769,-0.0041270147,-0.0206692301,-0.0308200731,0.0617009876,-0.0087394549,0.0179093925,-0.0542748261,-0.0628117044,0.0259516863,-0.0153751493],"Abstract_Vector":[0.206407596,-0.0735727157,-0.0060426928,-0.0051621905,-0.0507557075,0.063517705,0.0409731533,0.0155182972,0.0226471614,0.0334648569,-0.0483351252,0.0175017714,0.0278651373,0.0136079665,0.0084592296,0.0534402016,-0.0216264799,0.0183518602,0.0371385963,-0.0275324623,-0.000308212,-0.0389324593,0.0316603545,0.0570524914,-0.0128798061,-0.0436306129,-0.0513391616,0.0364908303,-0.0219871324,-0.0074746438,-0.0297355834,-0.0118665022,-0.0417016611,-0.0120251692,0.0824695585,-0.0301373648,-0.0434037345,-0.036612861,-0.0225050637,-0.0391570266,0.0325860105,0.0418894108,0.064008358,-0.0377351299,0.0354441895,-0.0266391954,0.0672839673,0.0464624151,-0.004751911,-0.0358057857,0.0018845506,0.0118993457,0.0035110506,-0.0131943718,0.0247214592,0.0409791048,0.0256740544,0.0137516068,-0.0059951083,-0.0320362037,0.0072779335,0.0186492962,0.0540695691,-0.0515727858,-0.0058755937,-0.0353228961,0.0216266558,-0.0421593883,-0.0626349707,0.0581881303,-0.0188179109,0.0378934678,0.0150980748,-0.0137149799,0.0300846401,-0.0295326845,0.0380736336,0.0127074002,0.035053319,-0.0170240401,0.0015226666,0.009326627,0.0376296327,-0.0009513644,-0.0646650618,0.0125096349,-0.0048140872,0.0125792169,0.0623330491,-0.0645015026,-0.0025190063,-0.0417335098,0.0256255665,0.0023257058,-0.0341804362,-0.0189535164,0.019526989,-0.0092027902,-0.0075901529,-0.002966038,0.0012201515,-0.0577144193,-0.0382995836,0.0035247179,-0.0236414708,-0.014157511,0.0231968036,-0.0179535373,0.0350283998,-0.0122465511,0.0182351122,0.0702692753,-0.025521655,0.0009855565,0.0364348036,-0.0257771501]},"354":{"Abstract":"Visualization tools are often specialized for specific tasks, which turns the user's analytical workflow into a fragmented process performed across many tools. In this paper, we present a component model design for data visualization to promote modular designs of visualization tools that enhance their analytical scope. Rather than fragmenting tasks across tools, the component model supports unification, where components-the building blocks of this model-can be assembled to support a wide range of tasks. Furthermore, the model also provides additional key properties, such as support for collaboration, sharing across multiple devices, and adaptive usage depending on expertise, from creating visualizations using dropdown menus, through instantiating components, to actually modifying components or creating entirely new ones from scratch using JavaScript or Python source code. To realize our model, we introduce Vistrates, a literate computing platform for developing, assembling, and sharing visualization components. From a visualization perspective, Vistrates features cross-cutting components for visual representations, interaction, collaboration, and device responsiveness maintained in a component repository. From a development perspective, Vistrates offers a collaborative programming environment where novices and experts alike can compose component pipelines for specific analytical activities. Finally, we present several Vistrates use cases that span the full range of the classic \u201canytime\u201d and \u201canywhere\u201d motto for ubiquitous analysis: from mobile and on-the-go usage, through office settings, to collaborative smart environments covering a variety of tasks and devices..","Authors":"S. K. Badam; A. Mathisen; R. R\u00e4dle; C. N. Klokmose; N. Elmqvist","DOI":"10.1109\/TVCG.2018.2865144","Keywords":"Components;literate computing;development;exploration;dissemination;collaboration;heterogeneous devices","Title":"Vistrates: A Component Model for Ubiquitous Analytics","Keywords_Processed":"collaboration;exploration;development;heterogeneous device;literate computing;dissemination;component","Keyword_Vector":[0.0663539798,-0.0257255274,-0.0316230746,0.0807099736,0.0350083521,0.0387989597,-0.0001390492,0.1149205069,-0.0711986524,-0.0222599085,-0.0923945898,-0.0093512885,-0.0306622695,-0.0105645022,0.0415564958,-0.1237811592,0.0140325852,0.0438920501,0.0282627811,-0.0290711117,0.0195994886,-0.0529105287,-0.0650153991,0.0212257265,-0.0533622988,-0.0218365039,0.123884735,-0.0706462075,-0.0551202906,0.0057215392,0.0250407368,-0.0188219853,0.005948408,0.0171290816,-0.0362940913,-0.0233200123,-0.0079716983,-0.017474028,0.0652374008,-0.0042528544,0.019493698,-0.0459220737,0.1113866758,0.0264216246,0.058824642,-0.0537313592,0.050383228,0.0241392568,-0.0697375955,-0.0213430425,0.0315388712,-0.0155306703],"Abstract_Vector":[0.2199620344,-0.0735371681,0.0316673923,0.0018370196,-0.0138576391,0.0122564778,-0.0229461585,-0.0051747195,-0.0147380363,-0.0359853243,0.0195105677,0.0141800289,-0.0284964544,0.0144849085,0.0039127746,-0.0014031312,0.0164231182,0.0050541566,0.0321501227,-0.0243364107,0.0289056486,0.0524777344,0.0170260843,-0.0132162252,0.0006458196,0.0542453643,-0.035677216,0.0309128354,0.0164355875,0.0137677327,0.0018429689,-0.0752381735,0.0017007411,0.0004587029,0.0273224831,-0.0093502529,0.0047421141,0.0127326423,0.0350604885,0.0046182074,-0.058930635,0.0176549278,-0.0036890544,0.0305618306,-0.000322057,-0.0190496243,-0.0018155096,-0.0246802745,-0.0216205789,-0.0071571454,0.0098493584,0.00469883,-0.0106585021,0.0143280003,0.0197615456,0.0321663786,-0.0411625431,-0.052360436,0.0238033131,-0.0005989507,0.0037766545,-0.0002055197,0.0116934417,0.0336312924,0.0442583106,-0.0456952791,0.0364488419,0.0209681639,-0.030497184,-0.0285526874,-0.0152305632,-0.0007491331,-0.0007015896,-0.0293670765,0.0142724135,0.0167437925,-0.0389456622,0.0382045714,0.0560745703,-0.0235763279,0.0225483499,-0.0174013975,0.0283199777,0.0075570479,0.0047966344,-0.0038381947,-0.0317919753,0.005966007,0.0171375471,-0.0043302418,-0.0075907192,0.0014569924,-0.0019951729,0.0168646658,-0.0201881151,0.0241939267,0.0023094873,0.0295995212,0.0178590191,0.0064765325,-0.0067493531,0.0513877748,-0.0339351438,0.052027706,0.023409791,-0.0253050675,0.0216009709,0.030460885,0.0235100141,0.0308628842,-0.0164731897,0.0197406336,0.0208930405,0.0190652289,-0.0103959097,0.0384329772]},"355":{"Abstract":"This paper presents a simple and effective two-stage mesh denoising algorithm, where in the first stage, face normal filtering is done by using bilateral normal filtering in a robust statistics framework. Tukey's bi-weight function is used as similarity function in the bilateral weighting, which is a robust estimator and stops the diffusion at sharp edges to retain features and removes noise from flat regions effectively. In the second stage, an edge-weighted Laplace operator is introduced to compute a differential coordinate. This differential coordinate helps the algorithm to produce a high-quality mesh without any face normal flips and makes the method robust against high-intensity noise.","Authors":"S. K. Yadav; U. Reitebuch; K. Polthier","DOI":"10.1109\/TVCG.2018.2828818","Keywords":"Robust statistics;face normal processing;tukey's bi-weight function;high fidelity mesh;differential coordinate","Title":"Robust and High Fidelity Mesh Denoising","Keywords_Processed":"tukey bi weight function;differential coordinate;high fidelity mesh;face normal processing;robust statistic","Keyword_Vector":[0.2115416833,-0.1657238108,-0.121312092,0.1317608857,-0.1314831179,-0.1150895163,0.1193507024,-0.0011805546,0.1184903837,-0.0496838445,0.074966355,0.0345191722,-0.0637481853,-0.0816055453,0.005007653,0.0930346255,-0.0568897998,0.0554179599,-0.0794376644,-0.0601035601,-0.0323319127,0.0940112656,0.0147807552,0.1509006893,-0.118135078,0.0896276977,-0.0065421954,0.0343617484,-0.007448138,0.08405825,0.006132721,0.0172504399,-0.1882755205,0.111416404,0.0641229241,-0.0328418832,-0.0450757179,0.2190918557,0.0950072323,0.1729208136,0.0679703953,-0.0647762931,0.0259771401,0.270795907,0.0362977384,-0.0444851322,0.09362229,-0.0232436161,-0.0575901186,-0.0155288215,-0.1049524296,0.0243555487],"Abstract_Vector":[0.1844335287,-0.1089101379,0.0270480168,-0.0102676974,0.0010967165,-0.0757837762,0.0192010277,0.0183973203,0.0175667003,0.1093399763,0.0063005394,0.0005961065,-0.113072301,0.0651210459,-0.0642128486,-0.0858526449,0.0259049415,-0.0506579488,-0.1091163587,0.0241010523,-0.0371402292,-0.0111477922,0.1091882109,0.0519171347,-0.0349468192,0.0688748504,0.0437246353,0.0844066031,-0.0437282995,-0.0512707418,0.0247662385,-0.0124044273,0.0197244089,-0.0620159069,-0.0082346009,0.0685051896,-0.1157500188,0.0227361879,-0.0324995863,-0.0021842565,-0.0268862187,0.0055917017,0.0519101806,-0.0461550889,0.0245105874,0.0351326017,0.0419964423,-0.0421028685,0.0839501948,-0.0985978492,-0.119349728,-0.0128411193,-0.0356436487,0.0930363311,-0.0521095868,0.0586977369,0.0146329605,0.0618270712,0.0594792756,0.0636685514,0.0043730803,-0.0130871209,0.058266437,-0.0453146819,-0.0824197923,-0.0526186253,-0.0229562427,0.1014801136,0.0281869051,-0.0791322767,0.0175042135,-0.0000456245,0.0113173995,0.0157515646,0.1077545336,-0.0353894224,0.069686018,-0.0451695221,0.0788081978,-0.0092874584,-0.0176994747,-0.0160224584,-0.0222023935,-0.0755299169,0.1600549734,-0.0034087224,0.0036145025,-0.0245402165,0.0538833898,-0.0053896546,0.010563016,-0.0066711676,-0.0305811766,-0.0313815751,-0.0629531981,-0.0076396358,0.0671392184,0.0574102113,0.0246901668,0.012114401,0.0363636799,0.0003980804,-0.0850845601,-0.0004556264,-0.0114087163,-0.0247794905,-0.0238505169,0.0239048831,0.051431566,-0.0236828281,-0.018726269,-0.0104074399,0.0192202022,-0.0419851401,0.0231461736,0.0225846467]},"356":{"Abstract":"Immersive navigation in virtual reality (VR) and augmented reality (AR) leverages physical locomotion through pose tracking of the head-mounted display. While this navigation modality is intuitive, regions of interest in the scene may suffer from occlusion and require significant viewpoint translation. Moreover, limited physical space and user mobility need to be taken into consideration. Some regions of interest may require viewpoints that are physically unreachable without less intuitive methods such as walking in-place or redirected walking. We propose a novel approach for increasing navigation efficiency in VR and AR using multiperspective visualization. Our approach samples occluded regions of interest from additional perspectives, which are integrated seamlessly into the user's perspective. This approach improves navigation efficiency by bringing simultaneously into view multiple regions of interest, allowing the user to explore more while moving less. We have conducted a user study that shows that our method brings significant performance improvement in VR and AR environments, on tasks that include tracking, matching, searching, and ambushing objects of interest.","Authors":"M. Wu; V. Popescu","DOI":"10.1109\/TVCG.2017.2778249","Keywords":"Augmented reality;virtual reality;navigation;occlusion management;multiperspective visualization;depth cues","Title":"Efficient VR and AR Navigation Through Multiperspective Occlusion Management","Keywords_Processed":"occlusion management;virtual reality;depth cue;multiperspective visualization;augmented reality;navigation","Keyword_Vector":[0.0901254434,0.0007673959,0.123922714,0.0224963433,-0.086248406,-0.0538266021,0.0226403186,0.0170055072,-0.0498245732,0.1406618233,-0.0601572122,0.0290968515,0.0628529438,0.0358309773,0.0396954178,0.0037520596,0.0004687482,0.0522511539,0.0554078522,-0.0201921032,-0.0589132917,0.0068896334,-0.0413626079,0.025813685,0.0158916521,0.0365440512,0.052063819,0.0233165091,-0.0339668704,0.0663027718,0.0330875461,0.011423992,-0.0320086403,-0.0140601225,-0.0288202781,-0.0083303313,-0.01981417,-0.0313280408,-0.0590936058,0.0227035026,0.0303976544,0.0039620719,-0.0559685883,-0.0008480388,-0.0068185227,0.0263178088,-0.0217214122,-0.0138451492,-0.0096464747,-0.0337145626,-0.000852982,-0.0808385496],"Abstract_Vector":[0.1501762616,-0.0144422112,0.0487232104,-0.0465691426,-0.03855908,0.0904436938,0.1279003871,-0.0061712031,0.0393776624,0.0450310722,-0.1108549611,0.0604745702,-0.0208018281,0.0478826939,-0.0543923902,-0.0452893467,0.0285348263,-0.0076180294,-0.0716406832,0.0318411844,-0.137156936,0.0599827128,0.401819422,-0.0358101646,-0.1044927559,-0.093191473,0.0108369455,-0.0468135417,0.1345805488,0.0451803577,0.0057166655,-0.0231149565,-0.0154890924,-0.0052089765,-0.0221270257,-0.0484769365,0.0189146707,0.0094411494,-0.0712355032,-0.0227952202,0.0192443261,0.0006080977,0.0136060062,0.0095760455,-0.0563671528,-0.0000543714,-0.0297389192,-0.0034023888,0.0146466464,0.0203467221,0.0856081721,0.0809668771,0.0336973499,-0.0221269373,0.0437146534,-0.0271097943,-0.0058379461,-0.0286371021,-0.0225940597,-0.0957954876,0.0032516086,0.012384453,0.0230278981,0.003350698,0.0240065267,0.07941841,-0.0279197752,0.0156481151,0.0198396263,0.0015450599,0.0146946152,0.02757553,0.0011774456,0.0297287695,-0.0399686147,0.0029814483,-0.0037579041,0.0411772997,-0.0264172469,0.0170815152,0.0140254442,-0.0359843091,0.0041739319,-0.0086175538,-0.0026489111,-0.002376543,-0.0163686841,0.0078565037,-0.0397757438,0.0192756807,-0.0542354833,-0.0060470728,-0.0192844221,0.0409983114,-0.0177716306,-0.0440120224,0.0301302127,0.005333889,0.0426764696,0.0361562596,0.0490167142,0.0099751931,0.0264350874,-0.0150267486,-0.033691329,-0.00407535,-0.0026778634,0.0497867471,0.0183699431,-0.0341620427,0.025510799,-0.0183894809,-0.0052071549,0.0065072868,0.0513618342,0.0252532046]},"357":{"Abstract":"Histograms are a fundamental tool for multidimensional data analysis and processing, and many applications in graphics and visualization rely on computing histograms over large regions of interest (ROI). Integral histograms (IH) greatly accelerate the calculation in the case of rectangular regions, but come at a large extra storage cost. Based on the tensor train decomposition model, we propose a new compression and approximate retrieval algorithm to reduce the overall IH memory usage by several orders of magnitude at a user-defined accuracy. To this end we propose an incremental tensor decomposition algorithm that allows us to compress integral histograms of hundreds of gigabytes. We then encode the borders of any desired rectangular ROI in the IH tensor-compressed domain and reconstruct the target histogram at a high speed which is independent of the region size. We furthermore generalize the algorithm to support regions of arbitrary shape rather than only rectangles, as well as histogram field computation, i.e., recovering many histograms at once. We test our method with several multidimensional data sets and demonstrate that it radically speeds up costly histogram queries while avoiding storing massive, uncompressed IHs.","Authors":"R. Ballester-Ripoll; R. Pajarola","DOI":"10.1109\/TVCG.2018.2802521","Keywords":"Integral histograms;tensor decomposition;multidimensional compression","Title":"Tensor Decompositions for Integral Histogram Compression and Look-Up","Keywords_Processed":"integral histogram;multidimensional compression;tensor decomposition","Keyword_Vector":[0.0456861905,0.0126202772,0.0607444666,0.0341870453,-0.008862743,-0.0366979559,0.02718571,-0.0085626168,-0.0312247575,0.0497492527,-0.00792637,0.0147326717,0.0234137922,0.021113762,-0.0063959389,-0.05584662,-0.030194539,-0.0225265593,0.0436951359,-0.0008915173,-0.0271298352,-0.0132341317,-0.0166163841,0.0101608119,0.0371076327,-0.0040941535,0.0251437585,0.0445674468,0.0036508065,-0.0027760695,-0.0122216793,-0.0219182167,0.0289749975,-0.0121947573,0.0690564829,-0.0012559968,0.071842081,0.0607229292,0.0235165497,-0.0299630554,0.0251820532,-0.0103886561,-0.0686417897,-0.0482565321,-0.0645750627,-0.0288412762,0.0407948024,0.074632438,0.0088014205,-0.0037548804,-0.0530279574,0.1069485398],"Abstract_Vector":[0.1994598264,0.0532329632,0.0000049534,0.0419090057,-0.0937743561,0.0383311426,0.0066613866,0.0104383669,-0.0278267904,-0.0076717644,-0.0126816116,-0.0260080837,-0.0019003664,0.0359862065,0.0075045053,0.068777071,0.0597189831,0.0683702744,0.0287745547,-0.081981825,0.0283319189,0.0004499685,0.0218310968,-0.0263713185,-0.0225639791,0.000180057,0.0411363817,0.040483766,0.0050322622,-0.00447334,-0.0169606064,-0.02464459,0.0013801743,0.018969055,0.0174775626,0.0135504,-0.0199808705,-0.0141130879,-0.0080237767,0.0092081467,-0.0353227496,0.0014891161,-0.0040220958,-0.0278611528,0.0603237338,0.0263923957,0.022133007,0.0464744363,0.0298945855,-0.0067469232,0.0254915956,0.0853798108,-0.0067858266,0.0484791969,-0.0472249169,0.0262971978,0.0289871502,-0.0370838392,0.0285516746,-0.0224230873,0.0228246749,0.0419850389,-0.0100290189,-0.0058004524,0.0137178694,-0.0258449248,0.0049863965,0.0122465554,0.0428668186,0.0488795493,-0.0079894277,0.0380905262,0.0112443965,-0.0171229449,0.0194261253,0.007209949,0.0185490516,0.0172944661,0.0002668919,0.0212129216,-0.0025061248,-0.0265864573,0.0169571546,-0.0067930686,-0.0199396854,0.0173474492,-0.0225668334,0.0407641207,0.0154035485,-0.0064139634,0.0150415941,0.0405743914,0.0122842545,0.0511218254,0.0032561039,-0.0339671716,-0.0280198187,0.0502243395,-0.0106754861,0.0941108782,-0.0031151213,-0.068202732,-0.0028294967,-0.0021622482,0.0295588271,-0.0032113948,-0.0259087604,-0.0153402904,0.0222400663,0.0255698677,-0.0313715545,-0.0189336808,0.0008017104,0.0089485062,-0.0108531522,-0.038558986]},"358":{"Abstract":"We introduce the Clutterpalette, an interactive tool for detailing indoor scenes with small-scale items. When the user points to a location in the scene, the Clutterpalette suggests detail items for that location. In order to present appropriate suggestions, the Clutterpalette is trained on a dataset of images of real-world scenes, annotated with support relations. Our experiments demonstrate that the adaptive suggestions presented by the Clutterpalette increase modeling speed and enhance the realism of indoor scenes.","Authors":"L. Yu; S. Yeung; D. Terzopoulos","DOI":"10.1109\/TVCG.2015.2417575","Keywords":"interactive 3D modeling,;scene modeling;scene understanding;indoor scenes;modeling tools;suggestive user interfaces;Interactive 3D modeling;scene modeling;scene understanding;indoor scenes;modeling tools;suggestive user interfaces","Title":"The Clutterpalette: An Interactive Tool for Detailing Indoor Scenes","Keywords_Processed":"suggestive user interface;indoor scene;scene understanding;modeling tool;interactive 3d modeling;scene model","Keyword_Vector":[0.0867579717,-0.0314414298,0.0251732437,-0.0453549442,0.0000053738,-0.003355398,-0.036152106,0.0054883029,0.0052062249,-0.0326557037,0.0051223271,0.0093126382,-0.0185470928,0.0274228934,-0.02583669,0.0267715304,0.000952135,0.0392627892,0.0063652391,-0.0015612467,-0.00784696,-0.0259290965,-0.0275673618,0.0125330612,-0.0059855293,0.0225659276,-0.0053489941,0.0177964635,-0.019346045,0.0392283293,0.0063378654,0.0059687138,-0.0006874088,-0.010129935,0.0251517254,0.0289754302,-0.0065419923,-0.0411122803,-0.0172596474,-0.0177489632,-0.0320346453,-0.0218915978,-0.0268993971,-0.0268408611,0.0047825225,-0.0230003091,0.0404485904,0.0059615419,-0.0119523921,0.0247100257,-0.0129763952,-0.0200233769],"Abstract_Vector":[0.2193854311,-0.0624831532,0.0001281949,-0.0444608638,-0.0464318721,0.0722696282,-0.0147427089,-0.0228136541,-0.0060765218,0.0155833761,-0.0353163346,0.0305440328,-0.0236688175,0.0157669296,-0.0281324389,0.0140834937,0.0230415527,-0.043730789,0.0329279656,-0.0315027496,-0.0017454326,-0.0223069189,-0.0124799472,-0.0746959977,-0.0167125767,0.0168824827,-0.0131595577,-0.0131019864,0.0302365557,0.0151138367,0.093764712,0.0137098861,-0.0144983227,-0.005447372,-0.0478176309,0.008243178,0.0487481892,-0.0234116915,0.0495655416,0.0218155097,0.03158058,-0.0659046448,-0.0083276149,0.0176892444,-0.0373046876,-0.0084992577,0.0416259764,0.0060920833,-0.0105932988,-0.0080142536,0.0002335538,-0.0398184635,-0.0744061223,-0.0300512123,0.0497898204,-0.0163527852,0.0235911775,0.0168807163,-0.0280305749,-0.0441563736,0.032507595,-0.0000144391,-0.0120753799,-0.0053770801,-0.0116978548,0.0012553472,-0.0073372494,-0.0029214121,0.046285886,-0.0396742519,-0.0275336234,0.0112821868,-0.0106069496,-0.0557478979,0.0224289801,-0.0035940036,0.0073283244,-0.0197457334,-0.00100506,-0.0412540641,0.032580537,-0.021708627,0.0105013326,-0.0092161846,-0.0104890814,0.0061912524,0.0222447818,-0.0091983611,0.0265887968,-0.0217652712,0.0026010007,-0.0022355765,0.0410669208,0.0049390212,0.006557932,-0.0007952632,-0.000670557,-0.0389998904,-0.024705806,-0.0157653579,-0.019620471,-0.0350801039,-0.0500821143,-0.0238604076,0.0020150781,0.0034507438,0.003916795,0.0104203621,0.00060359,0.0066304287,-0.0135267215,0.0043115839,0.0205488194,-0.0166745811,0.0215798186,-0.0044172862]},"359":{"Abstract":"3D object temporal trackers estimate the 3D rotation and 3D translation of a rigid object by propagating the transformation from one frame to the next. To confront this task, algorithms either learn the transformation between two consecutive frames or optimize an energy function to align the object to the scene. The motivation behind our approach stems from a consideration on the nature of learners and optimizers. Throughout the evaluation of different types of objects and working conditions, we observe their complementary nature - on one hand, learners are more robust when undergoing challenging scenarios, while optimizers are prone to tracking failures due to the entrapment at local minima; on the other, optimizers can converge to a better accuracy and minimize jitter. Therefore, we propose to bridge the gap between learners and optimizers to attain a robust and accurate RGB-D temporal tracker that runs at approximately 2 ms per frame using one CPU core. Our work is highly suitable for Augmented Reality (AR), Mixed Reality (MR) and Virtual Reality (VR) applications due to its robustness, accuracy, efficiency and low latency. Aiming at stepping beyond the simple scenarios used by current systems, often constrained by having a single object in the absence of clutter, averting to touch the object to prevent close-range partial occlusion or selecting brightly colored objects to easily segment them individually, we demonstrate the capacity to handle challenging cases under clutter, partial occlusion and varying lighting conditions.","Authors":"D. J. Tan; N. Navab; F. Tombari","DOI":"10.1109\/TVCG.2017.2734539","Keywords":"3D Tracking;Random Forest;6D Pose Estimation","Title":"Looking Beyond the Simple Scenarios: Combining Learners and Optimizers in 3D Temporal Tracking","Keywords_Processed":"Random Forest;3D tracking;6D Pose Estimation","Keyword_Vector":[0.0962030222,-0.0399404383,-0.0467117542,0.0503846727,0.005672655,0.0493978564,0.0589169371,-0.0227231489,-0.012990371,-0.0329697009,0.0162155116,0.0274156026,-0.0089159296,-0.0839393806,0.0646051749,0.0147524008,-0.0539944196,-0.0301237007,0.1071970792,-0.1314311178,0.0534928234,0.0027142523,-0.075413019,-0.0187205512,0.2078987082,-0.0186601043,-0.0032571023,0.0559031263,0.0070906496,-0.01660921,-0.0278085045,-0.0606935711,0.0392165728,0.0567068706,-0.0346428958,0.1073496728,0.1292320535,-0.0288320094,0.1029551229,0.0319918653,0.1311753125,0.0321157612,0.0075174076,0.0664740266,-0.062501513,0.0070569304,0.0482601633,-0.1776647416,-0.0120423495,0.0061633447,-0.0146015492,-0.029955416],"Abstract_Vector":[0.2845836506,-0.1776136189,-0.0123247732,0.149088177,0.1342404078,0.1323723881,-0.0160913668,-0.0575593847,-0.1540032564,0.1389473198,0.0761251128,-0.2168558823,-0.1126692716,0.1032347014,0.155211538,-0.001313817,0.0005788731,0.0480496673,0.0970099601,-0.0102464669,-0.0607755144,0.0356542854,-0.0125289045,-0.076697626,0.0627135888,-0.0198406144,-0.0358605196,-0.0662352921,0.0971208382,0.0263849062,-0.0514005693,0.0178047902,-0.0392795932,0.041203439,0.0714613138,-0.0019181058,-0.0047765108,-0.0083772909,0.0252025174,0.0019229447,0.009743321,0.085983633,0.0137606801,0.0051228745,0.0602362773,-0.006351005,0.0174473506,0.016275511,-0.0219594636,0.0125006524,0.0425535899,-0.0308781226,0.0469847877,0.0128176142,0.0304652852,0.0427497931,0.0681069399,-0.0351593631,-0.0022288768,-0.0009896954,0.0150905066,0.0053663341,0.0143700897,0.0208044081,0.0271987394,-0.0113708166,0.0506875819,0.0059215606,-0.0202476912,-0.0347821091,0.0154381854,0.045875111,0.0214090215,-0.0283072873,-0.0031708771,-0.0172190689,-0.021618639,-0.0451804538,-0.0118090987,0.0505941194,-0.0396042373,0.0180049303,-0.0444873377,0.0028665885,0.0107942163,0.0286340208,0.0022372098,0.0161439029,-0.0287432173,0.0162916641,-0.0569247969,-0.0059446062,-0.0315270987,0.0009379916,0.0128339783,-0.057604257,-0.0008886802,-0.0278967944,0.0504092203,-0.0034186562,0.007249393,-0.0072254706,-0.0173358473,-0.0279159669,0.0342568741,-0.0026828362,-0.0198578374,0.0273441829,-0.0169655515,-0.0195184546,-0.0099823973,-0.0032367164,0.0009998191,-0.0740527137,0.0074901075,0.0158354903]},"36":{"Abstract":"Designing a good scatterplot can be difficult for non-experts in visualization, because they need to decide on many parameters, such as marker size and opacity, aspect ratio, color, and rendering order. This paper contributes to research exploring the use of perceptual models and quality metrics to set such parameters automatically for enhanced visual quality of a scatterplot. A key consideration in this paper is the construction of a cost function to capture several relevant aspects of the human visual system, examining a scatterplot design for some data analysis task. We show how the cost function can be used in an optimizer to search for the optimal visual design for a user's dataset and task objectives (e.g., \u201creliable linear correlation estimation is more important than class separation\u201d). The approach is extensible to different analysis tasks. To test its performance in a realistic setting, we pre-calibrated it for correlation estimation, class separation, and outlier detection. The optimizer was able to produce designs that achieved a level of speed and success comparable to that of those using human-designed presets (e.g., in R or MATLAB). Case studies demonstrate that the approach can adapt a design to the data, to reveal patterns without user intervention.","Authors":"L. Micallef; G. Palmas; A. Oulasvirta; T. Weinkauf","DOI":"10.1109\/TVCG.2017.2674978","Keywords":"Scatterplot;optimization;perception;crowdsourcing","Title":"Towards Perceptual Optimization of the Visual Design of Scatterplots","Keywords_Processed":"perception;crowdsource;scatterplot;optimization","Keyword_Vector":[0.1677052907,-0.1262157144,0.0363860163,-0.2062125476,0.2009056174,0.0804196201,0.5327834381,-0.1158411047,-0.1068208605,-0.0986850556,-0.0193609124,0.1049153244,0.0677050932,0.0034152689,0.0063398259,0.0424600036,-0.0053429009,0.0444105393,-0.0213200648,-0.0311390075,0.0119875924,0.0264239939,0.0443871603,-0.04676548,0.0344566142,-0.0279477632,-0.0512563475,0.0125362273,-0.0255509229,0.011004308,-0.0204776255,-0.0109083352,0.0255594379,-0.0935422824,-0.0928153982,-0.0008384706,-0.0431792462,0.0420622394,0.0120325188,-0.0345043777,0.0281500307,-0.0490724044,0.0030382749,-0.0193726279,0.0080977146,0.0404042823,-0.0158415087,0.0080384763,-0.0173460263,0.0048653733,0.0128193429,0.0045775659],"Abstract_Vector":[0.2065100181,-0.1183743701,-0.0050072095,0.1920463202,0.143212047,0.1316535628,-0.0312447333,-0.0156461215,-0.044000391,-0.0093253571,0.0030455244,0.0156533607,0.055612822,-0.0382995043,-0.1066988757,0.0290840254,0.0980637027,0.0461821325,-0.0489134491,-0.0103620591,0.0269731915,-0.0304674442,-0.0064234263,0.0372415356,-0.0626993712,0.0154171193,-0.0372520366,0.059734129,-0.0219948204,-0.0249608663,0.0090702982,-0.0136168186,0.032581876,0.042603572,0.0552675521,0.0152934025,-0.0237225476,0.0184624511,-0.0562002544,0.0064265502,-0.0606235604,0.0157907514,-0.0951716477,0.0120805072,-0.0293570672,0.0172885047,0.0302773689,-0.0136707183,0.0091466829,-0.0197126496,-0.02451634,0.0578888185,0.0084278757,0.0707851961,0.003547352,-0.0081349801,0.0056207294,-0.0139681341,-0.0091591874,-0.039477109,0.0217489765,0.0004361809,-0.0279888384,-0.0158868884,0.0069360456,0.0197424796,-0.0006232169,-0.0110943353,-0.0596823473,-0.0190656218,-0.0102451463,0.0074616046,0.0303118041,-0.0565681806,-0.0007837295,0.0503479718,-0.0072975149,-0.0212396986,0.0115241287,0.0078248467,-0.0612939605,-0.0284856651,0.0188301651,0.022949261,0.0259969483,-0.0003915195,-0.0008103897,0.029869797,-0.0172494471,-0.016477806,0.0229453906,0.0083526697,-0.0137216243,-0.0037534044,-0.0314539951,-0.0302097475,-0.0030954177,0.0198833475,-0.0078095468,-0.0008563026,0.0087848659,-0.00040109,0.0237931171,-0.0168632568,0.0433725077,-0.0019238417,-0.0330222943,0.0190740423,-0.0104109801,-0.0144490167,-0.0073975333,-0.0122280392,0.0109224496,0.0002383375,-0.0245232756,0.0015263939]},"360":{"Abstract":"Accommodative depth cues, a wide field of view, and ever-higher resolutions all present major hardware design challenges for near-eye displays. Optimizing a design to overcome one of these challenges typically leads to a trade-off in the others. We tackle this problem by introducing an all-in-one solution - a new wide field of view, gaze-tracked near-eye display for augmented reality applications. The key component of our solution is the use of a single see-through, varifocal deformable membrane mirror for each eye reflecting a display. They are controlled by airtight cavities and change the effective focal power to present a virtual image at a target depth plane which is determined by the gaze tracker. The benefits of using the membranes include wide field of view (100\u00b0 diagonal) and fast depth switching (from 20 cm to infinity within 300 ms). Our subjective experiment verifies the prototype and demonstrates its potential benefits for near-eye see-through displays.","Authors":"D. Dunn; C. Tippets; K. Torell; P. Kellnhofer; K. Ak\u015fit; P. Didyk; K. Myszkowski; D. Luebke; H. Fuchs","DOI":"10.1109\/TVCG.2017.2657058","Keywords":"Augmented reality;displays;focus accommodation;perception;user study","Title":"Wide Field Of View Varifocal Near-Eye Display Using See-Through Deformable Membrane Mirrors","Keywords_Processed":"user study;display;focus accommodation;augmented reality;perception","Keyword_Vector":[0.200893927,0.0005768875,0.2323435642,-0.0042902996,-0.0810958865,-0.0574047179,0.0616239244,0.0948905543,-0.122690844,0.1357020443,0.1307010159,-0.0418221457,-0.0692049806,-0.0319024394,-0.0264845414,0.0084697215,-0.0687647172,-0.0017468403,0.0326807434,-0.011454264,0.022725465,-0.0024142611,-0.0765151971,-0.0347835775,0.0386861457,-0.0134576827,-0.0444966501,0.0385978203,0.0491545099,0.0130377524,-0.0475553851,-0.0351629505,0.0312020987,-0.0162441285,0.0997646772,-0.0169147979,0.0764425795,0.0234002186,-0.0239454283,0.0343127756,-0.0480718567,0.0338709275,0.0493022609,-0.0642960338,-0.0364537805,-0.0103285044,0.0354134357,-0.0117143255,-0.0316372149,0.0194334675,-0.041205669,0.0902919645],"Abstract_Vector":[0.2360692946,0.1035366573,0.1844810088,0.0823058157,-0.1147149216,-0.0218157523,-0.0642366457,0.0881851811,-0.0683670447,0.0395647869,-0.0316137425,-0.0141235811,0.0705262071,0.0000616538,0.0256442893,0.0324970606,0.0200096251,0.0011685084,-0.0370582477,0.0637493326,-0.0386279785,-0.0218623467,0.0001813521,-0.0705389331,0.0108698793,-0.0519771333,-0.0655499632,0.0271821477,-0.0026238259,0.020838671,-0.0235971462,0.0133284233,0.0391247041,0.0336065993,0.0317570601,-0.0420707283,-0.0517192417,-0.0575133841,0.0100698954,0.0315036401,-0.029043023,0.0204528838,-0.0035397631,-0.0035378345,0.0382211162,-0.0326474273,-0.0155651312,0.0052422661,-0.0061373543,0.0032439012,-0.0529684096,0.0313762312,-0.0258271153,0.0268987608,0.0005001508,0.00410998,-0.0327184776,-0.0337081529,0.0179279949,0.0163776001,0.0004764885,-0.0042692994,-0.0245986735,-0.0151858043,0.0237699866,-0.0102923166,0.0204306739,-0.0046812495,-0.0264393497,0.0027176257,0.0521766398,-0.0280854674,-0.0042510723,0.0023738359,0.0096655083,0.0186530622,0.0076604621,0.0133803489,0.014453745,-0.015105437,-0.0074033601,-0.0021789988,-0.0128520785,-0.0206857117,-0.000638974,-0.0572520104,0.036557879,0.014523819,-0.0095511159,0.0260909083,-0.0069606856,-0.0043587874,0.0239375316,0.0033550289,-0.0131350927,0.0240069745,-0.0123030733,-0.0415102057,-0.0071834382,-0.0006406708,-0.0090933397,-0.0331024539,-0.0158571285,-0.0116984013,-0.0225899635,0.0046639535,0.0034189519,0.010536832,0.0024671573,-0.0076841589,-0.004949037,0.0087249855,0.0318835548,0.0186633407,0.0392594117,-0.0558661261]},"361":{"Abstract":"PhenoLines is a visual analysis tool for the interpretation of disease subtypes, derived from the application of topic models to clinical data. Topic models enable one to mine cross-sectional patient comorbidity data (e.g., electronic health records) and construct disease subtypes-each with its own temporally evolving prevalence and co-occurrence of phenotypes-without requiring aligned longitudinal phenotype data for all patients. However, the dimensionality of topic models makes interpretation challenging, and de facto analyses provide little intuition regarding phenotype relevance or phenotype interrelationships. PhenoLines enables one to compare phenotype prevalence within and across disease subtype topics, thus supporting subtype characterization, a task that involves identifying a proposed subtype's dominant phenotypes, ages of effect, and clinical validity. We contribute a data transformation workflow that employs the Human Phenotype Ontology to hierarchically organize phenotypes and aggregate the evolving probabilities produced by topic models. We introduce a novel measure of phenotype relevance that can be used to simplify the resulting topology. The design of PhenoLines was motivated by formative interviews with machine learning and clinical experts. We describe the collaborative design process, distill high-level tasks, and report on initial evaluations with machine learning experts and a medical domain expert. These results suggest that PhenoLines demonstrates promising approaches to support the characterization and optimization of topic models.","Authors":"M. Glueck; M. P. Naeini; F. Doshi-Velez; F. Chevalier; A. Khan; D. Wigdor; M. Brudno","DOI":"10.1109\/TVCG.2017.2745118","Keywords":"Developmental disorder;Human Phenotype Ontology (HPO);Phenotypes;Topic models;Topology simplification","Title":"PhenoLines: Phenotype Comparison Visualizations for Disease Subtyping via Topic Models","Keywords_Processed":"Human Phenotype Ontology HPO;developmental disorder;topic model;phenotype;topology simplification","Keyword_Vector":[0.0533933437,0.0035750479,-0.0188607599,-0.015696588,0.0250370778,-0.0345138382,-0.0388185193,-0.0336277199,-0.0376487818,0.0361534415,-0.0019602546,0.0692048336,-0.0116301283,-0.0034459919,0.0295249474,-0.020914039,0.0396780356,-0.0348239425,0.0444888032,-0.0083523677,-0.0238397189,-0.0337544005,-0.0222348754,0.0159610697,0.0298925712,0.0635400696,-0.0020212784,-0.004443501,-0.0356110768,0.0663423751,-0.0433513672,-0.0237976139,-0.0055398515,-0.0300044407,-0.0472031503,-0.0203195051,-0.0224844541,0.0570060986,-0.0349341431,0.0068365172,0.0133765777,0.0286080627,-0.1227297161,-0.039835743,0.0236987828,-0.0050085085,-0.0488523592,0.0736368066,-0.0473140389,-0.0162669931,-0.0489726168,-0.1081678082],"Abstract_Vector":[0.1392955822,0.0649570193,-0.0128594159,0.0688688539,-0.0609234041,-0.0016040246,0.0038952357,0.0485424453,0.0074404895,-0.036762728,-0.0318464862,0.0191319312,-0.0409858053,0.0391080125,-0.0098247248,-0.0096087754,-0.0548770239,0.014843042,0.006681658,-0.0340224155,-0.0305488482,-0.0063246681,0.0275984256,0.0116570478,-0.0154482706,-0.0118396353,0.0220516483,0.051672464,-0.0409528527,-0.0090941904,0.0281394239,-0.0102535116,-0.0151491871,0.0270653863,-0.002900397,-0.0560667868,0.0223162328,0.0257444738,0.0181698173,0.0394598234,0.0469697946,0.0489869953,0.040742133,-0.0008775602,-0.0130354795,0.0019847749,-0.0164946099,0.0485800505,0.009964359,-0.0087604313,-0.0265857653,-0.0275698979,0.017170052,0.0173307845,0.0438806895,0.0337188159,0.0198200118,0.022915013,-0.0439737287,0.0078497902,0.0015037499,-0.0227871659,0.0128967496,-0.0300742332,0.0371973366,-0.0160884033,0.0250032483,-0.0224321508,0.0283722429,0.0339026594,-0.0054227045,-0.0070428137,0.0127591267,0.0338178941,0.0214732722,-0.0028274461,-0.0069166358,0.0113841931,0.0439183615,0.0030802133,-0.0092866296,-0.0230751488,0.0232514849,0.0042502602,0.0324299012,0.0096485132,0.0145964336,0.0176335261,-0.0171792712,0.0178437687,0.0077008441,-0.0347282115,-0.0013479855,-0.0049251364,-0.0278841881,-0.0587845316,-0.0078104529,0.045560044,-0.0065177799,0.0275760699,0.0131360059,0.0129744224,-0.0151456859,0.0106542118,0.0195052266,-0.0631398358,-0.0085606683,0.0301994689,0.0177055212,0.0201149394,-0.0653072481,0.0346400029,-0.0320399154,-0.0083313515,-0.0291536238,0.0146829635]},"362":{"Abstract":"The aim of Diminished Reality (DR) is to remove a target object in a live video stream seamlessly. In our approach, the area of the target object is replaced with new texture that blends with the rest of the image. The result is then propagated to the next frames of the video. One of the important stages of this technique is to update the target region with respect to the illumination change. This is a complex and recurrent problem when the viewpoint changes. We show that the state-of-the-art in DR fails in solving this problem, even under simple scenarios. We then use local illumination models to address this problem. According to these models, the variation in illumination only affects the specular component of the image. In the context of DR, the problem is therefore solved by propagating the specularities in the target area. We list a set of structural properties of specularities which we incorporate in two new models for specularity propagation. Our first model includes the same property as the previous approaches, which is the smoothness of illumination variation, but has a different estimation method based on the Thin-Plate Spline. Our second model incorporates more properties of the specularity's shape on planar surfaces. Experimental results on synthetic and real data show that our strategy substantially improves the rendering quality compared to the state-of-the-art in DR.","Authors":"S. H. Said; M. Tamaazousti; A. Bartoli","DOI":"10.1109\/TVCG.2017.2705687","Keywords":"Diminished reality;specularity;propagation;rendering;isocontours;brightest point;illumination variation","Title":"Image-Based Models for Specularity Propagation in Diminished Reality","Keywords_Processed":"bright point;propagation;diminish reality;specularity;isocontour;illumination variation;render","Keyword_Vector":[0.2398971959,-0.0911909098,0.0581097821,-0.0978524192,0.1488108794,-0.0041548977,0.0597967014,0.0667624661,0.0231838682,-0.0894314543,-0.0232383757,0.009034494,-0.0216367691,-0.0206381038,-0.0069832709,0.0683446796,-0.0011833662,0.0855436345,-0.0543970755,-0.0472054924,-0.0460483961,0.0352280235,-0.0238617247,-0.0552491361,0.1499017511,-0.0159105639,0.0129868148,0.0245574138,-0.0684587661,-0.0739143558,-0.0634468872,-0.0817552484,-0.0783213974,0.0007533734,0.0126342904,-0.0184503605,-0.0297678332,-0.0002434006,0.0509791157,-0.0000512008,-0.0017444441,0.0389578144,-0.0116198279,-0.0217839619,0.0185519941,-0.0256156678,-0.0277321753,-0.0495345388,0.0377948237,-0.0655211757,0.021922255,0.0629256987],"Abstract_Vector":[0.1804415099,-0.0374007105,-0.0078217375,0.1330795534,0.0803911313,0.0801783804,-0.0562460181,-0.0480668855,0.0864129712,-0.0392729828,-0.0520457535,0.0426178809,-0.0346810791,-0.0336063612,0.0761356332,0.1068408404,-0.029568691,0.0200466575,-0.0728903581,-0.0337153548,-0.0379720242,0.0108905533,0.0313175413,0.0934053488,-0.0226982384,-0.0333842616,0.0315759552,0.11359385,-0.0323360896,-0.0111329033,-0.0091238964,0.1125615112,0.0197243712,0.081740906,0.0070288947,0.0506745261,0.0157075718,0.0193231451,-0.0161575716,0.0103455138,-0.0486588927,0.0244886251,-0.0064189186,-0.0249118381,-0.0073739525,0.0158729014,0.024231668,0.0608390926,-0.0640280445,0.0381333492,-0.0856210605,0.1206601934,-0.0099098582,-0.0041838216,-0.0557721173,-0.0555532314,0.0169857249,-0.0014136225,-0.0521161575,0.0826729756,0.0149453177,-0.1176726876,-0.0276362009,0.0439272613,-0.0618681076,0.0321958297,-0.0278406001,0.0169884108,-0.0152439692,-0.0113693297,0.0320259691,0.0563234453,-0.0431214974,0.0009753471,-0.0426239485,0.0007058871,-0.0424880568,0.04968799,0.0615978007,0.0292873684,0.0412636511,0.0140705313,0.017676548,0.027906682,-0.0173650345,-0.0151467213,0.0698937079,-0.0427869133,-0.0345421798,-0.0127403648,-0.0260971491,-0.0437090341,0.0041681867,0.0071924139,-0.0130855453,0.0296236874,-0.0360033182,-0.0174937615,-0.0063432738,-0.0322062113,-0.0156343941,0.0337278735,-0.0295361665,-0.0682811832,-0.008618557,0.0268846312,-0.0135869224,0.0102797976,0.0171398132,0.0563480099,-0.0085696159,-0.0451101487,-0.0156185445,0.0350611277,-0.0418675273,-0.0066111948]},"363":{"Abstract":"Displays that can portray environments that are perceivable from multiple views are known as multiscopic displays. Some multiscopic displays enable realistic perception of 3D environments without the need for cumbersome mounts or fragile head-tracking algorithms. These automultiscopic displays carefully control the distribution of emitted light over space, direction (angle) and time so that even a static image displayed can encode parallax across viewing directions (Iightfield). This allows simultaneous observation by multiple viewers, each perceiving 3D from their own (correct) perspective. Currently, the illusion can only be effectively maintained over a narrow range of viewing angles. In this paper, we propose and analyze a simple solution to widen the range of viewing angles for automultiscopic displays that use parallax barriers. We propose the use of a refractive medium, with a high refractive index, between the display and parallax barriers. The inserted medium warps the exitant lightfield in a way that increases the potential viewing angle. We analyze the consequences of this warp and build a prototype with a 93% increase in the effective viewing angle.","Authors":"G. Lyu; X. Shen; T. Komura; K. Subr; L. Teng","DOI":"10.1109\/TVCG.2018.2794599","Keywords":"Automultiscopic displays;lightfields","Title":"Widening Viewing Angles of Automultiscopic Displays Using Refractive Inserts","Keywords_Processed":"lightfield;automultiscopic display","Keyword_Vector":[0.0589768392,-0.0273114797,-0.0342641334,0.0609669361,-0.0116735681,-0.0450133666,-0.0245311426,-0.062417386,-0.0493485448,-0.0364936664,0.0429782385,0.0085526258,0.0033367982,0.1962242068,-0.0064495287,0.0103400377,0.0397480595,0.0397720441,0.0090532577,-0.012936203,-0.0021364779,-0.0427384155,-0.0561328957,0.0482481952,0.015733301,-0.0232003503,-0.0324292272,0.0043045318,-0.0106376026,-0.0022302928,-0.0236887286,-0.0152450907,0.0050181185,-0.0201991961,-0.0124193733,-0.0360624532,0.0005221656,-0.0213614571,-0.0464522433,-0.0247731495,-0.0148384021,0.0138176655,-0.0060622436,0.0003158685,0.0315878375,0.0763026289,-0.0311970235,0.025075176,0.08444467,-0.028572493,-0.011390715,-0.0062700677],"Abstract_Vector":[0.1564291848,-0.0026180923,-0.0273392531,-0.0066110817,-0.0687580661,0.0355960346,-0.0190412712,0.0087773312,0.0092688337,-0.0304614712,0.0023071543,0.0030861619,-0.0816100399,0.011218805,-0.0410194109,0.0237054316,0.0334594243,0.0435537613,-0.0082633338,-0.1041124485,0.0330682712,-0.0397198268,0.0257626052,-0.0399879052,-0.0176114305,0.0110046584,0.0360386854,-0.0323338307,-0.0031570474,0.0158513924,0.0796098303,0.0104774478,-0.0331287555,-0.0422550109,0.0377534173,-0.0588029552,-0.0191938568,-0.0020056331,-0.0140037397,-0.0033585323,0.0253658719,0.033884437,-0.0304972102,-0.0250542833,0.0291182646,0.0366010419,0.0084278352,0.0028647632,-0.0367974545,0.0051624969,-0.0285583516,-0.0076870719,-0.0115925137,0.0250884502,0.0412809125,-0.0132349523,-0.0726360605,0.0347163518,0.0625981494,-0.0325982598,0.018397831,-0.0162046775,-0.0047166307,0.0176074,0.0098458633,0.0475989402,0.0027675266,0.0179633679,0.0153196825,-0.0017006626,0.0228255289,-0.0149856139,-0.0154371393,-0.0103392581,0.0043199479,0.0072926815,0.0042504628,0.0296779105,0.0116407443,0.0216105072,-0.0174954837,-0.0251649586,-0.0268582138,-0.0258970355,0.0140427738,0.015794333,-0.0184396842,0.0167283023,0.0043600306,0.0010241884,-0.0424760498,0.0556749038,-0.0044157712,-0.0153679836,0.0117786048,-0.0207677107,0.000701387,-0.0409229136,-0.0283221545,0.0128459624,0.0073921083,0.0234979458,-0.0260544934,-0.0158456405,-0.0415093221,-0.0178537788,-0.0144068843,-0.0066425235,-0.0010708278,0.0168733272,0.0074356608,-0.0303425942,-0.0210544909,0.0284387425,-0.0194076889,-0.0087694753]},"364":{"Abstract":"We present a direct manipulation technique that allows material scientists to interactively highlight relevant parameterized simulation instances located in dimensionally reduced spaces, enabling a user-defined understanding of a continuous parameter space. Our goals are two-fold: first, to build a user-directed intuition of dimensionally reduced data, and second, to provide a mechanism for creatively exploring parameter relationships in parameterized simulation sets, called ensembles. We start by visualizing ensemble data instances in dimensionally reduced scatter plots. To understand these abstract views, we employ user-defined virtual data instances that, through direct manipulation, search an ensemble for similar instances. Users can create multiple of these direct manipulation queries to visually annotate the spaces with sets of highlighted ensemble data instances. User-defined goals are therefore translated into custom illustrations that are projected onto the dimensionally reduced spaces. Combined forward and inverse searches of the parameter space follow naturally allowing for continuous parameter space prediction and visual query comparison in the context of an ensemble. The potential for this visualization technique is confirmed via expert user feedback for a shock physics application and synthetic model analysis.","Authors":"D. Orban; D. F. Keefe; A. Biswas; J. Ahrens; D. Rogers","DOI":"10.1109\/TVCG.2018.2865051","Keywords":"Visual Parameter Space Analysis;Ensemble Visualization;Semantic Interaction;Direct Manipulation;Shock Physics","Title":"Drag and Track: A Direct Manipulation Interface for Contextualizing Data Instances within a Continuous Parameter Space","Keywords_Processed":"ensemble visualization;direct manipulation;shock physic;Visual Parameter Space Analysis;semantic interaction","Keyword_Vector":[0.0504108916,0.0185896337,-0.0055902221,0.004364328,0.0309719231,-0.0124829804,-0.0371207003,-0.0153435138,-0.0738577587,-0.0528050665,0.0048946967,0.0582970328,0.0330328842,0.0090033946,0.069091135,0.0083272985,0.0200461287,0.0108475822,0.1166139376,-0.0191711014,0.0305852927,-0.0506384856,0.0927757637,0.0625873858,-0.0214839534,0.0794488349,-0.0267579515,0.0341560979,0.0193332258,-0.0306215623,0.0423720177,-0.0518464574,-0.0107216596,-0.0482290521,-0.0153272957,-0.0462313578,-0.0028590355,-0.0692729533,0.0273481941,0.0536961379,-0.0121943472,0.0358748917,-0.0767856205,0.024977666,-0.0596574609,0.0176972685,-0.0140061154,0.1084935615,0.0518050125,-0.0289451711,0.037993327,0.0376109691],"Abstract_Vector":[0.1720332925,-0.0176193208,0.0021928047,0.0097362553,-0.0505798733,0.0559328315,0.0011984285,0.0211501439,0.0954741377,0.0038206654,-0.0434141831,0.0392505309,-0.0295450307,0.0107808241,0.042730247,-0.0521277321,-0.0232140615,0.0855851713,0.0179381518,0.0873272266,0.004138248,-0.0524579377,-0.0303304143,0.0325325023,-0.0366894751,0.0264071468,0.1053781642,0.0246124142,0.0495184665,0.0306359941,0.0210925804,-0.0260326403,-0.0696237525,0.0101493307,0.0416454811,0.0026361782,-0.016251267,-0.0115779628,0.080809543,0.0491933705,0.0870365092,-0.0844277919,-0.0005944272,0.0021063332,-0.03741197,-0.0307111882,-0.0084630215,0.0250627092,0.0051667441,-0.0005185301,-0.0274985812,0.0601326098,-0.005596605,0.0070692652,0.0160449754,0.0017348731,0.0263038534,-0.0126933434,-0.0182323844,0.0419539999,0.0112601264,-0.0356226578,0.0098099661,0.0247273784,-0.010923177,0.0002856092,0.0465220036,0.033342138,0.0159478681,0.0289995741,0.0119654491,-0.0113531364,-0.0121793235,0.008544045,-0.0231344753,-0.0245898953,-0.0110658682,0.0287725923,-0.0725251142,0.0087554101,-0.0181111518,-0.0161362232,-0.0252980024,-0.0104399532,-0.0230251622,-0.0145612459,-0.0115568707,0.0613922502,-0.0524885155,-0.0104104147,-0.0057483333,0.0000601383,-0.0404309276,-0.0334155888,-0.0341171838,0.0252651612,-0.027031777,-0.0099193941,-0.0075094661,-0.0441260596,0.0180119123,-0.0115274085,-0.0011057778,0.0109214926,-0.0116386707,0.0193115435,-0.0007366746,0.0072915968,0.0039154599,0.0053440836,-0.0051246745,0.0250237681,-0.0123562909,0.0002326762,0.0231567789,0.027064802]},"365":{"Abstract":"We present a novel data-driven approach to populate virtual road networks with realistic traffic flows. Specifically, given a limited set of vehicle trajectories as the input samples, our approach first synthesizes a large set of vehicle trajectories. By taking the spatio-temporal information of traffic flows as a 2D texture, the generation of new traffic flows can be formulated as a texture synthesis process, which is solved by minimizing a newly developed traffic texture energy. The synthesized output captures the spatio-temporal dynamics of the input traffic flows, and the vehicle interactions in it strictly follow traffic rules. After that, we position the synthesized vehicle trajectory data to virtual road networks using a cage-based registration scheme, where a few traffic-specific constraints are enforced to maintain each vehicle's original spatial location and synchronize its motion in concert with its neighboring vehicles. Our approach is intuitive to control and scalable to the complexity of virtual road networks. We validated our approach through many experiments and paired comparison user studies.","Authors":"Q. Chao; Z. Deng; J. Ren; Q. Ye; X. Jin","DOI":"10.1109\/TVCG.2017.2648790","Keywords":"Traffic flow animation;crowd simulation;data-driven method;texture synthesis","Title":"Realistic Data-Driven Traffic Flow Animation Using Texture Synthesis","Keywords_Processed":"crowd simulation;traffic flow animation;datum drive method;texture synthesis","Keyword_Vector":[0.0430741951,0.0044464543,-0.0077892011,-0.0305703595,0.0174490773,-0.0318210338,-0.0538181346,-0.0153576604,-0.0956284344,-0.0390438741,0.0201428147,0.078069145,0.0131014085,0.0151296259,0.0766061619,0.0536259669,0.0818014705,0.0750863608,0.1488803062,0.0028665456,-0.0494933809,0.0678392424,0.0494383763,0.0797093641,-0.0686488079,-0.0124392097,-0.0452909213,0.0348723689,0.0618214442,-0.0531635817,-0.0084002693,-0.0711464886,0.1189819603,0.0125445751,-0.0515688195,0.0025213902,-0.0960663926,-0.051526182,-0.005289829,-0.0520456661,-0.0305538517,0.0647666336,-0.0236216899,0.0569980198,-0.0415842602,-0.0553718782,0.0732692638,0.0616259701,-0.0746011694,0.0571294585,0.0192747553,0.0428619361],"Abstract_Vector":[0.1100896115,-0.021689907,-0.0062554551,-0.003145717,-0.0546421323,0.0779987394,-0.0204229452,0.0089331582,0.0507598554,0.0149353233,-0.0060868112,0.022044204,-0.0228129178,0.0146634649,-0.0119510994,0.0607224187,0.0410390174,-0.0217140729,0.0545719973,-0.0544845787,0.010353766,-0.0367222794,0.008671002,-0.0401898566,-0.0086849974,-0.0052898881,0.0208891553,-0.02305203,-0.0096855946,0.0474493415,0.0743159809,0.0014135147,-0.0178020616,-0.0229423077,0.0586185471,0.0133221553,-0.0009731503,0.0102712022,-0.031204245,0.0182044764,0.0774356565,0.0170008521,0.0386709494,0.0048175325,0.0291068874,0.0019540212,0.0273130484,0.0183515703,0.0054555293,0.0014781111,-0.019168696,-0.0404438863,0.0003991934,0.0008542897,0.0200757583,-0.0050296292,0.0228988574,0.028365342,-0.0208298075,-0.0251970585,0.0236288682,0.0294296241,-0.0178559689,0.0361419357,0.0036571198,-0.0255244888,0.0087044838,-0.0598483153,-0.000371338,0.0925776803,0.0140361022,0.0324461312,-0.0161870358,0.0283488838,0.0345214013,-0.0390205484,0.0113095683,-0.0421092488,0.008652025,-0.0044264896,-0.0153895823,0.0107230506,0.0257397786,0.0114792785,0.0165667305,-0.0093763104,0.0030244161,0.005363255,-0.0299292819,-0.0035204318,-0.0071572481,-0.047418675,-0.0081621036,0.0142499196,0.0168529082,-0.0352823422,-0.0292948787,0.0041694162,-0.0103726782,-0.0144323084,-0.0202337615,0.0015509997,-0.0157050138,-0.011557783,0.0135926858,-0.0039100235,0.0211173543,-0.0205219837,0.0172578212,-0.0131207913,-0.0020584755,0.0035917634,0.0715802576,-0.0229571234,-0.011990001,0.0168392461]},"366":{"Abstract":"We have recently seen many successful applications of recurrent neural networks (RNNs) on electronic medical records (EMRs), which contain histories of patients' diagnoses, medications, and other various events, in order to predict the current and future states of patients. Despite the strong performance of RNNs, it is often challenging for users to understand why the model makes a particular prediction. Such black-box nature of RNNs can impede its wide adoption in clinical practice. Furthermore, we have no established methods to interactively leverage users' domain expertise and prior knowledge as inputs for steering the model. Therefore, our design study aims to provide a visual analytics solution to increase interpretability and interactivity of RNNs via a joint effort of medical experts, artificial intelligence scientists, and visual analytics researchers. Following the iterative design process between the experts, we design, implement, and evaluate a visual analytics tool called RetainVis, which couples a newly improved, interpretable, and interactive RNN-based model called RetainEX and visualizations for users' exploration of EMR data in the context of prediction tasks. Our study shows the effective use of RetainVis for gaining insights into how individual medical codes contribute to making risk predictions, using EMRs of patients with heart failure and cataract symptoms. Our study also demonstrates how we made substantial changes to the state-of-the-art RNN model called RETAIN in order to make use of temporal information and increase interactivity. This study will provide a useful guideline for researchers that aim to design an interpretable and interactive visual analytics tool for RNNs.","Authors":"B. C. Kwon; M. Choi; J. T. Kim; E. Choi; Y. B. Kim; S. Kwon; J. Sun; J. Choo","DOI":"10.1109\/TVCG.2018.2865027","Keywords":"Interactive Artificial Intelligence;XAI (Explainable Artificial Intelligence);Interpretable Deep Learning;Healthcare","Title":"RetainVis: Visual Analytics with Interpretable and Interactive Recurrent Neural Networks on Electronic Medical Records","Keywords_Processed":"healthcare;Interactive Artificial intelligence;XAI Explainable Artificial intelligence;interpretable Deep Learning","Keyword_Vector":[0.0634683496,0.0375862559,-0.0269742167,0.0519163875,0.0553245032,-0.0842492982,-0.0206081154,-0.1226137818,-0.0700183726,0.0177417257,-0.0072690981,0.0612511179,-0.0921851938,0.1253457051,-0.0265342087,-0.0999020459,-0.2007304612,-0.0507794002,-0.0439504936,0.1418290355,-0.0283098921,-0.0015066918,-0.1715661757,0.1778458165,0.0327213989,-0.2542752246,-0.1556880101,-0.0484328354,-0.0814229303,-0.1471399339,0.0664217102,-0.1344232364,-0.1525114017,-0.0125227976,0.0359547592,0.064363278,-0.0001963685,0.0765181002,-0.0244917285,0.0431787933,-0.0395821151,-0.0225646867,-0.1314220752,-0.0854910711,0.0093934884,-0.1034246851,-0.091526073,0.1537618304,-0.0021196523,-0.0201939261,-0.0062138354,-0.1051279538],"Abstract_Vector":[0.1751664628,0.107161558,0.0287668306,0.0908666148,-0.0643468583,-0.0337026105,-0.027849945,0.0235458804,-0.0452427548,-0.0407491475,-0.0288453272,-0.0233281478,-0.045489667,0.1004305979,-0.028450632,0.0425543182,0.0322982062,0.0424660103,-0.0058899688,-0.018567473,-0.0456753128,0.0218857845,-0.0185792376,-0.0532385283,-0.009788879,0.0300017081,0.0128966759,0.0129398196,0.0488658648,-0.0457556587,-0.0051652764,-0.0044549287,-0.0126533283,0.0715035462,-0.0153168438,-0.0314687936,0.0154208212,-0.0037395133,0.0217025823,0.0574301612,-0.018272836,-0.0229501126,-0.0108271622,-0.0052022927,-0.0386804471,-0.0238104621,0.019135239,0.0076336416,0.0097477235,-0.0085587099,-0.0115434998,-0.0133035776,-0.0139485466,0.0051448488,-0.0072832567,-0.0074571096,0.0075300317,-0.0187494323,-0.0285326008,-0.0070830661,0.0034964131,0.0036483118,-0.0010614556,-0.0046942431,0.0214126079,-0.0084445488,-0.0102226101,0.0017950814,-0.0511743565,-0.0235065498,-0.027941107,0.0055372792,-0.0373836441,0.000010678,0.0180585418,0.0041172915,-0.0249906812,0.0082756384,0.0180807291,-0.0143070821,-0.0101096643,0.0360115076,0.0241833089,0.0442924361,-0.0038641097,0.0392831078,-0.0056877097,0.0270506402,0.0129186257,0.001972027,-0.0042538885,0.0078166129,-0.0209651187,-0.0072387327,-0.0166069551,0.0169893711,-0.004100224,-0.0183304671,0.0092613289,0.0178945513,0.007659898,-0.0079669831,-0.0055870213,-0.0171341386,0.0183045491,-0.0247641542,0.0140443982,0.0096560198,0.0009881324,0.0053459021,-0.0148885283,0.020152366,0.0179722167,0.0156502562,0.0030795629,0.0216384906]},"367":{"Abstract":"State-of-the-art lighting design is based on physically accurate lighting simulations of scenes such as offices. The simulation results support lighting designers in the creation of lighting configurations, which must meet contradicting customer objectives regarding quality and price while conforming to industry standards. However, current tools for lighting design impede rapid feedback cycles. On the one side, they decouple analysis and simulation specification. On the other side, they lack capabilities for a detailed comparison of multiple configurations. The primary contribution of this paper is a design study of LiteVis, a system for efficient decision support in lighting design. LiteVis tightly integrates global illumination-based lighting simulation, a spatial representation of the scene, and non-spatial visualizations of parameters and result indicators. This enables an efficient iterative cycle of simulation parametrization and analysis. Specifically, a novel visualization supports decision making by ranking simulated lighting configurations with regard to a weight-based prioritization of objectives that considers both spatial and non-spatial characteristics. In the spatial domain, novel concepts support a detailed comparison of illumination scenarios. We demonstrate LiteVis using a real-world use case and report qualitative feedback of lighting designers. This feedback indicates that LiteVis successfully supports lighting designers to achieve key tasks more efficiently and with greater certainty.","Authors":"J. Sorger; T. Ortner; C. Luksch; M. Schw\u00e4rzler; E. Gr\u00f6ller; H. Piringer","DOI":"10.1109\/TVCG.2015.2468011","Keywords":"Integrating Spatial and Non-Spatial Data Visualization;Visualization in Physical Sciences and Engineering;Coordinated and Multiple Views;Visual Knowledge Discovery;Integrating Spatial and Non-Spatial Data Visualization;Visualization in Physical Sciences and Engineering;Coordinated and Multiple Views;Visual Knowledge Discovery","Title":"LiteVis: Integrated Visualization for Simulation-Based Decision Support in Lighting Design","Keywords_Processed":"visual Knowledge Discovery;Integrating Spatial and Non Spatial Data visualization;Coordinated and Multiple Views;visualization in Physical Sciences and Engineering","Keyword_Vector":[0.1762881306,-0.0964167195,-0.0178757729,-0.1549761848,0.0026090243,-0.1061271249,-0.1507736027,-0.057466969,-0.1574302332,0.0935688124,-0.1526541646,0.1522923961,-0.0397896362,0.0888577441,-0.2429470101,0.0760902148,-0.2623714206,0.0790038634,-0.1478702226,-0.0667319389,0.1787742041,-0.0195059889,0.1039978257,-0.1512379453,-0.1569282214,0.0738636205,0.0322079821,0.1493964677,-0.0177408902,-0.0933660737,-0.0081067001,0.1260495764,0.0806308849,0.1153363871,-0.0551234793,-0.0168420798,0.042975154,-0.0485508613,-0.0422770155,-0.078201495,0.0406741356,0.0919847936,-0.0182833374,0.1224365178,0.0187783809,-0.0941406646,-0.0569189651,0.001519921,0.0948597625,0.0161376245,-0.0048981799,-0.0539269158],"Abstract_Vector":[0.1585545729,-0.0989702184,0.0006637793,0.0144803554,-0.0270210942,0.0246256545,0.0008271541,0.0167261568,0.0351343792,-0.0183403956,-0.061117818,0.0426496705,0.0421622207,-0.0099217922,0.0681708894,0.004536961,-0.0241582514,-0.0002799827,-0.0028501562,-0.0142362177,-0.007216625,-0.0407287558,-0.0098884241,-0.0101051387,-0.0177466542,-0.0118059188,0.0018705849,-0.0017906104,-0.0650229504,-0.0386083243,0.0080815049,-0.0097335111,0.0268673953,-0.0265431158,-0.0089722319,0.0076675212,0.0309648843,0.0108499506,-0.0336202036,0.0041759194,0.0006341924,0.0076227157,0.0493714226,-0.0373097827,0.002931281,-0.0327449993,-0.0104462763,-0.0311721292,-0.0231243998,0.0304427393,0.0097566963,0.0141859957,0.0411726016,0.0222713838,0.0111475526,0.0034166837,-0.0127340427,-0.0173255391,0.0071757657,-0.0032582624,0.0195840484,-0.0387257107,-0.0036936515,0.002324371,-0.0179526354,-0.0187542662,0.0291489189,-0.0046964823,-0.0218346089,-0.009457663,0.0112586642,0.0029770756,0.0083120922,0.0080227571,-0.0232540068,0.0196876382,-0.0040115543,-0.0113037443,0.0464120507,-0.0143696658,0.0311772156,0.0133653491,-0.0025934689,-0.0018469877,0.0000587681,0.0172841737,0.0416903369,-0.0005174112,-0.0071165769,0.0182665149,-0.0211638091,0.0093104313,-0.0146880729,-0.00890046,-0.0033507863,0.0064499478,0.0192801932,-0.0043248603,0.0182903841,0.0236025143,0.0181604264,-0.0311952583,0.0401310966,0.0240398317,0.010763389,-0.0305868328,0.0086557679,0.0112853259,-0.007084739,-0.0013291557,-0.0095338985,-0.0131699523,-0.0255988458,0.006263381,0.0273487011,-0.0145671307]},"368":{"Abstract":"The increasing interest for reliable generation of large scale scenes and objects has facilitated several real-time applications. Although the resolution of the new generation geometry scanners are constantly improving, the output models, are inevitably noisy, requiring sophisticated approaches that remove noise while preserving sharp features. Moreover, we no longer deal exclusively with individual shapes, but with entire scenes resulting in a sequence of 3D surfaces that are affected by noise with different characteristics due to variable environmental factors (e.g., lighting conditions, orientation of the scanning device). In this work, we introduce a novel coarse-to-fine graph spectral processing approach that exploits the fact that the sharp features reside in a low dimensional structure hidden in the noisy 3D dataset. In the coarse step, the mesh is processed in parts, using a model based Bayesian learning method that identifies the noise level in each part and the subspace where the features lie. In the feature-aware fine step, we iteratively smooth face normals and vertices, while preserving geometric features. Extensive evaluation studies carried out under a broad set of complex noise patterns verify the superiority of our approach as compared to the state-of-the-art schemes, in terms of reconstruction quality and computational complexity.","Authors":"G. Arvanitis; A. S. Lalos; K. Moustakas; N. Fakotakis","DOI":"10.1109\/TVCG.2018.2802926","Keywords":"Spectral smoothing;orthogonal iteration;spectral denoising filtering;feature extraction;level noise estimation","Title":"Feature Preserving Mesh Denoising Based on Graph Spectral Processing","Keywords_Processed":"spectral denoising filtering;level noise estimation;feature extraction;spectral smoothing;orthogonal iteration","Keyword_Vector":[0.2040078932,0.0706407351,-0.0565523264,-0.0318722532,-0.0351575129,0.0516841809,0.0947787892,0.0089988208,-0.0005300766,-0.0364659442,0.0233338052,-0.0426105609,0.0170980228,-0.0559026143,-0.0495088004,-0.0677026276,-0.0153907258,0.0017335224,0.0526815571,0.0290161717,-0.0592562136,0.0049760429,-0.0235912865,0.1232073062,0.0231366832,0.0509282923,-0.0116181677,0.045657479,0.1343616732,0.0090340071,-0.0530871382,0.0883296707,-0.1392428686,0.1129048303,-0.0122314309,0.0407507333,0.098986503,-0.0409759377,0.1530948358,0.0262000531,0.0585033988,0.0110128045,-0.0635959015,0.1037074458,-0.0554702506,-0.0718950174,0.077940248,-0.0738881477,0.0664394644,-0.0044261929,-0.0138445755,-0.0320311683],"Abstract_Vector":[0.2301436402,0.026407429,-0.0746211178,0.0228622987,0.0949859708,0.0115552584,-0.0619310511,-0.0558664468,-0.0533765589,0.0141310933,-0.006287508,-0.0070409231,-0.0538510978,-0.0339010462,-0.1032291202,0.0790262711,0.064101479,-0.0297954585,-0.0063768375,0.0329691918,-0.0406876258,-0.0377073959,-0.0188069314,0.0425074039,-0.0090083608,0.0600897764,-0.0368068379,-0.054201898,0.0764834071,0.0151781687,0.0417365274,0.0228760785,-0.0005563998,-0.0744764273,0.0397089315,-0.0431826236,0.0493988982,-0.0293391905,-0.0369487357,0.0322782741,-0.0314946236,-0.0184127764,0.0350224068,-0.1011208231,0.0020507866,0.0573416301,-0.0246158499,0.0429614669,0.0673808257,-0.0298000551,-0.0074502132,0.0838260285,0.022392775,0.0675163951,-0.0283308165,0.0516188076,0.0144129929,-0.0241560671,0.0169475748,-0.0321480104,0.024363822,-0.033944126,0.024163857,-0.0468615963,0.0593237873,-0.008868801,-0.0292010811,0.0150880078,0.0266617029,0.0509438499,0.0502904296,0.0060173266,-0.0266369318,-0.0323942427,0.0115347609,0.0134378352,0.0265587438,0.0225626317,0.0050292727,-0.0141233446,-0.0151699193,-0.0034769883,-0.0088713467,-0.0047139502,0.0071620476,0.0321240125,0.0217501191,0.0114581936,0.0240911961,0.0105756027,0.0046898744,0.00568849,0.0074853971,0.0236847838,0.0019207927,-0.0049164208,-0.0079225358,-0.0257936379,-0.0039180195,0.0107975852,0.0083236851,-0.0064174232,0.007677027,-0.0322584053,-0.002449435,0.0097158533,-0.0240501053,-0.0219692341,-0.041110805,-0.0575355162,-0.0395807836,-0.0056197519,-0.0293273677,-0.0243697012,0.0263608741,0.0144704798]},"369":{"Abstract":"Eye-tracking has become an invaluable tool for the analysis of working practices in many technological fields of activity. Typically studies focus on short tasks and use static expected areas of interest (AoI) in the display to explore subjects' behaviour, making the analyst's task quite straightforward. In long-duration studies, where the observations may last several hours over a complete work session, the AoIs may change over time in response to altering workload, emergencies or other variables making the analysis more difficult. This work puts forward a novel method to automatically identify spatial AoIs changing over time through a combination of clustering and cluster merging in the temporal domain. A visual analysis system based on the proposed methods is also presented. Finally, we illustrate our approach within the domain of air traffic control, a complex task sensitive to prevailing conditions over long durations, though it is applicable to other domains such as monitoring of complex systems.","Authors":"P. K. Muthumanickam; K. Vrotsou; A. Nordman; J. Johansson; M. Cooper","DOI":"10.1109\/TVCG.2018.2865042","Keywords":"Eye-tracking data;areas of interest;clustering;minimum spanning tree;temporal data;spatio-temporal data","Title":"Identification of Temporally Varying Areas of Interest in Long-Duration Eye-Tracking Data Sets","Keywords_Processed":"spatio temporal datum;temporal datum;clustering;area of interest;minimum spanning tree;eye tracking datum","Keyword_Vector":[0.0295682517,0.009747372,0.0184787972,0.0175681284,0.0218213503,-0.0156940652,-0.0132787159,-0.0601384513,-0.0211802625,0.0031224343,0.0229043012,0.0341740053,-0.0552423796,0.0349161229,0.0784346195,-0.0661133655,-0.0064037758,-0.0246312885,-0.0206935914,0.0062962351,-0.0312252031,0.0073419641,0.0096992192,-0.0211997629,0.0069910819,0.0149546414,0.0069290349,-0.0068009753,0.0021926464,-0.0094304527,-0.0068309665,0.011604666,-0.020417708,-0.0030050983,-0.002491144,-0.0080622883,0.0192908865,-0.0018129089,-0.0159398761,0.020174659,0.0036340904,0.0181968971,-0.0089687683,-0.0140105139,-0.0004568553,0.0081797222,0.0072037362,-0.0276121816,0.0011652385,0.0235223695,-0.0260159759,0.0117711228],"Abstract_Vector":[0.1853575395,0.1320957829,0.0055911366,0.1321425484,-0.0561866978,-0.0353269733,0.002141568,0.0558076169,-0.0163804783,-0.0872069494,-0.0092439948,0.0018902697,-0.0343942167,0.0650240386,-0.0211492707,0.00778625,-0.0828759886,-0.039015634,0.0385068465,-0.0018708577,-0.0371598428,0.0165124434,0.0056866558,0.0324931499,0.0323812627,-0.0182603834,0.0228691089,0.0250274657,0.0286299042,-0.0337246808,0.0243488731,-0.014665862,-0.0878262366,-0.0005133906,-0.0210650709,-0.0296810987,-0.0213376845,0.0152679072,0.0009713203,0.0467409788,0.0011297266,0.0127052196,-0.043066508,-0.0103822896,-0.0236683715,0.0077308882,0.0291046756,0.0335861677,-0.0147958702,-0.036847767,-0.0292953189,0.0099397645,0.0178689829,-0.0090890074,-0.0243874674,-0.0245538094,-0.0143803752,-0.0612730973,0.0247891559,0.0333102177,-0.0126706457,-0.0088682514,0.0232769504,-0.0361451988,-0.0084280281,-0.0146105578,-0.0155719727,-0.0411938534,-0.0475726755,-0.02515314,0.0167111401,0.0537114418,-0.0590044417,-0.0261018281,0.0167814083,-0.0212184336,-0.0146346491,-0.0367018991,-0.048637398,0.027110994,0.0089166956,0.0504300553,0.0046146746,0.0177332505,-0.0035540006,-0.0116000253,0.00300564,-0.0226981904,-0.0562490522,-0.0155782895,0.0096313066,0.0346733936,0.0086485946,0.0084543127,-0.0287092081,-0.0004211324,0.0132233061,0.0393070327,0.0005104386,-0.0168149397,-0.0077065472,0.0041581229,0.0030832891,0.0214845165,-0.0157344826,-0.0077242483,-0.0414675284,-0.0366517742,0.0073325542,-0.0340790796,-0.0307199199,0.0139960944,0.0107437868,0.0032783745,0.0319305273,-0.0045944016]},"37":{"Abstract":"Multi-focal plane and multi-layered light-field displays are promising solutions for addressing all visual cues observed in the real world. Unfortunately, these devices usually require expensive optimizations to compute a suitable decomposition of the input light field or focal stack to drive individual display layers. Although these methods provide near-correct image reconstruction, a significant computational cost prevents real-time applications. A simple alternative is a linear blending strategy which decomposes a single 2D image using depth information. This method provides real-time performance, but it generates inaccurate results at occlusion boundaries and on glossy surfaces. This paper proposes a perception-based hybrid decomposition technique which combines the advantages of the above strategies and achieves both real-time performance and high-fidelity results. The fundamental idea is to apply expensive optimizations only in regions where it is perceptually superior, e.g., depth discontinuities at the fovea, and fall back to less costly linear blending otherwise. We present a complete, perception-informed analysis and model that locally determine which of the two strategies should be applied. The prediction is later utilized by our new synthesis method which performs the image decomposition. The results are analyzed and validated in user experiments on a custom multi-plane display.","Authors":"H. Yu; M. Bemana; M. Wernikowski; M. Chwesiuk; O. T. Tursun; G. Singh; K. Myszkowski; R. Mantiuk; H. Seidel; P. Didyk","DOI":"10.1109\/TVCG.2019.2898821","Keywords":"3D displays;Rendering;Accommodation;Perception","Title":"A Perception-driven Hybrid Decomposition for Multi-layer Accommodative Displays","Keywords_Processed":"perception;accommodation;render;3d display","Keyword_Vector":[0.0202638301,0.0141356207,-0.0156602295,-0.0029459201,0.0120484507,-0.0124589081,0.0044869966,0.0051846712,-0.0117098134,-0.0037305859,0.0063881561,0.0239312109,0.0031666712,0.0217421113,0.013022309,0.0130513484,-0.011610589,-0.0584336382,0.0366026826,-0.0129501055,0.0093575981,-0.0257471094,-0.0286957687,0.0181716257,-0.0082963281,-0.029780766,0.0883355288,0.0206747585,-0.0319104091,0.0140156297,-0.0018137696,0.0240004889,-0.0117222141,-0.0540314278,0.0208191342,0.0433454634,-0.0160198179,-0.0336293523,-0.0147897932,-0.0166385366,0.0226296833,-0.014401253,0.0118440372,0.0160042392,0.0030581549,-0.0445533695,0.0223298206,-0.0100982443,0.0268715966,0.0013709655,-0.0323762864,0.0349039792],"Abstract_Vector":[0.183169059,0.0266762645,-0.0276760566,-0.0484388919,-0.0461791258,0.1132841803,-0.0315604963,0.0032475371,0.041435727,-0.0078300719,-0.0927475611,-0.0185547874,-0.0921754259,0.018493224,-0.0100801964,0.0046950514,0.0653649229,0.0711201662,0.0444246681,-0.0323330503,-0.050018483,0.0328072473,-0.0298045334,-0.0297991576,-0.0574733672,-0.0395126392,-0.0887363439,0.083134533,-0.0393929774,-0.0461386655,0.0162944983,-0.0046291419,0.0475515829,-0.0206022279,-0.03556562,-0.0410238527,0.0719905124,-0.0042644362,-0.008656766,-0.0453118274,0.0477104102,-0.0597529669,0.0490254911,-0.0085049317,-0.0214094155,0.0659255187,0.0342997224,0.0190963398,0.0612482394,-0.0390837003,0.0579434158,0.0017069919,-0.0038482155,-0.0172763,-0.0179903163,0.0264063733,0.0269420372,-0.0352896371,0.0254164667,-0.0479707893,0.0188866187,-0.0112682526,-0.0144767375,0.005069021,0.0069143844,-0.0246622015,-0.0218485836,0.0042630217,-0.0278673426,-0.0231072894,0.0203197702,0.0094055163,-0.0072396115,0.0268271392,-0.0418969119,-0.0266255391,-0.0212728944,0.0308023471,-0.0224468026,-0.032452481,0.0398930564,0.0356570611,0.0239918118,-0.034803051,-0.0208497698,-0.0157780397,0.0090683799,-0.0323103136,-0.0419710843,-0.0273011867,0.0384610967,-0.0234429462,-0.0059630386,0.0208806674,-0.0724581181,0.0703373575,-0.0187093063,-0.0212282737,-0.018449661,0.0319396346,0.0032169337,0.0157397169,0.0466110643,-0.0572943975,0.0171211723,-0.0285753723,-0.0039302972,-0.0061091454,-0.0287671148,0.033229842,-0.0098568284,0.0129270618,0.0305933945,-0.0288929894,-0.0223658214,0.0226897116]},"370":{"Abstract":"Surface flow phenomena, such as rain water flowing down a tree trunk and progressive water front in a shower room, are common in real life. However, compared with the 3D spatial fluid flow, these surface flow problems have been much less studied in the graphics community. To tackle this research gap, we present an efficient, robust and high-fidelity simulation approach based on the shallow-water equations. Specifically, the standard shallow-water flow model is extended to general triangle meshes with a feature-based bottom friction model, and a series of coherent mathematical formulations are derived to represent the full range of physical effects that are important for real-world surface flow phenomena. In addition, by achieving compatibility with existing 3D fluid simulators and by supporting physically realistic interactions with multiple fluids and solid surfaces, the new model is flexible and readily extensible for coupled phenomena. A wide range of simulation examples are presented to demonstrate the performance of the new approach.","Authors":"B. Ren; T. Yuan; C. Li; K. Xu; S. Hu","DOI":"10.1109\/TVCG.2017.2720672","Keywords":"Shallow-water equation;flow on curved surfaces;finite volume method;coupled simulation","Title":"Real-Time High-Fidelity Surface Flow Simulation","Keywords_Processed":"shallow water equation;finite volume method;flow on curve surface;couple simulation","Keyword_Vector":[0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0],"Abstract_Vector":[0.1638238135,0.05533601,-0.0371240331,-0.1011835565,0.0096280204,0.0800579516,-0.1059113517,-0.0235856065,-0.0175484482,-0.0766870685,-0.0591198603,0.0369310084,-0.1258661477,0.0616036159,-0.0366097323,-0.0993243365,-0.0407248666,0.0904460463,-0.1465904449,-0.0153150711,-0.0153694406,0.048453249,-0.0713582846,-0.05251901,0.0482863536,-0.0399093563,-0.0710394412,-0.0401645858,-0.0192027097,0.0252930497,0.0025658355,0.0350010487,0.0190694028,0.0207765364,-0.0146426343,-0.0031668104,-0.0132771081,-0.0497445067,0.0293484676,-0.0514450776,0.0036062722,-0.0771953519,-0.0175680258,0.0077349488,-0.0044030193,0.0383166437,-0.0007126007,0.0589854834,-0.0422726835,-0.0058643647,0.0208967305,-0.0371626001,0.0163669552,0.0295378717,0.0190811075,0.0150741358,0.0449207589,-0.0215283062,0.0053263276,-0.0173631015,0.017448281,0.0408002469,-0.0139450385,-0.0031226467,0.0266516379,0.0171528693,-0.012448191,-0.0541464161,-0.0317518271,-0.0029866679,0.0119773378,-0.0547321685,-0.0158632572,0.0303191279,0.0447160939,0.0008396204,-0.061284464,0.0094774585,0.0135121443,-0.0196249148,-0.0095687303,0.0136195327,0.0772499819,-0.0005654877,0.0024108636,-0.0072575549,0.0018763637,0.0073125588,0.0197469406,0.0309302655,0.0127231856,-0.068063893,-0.0066552938,0.0104447718,0.0058194953,-0.0132868633,-0.0254551784,0.0056130118,-0.0326429516,-0.0532762051,-0.0047772755,-0.0040447181,0.0091005341,0.0145285083,-0.0020033485,0.0064867835,-0.0082512342,-0.0044006383,0.0261033694,0.0152066845,-0.0135445058,0.0445252218,-0.05042578,-0.0749843828,-0.0252879931,0.0465770311]},"371":{"Abstract":"Scatterplots are effective visualization techniques for multidimensional data that use two (or three) axes to visualize data items as a point at its corresponding x and y Cartesian coordinates. Typically, each axis is bound to a single data attribute. Interactive exploration occurs by changing the data attributes bound to each of these axes. In the case of using scatterplots to visualize the outputs of dimension reduction techniques, the x and y axes are combinations of the true, high-dimensional data. For these spatializations, the axes present usability challenges in terms of interpretability and interactivity. That is, understanding the axes and interacting with them to make adjustments can be challenging. In this paper, we present InterAxis, a visual analytics technique to properly interpret, define, and change an axis in a user-driven manner. Users are given the ability to define and modify axes by dragging data items to either side of the x or y axes, from which the system computes a linear combination of data attributes and binds it to the axis. Further, users can directly tune the positive and negative contribution to these complex axes by using the visualization of data attributes that correspond to each axis. We describe the details of our technique and demonstrate the intended usage through two scenarios.","Authors":"H. Kim; J. Choo; H. Park; A. Endert","DOI":"10.1109\/TVCG.2015.2467615","Keywords":"Scatterplots;user interaction;model steering;Scatterplots;user interaction;model steering","Title":"InterAxis: Steering Scatterplot Axes via Observation-Level Interaction","Keywords_Processed":"user interaction;model steering;scatterplot","Keyword_Vector":[0.1376404276,-0.1474168275,-0.1316814312,0.0478839384,-0.0733766873,-0.1365392027,-0.0203688589,0.042874841,-0.0153195824,0.0708922841,-0.0714292691,0.1021965478,-0.0551024974,0.0222302715,-0.1066035376,-0.0415256378,-0.1230323135,0.0016325354,-0.0092481026,0.188668643,0.2214030054,0.1612650515,0.0892573089,-0.037743871,-0.0361682064,0.1192455533,0.0442287913,0.1366429036,-0.0782408249,-0.0640286169,-0.15965516,-0.0752777986,-0.0179934054,0.0063147485,-0.0211858182,-0.1213437079,0.1281046996,-0.0586717123,-0.0741353188,-0.1136644946,0.0281713704,0.0003159174,-0.0042149891,-0.0338367825,-0.02189941,0.0006234172,0.0617563275,-0.0307418388,0.0707578471,-0.0683771994,0.0180827366,-0.0619525647],"Abstract_Vector":[0.1969968357,-0.1103044318,-0.0152865673,-0.0248548721,-0.0575084295,-0.0041590248,-0.0077643666,0.0109977051,-0.0519664686,-0.0127738224,-0.0012106228,-0.0326712498,-0.0075331299,-0.0063554926,-0.0191189327,0.0230917191,-0.0041974022,0.0214267886,-0.0088067965,0.0025424471,0.0387029553,-0.0386143762,0.0109598838,-0.0033256665,-0.0236528784,0.0042382778,0.0027422894,-0.0047798143,-0.0377587384,-0.0050136058,-0.0018282148,0.0069733375,-0.0618578312,-0.0275019006,-0.0490937678,0.0210516601,-0.0075085752,0.0088305098,-0.00120456,-0.0337383785,0.022096615,0.0007895618,0.0202891583,-0.0392308229,0.0536041211,-0.0152617213,-0.0261660611,0.0203235233,0.0102557217,0.0375273059,0.0357823445,0.0248465403,0.023721995,0.0103553275,0.0399297323,-0.0279393756,0.0089043627,-0.021780116,-0.0256697728,-0.0002345061,0.0377056175,-0.0258489966,-0.0511711817,-0.0540154668,-0.0086068237,-0.000439872,-0.0239180762,-0.0007520048,-0.0053947945,0.0134279836,0.0251977548,0.016824195,0.0485595978,0.0412889963,-0.0073703893,0.0151237315,-0.0348827482,0.0300703331,-0.0116994362,0.0313661973,0.0323677323,0.0017909986,-0.0302177826,0.0029465112,0.0304970406,0.0544594991,-0.0089332532,-0.0342071774,-0.0103279213,-0.0164739782,-0.015348168,0.0325394296,-0.0413194485,-0.0019161413,0.0267100454,0.0273589381,0.0097061108,-0.0358528054,0.0036312609,-0.0413457394,-0.0057431355,0.0225529533,-0.0262486504,-0.0130022179,0.0451091959,0.0529949749,-0.0009158951,0.0179059661,0.01274606,-0.0234612904,-0.0443323827,0.0230046736,0.0033421515,-0.0218699077,-0.001892722,0.0200716341]},"372":{"Abstract":"We present a new visualization approach for displaying eye tracking data from multiple participants. We aim to show the spatio-temporal data of the gaze points in the context of the underlying image or video stimulus without occlusion. Our technique, denoted as gaze stripes, does not require the explicit definition of areas of interest but directly uses the image data around the gaze points, similar to thumbnails for images. A gaze stripe consists of a sequence of such gaze point images, oriented along a horizontal timeline. By displaying multiple aligned gaze stripes, it is possible to analyze and compare the viewing behavior of the participants over time. Since the analysis is carried out directly on the image data, expensive post-processing or manual annotation are not required. Therefore, not only patterns and outliers in the participants' scanpaths can be detected, but the context of the stimulus is available as well. Furthermore, our approach is especially well suited for dynamic stimuli due to the non-aggregated temporal mapping. Complementary views, i.e., markers, notes, screenshots, histograms, and results from automatic clustering, can be added to the visualization to display analysis results. We illustrate the usefulness of our technique on static and dynamic stimuli. Furthermore, we discuss the limitations and scalability of our approach in comparison to established visualization techniques.","Authors":"K. Kurzhals; M. Hlawatsch; F. Heimerl; M. Burch; T. Ertl; D. Weiskopf","DOI":"10.1109\/TVCG.2015.2468091","Keywords":"Eye tracking;time-dependent data;spatio-temporal visualization;Eye tracking;time-dependent data;spatio-temporal visualization","Title":"Gaze Stripes: Image-Based Visualization of Eye Tracking Data","Keywords_Processed":"eye tracking;spatio temporal visualization;time dependent datum","Keyword_Vector":[0.1502577581,0.1752744001,-0.1295177375,-0.002700642,-0.0397074712,0.0412289887,0.0385842195,-0.0073667868,0.0086015263,0.0169600176,-0.0015502502,-0.0209700565,0.034413525,-0.0589502069,-0.0400856181,-0.0537738448,-0.0094573775,0.012604857,0.0106578069,0.0317980917,-0.0164204469,0.0476332211,0.0262620016,0.0289797288,0.0208803706,-0.0006538105,-0.0222353785,-0.0825359465,-0.0219436894,0.0400411403,0.0310147058,0.0126397517,-0.0201049692,0.0005156566,0.0386280924,0.0002676985,0.0520314251,-0.0590050649,0.0711095859,-0.034123499,0.0675511785,0.0412285055,-0.0775707134,0.1112983656,0.056109709,0.052316408,0.0594394893,0.0385322575,-0.0422216684,-0.0826546843,0.0701041524,-0.1061693078],"Abstract_Vector":[0.2209985576,-0.0043016143,-0.0856185125,0.0218401195,0.0870508137,-0.0746269525,0.0144069289,-0.013502091,0.0484770248,0.0751277039,0.0444934702,0.0135604747,-0.0027297068,-0.0127037708,-0.0427809825,0.0266062381,0.0225492556,-0.0453096544,0.028144465,0.023894347,-0.0175389435,0.0429115155,-0.0208926242,0.0003565084,0.0804897597,-0.0182437024,0.0261389161,-0.016576338,-0.0324071606,0.0939597131,-0.0083181032,0.0290897861,-0.0340296357,0.1148601046,-0.1202531251,-0.0094214149,0.0089421768,0.0680018252,0.0898224575,0.019994167,-0.0249637904,0.0566279217,0.035783082,-0.0269910717,-0.0008502725,0.0636326039,-0.0008772248,0.0556263433,0.0651689312,0.0591602702,-0.0333229172,0.0391452013,0.053649615,-0.0250320145,-0.0309264859,0.0394602989,0.0105339223,-0.0487119744,0.0305098453,-0.053150632,-0.018932065,-0.0814304675,-0.0489932391,-0.0244748653,-0.0279459195,0.0115719829,-0.0081703698,-0.0046337246,0.0046015827,0.0319426657,-0.0618340782,-0.0530084504,-0.0375693142,0.0076165525,-0.0046531484,0.0498064234,0.0478285547,0.0171567724,-0.0393047575,-0.0483892436,0.0143271176,-0.0290506126,0.0601084389,-0.0414608298,-0.0016494135,0.0692781324,0.022658761,0.0633157102,0.0480763117,0.043828466,-0.0312036183,-0.0161510448,-0.0363264376,0.0009890382,-0.0286261052,-0.0141261177,0.039388375,0.0153404511,0.020536574,-0.014204491,0.0284824518,-0.0026258922,0.0632724916,-0.0246155271,0.04677436,0.0329939692,0.0042330739,-0.0253315989,-0.0222185409,-0.0007556029,-0.0221599704,-0.005392343,0.0027260261,0.0690319597,0.011736388,0.0208566538]},"373":{"Abstract":"We introduce a new method to efficiently track complex interfaces among multi-phase immiscible fluids. Unlike existing techniques, we use a mesh-based representation for global liquid surfaces while selectively modeling some local surficial regions with regional level sets (RLS) to handle complex geometries that are difficult to resolve with explicit topology operations. Such a semi-explicit surface mechanism can preserve volume, fine features and foam-like thin films under a relatively low computational expenditure. Our method processes the surface evolution by sampling the fluid domain onto a spectrally refined grid (SRG) and performs efficient grid scanning, generalized interpolations and topology operations on the basis of this grid structure. For the RLS surface part, we propose an accurate advection scheme targeted at SRG. For the explicit mesh part, we develop a fast grid-scanning technique to voxelize the meshes and introduce novel strategies to detect grid cells that contain inconsistent mesh components. A robust algorithm is proposed to construct consistent local meshes to resolve mesh penetrations, and handle the coupling between explicit mesh and RLS surficial regions. We also provide further improvement on handling complicated topological variations, and strategies for remeshing mesh\/RLS interconversions.","Authors":"M. Yang; J. Ye; F. Ding; Y. Zhang; D. Yan","DOI":"10.1109\/TVCG.2018.2864283","Keywords":"Surface tracking;explicit mesh;remeshing;regional level set;multi-material;spectrally refined grid","Title":"A Semi-Explicit Surface Tracking Mechanism for Multi-Phase Immiscible Liquids","Keywords_Processed":"explicit mesh;remeshe;surface tracking;regional level set;spectrally refine grid;multi material","Keyword_Vector":[0.0302712456,-0.0165424556,-0.009193166,-0.0144881413,0.0145747336,-0.0072229791,-0.0474317773,-0.0060475239,-0.0443028255,0.0554687557,-0.0378342902,0.0179418767,0.0177720029,-0.026200322,0.0246771341,-0.0314028551,0.0431174857,0.007661784,0.0137247433,-0.0368818642,-0.0021908165,0.0444434802,0.0300692845,0.0214268452,0.018335342,-0.0248732406,0.0118592334,-0.0479563568,0.0034730809,0.0226901818,0.009497634,0.006875394,0.0028190147,0.0098519222,0.0309345792,0.0503460691,0.0089875933,0.0299707053,0.0383048075,-0.0002462586,-0.0089504821,-0.0007050859,-0.0068011266,0.0120182519,-0.0105897353,0.0350418253,-0.0042820084,-0.0148442676,0.0113092887,-0.0356565226,0.003603369,0.0107875829],"Abstract_Vector":[0.1374997457,-0.0845827791,0.0067725469,0.0109635461,-0.0599238678,0.0229555039,0.0106773608,0.0116371643,-0.0625359262,0.0047944239,-0.0389483259,0.0133604874,-0.0390281648,-0.0309613184,-0.0165639023,0.0106587133,-0.0369121188,-0.0441034049,-0.0158406484,0.0308565473,0.0183960486,0.0394943877,-0.0094218744,0.0070146553,0.013439587,0.0908274497,0.0120101004,0.0388602393,-0.0076236863,0.0052460733,-0.0209778234,-0.0508066078,0.0585473047,-0.0170237211,0.0423690525,-0.0263816102,0.01486545,0.0146244697,0.0146474495,-0.0362096095,-0.0081079716,0.0113026108,0.0300760323,-0.0027232887,-0.013933589,-0.0003360777,0.0178314445,-0.0207555307,-0.0362953163,0.0022126318,0.0040906957,0.0415005405,0.0032009387,-0.0072865209,0.0312452199,-0.0120596011,0.0169691223,-0.0156364115,0.0104834628,-0.0176057328,0.0031437608,-0.00477973,-0.0276629793,0.030474495,-0.0039398581,0.0051203848,-0.0006265348,0.0019307028,-0.0490657283,0.0346558313,-0.0060543957,0.0301041424,-0.0094556045,0.0155599762,0.0148372966,0.0012855128,-0.0044798793,0.0268478701,0.0197853571,0.0028879015,-0.030783396,0.0257477328,-0.0016820425,0.0230705271,0.0212167854,-0.0250062528,0.0166458828,-0.0050818106,-0.0157381242,-0.014327264,0.0285375389,-0.0094082669,0.0128105452,-0.0313844454,0.000039476,-0.0126912565,0.010356029,-0.0479981371,0.0334260901,0.0194539641,0.0105274369,0.0106519294,-0.0050452309,-0.0137093647,0.0130046978,-0.0083813498,-0.0242132993,-0.0221636832,0.0336620672,-0.0139843423,0.0279180949,-0.0061266666,-0.015365891,-0.0083416469,0.0193037739,0.0180511376]},"374":{"Abstract":"Data visualization systems have predominantly been developed for WIMP-based direct manipulation interfaces. Only recently have other forms of interaction begun to appear, such as natural language or touch-based interaction, though usually operating only independently. Prior evaluations of natural language interfaces for visualization have indicated potential value in combining direct manipulation and natural language as complementary interaction techniques. We hypothesize that truly multimodal interfaces for visualization, those providing users with freedom of expression via both natural language and touch-based direct manipulation input, may provide an effective and engaging user experience. Unfortunately, however, little work has been done in exploring such multimodal visualization interfaces. To address this gap, we have created an architecture and a prototype visualization system called Orko that facilitates both natural language and direct manipulation input. Specifically, Orko focuses on the domain of network visualization, one that has largely relied on WIMP-based interfaces and direct manipulation interaction, and has little or no prior research exploring natural language interaction. We report results from an initial evaluation study of Orko, and use our observations to discuss opportunities and challenges for future work in multimodal network visualization interfaces.","Authors":"A. Srinivasan; J. Stasko","DOI":"10.1109\/TVCG.2017.2745219","Keywords":"Multimodal interaction;network visualization;natural language input;direct manipulation;multitouch input","Title":"Orko: Facilitating Multimodal Interaction for Visual Exploration and Analysis of Networks","Keywords_Processed":"natural language input;multitouch input;network visualization;direct manipulation;multimodal interaction","Keyword_Vector":[0.092670281,0.0761862195,-0.0627694421,-0.0324276507,0.0120394366,-0.0200593044,-0.0594135127,-0.0130556237,-0.0784707126,0.0435448645,0.0248478314,0.0623690697,0.0495858817,0.0480012105,0.0317524077,0.040645929,0.055795446,0.002593428,0.1449158895,-0.0180749905,-0.0299476961,0.1664864063,0.1309638871,-0.0077847218,-0.0923943122,-0.0424153328,0.0668609794,-0.0685429019,0.0614995619,-0.1157060974,-0.0042500917,-0.0429168066,0.0324093468,-0.0140898883,0.012872814,-0.0055186543,-0.061744269,-0.0093877702,0.0328041906,0.0011298959,-0.0181028771,0.0166536881,0.0528495011,0.0175944263,0.009844933,-0.0114980445,-0.025929709,0.0453691201,-0.0085423038,0.0477049428,0.0121552539,0.0556924902],"Abstract_Vector":[0.1702428831,0.014651672,-0.0405595305,0.0100217662,-0.0656057845,0.0943468839,0.013219079,0.0429989794,0.1019916877,-0.0217881422,-0.0742584274,0.0493266356,-0.0823125746,-0.013133815,0.049446993,-0.0225675763,0.0608951529,0.1054013313,-0.0124816155,0.045716786,-0.0189924798,-0.0192436183,0.032154361,0.0011136817,-0.0347580661,-0.0257145679,0.0377501779,-0.0143622105,0.0221818396,0.0297308787,0.0472364447,0.004074308,0.0093585524,-0.0278693247,-0.0083710201,-0.0198028404,0.0286718437,-0.0316595763,0.0117738167,0.0291392174,0.0223436514,0.0274043949,-0.0054580856,0.0232462622,-0.0221306067,0.0198427488,-0.0180093966,-0.0307997437,0.0116613174,-0.0024370295,0.008351591,-0.0291565851,0.0094062636,-0.0147079614,0.0178273725,-0.0032953601,-0.0273263231,-0.0113861068,0.0082638737,-0.0218237411,0.0233803907,-0.0205967872,0.0246676967,-0.0306580694,-0.0382150323,0.0391162494,-0.0387407846,0.0181300516,-0.0165136951,-0.0542831896,0.0209918042,-0.0023835893,0.0240685827,0.0426744459,-0.0038043306,-0.003147358,-0.0031821849,0.0378583482,0.0004909119,0.0243790993,-0.0410184264,-0.0097399476,0.0012763598,-0.0580505472,-0.0066290069,-0.0468214683,0.0033102756,-0.0264819526,-0.0214424585,0.0119397197,-0.0047825388,-0.0068757469,-0.0232477577,0.0458622816,-0.008734152,-0.0126678276,0.0318997499,-0.0246992933,0.0131867075,0.0228885151,-0.0227686972,-0.0254766995,0.006460434,-0.0175120451,-0.0332018014,0.008176389,-0.0277955027,0.0249843494,0.0200936713,0.0174399371,0.0195379045,0.015267531,-0.0429325874,0.0120956367,0.0598295871,-0.0071289243]},"375":{"Abstract":"We present a novel approach for constructing a complete 3D model for an object from a single RGBD image. Given an image of an object segmented from the background, a collection of 3D models of the same category are non-rigidly aligned with the input depth, to compute a rough initial result. A volumetric-patch-based optimization algorithm is then performed to refine the initial result to generate a 3D model that not only is globally consistent with the overall shape expected from the input image but also possesses geometric details similar to those in the input image. The optimization with a set of high-level constraints, such as visibility, surface confidence and symmetry, can achieve more robust and accurate completion over state-of-the art techniques. We demonstrate the efficiency and robustness of our approach with multiple categories of objects with various geometries and details, including busts, chairs, bikes, toys, vases and tables.","Authors":"D. Li; T. Shao; H. Wu; K. Zhou","DOI":"10.1109\/TVCG.2016.2553102","Keywords":"RGBD camera;shape completion;single RGBD image","Title":"Shape Completion from a Single RGBD Image","Keywords_Processed":"single RGBD image;rgbd camera;shape completion","Keyword_Vector":[0.3754625241,0.3477282393,-0.2153633146,-0.1935497837,-0.0483255477,0.0878750634,-0.0011189768,0.1110929807,0.283227257,0.2161313862,0.1556717913,0.3116310615,-0.0766940712,-0.0423155187,-0.0348441668,-0.0983960068,0.0887937232,0.038547716,-0.00767522,-0.0552184774,-0.020169541,-0.1115963528,-0.0467460358,-0.0100885881,-0.0069357437,-0.0415342276,-0.0199837044,-0.0608173689,0.031500516,-0.0133968527,-0.022748258,0.0445293522,-0.0039414601,-0.039570001,-0.0399901957,-0.0418245224,0.0163877506,0.0221240064,0.0512803642,0.0299566733,-0.058448525,-0.0253305068,-0.0672457323,0.0219950474,-0.0342034077,0.0155668087,0.0053899682,-0.0586324464,0.0905166637,0.0126668959,0.0063595218,0.0208771014],"Abstract_Vector":[0.251443245,0.0235873477,-0.0297619636,-0.1114789912,0.0270364572,0.0866702076,-0.0265037003,-0.0401287985,0.0412746365,0.0392554403,-0.012632464,-0.0157084384,-0.003750681,0.0544587155,-0.0325810118,0.1287178828,0.0123754623,-0.0127421328,0.0779127046,-0.0390204835,-0.0746277348,-0.0382482756,-0.0163210136,-0.0154604267,-0.0246694424,0.0199872191,0.0046031124,0.0196825814,0.007592009,0.0810030125,-0.046970318,0.0450149697,-0.067063958,-0.0465223021,-0.0351264255,-0.0047817409,0.0087925632,0.0197735099,-0.0348694849,0.0300295943,0.0021989417,-0.0065172097,-0.0217954589,0.0043201565,0.0455423955,-0.0409281756,-0.0376744236,0.0051128295,-0.1029419989,0.0079551371,0.003334273,-0.0201996582,-0.0174360833,-0.0101427285,-0.0154530231,-0.0125241828,-0.0283214433,0.0213989296,0.0504347974,-0.0450531152,-0.0097592479,-0.0493639492,-0.0372239065,-0.0203932068,0.0141787444,0.0639753787,-0.0263319146,-0.0386746253,-0.0609733907,-0.0522880117,0.0882513638,0.0132768346,-0.0252813635,-0.0421000152,0.048409042,0.0289704855,-0.0203961314,-0.0222743384,0.0120294329,-0.022133376,0.0141041433,-0.0181201915,0.0349662774,-0.001934682,0.0147059619,0.0020773687,0.0002811473,0.0234595456,0.0104316926,0.0157889916,0.0051828264,0.000645057,-0.0058970607,-0.0585971213,0.0089682981,-0.0149313349,0.0137018043,-0.0240139556,-0.0272484951,0.019008208,0.0163501873,0.0355855789,-0.0227960746,0.0206095634,0.0080722912,-0.0010351862,-0.013461343,0.0147038174,-0.013509731,-0.0036703063,-0.0018151935,-0.0301674151,0.0362532858,-0.023605305,0.0450420643,0.0249295414]},"376":{"Abstract":"We present a formal approach to the visual analysis of recirculation in flows by introducing recirculation surfaces for 3D unsteady flow fields. Recirculation surfaces are the loci where massless particle integration returns to its starting point after some variable, finite integration. We give a rigorous definition of recirculation surfaces as 2-manifolds embedded in 5D space and study their properties. Based on this we construct an algorithm for their extraction, which searches for intersections of a recirculation surface with lines defined in 3D. This reduces the problem to a repeated search for critical points in 3D vector fields. We provide a uniform sampling of the search space paired with a surface reconstruction and visualize results. This way, we present the first algorithm for a comprehensive feature extraction in the 5D flow map of a 3D flow. The problem of finding isolated closed orbits in steady vector fields occurs as a special case of recirculation surfaces. This includes isolated closed orbits with saddle behavior. We show recirculation surfaces for a number of artificial and real flow data sets.","Authors":"T. Wilde; C. R\u00f6ssi; H. Theisel","DOI":"10.1109\/TVCG.2018.2864813","Keywords":"Flow visualization;recirculation;unsteady flow","Title":"Recirculation Surfaces for Flow Visualization","Keywords_Processed":"unsteady flow;flow visualization;recirculation","Keyword_Vector":[0.0404031457,-0.0077390701,-0.0226901688,-0.0143192434,-0.0000300671,-0.0255425118,-0.0268112961,0.0059542326,-0.0357485119,0.0035823846,-0.0144987951,0.0287436177,0.0043373685,0.0240620742,-0.0034832233,-0.0142864274,-0.0083232164,-0.0694275133,0.0345662959,-0.0137365344,0.024839047,-0.0668903129,0.0691826931,0.0252318794,0.0376222825,0.0710373381,0.0353149657,0.0416000168,-0.0945634263,0.0496759076,-0.0024988719,-0.0266712798,-0.0340683164,-0.0084618642,0.055148367,0.0912926897,-0.0981508107,-0.0195301445,0.0175591236,-0.0486389949,0.0314659419,-0.0361141445,-0.0161035289,0.0488541457,0.0570969883,0.0573213968,0.142269311,-0.0390362314,-0.0851284766,0.0313777145,0.056285659,0.0040523984],"Abstract_Vector":[0.1391459224,-0.0317373704,-0.0144763869,-0.0281155816,0.0102198901,0.0167947985,-0.0995935432,-0.0209818088,0.0153417116,-0.0842180819,-0.0490448407,0.0386917526,-0.0410239776,0.0952864364,0.0488605511,-0.0723600725,-0.0495430088,-0.0352093865,-0.1130794608,0.003953208,0.0578197038,-0.0041198837,-0.0319365249,0.0055067681,0.0173637692,0.0408590627,0.0325760312,-0.0574973734,0.0318984129,0.1051324125,0.13681245,0.0734722484,0.0515614745,0.035674014,-0.0301175527,0.0767651538,-0.0832164315,-0.0844130027,-0.0570817807,-0.0731183488,-0.1437885086,-0.1089242198,0.0729129941,0.117812522,0.0278573416,0.0524089605,-0.0240776637,0.0951779253,-0.0391240635,-0.0642806315,0.0920572883,-0.0270529122,-0.0369667356,0.069144855,0.0688110692,0.06630937,-0.0090515959,-0.029314095,-0.0829694906,-0.055462128,-0.0160499432,0.0193692623,0.0137911405,0.0228109589,-0.0253096767,-0.0138724894,0.023585704,0.0164062347,-0.0428850347,-0.0075205097,-0.047785156,-0.0102541905,-0.0579472035,0.0193144881,0.0012346934,0.0080656422,0.0010664549,0.0333288634,-0.0041580064,-0.0250344336,0.0002814902,-0.0197877151,0.0013622563,0.0265358828,-0.0031642948,-0.0166024543,-0.0098699907,0.025814844,-0.0127488817,-0.0122226681,0.0089900315,-0.0160127246,0.0342233508,-0.0163836746,0.0158493113,-0.0341699337,0.0240395094,0.0261999027,0.0152773504,-0.0035461679,-0.0031139404,-0.0246736474,0.0367431234,0.0030916653,0.0507962386,0.0219167405,-0.0197632204,-0.0216523417,-0.0235242313,-0.0006879054,0.006933348,0.0259761282,-0.0070401504,0.0066426657,0.0241356752,0.0010222368]},"377":{"Abstract":"Action sequences, where atomic user actions are represented in a labelled, timestamped form, are becoming a fundamental data asset in the inspection and monitoring of user behaviour in digital systems. Although the analysis of such sequences is highly critical to the investigation of activities in cyber security applications, existing solutions fail to provide a comprehensive understanding due to the complex semantic and temporal characteristics of these data. This paper presents a visual analytics approach that aims to facilitate a user-involved, multi-faceted decision making process during the identification and the investigation of \u201cunusual\u201d action sequences. We first report the results of the task analysis and domain characterisation process. Then we describe the components of our multi-level analysis approach that comprises of constraint-based sequential pattern mining and semantic distance based clustering, and multi-scalar visualisations of users and their sequences. Finally, we demonstrate the applicability of our approach through a case study that involves tasks requiring effective decision-making by a group of domain experts. Although our solution here is tightly informed by a user-centred, domain-focused design process, we present findings and techniques that are transferable to other applications where the analysis of such sequences is of interest.","Authors":"P. H. Nguyen; C. Turkay; G. Andrienko; N. Andrienko; O. Thonnard; J. Zouaoui","DOI":"10.1109\/TVCG.2018.2859969","Keywords":"Action sequence;event sequence;sequential pattern mining;visual analytics;cyber security;user behaviour","Title":"Understanding User Behaviour through Action Sequences: From the Usual to the Unusual","Keywords_Processed":"sequential pattern mining;cyber security;user behaviour;event sequence;action sequence;visual analytic","Keyword_Vector":[0.1154456224,-0.0586972688,-0.015245981,0.1385584214,0.0906119407,0.1390641548,-0.0304892547,0.118049575,-0.0454405057,-0.0021929648,-0.0796426342,-0.0410327409,0.0046838601,-0.0180712148,0.0354652293,-0.0476173526,0.0302246213,0.0039506503,-0.0346967006,-0.0210329706,-0.0385786255,-0.0371407898,0.0087791342,0.0174140585,-0.0538097514,-0.0220901379,0.0208374874,-0.0000192155,0.0035584523,-0.0600656948,-0.006057061,0.0614972654,-0.0473028059,-0.0763075756,0.0612617767,-0.0572774796,-0.0521406456,-0.0107682502,-0.0019713015,-0.0073437515,-0.0062206095,0.1607440454,-0.0237735046,-0.0128421531,-0.0125243662,0.0136095483,0.025074664,-0.0341204817,-0.0611868723,-0.0442342704,-0.0078003974,-0.0205449162],"Abstract_Vector":[0.171208563,-0.1077285332,0.024480593,-0.0202004309,-0.0287993396,-0.0202057553,0.0500905203,0.0058296325,-0.0298156497,0.0607815961,-0.0385345012,0.047342506,0.0079332409,0.005143665,0.0176595894,-0.0431910593,-0.0234050553,0.000764894,-0.0156566848,-0.0446287106,-0.0003384645,-0.0504165033,-0.02495298,-0.0019846792,-0.0290519146,0.0162188451,-0.0362155219,-0.0017441433,-0.0559968517,-0.0493727762,0.0009039245,-0.042599507,0.0082064597,-0.0015930157,-0.0214613768,-0.000671822,-0.0260723064,-0.020417318,0.0209431032,0.0022017027,-0.0257814553,-0.0011391016,0.0033753688,-0.0432593342,0.0330671983,0.0144800051,0.0117212965,-0.0379118229,0.0197188824,0.0102057931,-0.013209302,0.0400962602,-0.0004815301,0.0029019307,0.0014870645,-0.0033675167,-0.0291965748,-0.0085353884,-0.0134231228,0.0035342662,-0.0175921388,-0.0249846795,-0.0295623199,0.0083260748,-0.0343876885,0.0109095255,-0.0022408393,0.0093913848,-0.0055377281,0.0061563228,-0.0042033094,-0.0111008266,-0.0008447938,0.0303287893,0.0103188183,0.0015342816,-0.015950112,-0.0192828072,-0.0167907176,0.0347658297,-0.0131273116,-0.0018912419,0.0093824106,0.0236155846,0.0398903197,0.0020556474,0.0321659559,-0.0247863083,-0.0073185861,-0.0067639273,0.0275509742,0.0293656411,-0.0004677404,-0.041966134,-0.0374050352,0.0100029288,0.0068904074,-0.0099686631,0.0306330595,-0.0398540256,-0.0110822461,0.003700675,-0.0050728943,-0.0060839902,0.0256455612,-0.0128079493,-0.0059618923,-0.0144215816,0.0066758689,-0.0222149914,0.0204123225,0.0195494398,0.0016927521,-0.0066598904,-0.0031149152,0.0089742126]},"378":{"Abstract":"This paper presents a new approach for the visualization and analysis of the spatial variability of features of interest represented by critical points in ensemble data. Our framework, called Persistence Atlas, enables the visualization of the dominant spatial patterns of critical points, along with statistics regarding their occurrence in the ensemble. The persistence atlas represents in the geometrical domain each dominant pattern in the form of a confidence map for the appearance of critical points. As a by-product, our method also provides 2-dimensional layouts of the entire ensemble, highlighting the main trends at a global level. Our approach is based on the new notion of Persistence Map, a measure of the geometrical density in critical points which leverages the robustness to noise of topological persistence to better emphasize salient features. We show how to leverage spectral embedding to represent the ensemble members as points in a low-dimensional Euclidean space, where distances between points measure the dissimilarities between critical point layouts and where statistical tasks, such as clustering, can be easily carried out. Further, we show how the notion of mandatory critical point can be leveraged to evaluate for each cluster confidence regions for the appearance of critical points. Most of the steps of this framework can be trivially parallelized and we show how to efficiently implement them. Extensive experiments demonstrate the relevance of our approach. The accuracy of the confidence regions provided by the persistence atlas is quantitatively evaluated and compared to a baseline strategy using an off-the-shelf clustering approach. We illustrate the importance of the persistence atlas in a variety of real-life datasets, where clear trends in feature layouts are identified and analyzed. We provide a lightweight VTK-based C++ implementation of our approach that can be used for reproduction purposes.","Authors":"G. Favelier; N. Faraj; B. Summa; J. Tierny","DOI":"10.1109\/TVCG.2018.2864432","Keywords":"Topological data analysis;scalar data;ensemble data","Title":"Persistence Atlas for Critical Point Variability in Ensembles","Keywords_Processed":"ensemble datum;scalar datum;topological datum analysis","Keyword_Vector":[0.030063889,0.0078146366,0.031177148,-0.0062845843,-0.0146843282,-0.0188139001,-0.0265558731,-0.0227941303,-0.009220149,-0.0689045876,-0.0187869918,0.009685182,-0.0272934848,-0.0231851007,0.0067223802,-0.0162555073,0.0573038566,-0.004977004,0.0130659872,-0.0023145726,0.0403147624,0.0423949293,-0.0332224943,0.0389581518,-0.0236097329,0.0238071312,-0.0515486047,0.0172966683,-0.0439266377,0.029249362,-0.0257050465,-0.012095151,0.0462799987,-0.0329075369,-0.0158651826,0.0114981958,-0.0270986628,-0.085059959,-0.0286641592,0.044652028,-0.0314318136,0.0128299824,-0.0536256174,0.0674235683,-0.0443216681,-0.062294047,-0.0147359571,0.0334491181,0.013728271,0.031732418,0.0343850789,0.0355722274],"Abstract_Vector":[0.162350123,0.0752685946,0.0862574594,-0.0142177993,0.0352601984,-0.0150831795,0.0851702712,0.0180390588,-0.0103560858,-0.0110821848,0.011192389,0.0132119133,-0.0058332026,-0.0276703469,-0.0135690632,0.0014981607,-0.0397643609,0.0514861599,-0.006525277,-0.0357272938,0.0051946195,-0.0607791606,0.0029130665,0.0396856572,0.094624327,0.0104066825,0.0156041336,0.0172122113,0.0343855841,-0.0166774752,0.0284557494,0.033600951,-0.0461653531,0.0282401461,0.0244378954,-0.0224653572,0.0030475098,0.0239390416,-0.0365964815,-0.0192582011,0.0109368776,0.0184883453,0.0209553661,0.0079987586,-0.0168068375,0.0176283984,-0.0070774253,0.0136446261,0.0111278485,0.0059334649,0.0310276744,-0.0034006625,-0.0236727439,0.0307566874,-0.0010604416,-0.0028923735,0.0027033907,-0.0364948,0.058707322,-0.0075161135,-0.0012746028,-0.0222104901,0.0110242005,0.016590556,-0.0062632168,0.0088786431,-0.0077039061,0.0226427699,0.0172760861,0.0375300652,0.0098059179,0.0088176767,0.0176401208,0.0000104601,-0.0105305124,-0.0215811979,-0.023141481,0.0162206606,0.012293428,-0.0194699275,0.0191033881,0.0084592794,0.0103529613,-0.0109496409,-0.010819892,-0.0056437888,0.009544848,-0.0192802006,-0.0132688317,0.0082698198,0.0071036355,0.0272735185,-0.0125857029,0.0162486217,-0.007984371,0.0053205048,0.0093591521,0.0117539212,0.0078266719,0.0160218494,0.0282518039,-0.0338714409,0.0050279253,0.0046727556,0.0032306073,0.011779668,0.0124383743,-0.0430487877,-0.0161534239,-0.0248991647,-0.0001240409,0.0106334024,0.0092712412,-0.0056228661,0.0124450883,-0.0049002035]},"379":{"Abstract":"Finding similar points in globally or locally similar shapes has been studied extensively through the use of various point descriptors or shape-matching methods. However, little work exists on finding similar points in dissimilar shapes. In this paper, we present the results of a study where users were given two dissimilar two-dimensional shapes and asked to map a given point in the first shape to the point in the second shape they consider most similar. We find that user mappings in this study correlate strongly with simple geometric relationships between points and shapes. To predict the probability distribution of user mappings between any pair of simple two-dimensional shapes, two distinct statistical models are defined using these relationships. We perform a thorough validation of the accuracy of these predictions and compare our models qualitatively and quantitatively to well-known shape-matching methods. Using our predictive models, we propose an approach to map objects or procedural content between different shapes in different design scenarios.","Authors":"M. Hecher; P. Guerrero; P. Wonka; M. Wimmer","DOI":"10.1109\/TVCG.2017.2730877","Keywords":"2D mappings;shape matching;shape similarity;transformations;user studies","Title":"How Do Users Map Points Between Dissimilar Shapes?","Keywords_Processed":"user study;shape matching;transformation;2d mapping;shape similarity","Keyword_Vector":[0.1012342259,-0.017279943,0.0039937066,0.0171782188,0.0039902925,-0.0297389555,-0.0178489217,-0.0022248451,-0.0622963021,-0.0826494558,0.0396929242,0.0135019481,-0.0290409129,0.1068155846,0.0189105244,0.0076949968,0.0301889027,-0.0426769687,0.0630966601,-0.0278091955,0.0417950959,-0.0247340183,-0.081637096,0.0883805058,-0.0030459668,-0.0082570964,0.0747661017,0.0543128522,-0.1335734643,0.0577565975,-0.0465057985,0.0042485621,0.0488211969,-0.1149064125,0.0230838224,0.0377095911,-0.0584935504,-0.120611675,-0.053057624,0.0082628273,-0.0313312831,0.0366753786,-0.0312843202,0.1034215754,-0.0405301202,-0.0318010663,0.0063645798,0.0557344197,0.0589150635,0.0162276779,-0.0050838517,0.0572296427],"Abstract_Vector":[0.1423529795,-0.0315845698,0.0165369001,-0.0529561365,-0.0348559752,0.0130073507,0.0086020021,0.0248692665,0.0084229145,-0.0178623014,0.0420111033,-0.0097957576,0.0390312839,0.0246469346,-0.0591592512,0.0075810589,-0.0165824174,0.0874052375,0.0179578366,-0.0583298276,-0.0249702896,0.0238668257,-0.0099098649,0.0065118445,0.013773333,0.0459717937,-0.0071058684,0.0182412497,0.0164612817,-0.0140075371,0.0195727996,0.0166647327,-0.0271858466,-0.0174757953,-0.0356855897,-0.0364930267,-0.045756331,0.0105409125,0.0315051994,-0.0507550284,-0.0027595052,-0.0158606237,-0.0090555349,0.0108905328,-0.0187424195,0.0132147925,-0.0105720896,-0.0379053981,0.016137693,0.0459800325,-0.0052928416,0.0175643912,-0.0280530613,-0.0041442279,0.0562741288,-0.0080376134,0.0183968463,0.0288453416,-0.046909237,0.0013989398,0.0014793164,-0.0112364937,-0.0896041623,-0.0346999096,-0.0074712232,0.0049208666,0.0276185785,0.0854278267,0.0238139512,0.07412232,0.0250787482,0.0319074773,0.0402454449,0.0656189324,0.024224237,-0.0074913964,0.0136390598,-0.038399896,-0.0099759881,0.0059628473,0.0330075371,-0.0060833409,-0.0179489679,0.0026951742,-0.0151339476,0.0169181479,-0.0098203465,-0.0284222652,0.0299517603,0.016546436,-0.0254074975,-0.0025158869,-0.0227379749,-0.0194937848,-0.0233986666,0.028094981,-0.0070718962,-0.0418656477,-0.0034452877,-0.0296227753,-0.0312118047,0.030961625,-0.0234459839,0.0029101333,-0.040190822,0.0332888622,-0.0495484463,0.0253269253,0.0446026403,-0.0095795046,0.040067479,0.0227597791,0.0099488559,0.0415171044,-0.0276912715,0.0281326564]},"38":{"Abstract":"Reduced hair models have proven successful for interactively simulating a full head of hair strands, building upon a fundamental assumption that only a small set of guide hairs are needed for explicit simulation, and the rest of the hair move coherently and thus can be interpolated using guide hairs. Unfortunately, hair-solid interactions is a pathological case for traditional reduced hair models, as the motion coherence between hair strands can be arbitrarily broken by interacting with solids. In this paper, we propose an adaptive hair skinning method for interactive hair simulation with hair-solid collisions. We precompute many eligible sets of guide hairs and the corresponding interpolation relationships that are represented using a compact strand-based hair skinning model. At runtime, we simulate only guide hairs; for interpolating every other hair, we adaptively choose its guide hairs, taking into account motion coherence and potential hair-solid collisions. Further, we introduce a two-way collision correction algorithm to allow sparsely sampled guide hairs to resolve collisions with solids that can have small geometric features. Our method enables interactive simulation of more than 150 K hair strands interacting with complex solid objects, using 400 guide hairs. We demonstrate the efficiency and robustness of the method with various hairstyles and user-controlled arbitrary hair-solid interactions.","Authors":"M. Chai; C. Zheng; K. Zhou","DOI":"10.1109\/TVCG.2016.2551242","Keywords":"Hair simulation;interactive method;reduced model;adaptivity;collision correction","Title":"Adaptive Skinning for Interactive Hair-Solid Simulation","Keywords_Processed":"reduce model;hair simulation;collision correction;interactive method;adaptivity","Keyword_Vector":[0.1077494715,-0.1507963819,-0.1516050832,0.0843429428,-0.0852449869,-0.1856807905,0.0747665077,0.0415915824,0.0788991333,-0.0219414073,0.0247950496,-0.0947705474,-0.0518487429,-0.0552083229,0.0733281282,0.0060070311,-0.0197631573,0.0043528209,-0.0403937502,0.0130792894,-0.0224223676,-0.021313935,0.0476018084,-0.0003641167,0.0171825478,-0.0154838414,0.0385482258,0.0174006943,0.0237329744,-0.004817018,0.0398272042,0.0196127533,-0.0014606237,-0.0865364864,0.0355351953,-0.0608917915,0.0520235041,0.0071425848,-0.0919554675,-0.0158027923,-0.0760875705,0.0194256471,0.0913787492,0.0477339327,-0.0692998998,-0.021004051,0.0401100054,0.028457623,0.173425263,-0.0749240886,0.2770810097,0.0414001052],"Abstract_Vector":[0.221864704,-0.1150888468,0.0299822516,0.0108754512,-0.0824470584,0.0090243762,-0.0056961183,0.0127726307,-0.0684292291,0.0172712864,-0.0396225353,-0.0127814309,-0.096384383,-0.0248062043,-0.0440834087,0.0502427614,-0.0050829213,-0.0611350637,0.0002109873,0.0568696548,0.0375467543,0.0475145957,0.0167299512,0.055948388,0.0500569264,0.0889998654,0.0628536178,0.0754750372,0.0190892527,0.0290566601,0.014089406,-0.0578224892,0.0046562264,-0.0268265607,0.0176885819,-0.0555339847,0.0227298752,-0.000599339,-0.0339681968,-0.0116996339,0.0046005994,0.0110119042,0.0230491593,-0.0051690257,-0.0073516288,-0.006143191,0.0709341221,0.0264427601,-0.0449112857,-0.0117010556,0.0460441925,0.04374307,-0.0295024956,-0.0071535703,0.0331632079,-0.0245637989,-0.0395724181,0.0120885533,0.0077818979,-0.0146256922,0.0208453414,0.0319586098,-0.0071855234,0.0294259917,-0.0316164533,0.0040838933,0.0067071522,-0.0368037439,0.0042812744,0.002048041,0.0269224219,0.0690982568,0.0169804948,0.0218108737,0.0032160087,-0.005674912,-0.0129039918,-0.003272106,0.0332023738,-0.0114827861,0.0139684519,0.0110251093,-0.0007236364,-0.012611222,-0.0001965268,-0.0198778914,-0.0081031957,-0.0046537895,-0.0317715293,-0.0194878229,0.0192011592,0.0049281304,-0.0208638735,0.0031015886,0.0392860629,0.0221751674,0.0258425621,-0.0332768891,-0.0229287429,0.0516726357,0.0046743271,-0.0001627242,-0.0064842195,0.0416782162,0.0234244512,-0.013943788,0.0419401247,0.014985333,-0.0051354253,0.0322762235,-0.0116533353,0.0014008764,-0.0032497421,0.0307259859,-0.0115207009,-0.0003823443]},"380":{"Abstract":"Bezigons, i.e., closed paths composed of B\u00e9zier curves, have been widely employed to describe shapes in image vectorization results. However, most existing vectorization techniques infer the bezigons by simply approximating an intermediate vector representation (such as polygons). Consequently, the resultant bezigons are sometimes imperfect due to accumulated errors, fitting ambiguities, and a lack of curve priors, especially for low-resolution images. In this paper, we describe a novel method for vectorizing clipart images. In contrast to previous methods, we directly optimize the bezigons rather than using other intermediate representations; therefore, the resultant bezigons are not only of higher fidelity compared with the original raster image but also more reasonable because they were traced by a proficient expert. To enable such optimization, we have overcome several challenges and have devised a differentiable data energy as well as several curve-based prior terms. To improve the efficiency of the optimization, we also take advantage of the local control property of bezigons and adopt an overlapped piecewise optimization strategy. The experimental results show that our method outperforms both the current state-of-the-art method and commonly used commercial software in terms of bezigon quality.","Authors":"M. Yang; H. Chao; C. Zhang; J. Guo; L. Yuan; J. Sun","DOI":"10.1109\/TVCG.2015.2440273","Keywords":"clipart vectorization;clipart tracing;bezigon optimization;Clipart vectorization;clipart tracing;bezigon optimization","Title":"Effective Clipart Image Vectorization through Direct Optimization of Bezigons","Keywords_Processed":"clipart vectorization;bezigon optimization;clipart trace","Keyword_Vector":[0.0454512415,0.0253607709,0.0234466181,0.0286802444,0.024672746,-0.0350197036,-0.0213061652,-0.0242516118,-0.0410294499,-0.0056924835,0.0019500043,0.0430861446,0.0204716576,0.0420033437,0.0517156282,-0.0143147022,-0.0114704447,-0.0263340131,-0.0074131156,0.0161281147,-0.0207241541,-0.0227949362,-0.0330502046,0.0403761171,0.0171951978,0.0053301737,0.0502213031,0.0479251791,-0.0092834793,-0.0498687792,0.0350827316,-0.0422965962,-0.0222800389,-0.0077148575,0.0118331067,0.0138210918,0.0424425831,0.0438811465,-0.013711485,-0.013803845,0.0275692111,0.0378059565,-0.0701178704,-0.0102096443,-0.0211194136,0.0339985083,-0.0137833499,0.0735239627,-0.0175031837,-0.006899017,-0.00579581,0.0603296603],"Abstract_Vector":[0.1839447402,-0.0392972518,-0.0043805137,0.0181041688,-0.1094617333,0.0548125972,-0.0200400139,0.0137135402,-0.0228176719,-0.0189116993,-0.0314161337,0.0036054181,-0.0258220232,-0.011916851,-0.0014596694,0.0616755336,0.0791536156,0.0465919093,-0.0230356249,-0.0893158612,0.0416361172,-0.0128278381,0.0427212045,-0.0535251742,-0.0300186736,0.0294523915,0.0678055334,-0.029017717,-0.0385307963,0.0316334713,0.0141203297,-0.0110137737,-0.0031596051,-0.0111835088,-0.0082571995,0.0358368866,0.0123302088,-0.0000672426,-0.0198821792,0.0253042393,0.0284873446,-0.0084736642,0.0331682293,-0.0655503569,0.0269101866,-0.0068037645,0.0567564202,0.069523942,-0.0203724121,0.0413236762,-0.0365913926,0.0190713566,0.0196175145,0.0518373275,0.0023819881,0.0029516728,0.0121438711,0.0169233464,0.0176928059,0.0040756,0.0326168476,0.0311334784,-0.0350840183,-0.0183920482,-0.0197534651,-0.027587493,0.0126775307,-0.0232180659,0.0546047256,0.0627241983,-0.0372863111,-0.0059639793,-0.0302579175,0.0145972811,0.0052682534,0.046375609,-0.0356347987,0.0064209811,0.0369507643,0.0091477921,-0.0019905685,-0.0205558235,-0.0090334367,0.0060676014,-0.031520413,-0.0187184153,0.0103619633,-0.0351022106,-0.0043035065,-0.0171650162,-0.0319687687,0.0062844346,-0.0063699607,-0.0014057544,0.0248517468,0.0103757215,-0.009470021,0.0009594309,0.0102142508,0.0040444824,0.028737745,0.0224043041,-0.0007076196,-0.0247423055,-0.0053168428,-0.0379421792,0.0154455871,0.0420892566,0.0046262633,0.0075746934,-0.0526824865,0.0017592237,0.0141047087,0.0138157361,0.0369519346,0.0153488372]},"381":{"Abstract":"Turbulent vortices in smoke flows are crucial for a visually interesting appearance. Unfortunately, it is challenging to efficiently simulate these appealing effects in the framework of vortex filament methods. The vortex filaments in grids scheme allows to efficiently generate turbulent smoke with macroscopic vortical structures, but suffers from the projection-related dissipation, and thus the small-scale vortical structures under grid resolution are hard to capture. In addition, this scheme cannot be applied in wall-bounded turbulent smoke simulation, which requires efficiently handling smoke-obstacle interaction and creating vorticity at the obstacle boundary. To tackle above issues, we propose an effective filament-mesh particle-particle (FMPP) method for fast wall-bounded turbulent smoke simulation with ample details. The Filament-Mesh component approximates the smooth long-range interactions by splatting vortex filaments on grid, solving the Poisson problem with a fast solver, and then interpolating back to smoke particles. The Particle-Particle component introduces smoothed particle hydrodynamics (SPH) turbulence model for particles in the same grid, where interactions between particles cannot be properly captured under grid resolution. Then, we sample the surface of obstacles with boundary particles, allowing the interaction between smoke and obstacle being treated as pressure forces in SPH. Besides, the vortex formation region is defined at the back of obstacles, providing smoke particles flowing by the separation particles with a vorticity force to simulate the subsequent vortex shedding phenomenon. The proposed approach can synthesize the lost small-scale vortical structures and also achieve the smoke-obstacle interaction with vortex shedding at obstacle boundaries in a lightweight manner. The experimental results demonstrate that our FMPP method can achieve more appealing visual effects than vortex filaments in grids scheme by efficiently simulating more vivid thin turbulent features.","Authors":"X. Liao; W. Si; Z. Yuan; H. Sun; J. Qin; Q. Wang; P. Heng","DOI":"10.1109\/TVCG.2017.2665551","Keywords":"Wall-bounded turbulent smoke;fluid-solid Interaction;vortex shedding","Title":"Animating Wall-Bounded Turbulent Smoke via Filament-Mesh Particle-Particle Method","Keywords_Processed":"fluid solid interaction;Wall bound turbulent smoke;vortex shedding","Keyword_Vector":[0.1529806993,-0.0674609534,0.0524365831,-0.0777778867,0.0456746804,-0.0277192888,0.0498194623,0.0251422318,0.0131855404,0.0343565953,0.0025822467,-0.0435673605,0.0387484021,0.0151311213,0.072141266,-0.0094789344,0.011068103,-0.0120558668,0.0578623411,-0.0834685369,-0.0189967852,0.0174489043,-0.0413745738,-0.0371487987,0.1394313694,0.0006418721,-0.0082022583,0.0118344969,0.0051885705,-0.0271126749,-0.0050164717,-0.0352407674,-0.0105949559,0.0575438418,-0.0779673841,0.1124257441,0.0845908744,-0.0244547874,0.0284847611,-0.0022991351,0.1296053222,0.0770708306,0.0084227584,0.0093215343,-0.0511373903,-0.0040794534,-0.0937603235,-0.0278642637,-0.0211330183,0.0305234472,-0.0080378894,0.0111019411],"Abstract_Vector":[0.1863595151,-0.050964343,0.03766304,0.0895332009,0.0411837113,0.0836379015,-0.0127022515,-0.0152090413,-0.0083185329,0.0162878636,-0.031165504,-0.0338730286,-0.0115405226,0.0030951218,0.0952875396,0.0231185818,0.0233728573,0.0024671936,0.0249342036,-0.0028299369,-0.0323528859,0.0484254416,0.0051881292,0.0207976593,0.0297737352,0.013017932,0.0063237318,0.0309667213,0.0659251221,0.0334476819,-0.0342650211,-0.0098244921,-0.0257027086,0.0243846273,-0.0029466563,-0.0210256757,0.0324491723,-0.0332320712,0.0027917369,-0.0322777114,-0.022316163,0.0657343135,-0.0513845254,0.0258634737,0.0031636502,-0.0329289584,-0.0235026152,0.0442321953,-0.0221914162,0.0025960191,-0.0194738601,-0.050929173,0.0294563868,0.0157456079,0.0004636912,-0.0140314048,-0.0004596689,-0.0156117593,0.0475798173,-0.0420338584,0.0362082257,-0.0148240397,0.0022596613,-0.00484624,-0.0295642846,-0.0020649889,-0.0257701644,-0.0050088818,-0.0511392944,0.007960236,0.0051072173,0.0168660221,0.0195118064,0.022191559,0.0759254134,-0.028830008,0.0147801632,0.0202118506,0.0289764345,0.033707237,-0.0035828048,0.0187208386,0.0011119959,0.0039168069,0.0243499595,-0.0123061829,0.0047815285,-0.0029524588,-0.0284270432,-0.0130573675,-0.0099370572,-0.0106353351,0.0089730152,0.0067051869,-0.0183239178,0.0022452029,0.0091273629,0.0043188621,-0.0161937325,0.0125189152,0.0027310641,-0.0086459452,-0.0020298665,0.004380531,0.0272165257,0.0074623356,-0.0391503007,-0.0314497223,-0.0133626351,-0.0088366332,0.0156036126,0.0163364879,-0.0045759873,-0.0227026882,0.0149277245,-0.0018410502]},"382":{"Abstract":"The aspect ratio of a line chart heavily influences the perception of the underlying data. Different methods explore different criteria in choosing aspect ratios, but so far, it was still unclear how to select aspect ratios appropriately for any given data. This paper provides a guideline for the user to choose aspect ratios for any input 1D curves by conducting an in-depth analysis of aspect ratio selection methods both theoretically and experimentally. By formulating several existing methods as line integrals, we explain their parameterization invariance. Moreover, we derive a new and improved aspect ratio selection method, namely the $L_1$ -LOR (local orientation resolution), with a certain degree of parameterization invariance. Furthermore, we connect different methods, including AL (arc length based method), the banking to 45 $^\\circ$ principle, RV (resultant vector) and AS (average absolute slope), as well as  $L_1$ -LOR and AO (average absolute orientation). We verify these connections by a comparative evaluation involving various data sets, and show that the selections by RV and  $L_1$ -LOR are complementary to each other for most data. Accordingly, we propose the dual-scale banking technique that combines the strengths of RV and $L_1$ -LOR, and demonstrate its practicability using multiple real-world data sets.","Authors":"Y. Wang; Z. Wang; L. Zhu; J. Zhang; C. Fu; Z. Cheng; C. Tu; B. Chen","DOI":"10.1109\/TVCG.2017.2787113","Keywords":"Aspect ratio;parameterization invariance;line integral;banking to 45 $^\\circ$    ;orientation resolution","Title":"Is There a Robust Technique for Selecting Aspect Ratios in Line Charts?","Keywords_Processed":"aspect ratio;orientation resolution;parameterization invariance;line integral;banking to 45 circ","Keyword_Vector":[0.1771939357,-0.1426079537,-0.0495687008,0.0689983278,-0.0519749003,0.0993337861,-0.0911482658,-0.0797922288,-0.0658152285,0.1516043696,-0.0397154465,0.0954288243,0.0573929277,0.2406004165,-0.1315500419,0.1337845948,-0.0159034964,-0.0270185739,-0.1440810499,0.0228849848,0.1709684226,0.0878338126,0.0252936815,-0.0852531805,-0.1572830732,0.1455659305,-0.0012199511,0.0064171444,-0.0523504342,-0.1098787322,-0.0625875153,-0.0442753355,-0.0426947967,-0.0846602825,0.0526893555,-0.0713486497,0.0710110996,-0.087385969,0.0045559041,-0.0230990585,0.1195021423,-0.0265727487,-0.0044071719,-0.0300331873,0.0410606812,-0.1152836873,-0.0152815132,-0.04069187,0.0484454911,-0.0163482788,-0.0548554821,-0.0134142956],"Abstract_Vector":[0.2242756187,-0.1625068917,0.0060991455,-0.0203601837,-0.0650806496,-0.0296848125,0.0534589621,0.0438125082,-0.0395325809,-0.0026943911,0.0054017755,0.0168125242,0.0711248964,-0.027474061,0.0424947665,-0.0590118115,-0.0452651537,-0.0037140086,-0.0474213509,-0.0155837282,0.0093118371,-0.0409611536,0.0039239506,-0.040284617,-0.07259131,0.0051744504,-0.0157770958,-0.0108610371,-0.114747213,-0.1040225631,-0.0016521172,-0.0250848476,0.00705157,0.0097546251,-0.0160267251,0.0280051938,0.0181027864,-0.0167070579,-0.0235666829,0.0399469606,-0.0147907308,0.0141328616,0.0662961185,-0.1056626951,0.0131943489,0.0011585801,0.0195987747,0.0053477897,-0.0251771193,0.0529986742,0.0075911989,0.0244057684,0.0524194872,-0.0118500433,0.0529101073,-0.0415196969,-0.0513599271,-0.0200828143,-0.0363321931,-0.016037849,-0.0175300172,-0.0098272057,-0.0200622186,-0.0380480471,-0.0444847747,-0.0019101458,-0.0401868086,-0.0083352717,-0.0267534188,-0.0106374645,-0.0163090619,-0.051693432,0.002813662,0.0048183949,-0.0190686364,-0.0063657181,-0.0478718385,0.0030547294,-0.0091237402,0.0063310679,0.002750861,-0.0135291373,0.0200269209,-0.0117635259,0.0339299787,-0.0021636643,-0.0006424143,0.0169744965,-0.0273329463,-0.0184248089,0.0235213804,0.0464563409,-0.0073336336,-0.0352007906,-0.0062512864,0.0498950025,0.0367945233,-0.0136171784,-0.0141199803,0.0070441874,-0.0078662218,-0.0101833958,0.0194702683,0.022428978,0.0099947862,-0.0389733379,0.0154295223,0.0040266817,-0.0059333785,-0.0124965806,-0.0283270146,0.0183709857,-0.0089574802,-0.0061222604,-0.0197347625,0.0455638869]},"383":{"Abstract":"We present TopoAngler, a visualization framework that enables an interactive user-guided segmentation of fishes contained in a micro-CT scan. The inherent noise in the CT scan coupled with the often disconnected (and sometimes broken) skeletal structure of fishes makes an automatic segmentation of the volume impractical. To overcome this, our framework combines techniques from computational topology with an interactive visual interface, enabling the human-in-the-Ioop to effectively extract fishes from the volume. In the first step, the join tree of the input is used to create a hierarchical segmentation of the volume. Through the use of linked views, the visual interface then allows users to interactively explore this hierarchy, and gather parts of individual fishes into a coherent sub-volume, thus reconstructing entire fishes. Our framework was primarily developed for its application to CT scans of fishes, generated as part of the ScanAllFish project, through close collaboration with their lead scientist. However, we expect it to also be applicable in other biological applications where a single dataset contains multiple specimen; a common routine that is now widely followed in laboratories to increase throughput of expensive CT scanners.","Authors":"A. Bock; H. Doraiswamy; A. Summers; C. Silva","DOI":"10.1109\/TVCG.2017.2743980","Keywords":"Computational topology;join trees;branch decomposition;hierarchical segmentation;interaction;visualization system","Title":"TopoAngler: Interactive Topology-Based Extraction of Fishes","Keywords_Processed":"join tree;hierarchical segmentation;branch decomposition;computational topology;interaction;visualization system","Keyword_Vector":[0.1789565162,-0.2049403724,-0.1813772119,0.184139439,-0.0971604987,-0.2094356613,0.2419054638,0.0467422739,0.0406531929,-0.0721985589,0.0640381937,-0.016959282,-0.0022924499,-0.0719639598,0.0062322493,-0.071608859,-0.0138625539,-0.0303162435,0.0500797757,0.0056938313,0.0043743631,0.0782955312,-0.0364341745,0.1725228464,-0.0370290698,0.1401729133,0.050446565,0.015442265,0.2513018812,0.0486016372,-0.0546383351,0.179017437,-0.163993363,0.1889106213,-0.0640941021,0.0079512766,0.0160597535,-0.1200450854,0.2029373548,-0.0710801059,-0.097432859,0.0517227449,-0.0122955812,-0.028669649,-0.0553404219,-0.0990722671,-0.0698085963,0.0234800546,0.0822002294,0.0442724921,-0.0004398817,-0.0668586364],"Abstract_Vector":[0.3383032124,-0.1613116476,-0.0430554035,0.0416304431,-0.0037183418,-0.0653587494,-0.0301210304,0.0063089151,-0.0235867972,-0.0382552373,0.0212984891,-0.026919886,-0.0635450294,-0.0820015375,-0.0757263897,0.0525523379,-0.0088458156,-0.0215913518,-0.0356487481,0.0524475243,-0.0349090763,-0.0088054437,-0.0242758078,0.0865512604,-0.057705522,0.0308005219,-0.0808433185,-0.0606366161,0.0898607012,-0.0133415119,0.1278158014,0.0266781943,0.0722053298,-0.1399131184,0.0818360116,-0.0636957871,0.0330991326,-0.1044935334,-0.0225304691,0.0512914673,-0.0179144646,0.0639237887,-0.0486594252,-0.0505527085,-0.0334028604,0.0780527545,-0.0664060355,0.0303957084,0.0312036103,-0.040288187,-0.0683569844,0.0797711956,0.0626131965,-0.001515154,0.0251204295,0.0010963674,0.0603176521,0.0100004782,0.009648272,-0.0584503923,-0.0331874701,-0.0540499226,0.024431654,-0.0263186729,0.0317894497,-0.0518839559,-0.0528602782,0.0050656979,0.0189573972,-0.0054158326,0.0342643063,-0.0058809555,-0.0490586951,-0.0052030059,0.0501323016,0.0554847621,0.0374991569,-0.0179795178,-0.0122734358,0.0199106915,-0.0235746826,-0.009081836,-0.0090005443,-0.0063778598,0.0033400851,0.0086881734,0.0646780623,0.002169971,0.0230426718,-0.0019832164,0.0262311192,-0.0216918047,0.0029007517,0.0431451076,-0.0373663166,0.0141676672,0.015213099,-0.0100925086,0.0172121334,0.0072683464,0.0244363829,-0.015128129,-0.0032673147,-0.0604955857,-0.0485685805,0.0003608151,-0.0425400292,-0.0191715122,0.0190319143,-0.0366849026,-0.0209277567,0.0066705862,0.0494649145,-0.0266391072,0.0104136482,0.0456502394]},"384":{"Abstract":"Handheld scanning using commodity depth cameras provides a flexible and low-cost manner to get 3D models. The existing methods scan a target by densely fusing all the captured depth images, yet most frames are redundant. The jittering frames inevitably embedded in handheld scanning process will cause feature blurring on the reconstructed model and even trigger the scan failure (i.e., camera tracking losing). To address these problems, in this paper, we propose a novel sparse-sequence fusion (SSF) algorithm for handheld scanning using commodity depth cameras. It first extracts related measurements for analyzing camera motion. Then based on these measurements, we progressively construct a supporting subset for the captured depth image sequence to decrease the data redundancy and the interference from jittering frames. Since SSF will reveal the intrinsic heavy noise of the original depth images, our method introduces a refinement process to eliminate the raw noise and recover geometric features for the depth images selected into the supporting subset. We finally obtain the fused result by integrating the refined depth images into the truncated signed distance field (TSDF) of the target. Multiple comparison experiments are conducted and the results verify the feasibility and validity of SSF for handheld scanning with a commodity depth camera.","Authors":"L. Yang; Q. Yan; Y. Fu; C. Xiao","DOI":"10.1109\/TVCG.2017.2657766","Keywords":"Depth image refinement;handheld scanning;sparse-sequence fusion;surface reconstruction;supporting subset","Title":"Surface Reconstruction via Fusing Sparse-Sequence of Depth Images","Keywords_Processed":"sparse sequence fusion;support subset;handheld scanning;depth image refinement;surface reconstruction","Keyword_Vector":[0.1295474032,-0.069621725,-0.0341100358,0.0492867441,-0.0401750098,0.0754901947,-0.0132051965,-0.0087259352,-0.0517871713,-0.0453357131,0.0615668178,-0.0288851743,0.0143489917,0.0418082268,-0.0646099869,-0.0127718623,-0.0112342856,0.0418760968,0.0169783298,-0.0477718637,-0.0542808741,-0.0137096483,-0.0099656971,-0.01230365,-0.0084981032,-0.0172404275,0.0208377253,-0.0113769082,-0.049652617,0.0463564424,0.0331116292,-0.032403418,0.0172164463,0.0189087798,0.0051980433,-0.0423266209,0.0393682021,-0.0008102998,0.016710086,-0.0431035835,0.0100940213,-0.0276224273,0.0439556483,0.022851537,0.0735682097,0.027478962,-0.0032708564,-0.0675647479,-0.0466728694,-0.0283027909,0.0011489,-0.0278270519],"Abstract_Vector":[0.2426899669,-0.0755433975,-0.004112528,-0.0350659725,-0.026234706,-0.0575148464,-0.0141218441,0.0225974806,-0.0512132182,-0.0147905729,0.0654290739,-0.0610675768,-0.0039176253,-0.0069202549,-0.0792770341,-0.0497214652,-0.032807984,0.0266304268,-0.0041983172,0.0081780038,-0.053653828,0.0585997371,-0.0221985978,0.0550290126,-0.0374760608,-0.0231069601,0.011737091,0.0432009194,-0.0179196776,-0.0936935906,0.0417798698,0.0297410648,-0.0679075028,0.0045816206,0.0148608735,0.014278856,-0.0659822067,-0.0040588759,0.0265264403,-0.0267435756,-0.0001892147,0.0014156825,0.0404897748,-0.0661061824,-0.0430820588,0.0242771686,0.0438275062,0.03532038,-0.0121504342,-0.0300529456,-0.0037326257,0.0072787756,-0.0495529449,-0.0276787371,0.0111502063,0.0156513445,-0.0129761122,0.0239273691,-0.0121991832,0.0063998803,0.015561113,0.0651846912,-0.0122854625,0.0024764923,0.0057428335,0.0446513723,-0.0395856878,-0.0217523055,0.0184250458,-0.0251550699,0.0301515113,0.0265751878,-0.0011817749,0.0212324286,-0.0265815184,0.0179507754,-0.0471860271,0.0357018509,-0.0042128306,0.0243144288,0.0396263519,-0.0064591355,0.0245196082,-0.0280873048,-0.0019647541,0.0157387698,-0.0251122126,0.0129904713,0.0006029433,0.0452210252,0.0078553102,0.0586736892,-0.0134236031,-0.0022979493,0.0685914934,0.0263240623,0.0134338614,-0.0318727738,0.0137067664,-0.0083750316,0.024957846,0.0053780542,-0.0032800838,0.0139772722,0.0269630278,0.035069745,0.0172666907,-0.0302184593,-0.0090851599,0.0186579499,-0.0136311963,0.0190575231,-0.011111766,0.004163966,0.004579228,-0.0003812271]},"385":{"Abstract":"In this paper, we propose a novel machine learning-based voxel classification method for highly-accurate volume rendering. Unlike conventional voxel classification methods that incorporate intensity-based features, the proposed method employs dictionary based features learned directly from the input data using hierarchical multi-scale 3D convolutional sparse coding, a novel extension of the state-of-the-art learning-based sparse feature representation method. The proposed approach automatically generates high-dimensional feature vectors in up to 75 dimensions, which are then fed into an intelligent system built on a random forest classifier for accurately classifying voxels from only a handful of selection scribbles made directly on the input data by the user. We apply the probabilistic transfer function to further customize and refine the rendered result. The proposed method is more intuitive to use and more robust to noise in comparison with conventional intensity-based classification methods. We evaluate the proposed method using several synthetic and real-world volume datasets, and demonstrate the methods usability through a user study.","Authors":"T. M. Quan; J. Choi; H. Jeong; W. Jeong","DOI":"10.1109\/TVCG.2017.2744078","Keywords":"Volume Rendering;Machine Learning;Hierarchically Convolutional Sparse Coding","Title":"An Intelligent System Approach for Probabilistic Volume Rendering Using Hierarchical 3D Convolutional Sparse Coding","Keywords_Processed":"hierarchically Convolutional Sparse coding;volume Rendering;Machine Learning","Keyword_Vector":[0.0900295488,0.0133995746,-0.0100187363,0.0612190127,0.1201576321,-0.0459700602,-0.0574544649,0.0924102347,-0.0610483807,-0.0671188868,-0.0239726335,0.0336009861,-0.0113729521,0.0065469948,0.0552621825,0.0692098213,-0.0476689155,-0.0593852768,0.1930125922,-0.0904752994,0.0200747555,-0.0786682271,0.048657394,0.0681110527,0.0179618659,0.0757618805,0.0138040291,-0.0098250949,-0.0791735058,0.073562832,-0.0449291756,-0.000485456,-0.1190304845,-0.0688355146,0.0271592309,0.0461705425,-0.1098440587,0.060685309,-0.0333816742,0.0444116154,0.0834716773,-0.0265614722,-0.0365973904,-0.0123353857,0.0718121516,-0.054536051,-0.0177178584,0.034585551,0.0733689476,-0.0415121256,-0.0174455403,0.02188764],"Abstract_Vector":[0.1667825358,-0.0302283832,-0.0157149904,-0.0020693903,-0.0307353048,0.0645890082,-0.0222015023,0.0077338005,0.1185657778,-0.0419947557,-0.0439269263,-0.0148125244,-0.0826809472,0.0042135533,0.0150105334,-0.0091367048,-0.0005276502,0.0551356752,0.0488289287,-0.0017668873,-0.0028192565,0.0034500522,-0.0023066396,-0.021088902,0.0219429648,-0.027321589,0.0263438615,0.0433371133,-0.0076512188,-0.0499791053,-0.0173121671,-0.02850571,-0.0269279686,-0.0774844748,-0.0171208377,0.0154759669,-0.0039905082,-0.0131775802,-0.0212166246,-0.0246437515,-0.0034250695,0.0068734923,0.0244321786,-0.0017026084,-0.0419041322,-0.0098516659,-0.0213580812,-0.0103236575,-0.0169161531,0.0181604219,0.0443910217,-0.0113864307,-0.0009741817,0.019310949,0.0223027312,0.0038012413,-0.0210913362,-0.0267162123,-0.0079881957,0.0080784581,0.0206637337,-0.0003609028,-0.0128834422,-0.0108364024,-0.0137191952,-0.0634984661,0.0362297472,-0.0034907101,0.0065258668,-0.009835121,0.0364045689,-0.0241470522,-0.0309131569,0.0391796438,-0.014506845,0.0359570773,-0.0451957051,-0.0293259769,0.0475212575,-0.0140120672,0.0051715498,0.0046746288,0.0196652328,-0.0200817746,-0.013903806,0.0347972979,-0.0096287225,-0.025308927,0.0112616091,0.0061947532,-0.0134334508,0.0309213589,-0.0071731207,-0.0145105632,0.0018418075,0.0071809975,-0.016507449,-0.0105245275,-0.0141522782,0.0041964496,-0.0130989416,0.0332285317,0.0362243008,0.0274243954,0.0099237478,-0.0311343865,-0.0008456777,0.0088006042,0.0060989694,0.0222105091,-0.0185453649,0.0084186615,0.0247112986,0.0012379599,0.0526963768,0.0058838417]},"386":{"Abstract":"This paper presents Abstractocyte, a system for the visual analysis of astrocytes and their relation to neurons, in nanoscale volumes of brain tissue. Astrocytes are glial cells, i.e., non-neuronal cells that support neurons and the nervous system. The study of astrocytes has immense potential for understanding brain function. However, their complex and widely-branching structure requires high-resolution electron microscopy imaging and makes visualization and analysis challenging. Furthermore, the structure and function of astrocytes is very different from neurons, and therefore requires the development of new visualization and analysis tools. With Abstractocyte, biologists can explore the morphology of astrocytes using various visual abstraction levels, while simultaneously analyzing neighboring neurons and their connectivity. We define a novel, conceptual 2D abstraction space for jointly visualizing astrocytes and neurons. Neuroscientists can choose a specific joint visualization as a point in this space. Interactively moving this point allows them to smoothly transition between different abstraction levels in an intuitive manner. In contrast to simply switching between different visualizations, this preserves the visual context and correlations throughout the transition. Users can smoothly navigate from concrete, highly-detailed 3D views to simplified and abstracted 2D views. In addition to investigating astrocytes, neurons, and their relationships, we enable the interactive analysis of the distribution of glycogen, which is of high importance to neuroscientists. We describe the design of Abstractocyte, and present three case studies in which neuroscientists have successfully used our system to assess astrocytic coverage of synapses, glycogen distribution in relation to synapses, and astrocytic-mitochondria coverage.","Authors":"H. Mohammed; A. K. Al-Awami; J. Beyer; C. Cali; P. Magistretti; H. Pfister; M. Hadwiger","DOI":"10.1109\/TVCG.2017.2744278","Keywords":"Connectomics;Neuroscience;Data Abstraction;Interactive 3D Visualization","Title":"Abstractocyte: A Visual Tool for Exploring Nanoscale Astroglial Cells","Keywords_Processed":"interactive 3d visualization;Data Abstraction;connectomic;Neuroscience","Keyword_Vector":[0.2028202991,-0.1801967206,-0.0869122889,0.106764678,-0.1265967921,-0.0717775442,0.0856933566,-0.0187668891,0.1134762605,0.0213933681,0.0394743412,0.0114750003,-0.0472282535,-0.0514675785,0.0991424434,0.2085188399,0.0107767695,-0.0188609345,-0.1246085322,0.0726012414,0.0050257344,0.0229227276,0.0710419826,0.0835984838,-0.0455743829,0.0531919763,0.0111168082,-0.0471546243,-0.0263923022,-0.0367089044,-0.0157072659,0.0292619842,-0.0044330765,0.0275748364,0.0681910944,0.0226753904,0.0020559332,0.0122188742,0.0127656891,-0.0745947136,-0.0163583997,0.0282977732,0.0212304571,-0.0504056649,-0.0262548406,-0.0845108933,-0.0092234355,0.0240907241,0.0382238011,-0.0709146277,0.0318816243,0.0556018194],"Abstract_Vector":[0.194845858,-0.1379110547,0.0181251921,-0.0213669503,-0.0008942409,-0.112105003,0.0166413784,-0.0536089022,0.0336570833,0.1417575801,-0.0197315894,0.0290813868,-0.1338511696,0.0949580951,-0.0362426048,-0.0507998744,0.0435081461,-0.0402701058,-0.0732310982,-0.0234158982,-0.0115297447,-0.0369200182,0.0524702623,0.0057966163,-0.0342402411,0.0205382836,0.0442242967,0.0186311134,-0.0305031848,-0.0267783767,-0.0135861913,0.0304230192,-0.0194614632,-0.0032552861,-0.0127999492,0.0412636302,-0.0759060635,0.0269248233,-0.0530494012,-0.0114755198,0.0413580991,-0.0152807645,0.0272486266,0.0111844245,0.0780696501,0.010251038,0.0425061209,-0.1162918865,0.0504245786,-0.069370572,-0.0557787746,-0.0186952138,-0.0295675264,0.0539106961,-0.0604110868,0.0014726447,0.0392031331,0.0474035687,0.014534299,0.0785131511,-0.0387232197,0.0317686677,0.0373113512,-0.0570914499,-0.0295305439,-0.1063067122,0.0087933449,0.0198069451,-0.0588314594,-0.0862871727,0.0199035241,-0.076520752,-0.0007993296,0.0388473308,0.0392928586,-0.0297484052,0.0168673095,0.0085077921,0.0673866792,0.0411977852,0.0324921066,-0.0162576117,-0.0106583263,-0.0389940916,0.0886193481,-0.0322359199,0.0207439836,-0.0124345731,0.0150394684,-0.0237259441,0.0442600075,0.0369521655,-0.0231420538,-0.025088486,0.003347965,0.0083438517,0.0737057574,-0.0019055121,0.0274926669,-0.001501247,0.0022404324,0.0212349152,-0.0535453071,0.010334386,0.0052436061,-0.0334563471,0.0177533104,0.0276459879,-0.0127757073,-0.036443587,-0.0334251228,0.0136765929,0.0334608315,-0.0357184872,-0.0338552114,-0.0069716885]},"387":{"Abstract":"Immersive virtual- and augmented-reality headsets can overlay a flat image against any surface or hang virtual objects in the space around the user. The technology is rapidly improving and may, in the long term, replace traditional flat panel displays in many situations. When displays are no longer intrinsically flat, how should we use the space around the user for abstract data visualisation? In this paper, we ask this question with respect to origin-destination flow data in a global geographic context. We report on the findings of three studies exploring different spatial encodings for flow maps. The first experiment focuses on different 2D and 3D encodings for flows on flat maps. We find that participants are significantly more accurate with raised flow paths whose height is proportional to flow distance but fastest with traditional straight line 2D flows. In our second and third experiment we compared flat maps, 3D globes and a novel interactive design we call MapsLink, involving a pair of linked flat maps. We find that participants took significantly more time with MapsLink than other flow maps while the 3D globe with raised flows was the fastest, most accurate, and most preferred method. Our work suggests that careful use of the third spatial dimension can resolve visual clutter in complex flow maps.","Authors":"Y. Yang; T. Dwyer; B. Jenny; K. Marriott; M. Cordeil; H. Chen","DOI":"10.1109\/TVCG.2018.2865192","Keywords":"Origin-destination;Flow Map;Virtual Reality;Cartographic Information Visualisation;Immersive Analytics","Title":"Origin-Destination Flow Maps in Immersive Environments","Keywords_Processed":"virtual reality;immersive Analytics;flow Map;Cartographic Information visualisation;origin destination","Keyword_Vector":[0.0755550028,0.0077039915,-0.0257086146,0.0256644163,0.0605425917,0.014308725,-0.0219705347,0.0989416586,-0.0946427367,-0.0437967059,-0.1014493099,-0.0394122374,0.0265028113,0.0206470753,0.101843214,-0.0844645203,0.0767696033,0.1318163272,-0.0065834093,0.0225733437,0.1022726252,-0.0153836472,-0.0795738306,0.0536936423,-0.0197818165,0.0257792578,-0.0222299284,-0.0194985429,0.0049402139,0.0332882136,0.0343425342,0.0526850002,0.0337762305,0.0533517024,-0.02570396,-0.0582533346,-0.0797949181,0.0555336953,-0.0013103201,-0.0456993234,0.0330991338,-0.0125603611,-0.0402313414,-0.0733933343,-0.0641124767,-0.0120458533,-0.0101242622,-0.0804388741,-0.0074764197,0.0486427868,-0.0059284769,-0.0077199426],"Abstract_Vector":[0.1655726849,0.0429487925,0.0434077655,-0.0087021722,-0.0067887423,0.0061279023,0.0305288455,0.0133782108,0.0143301289,0.0118211716,-0.0279216188,0.0327075508,-0.0190276845,-0.0069096285,0.0043940188,-0.0334975011,0.0075729005,0.050804486,-0.0157882934,-0.0646819155,0.0197095041,-0.0059817991,0.0060751958,0.0559798901,0.0440248929,-0.0141157546,0.0255030195,0.0356551225,0.02863709,0.0017304028,-0.0176441048,0.0086961811,-0.0443001997,-0.0137679467,0.00651315,-0.0360609856,-0.0223942125,-0.0040934752,0.0181244816,0.0242728174,0.0297641064,0.0427934858,0.0139604591,-0.0022553349,0.0215843562,-0.0140949197,0.0142540093,0.0690611616,-0.0013523366,0.012095807,-0.0129723222,-0.0018321823,0.0154832196,0.0136989892,0.0264906723,-0.0025160368,0.0190282922,-0.0145167299,0.0503240626,-0.0136180166,0.0217976286,-0.0328929399,0.0454649535,-0.0169330377,-0.0297243983,-0.0231333171,0.015371594,0.0005604508,-0.0048217212,0.0173991047,-0.0311635181,0.0358069079,-0.0023713708,-0.0221392451,-0.0067562516,-0.0182958648,0.0101959282,0.0274655596,0.0207958281,-0.0240479229,0.0034365084,0.0351192754,0.0410593939,0.0167938888,-0.0358709204,0.0020416762,0.0097672969,0.0117011221,0.0263899812,-0.0188192612,0.0110998187,0.0200296911,-0.0115063538,0.0093088629,0.0216213166,-0.0005009805,-0.0096711508,0.0152669753,0.0181403354,0.0501527823,0.0112718723,-0.0099351006,-0.0159441142,0.0117374963,-0.001460957,0.0002702072,0.0002327824,-0.0221957602,-0.0034656813,-0.0039312369,0.0138768759,0.0253081085,-0.0051490818,0.0303039944,0.0176017337,-0.0004776937]},"388":{"Abstract":"The arrangement of objects into a layout can be challenging for non-experts, as is affirmed by the existence of interior design professionals. Recent research into the automation of this task has yielded methods that can synthesize layouts of objects respecting aesthetic and functional constraints that are non-linear and competing. These methods usually adopt a stochastic optimization scheme, which samples from different layout configurations, a process that is slow and inefficient. We introduce an physics-motivated, continuous layout synthesis technique, which results in a significant gain in speed and is readily scalable. We demonstrate our method on a variety of examples and show that it achieves results similar to conventional layout synthesis based on Markov chain Monte Carlo (McMC) state-search, but is faster by at least an order of magnitude and can handle layouts of unprecedented size as well as tightly-packed layouts that can overwhelm McMC.","Authors":"T. Weiss; A. Litteneker; N. Duncan; M. Nakada; C. Jiang; L. Yu; D. Terzopoulos","DOI":"10.1109\/TVCG.2018.2866436","Keywords":"Automatic layout synthesis;3D scene modeling;automatic content creation;position-based methods;constraints","Title":"Fast and Scalable Position-Based Layout Synthesis","Keywords_Processed":"automatic layout synthesis;position base method;3d scene model;constraint;automatic content creation","Keyword_Vector":[0.1494691815,-0.0832131185,-0.0285582243,-0.0077948834,0.0326233994,-0.0174973053,0.0408960747,-0.0462523397,-0.039350637,-0.0115806687,-0.0079364247,-0.0103086474,0.0034238702,0.1484343869,-0.0765241864,-0.0088921579,0.0454269102,-0.0004859841,0.0016928533,0.046780607,-0.0896362952,-0.0836617755,-0.0763093547,0.0277257341,0.0828901571,-0.1072173861,-0.0237539545,-0.0360859352,-0.0391259742,-0.1182226235,0.0754079346,-0.0787866094,-0.0909197484,0.1664559864,0.2927223416,-0.0386587965,-0.0849204767,-0.1144690299,-0.102578125,0.0567336863,-0.0946477616,0.0781441172,-0.0724380657,0.0104913524,-0.0230146723,0.1087011622,-0.0709868718,0.0272418096,0.0441239751,-0.0900452687,0.0202221117,0.0368620329],"Abstract_Vector":[0.169203468,-0.0735790607,-0.0168210857,0.0587242921,0.0081181901,0.0233154604,0.0250450794,-0.0016079566,-0.0200606808,0.0267077165,-0.0122681648,0.0884455751,0.0167140854,-0.008142286,-0.0164631787,0.0324110989,0.0047975348,0.0590768808,0.027259034,0.0023796976,0.1077216001,0.0601352124,0.0515345608,-0.0350424065,0.0415122503,0.082278685,-0.0440840863,-0.0251523515,-0.0087130708,-0.0305026609,-0.033281482,-0.0282580192,-0.0564129156,0.0016038273,-0.0847349399,-0.0744783287,-0.0548812254,-0.0520361637,0.0732435786,0.0243885179,-0.009871706,-0.0183821906,0.0910729112,-0.0304811048,0.0198139263,0.1027858124,-0.0266970588,-0.0368826277,-0.0355061846,-0.0460303965,0.0116549566,0.002690958,-0.0224840912,-0.0262233575,-0.0110042367,-0.0026099696,-0.0075842918,0.0449147148,-0.0146486089,0.0143871133,-0.010985583,-0.0469008931,-0.0253806497,0.0263358588,0.0255017432,0.0284824888,0.0233984126,0.013081725,0.039322745,0.0651001708,-0.0155064741,-0.0019112785,-0.0449695106,-0.0105765536,0.0039157473,-0.0337155246,-0.0266906696,-0.0149526025,0.0129736842,0.0084578889,0.0409585496,0.0393758691,-0.0446562052,-0.0208087578,0.0195266875,0.008566553,0.0377855654,0.0188156199,0.0132004045,-0.0217378211,0.0180804979,-0.0239983997,0.013568004,0.0037897958,-0.021237573,-0.0337053494,0.0001502611,-0.0362960634,-0.0101481071,-0.034718126,0.0295044344,0.0069141662,0.0308683073,-0.014738743,0.0210347011,-0.0509124944,-0.0238870236,-0.016653974,-0.0203686432,0.0078769741,0.0539899234,-0.0056888788,0.0277856163,-0.0152721816,0.0021593182,0.0207220129]},"389":{"Abstract":"The use of scatterplots is an important method for multivariate data visualization. The point distribution on the scatterplot, along with variable values represented by each point, can help analyze underlying patterns in data. However, determining the multivariate data variation on a scatterplot generated using projection methods, such as multidimensional scaling, is difficult. Furthermore, the point distribution becomes unclear when the data scale is large and clutter problems occur. These conditions can significantly decrease the usability of scatterplots on multivariate data analysis. In this study, we present a cluster-based visual abstraction method to enhance the visualization of multivariate scatterplots. Our method leverages an adapted multilabel clustering method to provide abstractions of high quality for scatterplots. An image-based method is used to deal with large scale data problem. Furthermore, a suite of glyphs is designed to visualize the data at different levels of detail and support data exploration. The view coordination between the glyph-based visualization and the table lens can effectively enhance the multivariate data analysis. Through numerical evaluations for data abstraction quality, case studies and a user study, we demonstrate the effectiveness and usability of the proposed techniques for multivariate data analysis on scatterplots.","Authors":"H. Liao; Y. Wu; L. Chen; W. Chen","DOI":"10.1109\/TVCG.2017.2754480","Keywords":"Data abstraction;scatterplot;glyph visualization;multilabel optimization","Title":"Cluster-Based Visual Abstraction for Multivariate Scatterplots","Keywords_Processed":"glyph visualization;scatterplot;multilabel optimization;datum abstraction","Keyword_Vector":[0.2995658077,-0.0819417348,0.0451575052,0.2090056364,-0.0582867346,0.3754918807,-0.0874041079,-0.1412676606,-0.0585517977,0.0223074515,0.0166739699,-0.0093598498,-0.0743612604,-0.1549944223,0.0211770846,0.1598758755,0.0252833198,0.0124338956,0.0359313692,-0.0158325647,0.0399790088,-0.0819894553,0.0821099715,0.0009556994,-0.0325426214,-0.0123947086,-0.0519401723,0.0493812957,-0.084206862,0.1024086759,-0.0200936426,-0.0326925612,-0.0485711835,-0.0031721626,0.0428867554,0.1289462606,0.0489903154,-0.029046756,-0.0706654596,-0.0521602556,0.0868383716,-0.0594786276,0.1007435028,0.0359238022,-0.1187506219,-0.0338442296,-0.1003425934,-0.0240512579,-0.0203627565,-0.0022292753,0.0195521382,-0.0771166265],"Abstract_Vector":[0.2539554143,-0.0733144167,-0.0269091997,-0.0299887389,0.0294077659,-0.1595993759,0.038723834,-0.0177156351,0.0735631796,0.0611447706,0.0363976317,0.0534777223,-0.0203832552,0.0528633702,0.0206085712,-0.0747779456,-0.0136265485,0.0421583031,-0.0180443273,-0.0432085595,0.0543457652,-0.0414385996,-0.019334081,0.015687891,0.0360566274,-0.0076465472,0.0254502937,0.0005344355,0.0140660465,0.0033828978,-0.019475964,0.0219145662,0.0656163132,0.0189195474,-0.0590514083,0.0033612491,0.0371376139,0.0358669974,-0.0809142983,0.0029353547,-0.008167819,0.0123447364,-0.0289181391,0.0202818835,0.0740968442,-0.0168003362,-0.0457243445,-0.0322798367,-0.0497897243,-0.0337746701,-0.0780789503,0.0204055941,0.0125330152,-0.047724342,0.0019105733,0.0195473755,0.0293296114,-0.0052749588,-0.0090390578,-0.0176617403,-0.0142343632,0.0642397478,0.0098892722,-0.0061103159,0.0830722186,-0.0126981639,-0.0174815097,0.0308623927,0.0143118936,0.0126970511,0.0274447781,0.0078611943,0.0347499075,0.0000618063,0.0056132384,-0.0125886242,-0.026580337,-0.0217441669,-0.0374566228,0.0085759408,0.0232405441,0.027553672,0.0240504282,0.0809136704,-0.0528359427,-0.0460896539,0.0740260278,-0.0034213866,0.0403359163,-0.0069644043,-0.0379811547,0.0378778098,0.0180827647,0.0265778882,0.021914784,-0.0613354753,-0.0143398795,-0.0153623026,-0.0386942913,-0.0280274722,0.016720761,0.0275241519,0.0530510077,0.0272810826,-0.0380245659,0.020033668,0.0311938726,-0.0527907533,-0.0005223172,0.0217754838,0.0362072283,-0.0062592316,-0.0078605169,0.0149665852,0.0239388158,-0.0004136019]},"39":{"Abstract":"We introduce a set of integrated interaction techniques to interpret and interrogate dimensionality-reduced data. Projection techniques generally aim to make a high-dimensional information space visible in form of a planar layout. However, the meaning of the resulting data projections can be hard to grasp. It is seldom clear why elements are placed far apart or close together and the inevitable approximation errors of any projection technique are not exposed to the viewer. Previous research on dimensionality reduction focuses on the efficient generation of data projections, interactive customisation of the model, and comparison of different projection techniques. There has been only little research on how the visualization resulting from data projection is interacted with. We contribute the concept of probing as an integrated approach to interpreting the meaning and quality of visualizations and propose a set of interactive methods to examine dimensionality-reduced data as well as the projection itself. The methods let viewers see approximation errors, question the positioning of elements, compare them to each other, and visualize the influence of data dimensions on the projection space. We created a web-based system implementing these methods, and report on findings from an evaluation with data analysts using the prototype to examine multidimensional datasets.","Authors":"J. Stahnke; M. D\u00f6rk; B. M\u00fcller; A. Thom","DOI":"10.1109\/TVCG.2015.2467717","Keywords":"Information visualization;interactivity;dimensionality reduction;multidimensional scaling;Information visualization;interactivity;dimensionality reduction;multidimensional scaling","Title":"Probing Projections: Interaction Techniques for Interpreting Arrangements and Errors of Dimensionality Reductions","Keywords_Processed":"dimensionality reduction;interactivity;information visualization;multidimensional scaling","Keyword_Vector":[0.1558047241,0.0388103512,-0.0610607666,0.0468376198,-0.0090233967,0.0114209122,-0.0002843594,-0.0293760116,-0.0194162839,-0.0031653586,0.0517300509,-0.0021857477,0.0209377649,0.1519183029,0.021334824,0.1347250134,-0.0180172159,-0.117128912,0.0673099689,-0.0743579981,0.0610354833,0.006219944,-0.078395428,0.0372392488,-0.0641939188,0.0232469799,0.1567268988,-0.016119249,-0.0645791409,0.0513484465,-0.027183875,-0.0332505515,-0.0256715791,-0.0312709646,0.0687482717,0.0123059564,-0.054202227,0.0308401217,0.0494838726,0.0491694781,-0.015760367,0.0027278524,0.0161396755,-0.0054965299,0.0124222579,-0.0500280948,-0.0486795556,-0.0726385824,-0.023442273,-0.0417981152,0.0554886581,-0.0343655524],"Abstract_Vector":[0.1522088019,0.0136700738,0.0005351354,0.0046014821,0.012010488,0.0221904316,0.0136316462,-0.0032931832,0.0846448949,0.0431822715,-0.0551389428,0.0294745532,-0.0560591688,0.0305479785,0.0109897711,-0.0487847604,0.0208932581,-0.0000053961,-0.0146740285,-0.0173883883,-0.0062187735,0.0059448127,0.0455488851,0.0381646656,-0.0178011326,-0.0220615181,-0.0056047011,0.0275990023,-0.0055977046,0.0226059043,0.0190460266,0.0442211391,0.0091991328,0.0212296192,-0.0048077556,-0.0003964365,0.0058719817,-0.0244330572,-0.0072807489,0.0233005686,-0.0045342617,0.0136601896,0.0075030191,0.005886235,0.053061345,0.0079821161,-0.0007146155,-0.0121289796,-0.0071856233,0.0166340649,-0.0372681619,0.0674663486,-0.0464651408,-0.0456578977,-0.020508286,-0.0178470241,0.0334396754,-0.0209805532,-0.0391398606,0.0187839343,-0.0191030035,0.0130171061,0.0029594773,0.0503929945,-0.0286667765,-0.0169728321,-0.0024316429,-0.0039423111,0.0112988811,0.0178263686,0.0005423951,-0.0142935604,-0.0058961619,-0.0009290868,0.0117630226,-0.01211596,-0.0045167925,0.0338082734,0.0193661078,0.026239689,0.0015273713,0.0007849083,0.0206754171,0.0120523112,-0.007674072,-0.0079042419,-0.0006428978,-0.0054964025,-0.0359101742,-0.0014205979,-0.0079408247,-0.0109696115,0.0175282159,0.0352301795,-0.0036346977,0.0019553026,0.001837615,-0.0140057223,-0.0285498835,-0.0085490528,0.001337431,0.0036470673,0.0066838924,-0.0203590906,-0.0162125747,-0.0200888424,-0.0163368883,0.0101612624,-0.0088591152,-0.031861605,0.0172680509,-0.0180753673,0.0025294162,0.0057963078,-0.0104467934,-0.0127643028]},"390":{"Abstract":"We present TimeLineCurator, a browser-based authoring tool that automatically extracts event data from temporal references in unstructured text documents using natural language processing and encodes them along a visual timeline. Our goal is to facilitate the timeline creation process for journalists and others who tell temporal stories online. Current solutions involve manually extracting and formatting event data from source documents, a process that tends to be tedious and error prone. With TimeLineCurator, a prospective timeline author can quickly identify the extent of time encompassed by a document, as well as the distribution of events occurring along this timeline. Authors can speculatively browse possible documents to quickly determine whether they are appropriate sources of timeline material. TimeLineCurator provides controls for curating and editing events on a timeline, the ability to combine timelines from multiple source documents, and export curated timelines for online deployment. We evaluate TimeLineCurator through a benchmark comparison of entity extraction error against a manual timeline curation process, a preliminary evaluation of the user experience of timeline authoring, a brief qualitative analysis of its visual output, and a discussion of prospective use cases suggested by members of the target author communities following its deployment.","Authors":"J. Fulda; M. Brehmer; T. Munzner","DOI":"10.1109\/TVCG.2015.2467531","Keywords":"System;timelines;authoring environment;time-oriented data;journalism;System;timelines;authoring environment;time-oriented data;journalism","Title":"TimeLineCurator: Interactive Authoring of Visual Timelines from Unstructured Text","Keywords_Processed":"time orient datum;timeline;journalism;system;author environment","Keyword_Vector":[0.0229354424,0.0052151672,0.0044023391,0.0192214543,0.0283456769,-0.0088244319,-0.0044361288,0.0086955482,-0.0130013166,-0.0087746628,0.0014998496,0.011305805,-0.0014391704,-0.0129578921,-0.0108459104,0.0099353437,0.0066571764,-0.0089918693,0.0031717508,-0.0017113061,0.022322107,-0.0098227239,0.0233156508,0.0063342019,0.0241265008,0.0096146138,-0.0127664329,-0.0049701759,0.0045913402,-0.0239748175,-0.0092162354,0.007342065,-0.0066707507,-0.0235081874,0.0252349821,0.0745796547,0.0704001827,-0.0146040449,-0.0422687082,-0.0216577122,0.0483983288,0.0114298222,-0.0365639324,0.0530767997,-0.010735833,0.0062441893,-0.0562404766,0.001329944,-0.0340197045,-0.02274813,0.0253708366,-0.0055292471],"Abstract_Vector":[0.2075657639,-0.056220551,-0.0102306749,0.0034862721,-0.0203566762,-0.0047586094,0.0120635544,0.018549506,-0.0206623775,0.0047742738,-0.0205448952,-0.0045488192,0.0098543205,-0.0018343,-0.0172392648,0.0077354983,0.0053738777,0.0295868253,-0.0032046271,-0.0011554778,0.0095258704,-0.0360558322,0.0357877077,-0.0231498308,0.0229946152,0.0334181685,-0.0123638243,-0.0245263075,-0.0556415956,-0.0193027536,0.0199362493,0.0102197636,-0.0502232832,0.0158090422,-0.0081924751,0.034035291,0.0179441293,0.0319628825,-0.0116874071,0.0273352423,-0.0260790802,0.0192970446,0.0346612752,-0.0758105281,0.0672574783,-0.0248450996,0.0342356502,0.0477557203,-0.0306670301,-0.0147804885,-0.0177827222,-0.0081242825,-0.0067152658,0.0358348542,0.0278594799,-0.0257246329,0.0384645808,-0.0198000495,0.0092683867,-0.0098438272,0.0174945419,0.049590389,-0.005286241,0.0546006274,0.0004955099,0.0060354266,-0.0461902827,0.0368669756,0.0323453818,-0.0121959104,0.0108096564,0.0484076373,-0.0122155657,0.0008971344,0.0095816249,-0.0370797876,-0.0110766794,-0.0241435084,-0.0173813249,0.0204710336,-0.0181599665,-0.0037340594,-0.0094147939,0.0220987577,0.0100105791,-0.0037394571,0.0051993455,0.0276417377,-0.0113999652,-0.0117226803,0.008720656,0.0391529131,-0.0357983745,0.0090115319,-0.0142874218,0.0443541649,0.0085583415,0.0002440536,-0.0357955945,0.0490555495,0.002035082,0.0395841217,-0.012748914,0.0072898148,-0.0049457494,-0.0192998051,0.0153255387,-0.0361141369,-0.0049911278,0.0040103577,0.0149277097,0.0226073614,-0.0209345754,-0.0083918145,0.0220228547,0.0190178544]},"391":{"Abstract":"A notable recent trend in time-varying volumetric data analysis and visualization is to extract data relationships and represent them in a low-dimensional abstract graph view for visual understanding and making connections to the underlying data. Nevertheless, the ever-growing size and complexity of data demands novel techniques that go beyond standard brushing and linking to allow significant reduction of cognition overhead and interaction cost. In this paper, we present a mining approach that automatically extracts meaningful features from a graph-based representation for exploring time-varying volumetric data. This is achieved through the utilization of a series of graph analysis techniques including graph simplification, community detection, and visual recommendation. We investigate the most important transition relationships for time-varying data and evaluate our solution with several time-varying data sets of different sizes and characteristics. For gaining insights from the data, we show that our solution is more efficient and effective than simply asking users to extract relationships via standard interaction techniques, especially when the data set is large and the relationships are complex. We also collect expert feedback to confirm the usefulness of our approach.","Authors":"Y. Gu; C. Wang; T. Peterka; R. Jacob; S. H. Kim","DOI":"10.1109\/TVCG.2015.2468031","Keywords":"Time-varying data visualization;graph simplification;community detection;visual recommendation;Time-varying data visualization;graph simplification;community detection;visual recommendation","Title":"Mining Graphs for Understanding Time-Varying Volumetric Data","Keywords_Processed":"time vary datum visualization;community detection;graph simplification;visual recommendation","Keyword_Vector":[0.0948064919,-0.0565624541,-0.0387104355,0.1160345724,0.0227761925,0.1585992462,-0.0099833182,0.1064203883,-0.0809920108,-0.0094709234,-0.0887748589,0.0103082127,-0.000420489,-0.0401934447,0.054940839,-0.1212862161,0.0103317082,0.0287475714,0.0233814132,-0.0361211718,0.0025135711,-0.0689324258,-0.0317280138,-0.0214362309,-0.0934664037,-0.0381794456,0.0344841753,-0.0524615281,0.0194601054,-0.0463888585,0.019615446,-0.0298869358,-0.0184047084,0.0558627795,-0.0596646469,-0.0398254871,-0.0066871139,-0.030748558,0.0229107345,0.0458523379,0.0367970173,-0.0285719474,0.0581720936,-0.0470823854,-0.0140708162,-0.0634149136,-0.0031970033,0.0161240974,-0.0285002203,0.0332653937,-0.0018560768,0.0537788962],"Abstract_Vector":[0.1944930491,-0.1034165159,-0.0178816959,-0.0085268655,-0.0226742306,-0.024489715,-0.0409029291,-0.0223234949,0.0263057555,-0.0079792601,-0.0302954004,-0.0515981504,-0.0498346635,-0.0255068148,-0.0143535771,0.0538633922,-0.0246274589,-0.0148467398,0.0313560844,0.0324139609,0.0066619551,-0.0079891504,0.0411827268,0.0218131277,-0.0194295092,-0.0100737905,-0.0614525631,0.0807138894,0.0087573613,0.0087205402,-0.0008081865,0.0332257731,0.0131717637,-0.0179919495,0.0637074928,-0.0142611738,-0.0092360153,-0.0427007927,0.0072549001,0.02164199,-0.0135608959,0.0150051109,0.0332450369,0.0194094042,0.0170382467,-0.0019316695,-0.0090697635,0.015710394,0.0197658893,-0.0441090723,-0.0014846001,0.0141730419,0.0328488186,-0.0023741484,-0.0158051036,-0.01205789,0.0114951562,-0.0004081614,-0.000661009,-0.0014746435,-0.0013751653,-0.0209896907,-0.0045974705,0.0039023073,0.0359414199,-0.0080334285,0.0608386097,0.0086637784,-0.0293101185,0.0096322165,-0.0166057316,-0.0118063917,-0.0390022988,-0.0116671668,-0.0101679462,-0.0073409715,0.0279498651,-0.0189090072,0.0026527027,-0.0143361769,0.031308008,0.0175660276,0.0483266586,-0.0229764479,0.0082782545,0.019726863,0.0268046225,0.021242051,-0.0327310846,0.0207601176,0.0002058734,-0.0303187874,-0.0029007449,0.0087964444,-0.0344567526,0.0263435335,-0.0006803869,-0.0049400879,0.0133251838,-0.0101808863,0.0420885703,0.0045719897,-0.0058339833,0.0016342613,0.0330371673,-0.0067415849,-0.0033428163,0.0094218791,-0.0057752908,0.0208829822,-0.0025988128,0.0057594314,0.0098219099,-0.0045969467,0.0059176818,-0.0072151206]},"392":{"Abstract":"We investigate whether the notion of active reading for text might be usefully applied to visualizations. Through a qualitative study we explored whether people apply observable active reading techniques when reading paper-based node-link visualizations. Participants used a range of physical actions while reading, and from these we synthesized an initial set of active reading techniques for visualizations. To learn more about the potential impact such techniques may have on visualization reading, we implemented support for one type of physical action from our observations (making freeform marks) in an interactive node-link visualization. Results from our quantitative study of this implementation show that interactive support for active reading techniques can improve the accuracy of performing low-level visualization tasks. Together, our studies suggest that the active reading space is ripe for research exploration within visualization and can lead to new interactions that make for a more flexible and effective visualization reading experience.","Authors":"J. Walny; S. Huron; C. Perin; T. Wun; R. Pusch; S. Carpendale","DOI":"10.1109\/TVCG.2017.2745958","Keywords":"active reading of visualizations;active reading;information visualization;spectrum of physical engagement","Title":"Active Reading of Visualizations","Keywords_Processed":"active reading;active reading of visualization;spectrum of physical engagement;information visualization","Keyword_Vector":[0.0616701803,-0.032012838,-0.0265905999,0.0817611034,0.0447008525,0.0582157461,0.014193274,0.1258656055,-0.0610496042,-0.0375062231,-0.0947433146,-0.0144920873,-0.0407711476,-0.0289310076,0.0268893283,-0.1088369844,-0.016946142,0.0680803726,0.014986515,-0.0524701985,0.0280945417,-0.037878482,-0.0822030195,-0.0029591341,-0.0688775471,-0.0226245267,0.066880781,-0.0808293408,-0.0413944639,0.0209621316,0.0538945605,-0.0729821413,-0.0040062318,0.0746828576,-0.0313748895,-0.0808809161,0.0082864834,-0.0007455288,0.0859498564,-0.0020549026,-0.0132654549,-0.0403406903,0.1124971745,0.019493451,0.0637592564,-0.0764263935,0.0248639612,-0.0315872836,-0.0874977388,0.0150509382,0.0520892232,0.0050940597],"Abstract_Vector":[0.185116667,-0.0391290299,0.0391420981,0.0490997777,0.0236098414,-0.0135704467,-0.0944039688,-0.0336589774,0.0114889944,-0.0210067584,-0.0110175072,-0.0261256983,-0.1019942469,-0.0243199805,-0.0354107274,-0.0231405277,-0.0086783658,0.0128764466,-0.0138722458,-0.0274826724,0.0456217216,-0.0367611702,0.0035909006,-0.0710048385,-0.0488887259,-0.0291539456,-0.0392435355,0.0323526662,0.0228090439,0.0028116927,0.026335699,-0.045613373,-0.0293336117,0.0037621829,-0.0752035481,-0.0204004488,0.0732690841,0.0200901181,-0.047570694,0.0149578257,0.0497822677,-0.0063209625,0.0787017763,0.0273143536,-0.0057552578,0.0054077022,-0.0466094341,-0.0097366618,0.0208429166,-0.0144643941,-0.0877491464,0.0393092293,-0.0174342303,-0.0071917388,0.0365422488,-0.0033476176,0.0440698491,-0.0784601122,0.0147912529,-0.0132181389,0.0167529014,0.0350851481,0.0521520747,0.0337720839,-0.0537184057,-0.0227401905,-0.0172029156,-0.028621774,-0.0161539897,0.1193063918,-0.0046041401,0.0469416844,0.0048004425,-0.0069332949,-0.0657570781,0.0036043593,0.0971851945,-0.0018683944,-0.0784851577,0.1257114338,-0.0399897563,-0.0406783495,-0.0518034433,-0.0345466894,-0.0094437386,-0.0096511972,-0.0331494274,-0.0343788008,-0.0368647067,-0.0271958731,0.1257281023,-0.0018063914,0.0365803121,-0.0288804115,0.0549688592,-0.0185347769,0.0115327713,0.0318070741,-0.0487657506,0.0152985719,-0.0177069844,0.0382196815,0.0062950719,0.0068473882,0.0264406423,-0.0275785849,0.0197044295,0.0099414438,-0.0364377419,-0.0299403774,-0.0117528406,0.0327761982,-0.0537316395,0.0482285495,0.0292989946,-0.0368646587]},"393":{"Abstract":"Although visualization design models exist in the literature in the form of higher-level methodological frameworks, these models do not present a clear methodological prescription for the domain characterization step. This work presents a framework and end-to-end model for requirements engineering in problem-driven visualization application design. The framework and model are based on the activity-centered design paradigm, which is an enhancement of human-centered design. The proposed activity-centered approach focuses on user tasks and activities, and allows an explicit link between the requirements engineering process with the abstraction stage - and its evaluation - of existing, higher-level visualization design models. In a departure from existing visualization design models, the resulting model: assigns value to a visualization based on user activities; ranks user tasks before the user data; partitions requirements in activity-related capabilities and nonfunctional characteristics and constraints; and explicitly incorporates the user workflows into the requirements process. A further merit of this model is its explicit integration of functional specifications, a concept this work adapts from the software engineering literature, into the visualization design nested model. A quantitative evaluation using two sets of interdisciplinary projects supports the merits of the activity-centered model. The result is a practical roadmap to the domain characterization step of visualization design for problem-driven data visualization. Following this domain characterization model can help remove a number of pitfalls that have been identified multiple times in the visualization design literature.","Authors":"G. E. Marai","DOI":"10.1109\/TVCG.2017.2744459","Keywords":"Design studies;Tasks and requirements analysis;Visualization models;Domain characterization;Activity-centered design;Functional specifications","Title":"Activity-Centered Domain Characterization for Problem-Driven Scientific Visualization","Keywords_Processed":"task and requirement analysis;domain characterization;functional specification;design study;visualization model;activity center design","Keyword_Vector":[0.0150827495,-0.012663317,-0.0060870457,-0.0053778326,-0.0006629324,-0.0154011179,0.0036198012,0.0043318238,-0.0002033311,0.0027670118,0.0043425082,-0.0008830692,-0.0127821635,0.0001998965,-0.0005532934,-0.0129473138,-0.0190829667,-0.0138681178,-0.0034850045,0.0015935748,0.0219310575,-0.0186319328,-0.0243201716,0.0082951147,-0.0040566281,-0.0111623921,-0.0167619556,0.0055823468,0.0130196762,-0.0204780411,0.0007054546,-0.0297578795,-0.007290805,0.0182986734,-0.0100064487,-0.0144926091,0.0076014225,-0.0128505603,0.0032260605,0.0151570702,-0.0128337097,0.0055476119,0.008066079,-0.0093545871,-0.0209866976,0.0091428141,-0.0085858711,0.0075725455,-0.0181630169,-0.0099419387,-0.0231467075,-0.0075447756],"Abstract_Vector":[0.1140992162,-0.0587767473,0.0047189081,0.007275653,-0.0445533908,0.0128764786,0.0189704536,0.0168441959,-0.0219102047,0.0005311634,-0.0347439674,0.0665820513,0.0128745338,-0.0065662303,0.0320121078,-0.0131941982,-0.0383649357,-0.0081823781,-0.0313588481,-0.0288718532,0.0228790588,-0.0309256403,-0.0125443118,-0.0204834881,-0.0316693449,0.0344185207,0.0007154683,-0.0137689411,-0.0421546606,-0.0248639133,-0.0161084051,-0.0070865251,0.0190579735,-0.008148354,0.0134786688,-0.0153801657,-0.0156493786,-0.0299454316,-0.0202238096,0.055982281,-0.0007891544,-0.004719485,0.0367452766,-0.0496087702,-0.0051820176,-0.0133981955,0.026089719,0.0152977947,0.00090941,0.0362715213,0.0077637616,0.0256204391,0.0364158676,-0.0156993911,0.0348018236,0.0062972305,-0.0357535195,-0.0266232324,0.0060439567,-0.024194268,-0.0015695884,-0.0065771848,-0.0036932658,-0.0268298532,-0.0326924673,-0.0290035135,0.0003542415,-0.0252100982,-0.0008838297,0.0127573583,-0.0042104492,-0.023103375,0.0060470486,0.0269275039,0.0062584562,-0.0022577766,0.0114657985,0.0009912047,0.0063641918,0.0029507966,0.0071950968,0.0038580224,-0.0096645962,-0.0095233294,-0.0081110082,-0.0214191057,0.0265571934,0.014446611,-0.032996022,-0.0138338084,0.0052445922,0.0340208961,0.009836925,0.0083875354,-0.0010725205,-0.014099557,-0.0060093224,-0.0134582128,0.0060524201,0.0041921778,-0.0014717424,-0.0004730661,0.0138955618,0.0039473572,0.0264274725,-0.0339178849,-0.0122607307,0.0162581495,0.025253261,-0.0005936624,-0.0137547929,0.0193276908,0.0340893682,-0.0030293294,0.0456374545,0.0183513124]},"394":{"Abstract":"A popular method of force-directed graph drawing is multidimensional scaling using graph-theoretic distances as input. We present an algorithm to minimize its energy function, known as stress, by using stochastic gradient descent (SGD) to move a single pair of vertices at a time. Our results show that SGD can reach lower stress levels faster and more consistently than majorization, without needing help from a good initialization. We then show how the unique properties of SGD make it easier to produce constrained layouts than previous approaches. We also show how SGD can be directly applied within the sparse stress approximation of Ortmann et al. [1], making the algorithm scalable up to large graphs.","Authors":"J. X. Zheng; S. Pawar; D. F. M. Goodman","DOI":"10.1109\/TVCG.2018.2859997","Keywords":"Graph drawing;multidimensional scaling;constraints;relaxation;stochastic gradient descent","Title":"Graph Drawing by Stochastic Gradient Descent","Keywords_Processed":"graph drawing;relaxation;stochastic gradient descent;multidimensional scaling;constraint","Keyword_Vector":[0.135611103,-0.1360345729,-0.1131396693,0.1037965021,-0.0786738679,-0.0693146003,0.0291950795,-0.0400561668,-0.0424475458,-0.0367290366,0.0236478604,-0.0170925087,0.0363339196,0.2299588099,-0.1242259078,0.0189368705,-0.0123112776,0.0414919589,0.0330351931,-0.1096991431,-0.0038394482,-0.0754234212,-0.0120292861,-0.0204280318,-0.0378936213,0.0259392472,0.0152330356,-0.018419224,-0.1000865268,0.0641914071,0.0947626129,0.0104537145,0.1546420047,0.120984864,-0.026290654,-0.0139233432,0.011555245,0.0623086417,0.0820852353,-0.0479423313,-0.0659040813,0.057812653,-0.0324822157,0.0830878181,0.067597505,0.009858847,-0.0243055466,-0.0580026871,-0.0598118931,-0.0327017279,0.0440618759,-0.0290873564],"Abstract_Vector":[0.244475152,-0.0939059309,-0.000700399,0.0093013879,-0.0312129817,-0.0660926577,0.0329728059,0.0060692655,-0.0594531069,0.0272924692,0.0402309131,0.0103982404,-0.0709283253,-0.064275002,-0.0191346906,0.0415655263,0.0145627948,0.0294296484,-0.0549985635,0.0168437327,-0.005016175,-0.0024254292,-0.010068611,0.0181846672,-0.088057919,0.0096087616,-0.0458389246,0.007407031,0.0082873319,-0.0243034074,0.0996304063,-0.005752239,-0.036926775,-0.0527556094,-0.0309025696,-0.0057908562,0.002853822,-0.0153505703,-0.0244760558,-0.0308835099,-0.0075970621,-0.0369018015,0.0039880464,-0.0266394515,-0.016947656,-0.0329223617,-0.0453017847,0.0093295658,-0.0286505945,-0.0238838993,0.0376330182,0.0071263912,0.0195201203,-0.0249262519,0.0223915321,-0.0181514318,0.0120602293,-0.001113999,-0.0004261133,0.019655303,0.0076944109,-0.0333125349,-0.0304304546,-0.0105423881,-0.0014933777,0.0087832167,0.0158243279,-0.0108640063,0.0518618323,0.0051321436,-0.0015277749,0.0086534421,0.0219984673,0.0523555624,0.0016596625,0.0038515765,-0.0655219812,0.0210407023,-0.0501566931,0.0259853006,0.0136316812,0.017365939,0.0182639283,-0.0064377469,0.0381769387,-0.0280688743,0.0028428662,-0.0210817662,0.0373653088,-0.0260409329,-0.0356519429,-0.0232711906,0.0386237668,-0.0267327318,0.0826011329,0.0719636711,0.0483704624,0.0185089267,-0.0308578058,0.0831755954,-0.0030136434,-0.0358919458,-0.0462246416,0.0034189461,-0.0003981488,-0.0221084247,0.0012798843,0.0336234104,0.0460924463,-0.0264988921,-0.0184223985,-0.023576773,-0.0171425817,-0.0256708564,-0.0271718497,0.0030503938]},"395":{"Abstract":"Analyzing social networks reveals the relationships between individuals and groups in the data. However, such analysis can also lead to privacy exposure (whether intentionally or inadvertently): leaking the real-world identity of ostensibly anonymous individuals. Most sanitization strategies modify the graph's structure based on hypothesized tactics that an adversary would employ. While combining multiple anonymization schemes provides a more comprehensive privacy protection, deciding the appropriate set of techniques-along with evaluating how applying the strategies will affect the utility of the anonymized results-remains a significant challenge. To address this problem, we introduce GraphProtector, a visual interface that guides a user through a privacy preservation pipeline. GraphProtector enables multiple privacy protection schemes which can be simultaneously combined together as a hybrid approach. To demonstrate the effectiveness of GraphPro tector, we report several case studies and feedback collected from interviews with expert users in various scenarios.","Authors":"X. Wang; W. Chen; J. Chou; C. Bryan; H. Guan; W. Chen; R. Pan; K. Ma","DOI":"10.1109\/TVCG.2018.2865021","Keywords":"Graph privacy;k-anonymity;structural features;privacy preservation","Title":"GraphProtector: A Visual Interface for Employing and Assessing Multiple Privacy Preserving Graph Algorithms","Keywords_Processed":"graph privacy;privacy preservation;structural feature;anonymity","Keyword_Vector":[0.0927887211,-0.1335998055,-0.1469287873,0.074440521,-0.0815772414,-0.1569173098,0.0268217203,0.006567792,0.0199480891,0.062504293,-0.0158477388,0.0972140806,0.0154581018,-0.1064624564,0.0472126519,-0.0129949771,0.0627584654,-0.0410845237,-0.0044079616,0.0626487949,-0.0262479376,-0.0193023394,-0.0118005565,-0.0145462695,0.0315485515,-0.001721745,0.0366477568,-0.0720735811,0.0151511048,0.026825973,0.0040602785,0.0269272096,0.0251932707,-0.0533511639,-0.0161266749,0.0261765442,-0.0058920087,0.003649042,0.0180386325,-0.0328919615,-0.0346868895,-0.0217462345,0.0044364187,-0.0035584906,0.0298330869,0.0335173355,-0.0139256187,0.0340292392,0.0018547425,0.001290211,0.0044797035,-0.0350548153],"Abstract_Vector":[0.1788854774,-0.158307948,0.0168088033,-0.0382157906,-0.0650332687,-0.0055444204,0.039117449,-0.0011322761,-0.0757540153,0.0073311424,-0.010087026,-0.0335871578,0.0134910308,-0.0259751895,-0.0436697477,0.0984042437,0.0067678377,-0.0226881459,0.0177579433,0.0666131501,-0.0013931633,-0.0050909795,-0.0411363944,0.0358722136,-0.0088593635,-0.0796620657,-0.0123874977,-0.0386980925,-0.035603611,-0.0057896806,0.0173036683,0.0020489845,-0.0232002357,0.0276758145,0.0460149939,0.019623292,-0.0312776673,-0.0037553249,0.0170052365,-0.0703167243,0.0341763888,0.0069299564,0.0320576059,-0.0416888057,-0.0290351675,-0.0268346271,0.0016036334,-0.0179223627,-0.0113078487,-0.0302180514,0.0201218189,-0.0027937946,-0.0173616041,-0.015224873,0.0292857211,-0.0075899389,-0.0145824866,0.0391164411,-0.0465978365,-0.0185263357,0.0123677841,0.0125321308,0.0352199761,-0.0027542513,-0.0337981552,-0.0015901286,-0.0220266818,-0.0395406985,-0.0005052979,0.0084092547,0.0017867771,0.0382062771,0.004115222,-0.0377511272,0.0054138851,0.0183017637,0.0174241368,-0.0127747727,-0.0129877228,0.0082128067,0.0199671399,-0.0451062389,0.0044465417,0.0002470185,-0.0146795671,0.0564854184,-0.006064712,0.0007215382,-0.0179678904,0.0237641372,-0.0612831859,-0.0417881326,0.0213981589,-0.0165805993,0.0171733664,-0.0152116808,0.0471251902,0.0197328278,-0.0276564073,-0.0108987485,0.0064915753,0.0052466062,-0.0048318682,-0.0039924339,0.0019374466,-0.0042186825,0.046438824,-0.0431701544,0.0096071242,-0.0007150833,-0.0541880608,0.0088559115,-0.0097735984,0.0612204452,-0.0012786727,-0.0239246366]},"396":{"Abstract":"Interactive visualization of large image collections is important and useful in many applications, such as personal album management and user profiling on images. However, most prior studies focus on using low-level visual features of images, such as texture and color histogram, to create visualizations without considering the more important semantic information embedded in images. This paper proposes a novel visual analytic system to analyze images in a semantic-aware manner. The system mainly comprises two components: a semantic information extractor and a visual layout generator. The semantic information extractor employs an image captioning technique based on convolutional neural network (CNN) to produce descriptive captions for images, which can be transformed into semantic keywords. The layout generator employs a novel co-embedding model to project images and the associated semantic keywords to the same 2D space. Inspired by the galaxy metaphor, we further turn the projected 2D space to a galaxy visualization of images, in which semantic keywords and images are visually encoded as stars and planets. Our system naturally supports multi-scale visualization and navigation, in which users can immediately see a semantic overview of an image collection and drill down for detailed inspection of a certain group of images. Users can iteratively refine the visual layout by integrating their domain knowledge into the co-embedding process. Two task-based evaluations are conducted to demonstrate the effectiveness of our system.","Authors":"X. Xie; X. Cai; J. Zhou; N. Cao; Y. Wu","DOI":"10.1109\/TVCG.2018.2835485","Keywords":"Image visualization;semantic layout;CNN;image captioning","Title":"A Semantic-Based Method for Visualizing Large Image Collections","Keywords_Processed":"image caption;CNN;image visualization;semantic layout","Keyword_Vector":[0.0164190701,-0.0099666784,-0.0047780294,-0.001115818,-0.0044084807,-0.0081478011,0.0063304573,0.0014333567,0.0051045633,-0.0136148127,0.006341356,0.0083107064,-0.0067516789,-0.0096726842,0.0083329846,0.0183044152,0.0025512377,-0.0097150996,-0.0014442108,-0.0076194909,0.0151111086,-0.0011566028,-0.001518765,0.0077563293,0.0044696468,0.0103637667,0.0020242579,0.0021082139,-0.0158296752,-0.0043774986,-0.0130074327,0.0046557883,-0.0025419962,-0.0045237608,-0.0051379058,0.000714887,-0.0144792169,-0.0022445779,0.0023608662,-0.0191062005,-0.000134255,0.0136820159,-0.0110291699,-0.0271870514,0.0104785839,-0.0158883345,-0.000818823,-0.0020247954,-0.0088515446,0.0189701749,-0.0043971169,0.0270711306],"Abstract_Vector":[0.1361596874,0.0237604767,-0.0245279021,-0.0250392082,-0.014235362,0.0384228938,0.0256507915,0.0238390576,0.0420708858,0.0127821871,-0.0187573374,-0.0204773693,-0.016018244,-0.0072593816,-0.0023648417,-0.0177802053,0.0127268657,0.0241887873,0.0142739848,0.0089463823,-0.0103704466,-0.0059241632,0.0137731388,0.0301029407,-0.0206649224,-0.0135144579,-0.0370632919,0.0367964509,-0.0114070827,0.006924935,0.0113237817,-0.0207144572,-0.0219745196,-0.0030602854,-0.0225014131,-0.0177725333,0.0134898249,0.0167165886,-0.0017645134,-0.0378366515,0.0349003217,0.009485838,0.0159875335,0.0197339581,0.0386371717,0.0372337289,0.0234545719,0.0276504675,0.0233359145,0.0050264303,0.0492634813,0.0327320269,-0.0029185903,-0.0138339303,-0.0189349696,0.0176302472,0.0299957538,-0.0477376271,-0.0052675346,0.0023869188,-0.0184192297,0.0258860788,-0.009698247,0.0128119316,-0.0108954866,0.0242323381,-0.0108114693,0.0012399713,-0.0166529931,0.0045456441,0.0045175965,-0.0332638236,-0.0002864224,0.0120156087,0.0078779025,-0.0047564484,0.0257281955,0.0036185844,-0.049135185,-0.0127315101,-0.0146744022,0.0334034772,-0.0021878868,-0.0325859456,-0.0249061648,-0.0161001873,-0.0027410384,-0.0186871793,-0.0208619094,-0.0140222255,-0.0067990585,0.0024409413,-0.0348073174,0.0041598735,-0.0137624254,0.0403009006,-0.0108184815,0.0070523397,-0.0042049815,0.0058959,-0.0001557269,-0.0376544656,-0.0117623392,-0.036744263,-0.0300926455,-0.0438327655,0.0116265711,0.0064260812,-0.0062895706,-0.0035998803,-0.0083305435,0.0193041507,-0.0052708259,0.00277187,0.0108663356,0.0160436116]},"397":{"Abstract":"Pseudocoloring is one of the most common techniques used in scientific visualization. To apply pseudocoloring to a scalar field, the field value at each point is represented using one of a sequence of colors (called a colormap). One of the principles applied in generating colormaps is uniformity and previously the main method for determining uniformity has been the application of uniform color spaces. In this paper we present a new method for evaluating the feature detection threshold function across a colormap. The method is used in crowdsourced studies for the direct evaluation of nine colormaps for three feature sizes. The results are used to test the hypothesis that a uniform color space (CIELAB) will accurately model colormapped feature detection thresholds compared to a model where the chromaticity components have reduced weights. The hypothesis that feature detection can be predicted solely on the basis of luminance is also tested. The results reject both hypotheses and we demonstrate how reduced weights on the green-red and blue-yellow terms of the CIELAB color space creates a more accurate model when the task is the detection of smaller features in colormapped data. Both the method itself and modified CIELAB can be used in colormap design and evaluation.","Authors":"C. Ware; T. L. Turton; R. Bujack; F. Samsel; P. Shrivastava; D. H. Rogers","DOI":"10.1109\/TVCG.2018.2855742","Keywords":"Colormapping;color perception","Title":"Measuring and Modeling the Feature Detection Threshold Functions of Colormaps","Keywords_Processed":"colormappe;color perception","Keyword_Vector":[0.19055012,-0.0324871624,0.0858889657,0.034351205,-0.0522392124,0.0603042765,-0.0567281517,-0.0402671368,0.0143954659,-0.1011863002,-0.0357096512,0.051452553,0.0573650987,-0.0132478588,-0.0751692046,-0.0859723533,-0.0698899592,-0.1040344337,-0.0396997101,0.0288415015,-0.0175029063,-0.0109549454,-0.0534487459,0.0805401575,-0.0202623923,-0.1060832716,-0.133784539,-0.0396135335,0.002981586,-0.0796596997,-0.0132479439,-0.1270989262,-0.0635319663,-0.0063381447,0.0539238097,0.024393605,-0.05817869,0.0083188866,0.0009093004,-0.0183742543,0.0135915668,0.0069256528,-0.0243918618,-0.0116619658,-0.0159167544,-0.0024276301,-0.0371119705,-0.0466008423,0.0357891104,-0.0038697826,0.0187219313,-0.0082089731],"Abstract_Vector":[0.2759903464,-0.1017070511,0.1515686582,-0.0955400723,0.0645961404,-0.0925512214,0.1261287656,-0.1238536794,0.0775253376,0.0487677754,0.0192335174,-0.003227125,-0.0935227053,0.1030632032,-0.0661630657,0.0454801747,0.0237581131,-0.0310472528,-0.0553293825,-0.0095679335,0.000400952,0.0348129015,0.0807126249,0.0035085161,-0.0522067463,-0.0160235308,0.0893244082,0.0138612882,-0.0586121177,0.0166170733,-0.0331224339,0.0368329795,0.0377892454,0.0135344241,0.0288453258,0.0376170784,-0.06171255,-0.0265281555,-0.0785856683,-0.0137848692,-0.0010910075,-0.0200283608,-0.0240425109,-0.0256115387,0.0677626733,0.0068552457,-0.0462638535,-0.0477158102,-0.0166238836,-0.0753678123,-0.1130090973,-0.0071482544,-0.0370605004,0.0423303287,-0.0323745513,0.0104959067,0.0309670495,0.0456741947,0.0330646644,0.0772614692,-0.0632383589,-0.0033145443,0.0295403831,-0.0448317262,0.0354186205,-0.0356624685,0.0048784871,-0.0028144513,-0.0496039991,-0.0475165165,0.0147826012,-0.0856277682,-0.0127003716,0.0363376917,0.0301978659,-0.0295279276,-0.0181929496,-0.0136686982,0.0317908711,0.0143569007,0.0008592609,-0.0360928441,-0.0435007816,-0.0506854701,0.0411590228,-0.0174001545,-0.0226938493,-0.0358965583,0.043602701,0.0196569387,-0.0154189096,0.0250534948,0.0200794755,-0.0041370627,-0.0183079934,-0.0101567924,0.0348953426,0.034022002,-0.0133459499,-0.0289621207,0.0011813564,0.0087654446,0.0151078779,-0.0453746338,-0.018602187,0.0061124146,0.0127841971,0.011036693,-0.0155798144,0.010581102,-0.0085493763,0.0463878327,0.0396563173,0.0337825904,-0.0245310015,-0.0214742278]},"398":{"Abstract":"Does it feel the same when you touch an object in Augmented Reality (AR) or in Virtual Reality (VR)? In this paper we study and compare the haptic perception of stiffness of a virtual object in two situations: (1) a purely virtual environment versus (2) a real and augmented environment. We have designed an experimental setup based on a Microsoft HoloLens and a haptic force-feedback device, enabling to press a virtual piston, and compare its stiffness successively in either Augmented Reality (the virtual piston is surrounded by several real objects all located inside a cardboard box) or in Virtual Reality (the same virtual piston is displayed in a fully virtual scene composed of the same other objects). We have conducted a psychophysical experiment with 12 participants. Our results show a surprising bias in perception between the two conditions. The virtual piston is on average perceived stiffer in the VR condition compared to the AR condition. For instance, when the piston had the same stiffness in AR and VR, participants would select the VR piston as the stiffer one in 60% of cases. This suggests a psychological effect as if objects in AR would feel \u201dsofter\u201d than in pure VR. Taken together, our results open new perspectives on perception in AR versus VR, and pave the way to future studies aiming at characterizing potential perceptual biases.","Authors":"Y. Gaffary; B. Le Gouis; M. Marchal; F. Argelaguet; B. Arnaldi; A. L\u00e9cuyer","DOI":"10.1109\/TVCG.2017.2735078","Keywords":"Augmented Reality;Virtual Reality;Haptic;Perception;Stiffness;Psychophysical Study","Title":"AR Feels \u201cSofter\u201d than VR: Haptic Perception of Stiffness in Augmented versus Virtual Reality","Keywords_Processed":"virtual reality;stiffness;augmented reality;haptic;perception;psychophysical study","Keyword_Vector":[0.3397511746,-0.1122449476,0.0721602789,-0.2710344069,0.0887232732,0.0568735425,0.0598016451,0.0851366878,0.2484589255,-0.0423237872,0.1582359792,0.1543878185,-0.1087800539,-0.0482438552,-0.0400363407,0.0089064833,0.0131025724,0.2063055645,-0.0539503033,-0.177008921,-0.0540703743,0.0045585032,-0.010085775,-0.0450410469,0.302206638,-0.0279197628,0.0162173033,0.127822805,-0.1552564082,-0.1606858204,-0.0274774141,-0.0837260369,-0.0423220157,0.0099628255,-0.0794375391,-0.0467055136,-0.0590755203,-0.0193038202,0.1393874264,-0.0260852732,0.0066195675,0.0439290704,0.0460386088,-0.0555313269,-0.0101875634,-0.0668778246,-0.0205699187,-0.0000021614,-0.040194095,0.0429026123,-0.0284611887,-0.0077133071],"Abstract_Vector":[0.2786993061,-0.1799094618,-0.0098023859,0.0123462859,0.0065689649,0.0046908248,0.0091700914,-0.0182316543,-0.0725352404,0.0273812464,0.1053395884,-0.1275598127,0.0046253359,0.0446717557,0.0394242108,-0.0389111998,-0.0176715638,0.031157919,0.0015979921,0.0021916641,-0.0130154039,0.0135268508,-0.0290596635,-0.0501997933,-0.0089940294,-0.0264500666,0.0073350104,-0.050401284,0.082901649,0.0434167648,-0.0172749655,0.0004674814,-0.0101656806,-0.0062632742,-0.0251517126,0.0162782876,0.0118378079,0.0093448847,0.0205135808,0.0303179746,0.0284982393,-0.0204049829,-0.0145322639,0.0408610075,0.1130812556,0.0125977013,0.0100729435,-0.0136231674,0.0088477508,0.0516827244,0.0475060907,-0.0168578695,0.0542277711,-0.0226815284,0.0451560785,-0.0341505545,0.0401632505,0.0115716083,-0.0082271121,0.0575669139,-0.0216998261,-0.0230347821,0.0322483645,-0.0292739773,0.0107999131,-0.0199570942,-0.0126255465,0.0195716328,-0.0334648457,-0.0358745147,0.0545812061,0.0365371546,0.0092549408,-0.0107037264,-0.0133174329,0.0015837116,-0.0365569434,0.0122272375,-0.0122807482,-0.0146428557,-0.0375602132,0.0208951828,-0.0184557765,0.0048526532,-0.0353502203,-0.02671573,0.0240955904,0.0156826468,-0.0476292108,-0.0035378917,-0.0112449279,-0.0077797883,-0.020712345,-0.0179350155,0.0164836157,-0.0237770962,0.012111307,-0.0137860931,-0.0132447729,-0.0100739159,0.0123129939,0.0086530065,-0.0034238055,0.0245763714,0.0362248903,-0.0457313102,-0.0163038495,0.0319803112,-0.0035796902,-0.0180277564,-0.0094089986,-0.0179009622,0.0164963498,-0.0294832582,0.0130990209,0.001666411]},"399":{"Abstract":"The relative location of human body parts often materializes the semantics of on-going actions, intentions and even emotions expressed, or performed, by a human being. However, traditional methods of performance animation fail to correctly and automatically map the semantics of performer postures involving self-body contacts onto characters with different sizes and proportions. Our method proposes an egocentric normalization of the body-part relative distances to preserve the consistency of self contacts for a large variety of human-like target characters. Egocentric coordinates are character independent and encode the whole posture space, i.e., it ensures the continuity of the motion with and without self-contacts. We can transfer classes of complex postures involving multiple interacting limb segments by preserving their spatial order without depending on temporal coherence. The mapping process exploits a low-cost constraint relaxation technique relying on analytic inverse kinematics; thus, we can achieve online performance animation. We demonstrate our approach on a variety of characters and compare it with the state of the art in online retargeting with a user study. Overall, our method performs better than the state of the art, especially when the proportions of the animated character deviate from those of the performer.","Authors":"E. Molla; H. G. Debarba; R. Boulic","DOI":"10.1109\/TVCG.2017.2708083","Keywords":"Motion retargeting;online performance animation;self-body contact;inverse kinematics;spatial relationship","Title":"Egocentric Mapping of Body Surface Constraints","Keywords_Processed":"motion retargete;self body contact;spatial relationship;inverse kinematic;online performance animation","Keyword_Vector":[0.1833356555,-0.147927946,-0.0861958271,-0.0778549981,-0.023275741,-0.1448605896,-0.067193753,-0.0227240769,-0.0296004138,0.0809724404,-0.0538525853,-0.0246849573,-0.0098643544,0.0083898142,-0.04049904,-0.0343979902,-0.0416305963,-0.0156006738,-0.0449047484,-0.0240261792,0.0793864496,0.0187672981,0.0309145096,0.0005221194,-0.0663170771,0.0594836086,0.0711005957,0.0164528468,-0.0296314934,0.0054225073,-0.0469568744,-0.0207357862,0.0052064066,-0.0574715925,-0.0213618061,-0.0245798466,0.0815195794,-0.0982224043,-0.0351950306,-0.0393604065,0.0644088285,0.0133266054,0.0149931331,0.0213124506,0.0594992113,0.0098168436,0.0854992672,0.0119022904,0.029739475,0.0068900676,-0.0504751319,-0.0477019162],"Abstract_Vector":[0.1573615439,-0.094431283,0.0008281051,0.0117388341,-0.0508176552,0.020247513,-0.0086891336,0.0167494938,-0.0096497525,-0.0212045641,-0.0518356035,0.0344240866,-0.0362909229,-0.0162581746,0.0189329921,-0.0052847253,-0.0199505585,-0.0673518588,0.0230790086,0.0099338056,0.0269660917,-0.0346998335,0.0052948249,-0.0219109227,-0.0008846145,0.0328761707,0.0162780167,-0.0537787797,-0.0210479061,-0.0368993145,0.0067610507,0.0065436423,0.0026267354,-0.0022230039,-0.0155115951,-0.0197992627,-0.0025467406,0.010039247,0.0039569911,-0.0264113914,0.0085250688,0.0185128462,-0.0165903034,-0.0208802152,-0.0297006998,0.0277852285,0.0234151469,-0.0178922028,0.015851033,-0.0025977557,-0.0004548633,0.0011439091,-0.0089454156,0.0083264396,-0.0289419403,0.0105715143,0.034162721,0.0092168764,-0.0028201355,0.0105124738,0.0123465404,-0.0271692363,-0.0175687732,0.0015989026,-0.0660335824,-0.0047824518,0.012990447,-0.0015184081,-0.000633627,0.0143597539,-0.0149358194,0.0069730412,-0.0476712664,0.0046568196,0.0027498374,-0.0145722301,-0.016926345,0.0042150374,-0.0007050414,-0.0025955884,-0.0332839324,0.0095459937,-0.0045503819,0.0214194364,-0.0227985343,0.0004394774,0.0052299852,0.0088395071,-0.0231882969,-0.0173349458,-0.0120344295,-0.0454523284,-0.0094335484,0.0013790851,0.0404539096,-0.0252926931,-0.0141857004,-0.0275124154,-0.0369791989,-0.0141652888,0.0320429284,-0.0542829673,-0.0171084443,0.0453999436,0.0021343006,-0.0012855005,-0.0007866051,-0.0175598104,0.0252157487,-0.0047177441,0.0154968749,0.0113935695,-0.0154876531,-0.000623696,-0.0393980779,-0.0034300912]},"4":{"Abstract":"Visual analysis of multidimensional data requires expressive and effective ways to reduce data dimensionality to encode them visually. Multidimensional projections (MDP) figure among the most important visualization techniques in this context, transforming multidimensional data into scatter plots whose visual patterns reflect some notion of similarity in the original data. However, MDP come with distortions that make these visual patterns not trustworthy, hindering users to infer actual data characteristics. Moreover, the patterns present in the scatter plots might not be enough to allow a clear understanding of multidimensional data, motivating the development of layout enrichment methodologies to operate together with MDP. This survey attempts to cover the main aspects of MDP as a visualization and visual analytic tool. It provides detailed analysis and taxonomies as to the organization of MDP techniques according to their main properties and traits, discussing the impact of such properties for visual perception and other human factors. The survey also approaches the different types of distortions that can result from MDP mappings and it overviews existing mechanisms to quantitatively evaluate such distortions. A qualitative analysis of the impact of distortions on the different analytic tasks performed by users when exploring multidimensional data through MDP is also presented. Guidelines for choosing the best MDP for an intended task are also provided as a result of this analysis. Finally, layout enrichment schemes to debunk MDP distortions and\/or reveal relevant information not directly inferable from the scatter plot are reviewed and discussed in the light of new taxonomies. We conclude the survey providing future research axes to fill discovered gaps in this domain.","Authors":"L. G. Nonato; M. Aupetit","DOI":"10.1109\/TVCG.2018.2846735","Keywords":"Multidimensional projection;dimensionality reduction;multidimensional scaling;error analysis;layout enrichment","Title":"Multidimensional Projection for Visual Analytics: Linking Techniques with Distortions, Tasks, and Layout Enrichment","Keywords_Processed":"multidimensional scaling;error analysis;dimensionality reduction;layout enrichment;multidimensional projection","Keyword_Vector":[0.0786684098,0.0585155004,0.0511800498,0.0394576101,0.0419602768,-0.0617514914,-0.0517152173,-0.1241251327,-0.0373474813,-0.0836996484,0.041722269,0.0759082229,0.004542701,0.0563298872,0.1918338665,-0.1056977664,0.0419979156,0.0267629089,-0.0713850719,0.0187848555,0.0109076378,-0.0119136985,0.0414607708,-0.0283730989,0.0069792757,0.0330701523,0.0000553705,0.1064413158,0.0024786977,0.0601380932,-0.0058879602,-0.0451205501,0.0403110448,-0.064287632,-0.0208642633,0.0071242762,-0.01350405,-0.0991219096,0.0341153636,0.0872786,-0.0510502109,0.0476868349,-0.0527550863,0.0434769383,-0.0431924644,-0.048650877,-0.0206080495,0.0374413349,0.0079619537,0.0001344661,-0.0063618889,0.0444461288],"Abstract_Vector":[0.2287508682,0.0981967128,0.1312663751,0.0860706551,-0.1774594393,0.0648250181,-0.1029245205,-0.0981277124,0.081219647,0.0749627696,0.0320629694,0.0623275895,0.0286129839,0.0640572914,0.035049296,-0.0709525303,-0.040044101,-0.0929824973,-0.0262157728,0.0214859103,0.0608617194,0.0175993674,-0.0064937302,-0.0068377247,0.0287849879,0.0115438331,-0.060811752,-0.0158751908,-0.0019345454,-0.0496974426,0.041263691,0.0484473304,-0.0521677635,0.0054616887,-0.0325646102,0.0151916967,0.0130385953,-0.0338318986,-0.0517000493,-0.0323076696,0.0200969452,0.0521889916,0.0123265181,0.0774239442,-0.0016485304,-0.0238518712,0.0353931881,0.0090505136,-0.0133856364,-0.0160666157,0.0167806392,-0.0363807759,0.0028755949,-0.0081193031,0.0349923097,-0.0001394835,0.0595155548,0.0247811865,-0.0233651159,0.031613013,-0.0220418203,0.0067860072,-0.0184433524,-0.0559197352,0.0279756663,0.0167503111,0.0011359314,-0.0204448835,0.041624901,0.0654669909,-0.0581788704,0.0270020234,-0.0361239709,0.0003150503,-0.0332246428,0.0372906218,0.053986808,-0.0432217593,0.0057466912,-0.0445129329,0.033752806,-0.0046289251,0.0884610507,0.0196204894,0.0201302962,-0.0325835839,0.0058778355,0.0711854701,0.0194961373,-0.0610129368,0.0171673376,0.0082906464,0.0050553319,-0.0570384378,-0.0041490416,-0.0115519419,0.0497502744,-0.0258597669,-0.0295449073,-0.003452191,-0.0263050628,0.0300551425,-0.0228743689,-0.0410306534,-0.0146317769,0.0257794903,0.0247634866,0.0108086688,0.0043640167,-0.0036730373,-0.000627283,0.0269373767,0.0236392335,-0.010255323,-0.052232725,-0.0137209562]},"40":{"Abstract":"We present a system to combine arbitrary triangle mesh animations with physically based Finite Element Method (FEM) simulation, enabling control over the combination both in space and time. The input is a triangle mesh animation obtained using any method, such as keyframed animation, character rigging, 3D scanning, or geometric shape modeling. The input may be nonphysical, crude or even incomplete. The user provides weights, specified using a minimal user interface, for how much physically based simulation should be allowed to modify the animation in any region of the model, and in time. Our system then computes a physically-based animation that is constrained to the input animation to the amount prescribed by these weights. This permits smoothly turning physics on and off over space and time, making it possible for the output to strictly follow the input, to evolve purely based on physically based simulation, and anything in between. Achieving such results requires a careful combination of several system components. We propose and analyze these components, including proper automatic creation of simulation meshes (even for non-manifold and self-colliding undeformed triangle meshes), converting triangle mesh animations into animations of the simulation mesh, and resolving collisions and self-collisions while following the input.","Authors":"Y. Li; H. Xu; J. Barbi\u010d","DOI":"10.1109\/TVCG.2016.2620467","Keywords":"Computer graphics;animation;physically based modeling;animation system;directable simulation;FEM;collisions","Title":"Enriching Triangle Mesh Animations with Physically Based Simulation","Keywords_Processed":"directable simulation;FEM;animation system;computer graphic;physically base modeling;collision;animation","Keyword_Vector":[0.0034499996,0.0042334038,-0.0018640643,0.0009416437,-0.0022021887,0.0010006311,0.0000298549,-0.0021251167,-0.0037270277,0.0036009913,-0.0009890214,-0.0008263743,-0.003099591,-0.0017153898,-0.0027356937,0.0027131326,0.0050506908,-0.0023026852,0.0042212018,-0.0012661327,0.0028022828,0.0015064811,0.0028011401,-0.0005611442,-0.0021765996,-0.0011795553,0.0030668653,-0.0009725091,0.0070204739,-0.0060286645,0.0042477385,-0.0048227936,-0.0027539536,-0.0019229966,-0.0028518366,-0.0005191348,0.0008070239,0.0023615007,-0.0047023724,0.0069023493,0.0001632739,0.0014914994,-0.0061784913,0.0003888127,-0.0021888812,0.0017881272,0.0001897256,-0.0037702693,0.0015136934,0.0026077378,-0.000546452,0.0056729528],"Abstract_Vector":[0.1282185609,-0.0438921042,0.0162276686,0.0152432444,-0.0375633705,0.0336337309,-0.0103500466,0.0114090621,-0.0116523639,-0.0209924963,-0.0240675991,0.0413548723,-0.0144020347,-0.0062576461,0.0105201015,-0.0035360823,-0.036504543,-0.0047503564,0.0004325278,-0.0107882339,0.0302690035,-0.0246232243,-0.0013530374,-0.016066473,-0.0023193727,0.0211564551,-0.0040766148,-0.0298253453,0.0010532338,-0.0029787496,0.028958543,0.0400317587,-0.0257233814,-0.0096518192,-0.0092006068,-0.0074928284,0.0102654598,0.0052585615,0.0044487332,0.007438629,0.0241003339,0.0306038477,0.0450665412,0.0347530157,-0.0061602486,0.0071585632,-0.0080329454,-0.0065587892,0.0396336106,0.0199801921,0.0077543147,-0.0144061149,0.0008629568,0.0175522918,-0.0198500545,0.0057010286,0.0242035213,-0.0171235105,0.0452691204,-0.011777568,-0.0050334385,0.0153272158,-0.0198612157,0.0051883526,-0.0226894966,-0.0436366708,0.0066011005,0.0037574316,0.002244767,0.0214392357,0.032401467,0.0073905351,-0.0379240672,-0.0141139512,0.0274607718,-0.0362860528,0.0134446915,0.0285167176,0.004402634,0.0012760269,-0.0265093276,0.0020756314,-0.0198215497,0.009215889,-0.0270005963,-0.0361248168,-0.0475827981,-0.0098528089,-0.033468486,-0.0345764935,-0.0341492756,-0.0470275833,-0.0430686231,0.0287788194,-0.0135061678,-0.0011393515,0.0107392456,-0.0093613794,-0.0310698401,0.0141699551,0.0168474529,0.0419849393,-0.0243390596,-0.0194073292,0.029245217,-0.0208859252,0.0236061041,-0.0044030867,-0.0090288119,-0.0142948557,0.0437694932,0.0365716076,-0.0208179531,-0.0159073076,-0.0063572806,0.0161428785]},"400":{"Abstract":"In immersive Virtual Reality systems, users tend to move in a Virtual Environment as they would in an analogous physical environment. In this work, we investigated how user behaviour is affected when the Virtual Environment differs from the physical space. We created two sets of four environments each, plus a virtual replica of the physical environment as a baseline. The first focused on aesthetic discrepancies, such as a water surface in place of solid ground. The second focused on mixing immaterial objects together with those paired to tangible objects. For example, barring an area with walls or obstacles. We designed a study where participants had to reach three waypoints laid out in such a way to prompt a decision on which path to follow based on the conflict between the mismatching visual stimuli and their awareness of the real layout of the room. We analysed their performances to determine whether their trajectories were altered significantly from the shortest route. Our results indicate that participants altered their trajectories in presence of surfaces representing higher walking difficulty (for example, water instead of grass). However, when the graphical appearance was found to be ambiguous, there was no significant trajectory alteration. The environments mixing immaterial with physical objects had the most impact on trajectories with a mean deviation from the shortest route of 60 cm against the 37 cm of environments with aesthetic alterations. The co-existance of paired and unpaired virtual objects was reported to support the idea that all objects participants saw were backed by physical props. From these results and our observations, we derive guidelines on how to alter user movement behaviour in Virtual Environments.","Authors":"A. L. Simeone; I. Mavridou; W. Powell","DOI":"10.1109\/TVCG.2017.2657038","Keywords":"Virtual reality;Locomotion;User behaviour","Title":"Altering User Movement Behaviour in Virtual Environments","Keywords_Processed":"virtual reality;user behaviour;locomotion","Keyword_Vector":[0.0735941244,0.0753162021,-0.046947625,0.0105937689,0.0091065455,-0.0160555553,0.0063277731,-0.0197151143,-0.0156684596,-0.0328949392,0.0288183084,0.0197148995,-0.0046782844,0.0333288454,0.0551883747,0.0509025388,-0.0391495483,-0.100533626,0.0672622018,-0.0696875439,0.0397578885,-0.014099013,-0.0735397089,0.051417539,-0.0121369619,-0.0533074609,0.1122180548,0.0506977788,-0.0353286658,0.0177429958,-0.0882911474,0.0124873168,-0.0030925374,-0.0514165449,0.0253658616,0.0336717358,-0.0774432653,0.0184880569,-0.0248161225,0.0231343832,0.0161252562,-0.0092526772,-0.026330641,0.0368027202,0.0238289213,-0.0154411757,0.0040018472,-0.0135333225,-0.0087739472,0.0431746824,-0.0185625102,0.0128960752],"Abstract_Vector":[0.1938401506,0.0203283012,0.0211956264,-0.0500003659,-0.014492236,0.0766135232,0.0713007881,0.0281283397,0.1045964382,0.0501602999,-0.0658929665,-0.0129750083,-0.0475794252,0.003975399,0.0001653832,-0.0518051862,0.0780305044,0.0064742261,0.0089445899,-0.0125364305,-0.0051186552,-0.0234352191,0.0454716887,-0.0073726098,-0.0070217911,-0.0449559041,-0.0085936673,0.0128804278,-0.0677071036,-0.0262060854,0.0674952928,-0.0420545513,-0.024094488,-0.0242709179,0.0047948824,0.037000417,0.0096999278,-0.0191177755,0.0110619448,0.0096235315,0.0895224087,0.0315216063,0.0510427625,0.0758431386,0.0208781085,-0.0086289404,-0.0195643095,0.0602052392,0.0194939002,0.0237663035,0.0163920555,0.0015241724,-0.0431816065,0.0255710265,0.0250510695,0.0266758312,0.003709453,-0.0383552754,-0.0177804685,-0.0625217597,0.0511377657,-0.0545872301,0.0144699834,-0.0242556701,-0.0167802535,0.0007358787,0.0173260318,-0.0053631464,-0.0203312543,-0.0057875773,0.027275134,0.0883883589,0.0368363402,0.0811656832,-0.0010121599,-0.0385330563,0.0177420979,0.0783970258,0.0159791362,-0.0572384013,-0.0381129576,0.0131899371,0.0153862097,-0.0484988389,-0.0503360355,-0.0064623437,-0.0389062524,-0.0299478213,0.0573824936,-0.0563423289,0.0555778513,-0.0028932572,-0.0145908025,-0.0666495361,-0.0109964726,0.0005687556,0.0071174286,-0.0252867786,-0.0209077501,0.0001504464,0.0209637821,-0.0109951749,0.0050180192,0.0012660782,-0.08782114,-0.0268780104,-0.0176234648,0.0191327179,-0.0055315288,-0.0522201695,-0.019829168,-0.0098619202,0.000174573,-0.026552089,0.0845236819,-0.0004036541]},"401":{"Abstract":"Modularity, modifiability, reusability, and API usability are important software qualities that determine the maintainability of software architectures. Virtual, Augmented, and Mixed Reality (VR, AR, MR) systems, modern computer games, as well as interactive human-robot systems often include various dedicated input-, output-, and processing subsystems. These subsystems collectively maintain a real-time simulation of a coherent application state. The resulting interdependencies between individual state representations, mutual state access, overall synchronization, and flow of control implies a conceptual close coupling whereas software quality asks for a decoupling to develop maintainable solutions. This article presents five semantics-based software techniques that address this contradiction: Semantic grounding, code from semantics, grounded actions, semantic queries, and decoupling by semantics. These techniques are applied to extend the well-established entity-component-system (ECS) pattern to overcome some of this pattern's deficits with respect to the implied state access. A walk-through of central implementation aspects of a multimodal (speech and gesture) VR-interface is used to highlight the techniques' benefits. This use-case is chosen as a prototypical example of complex architectures with multiple interacting subsystems found in many VR, AR and MR architectures. Finally, implementation hints are given, lessons learned regarding maintainability pointed-out, and performance implications discussed.","Authors":"M. Fischbach; D. Wiebusch; M. E. Latoschik","DOI":"10.1109\/TVCG.2017.2657098","Keywords":"Real-time interactive systems;virtual reality systems;software architecture;multimodal processing","Title":"Semantic Entity-Component State Management Techniques to Enhance Software Quality for Multimodal VR-Systems","Keywords_Processed":"software architecture;real time interactive system;multimodal processing;virtual reality system","Keyword_Vector":[0.095853838,-0.0609713728,0.065430811,-0.001288764,0.008817286,0.1034315534,0.1918770122,-0.066760416,-0.0093491516,0.0397044139,0.0338053897,0.0471832219,0.0242979804,0.0484640937,0.0789155422,0.1523516564,0.0283708496,-0.0269927862,-0.1070311536,0.0818416039,0.0621150463,0.0089623957,0.0863708052,0.061377395,-0.0887790107,0.0289847888,-0.0254028343,-0.0430692407,-0.0714966596,0.0312738713,-0.0700439388,-0.0116295532,0.0164440494,0.0344287619,-0.0103253579,0.0707373848,-0.0038843863,0.050278148,-0.001876775,-0.001239003,-0.0234697678,-0.028266691,0.0333258857,-0.0045709414,0.014085865,-0.0268139499,-0.0004578974,0.0226754493,0.0431540286,-0.0748059387,-0.010436256,0.0032493779],"Abstract_Vector":[0.2171498791,-0.0800337967,0.0320779815,0.0582270354,0.0548082072,-0.0507152358,-0.0172456597,-0.0253121192,0.0775444664,0.0286246656,0.0000185748,0.0844345158,-0.004842833,0.0197213511,-0.0228343387,-0.0820666474,-0.0165573268,-0.0374531117,-0.0594912943,-0.0621632188,0.0028802789,-0.0804228961,-0.0797782309,0.0421614904,-0.06623933,-0.0044339831,0.0540864345,-0.0033145172,-0.0275433661,-0.0023187439,-0.0380133931,-0.0821714781,-0.0131889586,-0.0348125584,-0.0345933622,0.0152663432,-0.010375252,-0.0377967148,-0.0455847589,0.0136910243,-0.0201250034,0.0322976648,-0.0062369936,0.0329000046,-0.0397765549,0.0037095607,0.0513249157,-0.0627718017,0.0141769379,-0.0562074935,-0.0222888643,-0.0071910662,-0.0047723207,-0.0444682896,0.0137498331,-0.0258633814,-0.0276204553,0.0013875884,0.0259299873,-0.0474368043,0.0071318661,0.015410846,0.0454973519,-0.0698365556,0.0885373272,-0.0035016206,-0.0113458409,-0.0467039354,0.0702358505,0.0524914433,0.0689889353,0.0489066007,-0.0054925696,-0.0009818038,-0.0050987567,0.0499957252,0.0070140635,0.0123914079,-0.0547761613,-0.020586603,0.0016886653,-0.0280429285,-0.0288072516,0.0040926481,-0.0824708459,-0.042575989,0.0220615542,0.0484889291,0.0133504148,-0.0085338539,-0.043592975,0.002257297,-0.0299041041,-0.0001781624,0.0027239624,-0.0157341947,0.0011011895,0.0236180379,0.0019667866,-0.0434795524,-0.0051993191,-0.0572792108,-0.0277626566,0.0066612938,0.0143617301,-0.0104036,-0.0122747048,0.0026236187,-0.0272574934,0.043853214,0.0217454406,0.0190084988,0.0054886256,0.044495019,0.001561158,0.0390426859]},"402":{"Abstract":"Retrieving salient structure from textured images is an important but difficult problem in computer vision because texture, which can be irregular, anisotropic, non-uniform and complex, shares many of the same properties as structure. Observing that salient structure in a textured image should be piece-wise smooth, we present a method to retrieve such structures using an L0 minimization of a modified form of the relative total variation metric. Thanks to the characteristics shared by texture and small structures, our method is effective at retrieving structure based on scale as well. Our method outperforms state-of-art methods in texture removal as well as scale-space filtering. We also demonstrate our method's ability in other applications such as edge detection, clip art compression artifact removal, and inverse half-toning.","Authors":"Y. Sun; S. Schaefer; W. Wang","DOI":"10.1109\/TVCG.2017.2711614","Keywords":"Texture removal;scale-space filtering;image smoothing;  $L_0$      sparsity","Title":"Image Structure Retrieval via $L_0$  Minimization","Keywords_Processed":"texture removal;image smooth;scale space filtering;sparsity","Keyword_Vector":[0.159170189,0.194563744,-0.1335629103,-0.062382011,-0.0231323501,-0.0133572592,-0.0337404342,-0.0331605443,-0.036848905,0.0517512155,-0.036079869,0.0396430949,0.0108816563,0.0139423833,-0.087904727,0.050592334,-0.156866662,0.0023954881,-0.044771492,0.0332338925,0.0367673775,-0.057757907,0.0825482292,-0.0954098804,-0.0647629247,-0.0383423631,-0.0640485309,0.0215814804,0.0057236208,-0.0321889399,0.0868662691,0.2029487393,0.1347529395,0.2129908853,-0.0725089033,0.0352734013,-0.1048184133,0.0160815242,0.060881434,-0.098366011,0.1667368941,-0.0023933917,-0.0399512427,0.1622255369,-0.0399338854,0.0425539894,-0.0260800585,0.1640433469,0.0216043454,-0.0513260997,0.1055139084,-0.121666957],"Abstract_Vector":[0.2623433138,-0.0738499182,-0.0014872657,-0.0786686297,-0.0497736497,0.0468775043,0.0345953367,0.0358829987,0.0055381113,-0.0133203705,-0.0144929372,-0.0512520606,0.0181272916,0.0100436813,0.0001796762,-0.005819345,-0.0531032362,0.013791781,0.0291271437,0.0309819996,-0.0147946551,0.0004898069,-0.0330712724,-0.0104253365,-0.076804637,-0.0035797278,-0.0695427509,0.0230583953,0.0030134072,-0.0701862877,-0.0143421757,-0.0211614925,-0.000647777,0.0408248543,-0.0368860085,0.0187097865,-0.0211227143,0.0342760411,-0.0550778091,-0.0435262523,-0.0359967067,0.0288937989,0.0077361212,-0.0165409424,0.0579787689,0.0649432862,0.063391792,0.0604628991,0.0133123774,-0.007747316,0.0448094377,0.0246428567,-0.0564550939,-0.0437884337,0.0141470454,-0.0154020427,0.0236532912,0.0240027552,-0.0263297061,-0.0269342868,-0.0155151933,0.0119988178,-0.0573982135,0.0424316035,0.0139843419,0.0486389929,-0.0131371563,-0.0099732803,0.0032878301,-0.0146860044,-0.0159980139,0.0001808273,0.0393169571,0.0017796546,0.0085308527,0.0106469649,-0.0170843996,0.0068578178,-0.0271909471,0.0089128822,0.0168140395,0.0220568209,0.0062090532,-0.0258556813,0.0148656702,-0.0336977887,-0.0148300924,0.0170990915,-0.0160548708,-0.0116223113,-0.0072985962,0.0039289126,-0.0443724775,-0.0178436242,-0.0080905544,0.0333777205,0.0251782505,-0.0186390152,-0.0035179463,0.0039965698,0.02471621,-0.0183081585,-0.0028843542,-0.0340959143,0.0202663694,0.0135092552,-0.0194828373,-0.0077493081,0.0213755479,-0.0286366653,-0.0078121008,0.0070182021,0.0283199526,0.0260351172,-0.0008873308,-0.0137648451]},"403":{"Abstract":"Many approaches for analyzing a high-dimensional dataset assume that the dataset contains specific structures, e.g., clusters in linear subspaces or non-linear manifolds. This yields a trial-and-error process to verify the appropriate model and parameters. This paper contributes an exploratory interface that supports visual identification of low-dimensional structures in a high-dimensional dataset, and facilitates the optimized selection of data models and configurations. Our key idea is to abstract a set of global and local feature descriptors from the neighborhood graph-based representation of the latent low-dimensional structure, such as pairwise geodesic distance (GD) among points and pairwise local tangent space divergence (LTSD) among pointwise local tangent spaces (LTS). We propose a new LTSD-GD view, which is constructed by mapping LTSD and GD to the$x$axis and$y$axis using 1D multidimensional scaling, respectively. Unlike traditional dimensionality reduction methods that preserve various kinds of distances among points, the LTSD-GD view presents the distribution of pointwise LTS ($x$axis) and the variation of LTS in structures (the combination of$x$axis and$y$axis). We design and implement a suite of visual tools for navigating and reasoning about intrinsic structures of a high-dimensional dataset. Three case studies verify the effectiveness of our approach.","Authors":"J. Xia; F. Ye; W. Chen; Y. Wang; W. Chen; Y. Ma; A. K. H. Tung","DOI":"10.1109\/TVCG.2017.2744098","Keywords":"High-dimensional data;low-dimensional structure;subspace;manifold;visual exploration","Title":"LDSScanner: Exploratory Analysis of Low-Dimensional Structures in High-Dimensional Datasets","Keywords_Processed":"manifold;visual exploration;subspace;high dimensional datum;low dimensional structure","Keyword_Vector":[0.0170999077,0.0085607522,-0.0062070991,0.0032020855,0.0020494571,-0.004647304,0.0103124943,-0.0054512326,-0.0052757751,-0.0080385733,0.0010816782,-0.0147690942,-0.0051513749,0.0137091088,-0.0042707321,-0.0015730943,0.0109463339,-0.0165009652,-0.0016240315,0.0054466846,0.0033995501,-0.0157181754,0.0080902362,0.019361287,-0.0065383719,0.0163666372,0.0165414246,0.0075798002,-0.0009062673,-0.0114177078,-0.0188923369,0.0017243796,-0.0110052581,-0.0003893384,-0.0035606493,0.0306735509,0.0100079677,0.0104691089,-0.0057004929,-0.0075567805,-0.0054625814,0.0222092054,-0.0297859135,-0.017959453,0.0098494522,-0.0070894737,0.0138461491,0.0036699873,-0.0197365725,0.0135651076,-0.0120638032,0.0332144049],"Abstract_Vector":[0.148031344,0.1135917395,-0.0799636722,-0.0166319481,-0.0260313136,0.0450649343,0.0602995933,-0.004789119,0.0474763822,0.0880493226,0.0091481497,0.0471052748,-0.0575649278,-0.0310834166,-0.0330529956,-0.0181826707,-0.0436019984,-0.0181694254,0.0254342886,-0.0690697378,-0.0349705438,-0.0275943421,0.0788405601,-0.0833713987,-0.0594286447,-0.0435716332,0.0231365987,0.0502759676,-0.0327491174,0.0284648973,0.0517987411,-0.0627130761,-0.0507441948,-0.0160499,0.05264933,-0.0238027204,-0.0607249842,-0.0005332876,0.0353248748,0.0222627014,-0.0339507813,-0.0235544087,-0.0100200551,-0.006550583,0.0653311112,-0.034683388,-0.0285588043,0.0271593688,-0.0597078561,0.0054163033,0.0170628265,-0.0841644323,-0.0133357149,0.035525468,-0.0624102974,0.0054127303,-0.0196517089,0.0099598397,-0.0011843169,-0.0674973847,0.0228888452,-0.0287617233,0.0483972278,-0.0745103666,-0.0577398437,0.0405985941,0.003573663,-0.0258936159,-0.0160577341,0.0939519913,0.1022850668,0.0628911468,0.0206291483,-0.0339721287,-0.0055044687,0.0233447044,0.0399203832,-0.0305113731,0.0204824647,-0.0408144814,-0.0136298794,-0.0196275539,0.043043876,-0.0076174264,-0.0828002439,-0.0053231643,-0.0580975805,0.0608768142,0.1001290632,-0.0450980873,0.0205538757,-0.0000520945,0.0476804361,-0.0537018701,0.0422356663,0.0445104491,0.0358932982,-0.007425188,0.00554413,-0.030846266,0.012317934,0.0232759076,-0.0099454716,-0.0064690751,-0.0412091012,0.0072050294,-0.0094887726,-0.018180383,-0.0490940172,0.0132169362,0.0048111673,-0.0315418563,-0.0117263441,-0.0317126676,0.0689270296,-0.0174917897]},"404":{"Abstract":"Topic modeling algorithms are widely used to analyze the thematic composition of text corpora but remain difficult to interpret and adjust. Addressing these limitations, we present a modular visual analytics framework, tackling the understandability and adaptability of topic models through a user-driven reinforcement learning process which does not require a deep understanding of the underlying topic modeling algorithms. Given a document corpus, our approach initializes two algorithm configurations based on a parameter space analysis that enhances document separability. We abstract the model complexity in an interactive visual workspace for exploring the automatic matching results of two models, investigating topic summaries, analyzing parameter distributions, and reviewing documents. The main contribution of our work is an iterative decision-making technique in which users provide a document-based relevance feedback that allows the framework to converge to a user-endorsed topic distribution. We also report feedback from a two-stage study which shows that our technique results in topic model quality improvements on two independent measures.","Authors":"M. El-Assady; R. Sevastjanova; F. Sperrle; D. Keim; C. Collins","DOI":"10.1109\/TVCG.2017.2745080","Keywords":"Topic Model Configuration;Reinforcement Learning;Feature Detection and Tracking;Iterative Optimization","Title":"Progressive Learning of Topic Modeling Parameters: A Visual Analytics Framework","Keywords_Processed":"Reinforcement Learning;feature Detection and tracking;Topic Model Configuration;Iterative Optimization","Keyword_Vector":[0.0648426966,-0.0263717041,0.0483027422,0.064718144,-0.0254025978,0.0619570587,0.0117980129,-0.0679629886,0.0297668705,0.0707113591,0.0296759328,0.008613584,-0.0340354003,0.0462556657,0.0815047632,0.1734304887,0.0169248639,-0.0242894096,-0.1197160186,0.0234032429,-0.0055322861,-0.0026396536,0.0368421151,0.127536795,-0.0358693053,0.0522664158,0.0222066067,-0.0365805217,-0.0610563035,-0.0231560817,-0.0576325211,-0.0090365726,-0.012162814,0.035301765,0.0291506724,0.0695040847,0.0103113453,0.0319838305,0.0072166738,-0.0315132054,0.0400843534,-0.0287057729,-0.0074045448,0.0195157845,-0.0171886361,-0.0891621104,-0.0071839889,0.046986833,0.0056629678,-0.0412357587,-0.0227239633,0.0455621663],"Abstract_Vector":[0.2715996097,0.0184691581,-0.0142736928,0.03616616,-0.008626031,-0.1345134175,-0.0000036434,-0.0322599501,0.0879335576,0.0613901859,0.0147533288,0.0343820027,-0.0026459565,-0.0085903438,0.0191338593,-0.0477101344,0.0214024045,-0.0026218361,-0.0590709627,-0.0412124705,-0.0381279697,0.0111538441,-0.0134498429,0.0399578752,-0.0489757636,0.0255592352,0.0084990021,0.0148898225,-0.0978890601,0.0048689393,0.0511892369,-0.000695208,0.0117983487,-0.0094611106,0.0157728211,-0.0290585279,0.013358382,-0.0136842163,0.03790171,0.0612131399,0.0628429415,0.0296403965,0.0259980725,-0.013592677,-0.0245618239,0.0135476149,0.0142384398,0.066709282,0.0114609274,-0.007574775,-0.0051805238,-0.0449457812,-0.0400514326,-0.0420195046,0.0359238121,-0.037971922,0.0324796678,-0.0242785583,-0.0514463205,0.034805711,-0.0383212637,0.0350696288,0.0087247423,0.0130555481,-0.0257702899,-0.0175857781,0.0379239588,-0.0070532824,0.061682867,0.0817030803,-0.0337818533,-0.0030910572,0.0277701353,0.0216467002,0.0432369766,0.0406607828,-0.0051974406,0.0207643271,0.003395551,0.0224911297,-0.0452419143,-0.0788592081,0.0057578322,0.0149573022,-0.0336181491,-0.0363813448,-0.0014529183,0.0423756891,0.0182167031,-0.0019771886,0.0043450358,-0.020298941,-0.0779564963,-0.0241629436,0.0196961954,-0.0432985704,-0.0004243872,0.0119959397,0.0294937676,0.0312049911,-0.0231849588,-0.0359508861,0.0038195468,0.0344263428,0.025131075,-0.0167893615,0.0162239697,0.035743806,-0.0087013941,0.001249169,0.0015352792,0.041978186,-0.0219276205,-0.0353980616,-0.0143244475,0.0197697974]},"405":{"Abstract":"The increasing availability of spatiotemporal data continuously collected from various sources provides new opportunities for a timely understanding of the data in their spatial and temporal context. Finding abnormal patterns in such data poses significant challenges. Given that there is often no clear boundary between normal and abnormal patterns, existing solutions are limited in their capacity of identifying anomalies in large, dynamic and heterogeneous data, interpreting anomalies in their multifaceted, spatiotemporal context, and allowing users to provide feedback in the analysis loop. In this work, we introduce a unified visual interactive system and framework, Voila, for interactively detecting anomalies in spatiotemporal data collected from a streaming data source. The system is designed to meet two requirements in real-world applications, i.e., online monitoring and interactivity. We propose a novel tensor-based anomaly analysis algorithm with visualization and interaction design that dynamically produces contextualized, interpretable data summaries and allows for interactively ranking anomalous patterns based on user input. Using the \u201csmart city\u201d as an example scenario, we demonstrate the effectiveness of the proposed framework through quantitative evaluation and qualitative case studies.","Authors":"N. Cao; C. Lin; Q. Zhu; Y. Lin; X. Teng; X. Wen","DOI":"10.1109\/TVCG.2017.2744419","Keywords":"Anomaly Detection;Visual Analysis","Title":"Voila: Visual Anomaly Detection and Monitoring with Streaming Spatiotemporal Data","Keywords_Processed":"Anomaly Detection;Visual Analysis","Keyword_Vector":[0.1179457845,-0.07615308,-0.1143679518,0.0371674887,-0.0363383185,-0.1500549804,-0.0329981434,-0.0291078127,-0.0819651647,-0.0146808989,-0.016945316,0.1296258911,-0.0051662459,-0.0198104371,-0.0284654779,0.0254649519,-0.0323643023,0.0019079589,0.1077677979,0.0442889737,-0.064038889,0.0577544008,-0.0413215261,0.1315201125,-0.0930823551,-0.044210612,-0.0972383619,-0.0251848475,0.0387887954,0.0046097228,-0.0158300914,-0.145663137,-0.1091297093,0.0681259898,0.014879044,0.0560898759,-0.0990480804,0.0005732863,0.0624144663,-0.0320385546,-0.0202553231,0.0370358354,-0.0442940549,-0.0866433331,0.1127451113,0.0153920512,-0.0147379919,0.053623047,-0.0276379589,-0.0693408632,0.1336520786,-0.0039594076],"Abstract_Vector":[0.2128818321,0.012763704,0.0401904605,0.0261795892,-0.1223340313,0.055914417,0.0002712556,-0.0378048777,0.0673059921,0.0117344767,-0.0282432459,-0.018673033,-0.0647072765,0.0236804179,0.002046207,-0.0203198305,0.0531712446,-0.02240157,-0.0554772944,-0.0633734339,0.0611443914,-0.015059114,0.0272027939,-0.0159522661,-0.0374756352,-0.0420668539,0.0433130687,0.0006645307,0.0050411006,0.014631863,0.0435497111,0.0261241796,0.0189411486,0.0201285643,0.0065633988,0.0074544752,-0.0460622959,-0.0262442207,0.0250028576,0.0557471977,-0.0054405319,0.0162418646,0.0497381551,0.0518090021,0.0329691738,0.0258548689,-0.0068592069,0.0726796252,-0.035025501,-0.0067178859,0.0028233795,-0.0456542376,-0.0022433541,0.0500903915,0.0265403157,-0.0544404752,0.0153036198,0.0387333807,-0.0122534294,0.0049831907,0.0620514504,-0.0008807737,-0.0179137127,0.024098391,0.0319491494,-0.020693235,0.0821899365,-0.026385815,0.0100854219,0.0558360604,0.0271483499,-0.0328595043,0.0109571179,-0.0093253196,0.0521990294,-0.0202989504,0.0383463866,-0.0692058047,0.0320171197,-0.0054415445,0.0193888701,-0.0016906842,-0.004312373,-0.0602340054,-0.055218145,-0.0094653837,0.0228598344,0.0253114909,0.0076449676,-0.0882739838,0.0429330757,0.0047992396,0.0246372466,-0.0954467349,-0.0185191701,0.0042635418,0.0509217699,-0.0264340365,-0.0343899132,-0.0424000215,0.0218473766,0.0322258971,-0.0455544378,-0.0254335546,-0.0409599842,0.0271829022,-0.048150645,-0.0151340118,-0.0229542148,-0.0161815256,-0.0118240386,-0.0269704578,0.0482845342,-0.0062833562,0.0579901398,-0.0220360218]},"406":{"Abstract":"Interactive wall-sized displays benefit data visualization. Due to their sheer display size, they make it possible to show large amounts of data in multiple coordinated views (MCV) and facilitate collaborative data analysis. In this work, we propose a set of important design considerations and contribute a fundamental input vocabulary and interaction mapping for MCV functionality. We also developed a fully functional application with more than 45 coordinated views visualizing a real-world, multivariate data set of crime activities, which we used in a comprehensive qualitative user study investigating how pairs of users behave. Most importantly, we found that flexible movement is essential and-depending on user goals-is connected to collaboration, perception, and interaction. Therefore, we argue that for future systems interaction from the distance is required and needs good support. We show that our consistent design for both direct touch at the large display and distant interaction using mobile phones enables the seamless exploration of large-scale MCV at wall-sized displays. Our MCV application builds on design aspects such as simplicity, flexibility, and visual consistency and, therefore, supports realistic workflows. We believe that in the future, many visual data analysis scenarios will benefit from wall-sized displays presenting numerous coordinated visualizations, for which our findings provide a valuable foundation.","Authors":"R. Langner; U. Kister; R. Dachselt","DOI":"10.1109\/TVCG.2018.2865235","Keywords":"Multiple coordinated views;wall-sized displays;mobile devices;distant interaction;physical navigation;user behavior;user movements;multi-user;collaborative data analysis","Title":"Multiple Coordinated Views at Large Displays for Multiple Users: Empirical Findings on User Behavior, Movements, and Distances","Keywords_Processed":"mobile device;user movement;user behavior;multi user;wall sized display;collaborative datum analysis;physical navigation;distant interaction;Multiple coordinate view","Keyword_Vector":[0.0539863171,-0.0002959852,-0.0035015989,0.0145151491,0.0636698739,-0.007968246,0.0677562927,-0.0532739637,-0.0518786853,-0.0087580643,-0.000943156,0.0250542721,0.0006105635,0.0590318892,-0.0030017949,-0.0453917989,0.0393010903,0.0042906448,0.0237765797,-0.0425478683,0.0628374726,0.0656423384,-0.0676806801,0.0143682499,0.0498710812,0.044967962,0.0298156626,-0.0490027997,0.041425039,0.0359564068,-0.0025796727,0.0042953865,0.0206384999,0.0640306144,-0.0511453556,0.1078358144,0.0455657923,0.0651653149,-0.078212486,0.0019759504,0.0705879769,0.0606451194,-0.0456071911,-0.0395222889,-0.0533089927,-0.0543408228,0.0028644162,0.0982014032,0.0124978101,-0.0412209769,0.0092931195,-0.0107214854],"Abstract_Vector":[0.098497515,-0.0309531838,-0.012229307,-0.0073504782,-0.0241218919,0.0377335641,-0.0120918042,0.0029819254,-0.0007733669,0.012880986,-0.0303369121,0.0187500372,-0.0332521871,-0.0127281189,-0.0235368902,0.0321787827,0.006452208,0.0121910334,0.0128758077,0.0209670082,-0.0273983158,0.0024916448,-0.0120829718,0.0139062179,0.0268004632,-0.0419103633,-0.0235360214,0.0029471148,0.0016051719,0.0235242533,0.005569592,0.0078318395,0.0051484171,0.0116439281,0.0231470559,-0.0350094318,0.0164660742,-0.030923259,-0.0127668027,-0.0306148996,0.0014050306,0.0384886976,0.0205638224,-0.0161360889,0.006446801,-0.0382296481,0.0310084014,-0.0026760677,-0.0045483123,-0.0066321265,-0.0085828151,0.0072215281,0.0277037481,0.0181232119,-0.0007230715,-0.0066230829,0.0349897023,0.0337357369,-0.0284785682,-0.0126246587,0.0121386889,0.0131028168,0.0095264861,-0.0104604177,-0.0089726746,0.0000681203,-0.0034846546,-0.0019131014,0.0088521662,0.0181308059,-0.0062980178,0.000480227,0.0239622732,-0.0020268016,0.0250263941,0.018900903,-0.0057021461,-0.0113005589,0.0279195284,0.0062810952,-0.0311599749,-0.0229343087,0.0044763556,-0.0032076397,0.0140038511,-0.0324198496,0.0044969489,-0.0069354768,-0.0089119536,-0.0110363554,0.0074041787,-0.0055951982,-0.0139987687,0.0041537525,0.0235424634,-0.0035305845,0.0102922226,0.0148047636,-0.0320584125,-0.0080134141,-0.0257454218,0.0026738997,0.0106723777,-0.0050080575,-0.0008174006,-0.015511729,-0.003785444,-0.0183445923,-0.0075328749,0.0051232302,0.0027589992,0.0134315346,0.0089527377,-0.0364777837,-0.0020540833,-0.0056946771]},"407":{"Abstract":"Augmented Reality offers many applications today, especially on mobile devices. Due to the lack of mobile hardware for illumination measurements, photorealistic rendering with consistent appearance of virtual objects is still an area of active research. In this paper, we present a full two-stage pipeline for environment acquisition and augmentation of live camera images using a mobile device with a depth sensor. We show how to directly work on a recorded 3D point cloud of the real environment containing high dynamic range color values. For unknown and automatically changing camera settings, a color compensation method is introduced. Based on this, we show photorealistic augmentations using variants of differential light simulation techniques. The presented methods are tailored for mobile devices and run at interactive frame rates. However, our methods are scalable to trade performance for quality and can produce quality renderings on desktop hardware.","Authors":"K. Rohmer; J. Jendersie; T. Grosch","DOI":"10.1109\/TVCG.2017.2734426","Keywords":"Augmented Reality;Mixed Reality;Differential Rendering;Color Compensation;Impostors Tracing;GPU-Importance Sampling;Mobile AR;Scene Reconstruction;Light Estimation;Material Estimation;Depth-Sensing;Point Clouds;Global Illumination","Title":"Natural Environment Illumination: Coherent Interactive Augmented Reality for Mobile and Non-Mobile Devices","Keywords_Processed":"GPU Importance Sampling;Color Compensation;Differential Rendering;Light Estimation;scene Reconstruction;impostor trace;mixed reality;Material Estimation;augmented reality;depth Sensing;mobile AR;Global Illumination;point cloud","Keyword_Vector":[0.0630539525,-0.0212107546,0.0181885566,-0.0328498216,0.004395907,0.001803989,-0.0191900717,0.0006127819,0.0044526062,-0.0209364275,-0.0003340338,-0.0037808131,-0.0120001502,-0.0108452555,-0.0078049983,0.0115633597,-0.0139884344,0.0188564416,-0.0065514533,0.0077769678,-0.0192381975,-0.0082808687,-0.0147764264,-0.0146246864,0.0051969838,0.0149838078,-0.0003326624,0.0031197236,-0.0035629314,0.0035143157,0.0152250911,0.0077545525,0.0098040507,-0.0053596497,0.0010800278,-0.0025686713,0.0046640368,-0.0011446577,-0.0060946172,-0.0063679627,-0.022353665,0.0114409276,0.0066528801,0.0091404952,0.0014143213,-0.0022588553,0.0068936255,0.0031875668,0.0018506054,0.0013903167,0.0118956843,-0.0055688841],"Abstract_Vector":[0.1906334109,-0.1338866039,0.0101453524,-0.0273362794,-0.0678418383,0.0059708427,0.0254638397,0.017179338,-0.0624339124,-0.0194306563,-0.0067861724,0.0154232026,0.0626744338,-0.0167091654,0.0117742734,-0.0116170933,-0.0596008884,0.0068780979,-0.0092719283,-0.0124215159,-0.0161736833,0.0057370573,-0.0052632254,-0.0122745901,-0.037626535,0.0163951858,-0.0215457161,-0.0163678523,-0.0347792695,-0.0258062883,-0.0023743833,-0.0174251109,0.0297870878,-0.0113272904,-0.005276778,0.0038589262,-0.0135473803,-0.029416864,0.0192975239,0.005779944,-0.0142226609,-0.0018209646,0.0240500127,-0.0462349437,0.0045557598,-0.0054853883,0.0157754677,-0.0035257644,0.0001469717,0.0071717106,0.020679605,0.0056517068,0.0266696552,0.0065339115,-0.0101170339,-0.0016268255,-0.0104917757,0.0090303591,-0.0172067151,-0.0100405922,0.0005427983,-0.0401719693,-0.0022719765,-0.023493253,-0.0090477626,0.015453226,0.0000866126,0.0012524117,-0.0144168886,0.0041474589,0.0105850986,0.0073630802,0.0269415093,0.0185047629,-0.0047607405,0.0121606803,-0.0516843422,0.011309431,-0.0039422436,-0.0072041714,-0.0119184601,0.0011680831,0.0075846649,0.0053600131,0.0368968662,-0.0116250836,0.012133125,0.0122147844,-0.0055289217,-0.0495533096,0.0119709607,0.007144886,0.0085515201,-0.0041270316,-0.0266923139,0.004956421,0.0129614518,0.0189782668,-0.0271731493,0.0140840999,0.0112453434,0.0104853453,0.0052331217,-0.0052067306,0.0215569971,-0.007866134,-0.0180325978,0.0149652229,-0.0003488983,-0.0046085765,0.0045606977,0.0464684957,0.0045027103,0.0271972289,-0.006855364,0.0256878987]},"408":{"Abstract":"Sketching designs has been shown to be a useful way of planning and considering alternative solutions. The use of lo-fidelity prototyping, especially paper-based sketching, can save time, money and converge to better solutions more quickly. However, this design process is often viewed to be too informal. Consequently users do not know how to manage their thoughts and ideas (to first think divergently, to then finally converge on a suitable solution). We present the Five Design Sheet (FdS) methodology. The methodology enables users to create information visualization interfaces through lo-fidelity methods. Users sketch and plan their ideas, helping them express different possibilities, think through these ideas to consider their potential effectiveness as solutions to the task (sheet 1); they create three principle designs (sheets 2,3 and 4); before converging on a final realization design that can then be implemented (sheet 5). In this article, we present (i) a review of the use of sketching as a planning method for visualization and the benefits of sketching, (ii) a detailed description of the Five Design Sheet (FdS) methodology, and (iii) an evaluation of the FdS using the System Usability Scale, along with a case-study of its use in industry and experience of its use in teaching.","Authors":"J. C. Roberts; C. Headleand; P. D. Ritsos","DOI":"10.1109\/TVCG.2015.2467271","Keywords":"Lo-fidelity prototyping;User-centred design;Sketching for visualization;Ideation;Lo-fidelity prototyping;User-centred design;Sketching for visualization;Ideation;C4240C;C4210L;C1140Z;C1160","Title":"Sketching Designs Using the Five Design-Sheet Methodology","Keywords_Processed":"sketch for visualization;c1140z;c1160;c4210l;User centre design;lo fidelity prototyping;C4240C;ideation","Keyword_Vector":[0.0247806419,-0.0140281438,-0.0208687201,0.029550344,-0.0217240765,0.0019702253,0.0184914476,0.0022567248,-0.0021798107,-0.0013177328,0.0015201943,0.0120274055,0.0098502205,-0.0179227846,0.0055264746,-0.0249214781,-0.0042559975,-0.0066025397,0.0419192751,-0.0113278671,-0.0024238165,0.0006908504,-0.0212987996,0.0088671889,0.0286100868,-0.0143301764,-0.0057066754,0.0190075324,0.014492349,0.025038927,0.0188624541,-0.0206334821,0.006098047,0.0208163828,-0.0086893216,0.0721302972,0.0549022114,-0.0149707041,0.0122188291,0.0375371436,0.0930706399,0.0277146142,-0.0017045819,0.0293518862,-0.0292620001,-0.0091327167,0.0594186354,-0.0666558926,-0.0080572118,-0.0017622087,-0.0142085889,0.0209346735],"Abstract_Vector":[0.1763488321,-0.0311672505,0.0094678068,0.0194031347,-0.031225937,0.0475965233,-0.0075750499,0.0092422355,0.0148624541,-0.0327270148,-0.0613439387,0.0249827785,-0.0311311709,-0.0106450936,0.0234794107,0.0263871226,-0.0036691599,-0.0123385806,0.0615322876,-0.0121577459,-0.0387687106,-0.0283064965,0.0285824538,0.0719048517,0.0207771748,-0.0227033954,0.0149685894,0.0258532721,0.010487411,0.0334079329,-0.0762846631,0.0523547183,-0.0170465113,-0.0333918496,-0.0126416871,0.006443678,0.0020373976,-0.0349655054,0.0041527459,0.0250009876,-0.0231106009,0.0308149464,0.019764485,0.0004635218,-0.0251058111,-0.0070028144,0.0175352476,0.0339887843,0.009175461,-0.0361533059,-0.0056224342,0.0247027276,0.0014697349,0.0231348193,-0.0112773601,-0.0069584692,0.024795414,0.0077294335,0.0103770383,0.0030525378,0.0197912862,0.005146234,0.0314329523,-0.0293787509,0.0196008358,0.0064909804,-0.0334331163,-0.0002732051,-0.0292319187,0.0380446733,0.0165553966,-0.0026210714,0.0006392837,0.0220461665,0.0100015964,-0.0016024773,-0.0027207306,0.0500988362,0.002133113,-0.0059793477,-0.0132606168,0.0050237204,0.0000329488,0.0482149248,-0.0058829071,-0.0102313433,0.0112048489,-0.0231584938,-0.0043110668,-0.0425095209,0.0053578023,-0.015509737,0.0317086609,-0.0004962132,-0.0080592058,0.0044189291,-0.0082898302,0.0019143564,-0.0301050999,0.0137770023,-0.028981282,0.0195029539,0.0124616881,0.0101347438,-0.002633895,0.0347402577,-0.0014032891,0.0131754631,-0.0235123315,0.0322321225,0.0021634756,-0.0029485697,-0.0425687872,-0.0010797579,0.0079402713,0.0151182748]},"409":{"Abstract":"Visualization research often seeks designs that first establish an overview of the data, in accordance to the information seeking mantra: \u201cOverview first, zoom and filter, then details on demand\u201d. However, in computational fluid dynamics (CFD), as well as in other domains, there are many situations where such a spatial overview is not relevant or practical for users, for example when the experts already have a good mental overview of the data, or when an analysis of a large overall structure may not be related to the specific, information-driven tasks of users. Using scientific workflow theory and, as a vehicle, the problem of viscous finger evolution, we advocate an alternative model that allows domain experts to explore features of interest first, then explore the context around those features, and finally move to a potentially unfamiliar summarization overview. In a model instantiation, we show how a computational back-end can identify and track over time low-level, small features, then be used to filter the context of those features while controlling the complexity of the visualization, and finally to summarize and compare simulations. We demonstrate the effectiveness of this approach with an online web-based exploration of a total volume of data approaching half a billion seven-dimensional data points, and report supportive feedback provided by domain experts with respect to both the instantiation and the theoretical model.","Authors":"T. Luciani; A. Burks; C. Sugiyama; J. Komperda; G. E. Marai","DOI":"10.1109\/TVCG.2018.2864849","Keywords":"theory;visualization design;details-first model;discourse paper;computational fluid dynamics","Title":"Details-First, Show Context, Overview Last: Supporting Exploration of Viscous Fingers in Large-Scale Ensemble Simulations","Keywords_Processed":"computational fluid dynamic;theory;detail first model;visualization design;discourse paper","Keyword_Vector":[0.0883181746,0.0589477735,0.1488774786,0.0868447036,-0.0631045308,-0.0623535674,0.0195580364,-0.0817757267,0.0215017794,-0.0306947439,-0.0649650375,-0.0335180779,-0.0880143976,-0.077194402,-0.1460587853,0.0056394685,0.2441634597,-0.0640872592,0.0574650055,-0.0321474267,0.0820455804,-0.0106584346,0.0404775798,0.0923172155,-0.0398817469,-0.121145813,-0.0108337711,0.0278731627,-0.1678666323,-0.062632718,0.0215254261,0.1992574228,0.1223320909,0.1177066951,-0.1515386375,-0.1229965943,-0.0057257866,-0.0588377002,-0.1366705179,0.1633559784,-0.0469747431,-0.0314322483,-0.0784335973,-0.1417307843,-0.0376143507,-0.0660122747,-0.0422476145,-0.0149944985,0.015792035,-0.0347564454,0.0133823914,-0.0577291775],"Abstract_Vector":[0.1885300966,0.0558986127,0.1449401219,-0.0198778545,0.087612046,-0.0667784602,0.0263540446,-0.0146866182,-0.0088255879,0.057372603,-0.079068139,0.0451388796,-0.0220286659,0.0591171113,-0.0600017771,0.0146503544,0.0136241471,0.0544757322,0.0035520096,-0.0708338152,-0.0105447605,-0.0329346843,-0.0448653461,-0.0146175126,-0.04577528,0.0561122344,-0.0322944765,0.0445824539,0.0809285533,-0.0341305415,0.0529632324,-0.0386667405,-0.0874233955,-0.0112444669,0.0345251014,0.0508519114,0.0508387934,-0.0756359375,0.0245374549,-0.0686098537,-0.0320605241,-0.0078981935,0.0364579745,0.0380773814,-0.0473005605,0.0238205435,0.1551202869,0.0071784713,0.06931622,0.0535840094,-0.0823469437,-0.0919532732,0.1956954181,-0.0187323749,-0.0603978933,0.061788959,-0.0568951749,-0.0859989852,-0.1163982471,0.0938404753,-0.1239339563,0.0273815032,0.0026805073,0.0391976031,0.006945531,0.095913386,0.0498047504,-0.1030969373,0.0236468445,-0.0402872315,0.0681252045,-0.0374175877,-0.0275895851,-0.01622913,-0.0009155898,-0.0158974772,0.0123461474,0.0869494156,-0.0099204493,0.0003010926,-0.0175594479,0.00633293,-0.0251132869,-0.0265513219,0.0446608475,-0.0599682703,0.0111408037,0.0260572379,0.0726370074,0.0189250942,0.0176535964,0.0108807787,0.0295650712,-0.0202744479,-0.0161445521,-0.015904705,-0.0223106214,0.0053434126,-0.0249793957,0.0185676088,0.0073019367,0.0168334605,-0.0029485362,0.0258403958,0.0259671167,0.0141726591,0.0381133277,-0.0226595733,0.0297913902,-0.0421949995,-0.0028895076,0.0008673296,0.0066138729,0.030344478,-0.0037548008,-0.0026671571]},"41":{"Abstract":"Parallel coordinates plots (PCPs) are a well-studied technique for exploring multi-attribute datasets. In many situations, users find them a flexible method to analyze and interact with data. Unfortunately, using PCPs becomes challenging as the number of data items grows large or multiple trends within the data mix in the visualization. The resulting overdraw can obscure important features. A number of modifications to PCPs have been proposed, including using color, opacity, smooth curves, frequency, density, and animation to mitigate this problem. However, these modified PCPs tend to have their own limitations in the kinds of relationships they emphasize. We propose a new data scalable design for representing and exploring data relationships in PCPs. The approach exploits the point\/line duality property of PCPs and a local linear assumption of data to extract and represent relationship summarizations. This approach simultaneously shows relationships in the data and the consistency of those relationships. Our approach supports various visualization tasks, including mixed linear and nonlinear pattern identification, noise detection, and outlier detection, all in large data. We demonstrate these tasks on multiple synthetic and real-world datasets.","Authors":"H. Nguyen; P. Rosen","DOI":"10.1109\/TVCG.2017.2661309","Keywords":"Correlation;parallel coordinates plot;large data visualization","Title":"DSPCP: A Data Scalable Approach for Identifying Relationships in Parallel Coordinates","Keywords_Processed":"parallel coordinate plot;correlation;large datum visualization","Keyword_Vector":[0.0537064383,-0.0399641197,-0.0229905607,-0.0282201857,0.0345689895,-0.0333769433,0.0317124702,-0.01091167,-0.0310683032,-0.028698246,-0.0239704984,0.0258560933,-0.0047546007,-0.0085916932,0.0207125528,0.0204574297,-0.0526285899,-0.0003499081,-0.030619508,-0.0907883357,-0.0119166264,-0.0127893564,0.0291251157,-0.0285609666,0.1009730262,0.0003689933,0.0142573206,0.0859007425,-0.0956263707,-0.0272394926,-0.0097936205,-0.0481441353,-0.0252492378,0.0839559756,-0.0900376488,0.0564116456,-0.1673025334,-0.017313167,0.1506987659,-0.0585936689,0.0307042159,0.0338934266,-0.0803749018,-0.0585861297,-0.0057150882,-0.0133999196,0.0989499957,-0.0137043033,-0.0930308717,0.0019305925,-0.0123802639,0.0877555795],"Abstract_Vector":[0.1838980259,-0.1178685036,0.0104205224,0.0323180193,-0.0536129909,0.0959755711,0.0116674936,0.0224360829,-0.023377525,-0.0192342998,0.0000469694,0.0216072288,-0.0247291132,0.0198929006,0.0943476413,-0.0963388073,-0.0597941573,-0.0077418277,0.0263191548,-0.103647236,0.0161439087,0.0046157985,-0.0146835989,-0.0482784864,-0.0328918687,-0.0147260597,-0.0077545148,-0.1296229587,0.0097709537,-0.0231739556,0.0862936762,-0.0526025251,0.0086986133,-0.0070890578,-0.0487347902,-0.0795270085,0.0671914144,-0.0276955813,0.0302615313,-0.0084006737,-0.0073908302,0.0771459069,-0.0391805984,-0.0291341503,-0.0612337199,0.0343788149,0.0053367922,0.070592192,-0.0173007339,0.0408428008,-0.0193791739,0.0014741875,-0.0363074468,-0.01197811,-0.0060172434,-0.0272842596,0.0024691758,0.0169387161,0.0212972849,0.0096975547,-0.060924649,0.0296550647,-0.002700424,-0.023818497,-0.0209370532,-0.0077092029,0.0205471057,-0.0120637745,0.0364785489,-0.0067296085,0.0247252963,0.0399731529,-0.0390715267,-0.0268312939,-0.0249692864,-0.0024303795,-0.0047766892,-0.0317477163,-0.0173192913,-0.0059513144,-0.027268212,-0.0298447909,-0.0493811967,-0.0299888093,-0.0300498499,0.0025143004,0.0149364719,-0.0324630876,-0.0149052105,0.0236069454,0.013732874,-0.0343479067,0.0106647282,-0.0230543789,-0.0553940071,-0.0189418155,-0.017699227,0.0008976601,-0.000055795,-0.0089707702,-0.0055139688,0.0387204411,-0.0208004117,-0.0501676283,0.0483938275,-0.0749117961,-0.012724016,-0.0174587248,0.0045183266,-0.0019248883,-0.0260240713,0.0424885438,0.0122645566,0.0119920364,-0.0446341822,0.0068983069]},"410":{"Abstract":"Computational fluid dynamic (CFD) simulations of blood flow provide new insights into the hemodynamics of vascular pathologies such as cerebral aneurysms. Understanding the relations between hemodynamics and aneurysm initiation, progression, and risk of rupture is crucial in diagnosis and treatment. Recent studies link the existence of vortices in the blood flow pattern to aneurysm rupture and report observations of embedded vortices - a larger vortex encloses a smaller one flowing in the opposite direction - whose implications are unclear. We present a clustering-based approach for the visual analysis of vortical flow in simulated cerebral aneurysm hemodynamics. We show how embedded vortices develop at saddle-node bifurcations on vortex core lines and convey the participating flow at full manifestation of the vortex by a fast and smart grouping of streamlines and the visualization of group representatives. The grouping result may be refined based on spectral clustering generating a more detailed visualization of the flow pattern, especially further off the core lines. We aim at supporting CFD engineers researching the biological implications of embedded vortices.","Authors":"S. Oeltze-Jafra; J. R. Cebral; G. Janiga; B. Preim","DOI":"10.1109\/TVCG.2015.2467203","Keywords":"Blood Flow;Aneurysm;Clustering;Vortex Dynamics;Embedded Vortices;Blood Flow;Aneurysm;Clustering;Vortex Dynamics;Embedded Vortices","Title":"Cluster Analysis of Vortical Flow in Simulations of Cerebral Aneurysm Hemodynamics","Keywords_Processed":"Aneurysm;clustering;Vortex dynamic;blood Flow;embed vortex","Keyword_Vector":[0.0360466513,0.0047411184,-0.0173446404,0.0239397542,0.0098490603,0.0004625733,-0.0026979412,0.0050875697,-0.015663348,-0.0234035687,0.0143666703,0.0114284788,0.009980822,0.0517929087,0.0080405897,0.0057398454,0.0322653372,0.0031251144,0.0535662346,-0.0632862754,0.0364964433,0.081949962,-0.0974781563,-0.0326725972,0.0436229817,0.0356679968,-0.0080395856,-0.0322494859,0.0743771637,0.017448569,0.0108234736,0.0058849907,0.0098253849,0.0870337335,-0.0185632556,0.098008788,0.049918809,-0.004089456,-0.0127497494,0.0346264762,0.0431908565,0.0895071643,0.0134545987,-0.0234024046,-0.0481049319,-0.0212657979,0.0448735149,0.0218635125,0.0488851139,0.0260443271,0.0050565412,-0.0540522861],"Abstract_Vector":[0.1647208568,0.0086475076,-0.0455719322,-0.0499455064,-0.0334719491,0.0980492827,-0.0046624375,-0.0058425358,0.0302927903,0.0010007199,-0.0675012806,0.0120623123,-0.0110390598,0.031495134,0.0235763303,0.0152151226,0.0245947767,0.0240857517,-0.028202915,-0.0603757976,-0.0107881369,-0.047745997,-0.0016981075,0.0015676043,0.0251529534,-0.0083336658,-0.0026317137,0.0224594355,-0.0188135555,0.039012582,0.0241923777,0.0070261186,0.0491146611,-0.0199581465,0.0146676958,0.0396537582,0.0055697575,0.0126368404,-0.0072945594,0.0303756638,0.1367314164,0.0686547955,0.0219312393,0.0740351235,-0.0053365705,-0.0035453273,-0.0054770795,0.0172647143,-0.0334383889,-0.0864447395,0.0154698165,0.0400922448,-0.0101043637,0.0657426513,0.0343398862,0.0046800446,-0.0531165215,-0.0025277268,-0.0562521556,-0.0279580149,-0.0330984837,-0.0211620505,0.0196903993,-0.0044561934,0.0258865803,0.0297830077,-0.0090116479,0.0519165989,-0.0012804145,-0.0392654222,0.0161057875,0.0123344532,-0.0109956733,-0.0104843538,0.0166665987,-0.0065276919,0.0329153336,0.0792497453,-0.0051629823,0.024912818,-0.0064038129,-0.0045509164,0.0288431802,-0.0152025512,0.0062829166,-0.0149642762,0.0148569941,0.0123340104,0.0036442553,0.0244702323,0.0079376904,0.0115278836,0.0163035457,0.0207580325,0.0035346925,-0.030487587,0.044365785,-0.0048868143,-0.0180441502,0.0023258793,0.0181034987,-0.0249667244,0.0210858576,-0.0065102777,-0.0400323063,-0.010185537,-0.0044328961,0.0240143254,0.0538477085,-0.0103583582,0.0359372682,-0.0357942128,0.0104354401,-0.0025489638,0.0321744472,-0.0040416827]},"411":{"Abstract":"We present a new method to visualize from an ensemble of flow fields the statistical properties of streamlines passing through a selected location. We use principal component analysis to transform the set of streamlines into a low-dimensional Euclidean space. In this space the streamlines are clustered into major trends, and each cluster is in turn approximated by a multivariate Gaussian distribution. This yields a probabilistic mixture model for the streamline distribution, from which confidence regions can be derived in which the streamlines are most likely to reside. This is achieved by transforming the Gaussian random distributions from the low-dimensional Euclidean space into a streamline distribution that follows the statistical model, and by visualizing confidence regions in this distribution via iso-contours. We further make use of the principal component representation to introduce a new concept of streamline-median, based on existing median concepts in multidimensional Euclidean spaces. We demonstrate the potential of our method in a number of real-world examples, and we compare our results to alternative clustering approaches for particle trajectories as well as curve boxplots.","Authors":"F. Ferstl; K. B\u00fcrger; R. Westermann","DOI":"10.1109\/TVCG.2015.2467204","Keywords":"Ensemble visualization;uncertainty visualization;flow visualization;streamlines;statistical modeling;Ensemble visualization;uncertainty visualization;flow visualization;streamlines;statistical modeling","Title":"Streamline Variability Plots for Characterizing the Uncertainty in Vector Field Ensembles","Keywords_Processed":"ensemble visualization;streamline;statistical modeling;flow visualization;uncertainty visualization","Keyword_Vector":[0.051318506,0.0029289287,0.0127207633,0.0394587993,0.075632407,-0.0114467692,-0.0049076348,0.0727022689,-0.0133599864,-0.0238892787,-0.0402168879,-0.0173415923,-0.0141367447,0.0167744978,0.0124386376,0.0268509428,0.0357377088,-0.0056925839,-0.0229000471,-0.0029502418,0.0120051963,0.0004034013,0.0142822268,0.009695989,-0.0162702173,0.0158559698,-0.0216079684,-0.0000079864,-0.0320478914,0.0463947814,-0.0354723219,0.0091219462,0.0000289169,-0.0183498498,-0.0068496562,0.0233615745,-0.0164822731,0.0328576959,0.0034319144,-0.0195370312,0.0200379419,0.061944003,-0.0627660709,-0.0224063029,-0.0160021506,0.0442754705,-0.0154733954,-0.0270142942,-0.0104134865,0.0186433017,-0.052258944,-0.011801443],"Abstract_Vector":[0.1324565873,-0.0075893972,0.0110386222,-0.0115252952,-0.0357346502,-0.0055611794,0.017915221,-0.0087135634,0.0566103846,0.0157516331,-0.0140887953,0.0125722463,0.00463358,0.0325890044,-0.0151249506,-0.0254332966,0.0011758168,-0.0007113194,-0.0095665369,-0.0459684869,-0.0269493238,-0.0019453519,0.0967281415,-0.0040353607,-0.0185138823,-0.0251382858,0.0111278152,0.0060144331,-0.0505800253,-0.0167960366,0.1033905643,-0.021282669,-0.0434038068,0.0075592271,0.0781668652,0.0030715351,-0.0134738232,-0.0014195146,0.0352351854,0.0159046998,0.0111937904,0.024747412,0.0641640097,0.0490805097,-0.008252746,-0.0271542073,0.0188224814,0.0986269787,-0.0257171678,0.0482098403,-0.0298046899,-0.0272535996,-0.0207942999,0.0010658232,0.0404110251,0.0505671146,0.0633812905,0.0667998135,-0.005243123,-0.027880385,0.0773435243,0.0100804459,0.0641351998,-0.048800224,0.0061327628,-0.0087378835,0.0757270539,-0.0650822249,-0.0373105291,0.1471067631,-0.0142884743,0.0752665635,0.1060513433,0.012705728,0.0685743685,-0.0451552839,0.0607221305,-0.0267806494,0.0223375601,-0.0537850994,-0.0269489729,0.0705882674,0.1045920686,-0.0593486128,-0.1426642386,0.0222046236,-0.0228640053,0.0168580353,0.1428061019,-0.0666144518,0.0620769027,-0.006977688,0.0291333876,-0.0963802257,0.0325417971,0.040653372,-0.0224256659,0.0094349622,-0.0221597208,-0.1038793158,-0.070309322,-0.0048428665,-0.0296171461,-0.042902342,-0.0745987684,-0.0072426722,-0.0398463085,-0.0290297226,-0.0261431857,-0.04076863,0.0193696218,-0.0354559941,0.0390032289,-0.0391351306,0.0445309496,-0.0665278476]},"412":{"Abstract":"We further describe and analyze the idea of hashed alpha testing from Wyman and McGuire\u00a0[1] , which builds on stochastic alpha testing and simplifies stochastic transparency. Typically, alpha testing provides a simple mechanism to mask out complex silhouettes using simple proxy geometry with applied alpha textures. While widely used, alpha testing has a long-standing problem: geometry can disappear entirely as alpha mapped polygons recede with distance. As foveated rendering for virtual reality spreads, this problem worsens as peripheral minification and prefiltering introduce this problem on nearby objects. We first introduce the notion of stochastic alpha testing, which replaces a fixed alpha threshold of $\\alpha _\\tau =0.5$ with a randomly chosen $\\alpha _\\tau \\in [0..1)$. This entirely avoids the problem of disappearing alpha-tested geometry, but introduces temporal noise. Hashed alpha testing uses a hash function to choose $\\alpha _\\tau$ procedurally. With a good hash function and inputs, hashed alpha testing maintains distant geometry without introducing more temporal flicker than traditional alpha testing. We also describe how hashed alpha interacts with temporal antialiasing and applies to alpha-to-coverage and screen-door transparency. Because hashed alpha testing addresses alpha test aliasing by introducing stable sampling, it has implications in other domains where increased sample stability is desirable. We show how our hashed sampling might apply to other stochastic effects.","Authors":"C. Wyman; M. McGuire","DOI":"10.1109\/TVCG.2017.2739149","Keywords":"Anisotropy;alpha map;alpha test;hash;hashed alpha test;stable shading;stochastic sampling","Title":"Improved Alpha Testing Using Hashed Sampling","Keywords_Processed":"stochastic sampling;Anisotropy;hash;stable shading;alpha map;hashed alpha test;alpha test","Keyword_Vector":[0.1005022992,-0.1224078809,-0.1566733375,0.1164150857,-0.0806246844,-0.1838881909,0.0674876611,-0.0033111309,0.0413849028,-0.0020320907,0.0090491592,0.0329054985,-0.0392182777,-0.0330845623,0.0191067264,-0.0081640555,-0.0949833416,-0.0177523972,-0.02035465,0.0853664713,0.0186110687,0.0212560809,-0.0310594048,0.0785760043,-0.0056315576,-0.1153929984,-0.0692711935,-0.0098550252,-0.0121715865,-0.0638910348,0.0688146017,-0.0424766409,-0.0176135399,-0.0171435155,0.0268524341,0.0243448955,-0.0263320102,-0.0363686945,0.0243590126,0.0070577252,-0.0362858993,-0.0194991281,0.0036267903,-0.0250025954,-0.0282284126,-0.0011370353,-0.0490613703,-0.0069841842,0.0462416897,0.0149719417,0.0425354643,-0.019026549],"Abstract_Vector":[0.2025017574,-0.1154377909,0.0114883792,0.0058165109,-0.1245102691,0.0171361156,0.058154772,0.0413793729,-0.0861544661,-0.0207445862,-0.0004678837,0.0067045292,-0.0834828495,-0.0141530173,-0.0095670034,0.0389632124,0.074086752,0.0815853084,-0.021403521,-0.1216384206,0.1076403537,-0.0008799385,0.0172905259,0.029646215,-0.0711157786,0.0255034375,0.0302423245,-0.0259501867,0.0036898022,-0.0580821929,-0.0088471956,-0.0092286274,-0.0081578475,0.0177219743,-0.0623053013,-0.0545094147,0.0093728045,-0.0683110612,0.011913088,0.0025275325,-0.0399713462,0.0007816211,-0.0521255583,-0.0040822715,0.0083948435,0.0361319499,0.0180527539,0.0300884274,0.0245884428,0.0244625073,-0.0027144185,0.050391399,-0.0128257354,0.0093277125,0.0076846754,-0.0117415876,0.0261974027,0.0077743566,0.0249287598,0.0038204341,0.0020195137,-0.0084862847,-0.0294132998,0.0240640194,0.003857567,-0.0113818496,0.0056849646,0.0071238739,-0.0627020345,0.0206990731,-0.0339108949,0.0203803597,-0.0200880884,0.0105430672,-0.0236448904,0.0388369135,-0.028168708,0.0529022376,0.0126238583,0.013242554,-0.0047552655,0.0034385587,0.0433850155,-0.0037018636,0.016658843,-0.0469611634,0.0115494509,0.0681824943,0.0195674221,-0.0771580997,0.0419613779,-0.0333340985,0.0081598643,-0.0158226081,0.0113325018,0.0266682896,0.0003271678,-0.0048381948,-0.0148902332,0.0311414549,0.0070765591,0.0175675029,0.0115659993,-0.0214101516,0.0146080906,-0.0145704443,-0.0017044819,-0.0012237136,0.0073309654,0.0185466164,0.0062072205,-0.0201267397,0.0209657744,-0.024102956,-0.034665137,-0.0152824318]},"413":{"Abstract":"While ASCII art is a worldwide popular art form, automatic generating structure-based ASCII art from natural photographs remains challenging. The major challenge lies on extracting the perception-sensitive structure from the natural photographs so that a more concise ASCII art reproduction can be produced based on the structure. However, due to excessive amount of texture in natural photos, extracting perception-sensitive structure is not easy, especially when the structure may be weak and within the texture region. Besides, to fit different target text resolutions, the amount of the extracted structure should also be controllable. To tackle these challenges, we introduce a visual perception mechanism of non-classical receptive field modulation (non-CRF modulation) from physiological findings to this ASCII art application, and propose a new model of non-CRF modulation which can better separate the weak structure from the crowded texture, and also better control the scale of texture suppression. Thanks to our non-CRF model, more sensible ASCII art reproduction can be obtained. In addition, to produce more visually appealing ASCII arts, we propose a novel optimization scheme to obtain the optimal placement of proportional-font characters. We apply our method on a rich variety of images, and visually appealing ASCII art can be obtained in all cases.","Authors":"X. Xu; L. Zhong; M. Xie; X. Liu; J. Qin; T. Wong","DOI":"10.1109\/TVCG.2016.2569084","Keywords":"ASCII art synthesis;non-classical receptive field modulation;texture suppression","Title":"ASCII Art Synthesis from Natural Photographs","Keywords_Processed":"ASCII art synthesis;non classical receptive field modulation;texture suppression","Keyword_Vector":[0.1747353696,-0.2129480768,-0.1705517173,0.1981526594,-0.1412433709,-0.007095469,0.1210559351,-0.0276944732,0.006891689,-0.0379744285,0.0705155963,-0.0382564849,0.0555039396,0.0321930252,-0.0507726522,-0.0472448342,0.0150078166,-0.0160980176,-0.0028548004,-0.039323845,-0.0458989179,-0.0137709995,0.0480193302,0.0644880415,0.0001937574,0.0311835051,-0.0040170361,0.0445318842,0.0682724189,-0.0322912725,-0.0200946309,0.098745513,-0.0993527666,0.0340618614,0.0406905208,-0.0937085053,-0.0051360086,-0.0636308785,0.0624073325,-0.092209861,-0.0694308535,0.0695572538,-0.0182691891,-0.0302377559,-0.0213736266,-0.0050852523,-0.0669711623,-0.0262186708,0.0112401344,-0.0473139845,0.0097732948,-0.0272774586],"Abstract_Vector":[0.2070378461,-0.0812819134,0.0345714887,-0.0138815358,-0.0006258469,-0.0366999681,-0.0433769306,-0.0377007308,-0.0200835159,0.0018776644,0.0035402739,-0.0332447804,-0.0798088927,0.0019478518,-0.0487883218,-0.0415124848,0.0090168025,-0.0554237327,0.0059320729,-0.0409231579,-0.0328231236,-0.0687064647,0.0619976465,-0.011842649,0.0157795507,0.0527474831,-0.0265200491,0.0282373044,-0.0268035796,-0.0119586483,0.1124411779,-0.0025280982,-0.0340032645,-0.0667519628,0.0666219692,0.0209969065,-0.02184766,0.0275270957,0.0286707387,0.0039818772,-0.0233028099,-0.057848072,-0.012485987,-0.0162217631,0.0561546174,0.0328504881,0.0076495553,0.024321529,0.0151631633,0.0628170699,-0.0589318198,0.0583331249,-0.0333816726,-0.0045476542,0.0453248564,0.0028923762,-0.0117432911,0.0778546022,0.0386745445,0.0491874753,-0.0055295337,-0.0325136044,-0.0050868974,-0.057126255,0.0348374866,0.0455958168,-0.0677242052,-0.0145146071,0.0040939048,0.0066251628,0.0481326954,-0.0584153883,-0.0161549535,-0.0071307239,0.0201831129,0.0333367456,-0.006995426,0.0129405722,0.0447589323,0.0320741452,-0.0230481768,-0.0282905041,0.0178991188,-0.0679321254,0.0055917883,-0.0144855302,-0.0308395576,-0.0162828204,-0.010491872,0.0695540603,0.0511646959,0.0302775537,0.0378418322,0.0593867272,0.0414720702,0.0247465967,0.0265982382,-0.0021904294,-0.105154963,-0.0381797842,-0.0545350338,-0.050069089,0.0383250329,0.0243804373,0.0278022372,0.0047475448,0.0197415466,0.0665080636,-0.0147087325,0.0014117843,0.0064473675,0.0348470652,-0.0043439321,0.012682922,0.0120323699,-0.0571751645]},"414":{"Abstract":"We present a geometric calibration method to accurately register a galvanoscopic scanning laser projection system (GLP) based on 2D vector input data onto an arbitrarily complex 3D-shaped projection surface. This method allows for accurate merging of 3D vertex data displayed on the laser projector with geometrically calibrated standard rasterization-based video projectors that are registered to the same geometry. Because laser projectors send out a laser light beam via galvanoscopic mirrors, a standard pinhole model calibration procedure that is normally used for pixel raster displays projecting structured light patterns, such as Gray codes, cannot be carried out directly with sufficient accuracy as the rays do not converge into a single point. To overcome the complications of accurately registering the GLP while still enabling a treatment equivalent to a standard pinhole device, an adapted version is applied to enable straightforward content generation. Besides the geometrical calibration, we also present a photometric calibration to unify the color appearance of GLPs and standard video projectors maximizing the advantages of the large color gamut of the GLP and optimizing its color appearance to smoothly fade into the significantly smaller gamut of the video projector. The proposed algorithms were evaluated on a prototypical mixed video projector and GLP projection mapping setup.","Authors":"P. Pjanic; S. Willi; A. Grundh\u00f6fer","DOI":"10.1109\/TVCG.2017.2734598","Keywords":"Projector-camera systems;Calibration and registration of sensing systems;Display hardware;including 3D;stereoscopic and multi-user Entertainment;broadcast","Title":"Geometric and Photometric Consistency in a Mixed Video and Galvanoscopic Scanning Laser Projection Mapping System","Keywords_Processed":"display hardware;calibration and registration of sensing system;projector camera system;broadcast;stereoscopic and multi user Entertainment;include 3d","Keyword_Vector":[0.1070175769,-0.1475133115,-0.145703451,0.0796753805,-0.0644071968,-0.1503399292,0.1338137961,0.0081217542,0.0615075607,-0.025059316,0.018182163,0.0054247073,-0.0177495705,-0.1066258397,0.0429713335,0.0179633786,-0.021622702,0.0368476979,-0.0372185146,-0.0710990859,0.0041655544,0.0514231402,0.0201786314,-0.0186593273,0.1391827477,0.0003436825,0.0298768987,0.0370247077,-0.0658538895,-0.0995431992,-0.0125704675,-0.0581756349,0.0112987694,0.0365895981,-0.0505908942,-0.0366791152,-0.0352796078,-0.007268982,0.1051913167,-0.0378349312,0.0380100577,-0.0016394279,0.015862394,-0.0200081073,0.0014395601,-0.0292474948,-0.0639015384,-0.0123138694,0.000210035,0.0073500178,-0.0094889329,0.0087096093],"Abstract_Vector":[0.2087620927,-0.1307843207,-0.0073809954,0.0794993675,0.028729634,0.092885087,0.057748031,-0.006945506,-0.1687117702,0.1053946366,0.0619800972,-0.1526279253,-0.1014879965,0.0616446256,0.2246697764,-0.073632787,-0.0406626414,0.0750359799,-0.006480729,-0.0186611382,-0.0693455212,0.0322702054,-0.036658457,-0.0282181489,-0.0393377942,-0.0057471903,-0.0042281063,-0.0455186351,0.0493083559,0.0321545526,-0.0101993777,0.0276691506,0.0244016178,-0.0030778361,-0.003502335,-0.0207412676,0.0670708501,0.0140499201,-0.0047600492,0.0491132908,0.0071563907,0.0280087767,0.014740892,-0.0230270322,-0.001747186,-0.0029381633,0.0306693701,0.0040703762,0.0034736219,0.0520852305,0.0281108806,-0.0434953689,0.0119692119,-0.0187897172,0.084694385,0.02687027,0.0108193414,0.0096159686,-0.0029013566,-0.0148271999,-0.0272214841,-0.0040328155,-0.0188133213,-0.0252081421,0.0184277765,-0.0301916854,0.0138869752,-0.0109460405,0.0332741809,0.0187463144,-0.0287311197,-0.007999549,-0.0172718829,-0.0085941589,-0.005879102,-0.0199402948,-0.0173699631,-0.0221034566,0.0090555372,-0.0021376469,0.0527770542,0.0028608852,-0.0171519882,-0.0544757175,-0.0027645276,0.0091661898,-0.0638743652,-0.0169815444,-0.0145490814,-0.016171245,-0.0091068293,0.0253625516,0.0217506484,-0.0307164997,-0.0376804949,-0.0338628125,0.0275137633,0.0070163698,-0.0287741734,0.0006381495,0.0059679657,0.0055712124,-0.0192802325,-0.0541128236,0.0305122307,-0.0132271101,-0.0253771431,0.0179671863,-0.0082477464,0.0595957179,0.0194423047,0.0034817393,0.035683668,-0.0080725673,0.0364161471,0.0225310526]},"415":{"Abstract":"Virtual Reality (VR) applications allow a user to explore a scene intuitively through a tracked head-mounted display (HMD). However, in complex scenes, occlusions make scene exploration inefficient, as the user has to navigate around occluders to gain line of sight to potential regions of interest. When a scene region proves to be of no interest, the user has to retrace their path, and such a sequential scene exploration implies significant amounts of wasted navigation. Furthermore, as the virtual world is typically much larger than the tracked physical space hosting the VR application, the intuitive one-to-one mapping between the virtual and real space has to be temporarily suspended for the user to teleport or redirect in order to conform to the physical space constraints. In this paper we introduce a method for improving VR exploration efficiency by automatically constructing a multiperspective visualization that removes occlusions. For each frame, the scene is first rendered conventionally, the z-buffer is analyzed to detect horizontal and vertical depth discontinuities, the discontinuities are used to define disocclusion portals which are 3D scene rectangles for routing rays around occluders, and the disocclusion portals are used to render a multiperpsective image that alleviates occlusions. The user controls the multiperspective disocclusion effect, deploying and retracting it with small head translations. We have quantified the VR exploration efficiency brought by our occlusion removal method in a study where participants searched for a stationary target, and chased a dynamic target. Our method showed an advantage over conventional VR exploration in terms of reducing the navigation distance, the view direction rotation, the number of redirections, and the task completion time. These advantages did not come at the cost of a reduction in depth perception or situational awareness, or of an increase in simulator sickness.","Authors":"L. Wang; J. Wu; X. Yang; V. Popescu","DOI":"10.1109\/TVCG.2019.2898782","Keywords":"VR exploration;occlusion removal;disocclusion portal;multiperspective visualization","Title":"VR Exploration Assistance through Automatic Occlusion Removal","Keywords_Processed":"disocclusion portal;vr exploration;occlusion removal;multiperspective visualization","Keyword_Vector":[0.0362508724,-0.0111021606,-0.0127709912,-0.006365063,0.0111633601,-0.0211135735,-0.0292950436,-0.0225070463,-0.0506025626,0.027660737,-0.0274188247,0.015038625,0.018354544,-0.002283491,-0.032900303,-0.0271068917,0.0092540924,0.0155165349,-0.00783699,-0.0567599774,0.0210828233,0.048701486,0.0586933072,0.0193041119,0.0145568127,-0.0403215029,0.0685118223,-0.0235008812,-0.0341801997,-0.0084739278,-0.0114138107,0.023346011,-0.0258178438,-0.0260951481,0.0326731132,0.0314863753,0.0247395164,0.0057873539,-0.0046098668,0.0551061379,0.0053401853,-0.0117025817,-0.0049210692,0.0110275429,-0.0444958011,0.0030211453,0.0062396587,0.0052061113,0.0005015597,-0.0384964784,0.0032002494,-0.0091931847],"Abstract_Vector":[0.1779245109,-0.0006818251,-0.0278359542,-0.0087903315,-0.03781409,0.0321453423,-0.001568564,0.0112207894,0.0382684203,0.0147172745,-0.0448122957,0.0159793816,-0.0331697907,0.0035531887,-0.0299940871,0.0418148533,-0.0092462428,0.0171044717,0.005444328,-0.0185569041,-0.0233481753,-0.0234139697,0.0904579011,-0.00708099,-0.0099807607,-0.0489844377,-0.0038746207,-0.0110822663,-0.0223184395,0.0507671379,-0.0167344542,0.0182331487,-0.0352407091,-0.0154360546,0.0856311863,-0.036490143,0.0292280272,-0.0222947307,0.0119861354,0.0396916588,0.0378252535,0.0375863517,0.0231229964,0.0607937262,0.0348130015,-0.0116831873,-0.0159347311,0.0059194678,0.0064571191,-0.023043503,-0.0165045805,0.0237639075,-0.0059800338,0.0100771989,0.0274313174,0.0457915623,-0.0065435481,-0.0483151525,-0.0226848743,-0.0515924149,0.0002746777,-0.0145553371,-0.0006451191,0.0440678846,0.0165767418,-0.032870969,0.0244922877,0.0396987356,-0.0213695614,-0.020122762,0.0194026911,0.030603059,0.0005144105,-0.0021423056,-0.0068404842,0.0056996808,-0.0162696014,0.0330731244,0.0054370495,-0.0257231806,0.0136006383,0.0171858496,-0.0265081911,-0.0105596477,0.0112001815,-0.0195939196,-0.0016815826,-0.0321089169,-0.0006755445,0.0254055092,-0.007124285,0.0229637531,-0.0318732306,-0.0096592593,-0.0260892909,-0.0170585052,-0.0142331016,-0.0108052491,-0.0115228379,0.0238081272,0.012701795,0.0155614696,0.0410946112,0.0402118414,-0.0350120263,0.037748622,0.0183855127,0.0195885489,0.0065302232,0.011780672,-0.0014492652,-0.0292745049,-0.0090949121,-0.0412715788,0.0082292826,-0.003375126]},"416":{"Abstract":"Software for computer animation is generally characterized by a steep learning curve, due to the entanglement of both sophisticated techniques and interaction methods required to control 3D geometries. This paper proposes a tool designed to support computer animation production processes by leveraging the affordances offered by articulated tangible user interfaces and motion capture retargeting solutions. To this aim, orientations of an instrumented prop are recorded together with animator's motion in the 3D space and used to quickly pose characters in the virtual environment. High-level functionalities of the animation software are made accessible via a speech interface, thus letting the user control the animation pipeline via voice commands while focusing on his or her hands and body motion. The proposed solution exploits both off-the-shelf hardware components (like the Lego Mindstorms EV3 bricks and the Microsoft Kinect, used for building the tangible device and tracking animator's skeleton) and free open-source software (like the Blender animation tool), thus representing an interesting solution also for beginners approaching the world of digital animation for the first time. Experimental results in different usage scenarios show the benefits offered by the designed interaction strategy with respect to a mouse & keyboard-based interface both for expert and non-expert users.","Authors":"F. Lamberti; G. Paravati; V. Gatteschi; A. Cannav\u00f2; P. Montuschi","DOI":"10.1109\/TVCG.2017.2690433","Keywords":"Tangible user interfaces;natural user interfaces;motion capture;human-machine interaction;computer animation","Title":"Virtual Character Animation Based on Affordable Motion Capture and Reconfigurable Tangible Interfaces","Keywords_Processed":"tangible user interface;motion capture;natural user interface;computer animation;human machine interaction","Keyword_Vector":[0.0232894949,-0.0108494721,0.0063515866,0.0030397139,-0.0190260379,-0.0022456791,-0.0087422641,0.0117789681,-0.0413693431,-0.007551625,0.0114394712,0.0068455038,-0.0108379175,0.0092574932,-0.0287743968,0.0289448988,0.0292905303,0.0010410508,-0.0180334068,-0.0232958752,-0.0006065788,0.0137307887,0.0399493666,0.0339031329,-0.0218909504,-0.0220543331,0.046496041,0.0121696045,-0.0575010779,-0.0107727555,-0.0084595141,0.0534775725,-0.0084646895,-0.0025364444,0.0391491877,-0.0379396192,0.0464289049,-0.0420334731,0.0098122425,0.0572119521,0.0089117772,0.0165396729,-0.0036964029,-0.0081115723,-0.0174380778,-0.0614104925,0.0154316754,-0.0008193429,-0.0046710621,-0.0054159483,0.0211136378,-0.0295358367],"Abstract_Vector":[0.1986140317,-0.0255945025,-0.0467033074,0.0291373521,-0.0363008195,0.0247092487,0.0628029555,0.0289507851,0.004998457,0.033943911,-0.0464878251,0.0401363534,-0.0614705483,-0.0279166066,-0.0158673116,-0.0425502431,-0.0287857433,-0.0414878747,-0.0012890077,0.0243299625,-0.0998296412,0.015766474,0.1884005646,0.0069749395,0.0054827788,-0.0782518054,0.0064119618,-0.0746985798,0.1017043214,0.0093648636,-0.0344563807,0.0322081657,-0.0593928243,-0.0204580485,0.0222266417,-0.0620350742,0.0192726525,0.0194286866,0.0008034654,-0.0006300955,-0.0246487157,0.0382534234,0.0648000396,0.0564860304,0.0089603838,0.0254389003,-0.0102264335,-0.0178469756,-0.0244927455,-0.0038473588,0.0250307475,0.0798598172,0.0242623857,-0.0210007193,0.0217372802,-0.0529374958,0.0225407676,-0.0757767837,-0.0143445942,-0.0272588765,0.0002896997,0.0469667735,-0.0005594852,-0.04207114,0.0003078089,0.0490070931,-0.0163280018,-0.0066947521,-0.0047118685,0.0380814651,0.0171981349,0.0011652401,0.0037834225,-0.0029288557,0.0044563032,0.0185270527,-0.0051646748,-0.0169970634,-0.000487483,0.0027835073,-0.0009625437,0.0085325901,0.000761101,-0.039792089,-0.0240499919,-0.0278407011,-0.049353712,-0.0147699742,-0.0140181289,-0.0067841991,-0.0001888807,0.0176369696,-0.0148007447,0.0533636683,-0.021426654,0.0000684596,-0.0098321354,0.0136367186,0.0385297196,0.0058265365,-0.006885328,-0.0217615611,0.0389783014,0.0162542175,0.0079341066,0.0205351277,-0.0012930488,0.0230732566,0.0003572771,0.0137211704,-0.006246044,-0.0095104676,0.0054895313,-0.0281128108,-0.0025614665,0.0084013427]},"417":{"Abstract":"Constructing distributed representations for words through neural language models and using the resulting vector spaces for analysis has become a crucial component of natural language processing (NLP). However, despite their widespread application, little is known about the structure and properties of these spaces. To gain insights into the relationship between words, the NLP community has begun to adapt high-dimensional visualization techniques. In particular, researchers commonly use t-distributed stochastic neighbor embeddings (t-SNE) and principal component analysis (PCA) to create two-dimensional embeddings for assessing the overall structure and exploring linear relationships (e.g., word analogies), respectively. Unfortunately, these techniques often produce mediocre or even misleading results and cannot address domain-specific visualization challenges that are crucial for understanding semantic relationships in word embeddings. Here, we introduce new embedding techniques for visualizing semantic and syntactic analogies, and the corresponding tests to determine whether the resulting views capture salient structures. Additionally, we introduce two novel views for a comprehensive study of analogy relationships. Finally, we augment t-SNE embeddings to convey uncertainty information in order to allow a reliable interpretation. Combined, the different views address a number of domain-specific tasks difficult to solve with existing tools.","Authors":"S. Liu; P. Bremer; J. J. Thiagarajan; V. Srikumar; B. Wang; Y. Livnat; V. Pascucci","DOI":"10.1109\/TVCG.2017.2745141","Keywords":"Natural Language Processing;Word Embedding;High-Dimensional Data","Title":"Visual Exploration of Semantic Relationships in Neural Word Embeddings","Keywords_Processed":"word Embedding;high Dimensional Data;Natural Language Processing","Keyword_Vector":[0.2148889551,-0.0840140949,-0.0312538613,-0.0580964084,0.0408790584,-0.1185684774,-0.0900835102,-0.018670224,-0.0672130828,-0.0233076979,0.0872737907,0.0266663228,0.0072282944,0.0522444606,0.0518709843,0.1274229394,0.0491720928,0.0796996145,0.2304637888,0.0087082461,-0.1878274607,0.0657693835,0.1154940732,0.0362549897,-0.1142790872,-0.0762940017,-0.0756346922,0.0232930453,0.1517867332,-0.1669439372,-0.0382928389,-0.1229094735,0.1774440749,0.0346617837,-0.0051140141,-0.0792818502,-0.0283524695,0.0376733309,-0.0399471334,-0.0958099304,-0.0511200653,0.0429656211,0.1001386306,0.0246771466,-0.0266148046,-0.0545723693,0.047943673,0.0943184707,-0.1002541348,0.0091000321,0.0236865802,0.0258653393],"Abstract_Vector":[0.1846337271,-0.1382808808,0.0170402423,-0.0172583481,-0.1001925561,0.0408531729,0.0635013789,0.0688543533,-0.04727947,-0.009590385,-0.0204570555,0.0147846031,0.1010958889,-0.0197577386,-0.007911135,-0.0436961183,-0.0441202282,0.0196993472,0.0101197898,0.0005241998,-0.0039283703,0.0176168892,-0.012843638,0.0059062343,-0.0151347519,0.0018931543,-0.0260505704,0.0242642575,0.0640559423,0.0202355618,-0.0200238189,0.0308846782,0.0070892547,-0.0442359017,-0.0341438263,-0.0012599193,-0.0073855281,0.0066910511,0.0330632847,0.0188652927,-0.0019932128,-0.0105080524,0.0088785536,0.027503902,0.0035954578,0.018093198,0.0167811022,-0.0069958283,0.0181417662,-0.0098952253,-0.0036334004,-0.0391131429,-0.0840466541,-0.0247249423,-0.0672601462,-0.0130563094,0.0278350506,0.0341349598,0.0300379877,0.0182308145,0.0141705883,0.0258024871,-0.0011195849,0.0210471025,0.0007685467,-0.0333707946,0.0147448469,-0.0003489115,0.043180093,-0.0146782251,0.0369833695,0.0469373813,0.0371953634,-0.0452666881,-0.0431564013,-0.0483688954,0.0190036578,0.0222785423,0.0198256325,-0.0468821958,0.0117598323,0.0041850827,0.0066561477,0.0072018779,0.0393636507,0.0364370753,0.0012258184,0.0387981644,0.0118201993,0.0011306919,0.0180258687,0.0246127161,-0.0215678956,0.0086471489,0.0490885286,-0.0045704462,-0.0359493849,-0.0183442345,-0.000853963,-0.0367039176,-0.0125122516,-0.00793389,0.0103514956,-0.0088557037,0.0207458273,0.0635242572,0.0195821671,0.0503111515,0.0145128532,-0.0446008709,-0.0400066627,0.0045934259,-0.0017207587,0.002268428,0.0332720849,-0.0112708095]},"418":{"Abstract":"Event sequence data such as electronic health records, a person's academic records, or car service records, are ordered series of events which have occurred over a period of time. Analyzing collections of event sequences can reveal common or semantically important sequential patterns. For example, event sequence analysis might reveal frequently used care plans for treating a disease, typical publishing patterns of professors, and the patterns of service that result in a well-maintained car. It is challenging, however, to visually explore large numbers of event sequences, or sequences with large numbers of event types. Existing methods focus on extracting explicitly matching patterns of events using statistical analysis to create stages of event progression over time. However, these methods fail to capture latent clusters of similar but not identical evolutions of event sequences. In this paper, we introduce a novel visualization system named EventThread which clusters event sequences into threads based on tensor analysis and visualizes the latent stage categories and evolution patterns by interactively grouping the threads by similarity into time-specific clusters. We demonstrate the effectiveness of EventThread through usage scenarios in three different application domains and via interviews with an expert user.","Authors":"S. Guo; K. Xu; R. Zhao; D. Gotz; H. Zha; N. Cao","DOI":"10.1109\/TVCG.2017.2745320","Keywords":"Visual Knowledge Representation;Visual Knowledge Discovery;Data Clustering;Time Series Data;Illustrative Visualization","Title":"EventThread: Visual Summarization and Stage Analysis of Event Sequence Data","Keywords_Processed":"Illustrative visualization;visual Knowledge Discovery;Data Clustering;Time Series Data;visual Knowledge Representation","Keyword_Vector":[0.0949717365,-0.0317715478,0.025569256,-0.0479972685,0.0048499096,-0.0009165254,-0.0316735499,0.0011670554,0.0065285317,-0.0218856665,-0.0003618481,0.0064507685,-0.0065694275,-0.0159642948,-0.0235264511,0.0243556766,-0.0077223025,0.0250624472,-0.0142702557,0.0107358752,-0.0295753532,-0.0084283764,-0.0232290349,-0.0247102993,0.0192320516,0.0096874736,0.0068936688,0.006728411,-0.0016447349,0.0037379612,0.0022501303,0.009924195,-0.0018028674,-0.0172430088,-0.0056963925,-0.0125872004,0.0160562721,-0.0088165827,-0.0143676171,-0.0184448912,-0.0319565406,-0.0025891062,0.0197806429,-0.0088557473,0.004965548,-0.0065080784,0.0228022186,0.010492205,0.0088007705,0.0142442435,0.0098886781,0.0195539406],"Abstract_Vector":[0.1188814151,-0.0765673316,0.0199848053,-0.0049553702,-0.0245786844,-0.0032205438,0.0113982561,0.0036006743,-0.0152649887,0.0436511945,-0.0164610554,0.0239662191,-0.0294374788,0.0184230688,-0.0227068467,-0.0275923895,-0.0228037495,-0.0332503786,-0.048849516,-0.0066013533,-0.016506109,0.0120894505,0.042657065,0.0194884202,-0.0240514583,0.0164737782,0.0330614704,0.0420853509,0.0076661193,-0.0087745358,-0.0141159047,-0.0071335133,0.0200784372,-0.0165516965,-0.010057741,0.0373939378,-0.0554829194,0.007161744,-0.0144774552,-0.0139709844,0.0204244463,0.0048736283,0.0206383867,-0.0156126667,0.0242278067,-0.0334934559,0.0428213132,-0.1067085834,0.0161199616,-0.0602723402,-0.0289184924,-0.0443108359,-0.0073942816,0.0669463107,-0.0661600436,0.0155346596,0.0136745519,-0.0030536791,0.0313575069,0.0237698735,-0.0083182561,-0.0243344317,0.009108864,-0.0352656984,0.0200801133,-0.0410971298,0.0420070829,0.0167469269,-0.0349657837,-0.0275584427,0.0032928638,-0.0457238102,-0.0541672172,-0.00658649,0.0291542427,-0.0083376176,0.0243864298,-0.0377593531,0.0218942116,0.0346208088,0.0023067822,-0.0381372216,-0.0436268809,0.0019473719,0.1086308388,-0.0009525588,0.008298567,-0.0120606725,0.0227552928,-0.0353875932,-0.0089712401,-0.0507268922,-0.0487578646,0.008368525,0.0444987877,-0.024450701,-0.0052326578,-0.0030440369,-0.0290112406,0.0083350387,-0.0213443473,0.0548305031,-0.0123713122,-0.0028775849,-0.033897953,0.0042070932,-0.0153157716,-0.0298591568,0.013584055,0.0065913059,-0.0387483552,0.0572967677,0.0502583326,-0.0014325637,-0.0260205336,-0.0335410876]},"419":{"Abstract":"Using synthetic videos to present a 3D scene is a common requirement for architects, designers, engineers or Cultural Heritage professionals however it is usually time consuming and, in order to obtain high quality results, the support of a film maker\/computer animation expert is necessary. We introduce an alternative approach that takes the 3D scene of interest and an example video as input, and automatically produces a video of the input scene that resembles the given video example. In other words, our algorithm allows the user to \u201creplicate\u201d an existing video, on a different 3D scene. We build on the intuition that a video sequence of a static environment is strongly characterized by its optical flow, or, in other words, that two videos are similar if their optical flows are similar. We therefore recast the problem as producing a video of the input scene whose optical flow is similar to the optical flow of the input video. Our intuition is supported by a user-study specifically designed to verify this statement. We have successfully tested our approach on several scenes and input videos, some of which are reported in the accompanying material of this paper.","Authors":"A. Baldacci; F. Ganovelli; M. Corsini; R. Scopigno","DOI":"10.1109\/TVCG.2016.2608828","Keywords":"Multimedia content production;video similarity;2D vector field comparison;computer animation","Title":"Presentation of 3D Scenes Through Video Example","Keywords_Processed":"video similarity;2d vector field comparison;Multimedia content production;computer animation","Keyword_Vector":[0.1835231766,-0.0957618638,-0.0039451292,0.0765882805,-0.0433229807,0.2209576089,-0.0581615463,0.0443130595,0.0402044669,-0.0447938829,0.0353617879,0.0102849026,-0.0017332363,-0.000357857,-0.0596171575,-0.1135717497,-0.0983568164,-0.0012684754,0.1065175137,0.2982408584,0.0664658831,0.1103135975,0.0966377709,-0.00388549,0.0866635363,0.0062310207,0.0164161071,0.1019503182,-0.0898362102,0.0604884615,-0.1413639164,-0.0637133311,0.0294551659,0.0791283756,-0.0625107125,0.0091873383,0.0465139321,0.020247712,-0.0366928486,-0.0385439956,-0.0789763048,-0.0044479225,0.0324190727,0.0300753417,0.0044407179,0.0958834858,0.0145758373,0.0224636626,-0.0113125935,-0.0054890741,0.0207024015,0.0397365806],"Abstract_Vector":[0.1666043931,-0.0830294085,0.0132706458,-0.0073612798,0.0255117254,-0.1034620421,-0.0641574725,-0.030920545,0.0329398676,-0.0510901627,0.0547652548,-0.0953808136,-0.0482395983,-0.0428058087,0.0067403986,0.0151213298,-0.037682128,-0.0326881231,-0.015883968,0.0202417427,0.0755273978,-0.0778576841,-0.0186360627,-0.0375566359,-0.0403997914,-0.0521640342,-0.0201055893,0.04145015,0.0806994436,0.0184571421,-0.0807238877,-0.055654486,0.023153964,-0.0100800481,-0.1398188245,0.0589929697,0.0505600058,-0.0521705358,-0.0958367414,0.0640724466,0.0294913714,-0.0148501444,0.0110399666,0.0283239773,-0.0370147263,-0.0539980753,0.0006462487,-0.1013222182,-0.0115597739,-0.0182590406,-0.0619078524,-0.0450184203,-0.0498454363,0.0532738188,0.0298328865,-0.070586685,0.1055600183,-0.0738666927,-0.020338734,0.0048324344,0.0000794013,-0.0271271379,0.0408763516,0.0266030137,0.030043188,0.0729273899,-0.0562879388,-0.040945835,0.0151501496,0.0162296433,-0.006434337,0.012223036,0.0246496323,-0.0212172732,0.0778964439,-0.0518624417,-0.0320474679,0.0325857584,-0.0386222388,-0.0296186885,0.076605225,0.0186441487,-0.065386242,-0.0820824072,-0.0227683555,0.0849064362,-0.027935596,0.0436022842,0.0400792932,0.0013552006,0.0136445845,0.0078848004,0.0651167466,0.049737728,-0.0620528061,0.0135970708,0.0212860305,0.0320403962,0.0363562313,-0.0213681928,-0.0067327016,0.0244883781,-0.0092601777,0.0668711492,0.0107705193,0.0091522182,-0.0323922478,0.0211980899,-0.0102986735,-0.0486137691,-0.0040977859,-0.0215484424,0.0144747198,-0.0275228401,-0.035112717,-0.042238248]},"42":{"Abstract":"Offsetting-based hollowing is a solid modeling operation widely used in 3D printing, which can change the model's physical properties and reduce the weight by generating voids inside a model. However, a hollowing operation can lead to additional supporting structures for fabrication in interior voids, which cannot be removed. As a consequence, the result of a hollowing operation is affected by these additional supporting structures when applying the operation to optimize physical properties of different models. This paper proposes a support-free hollowing framework to overcome the difficulty of fabricating voids inside a solid. The challenge of computing a support-free hollowing is decomposed into a sequence of shape optimization steps, which are repeatedly applied to interior mesh surfaces. The optimization of physical properties in different applications can be easily integrated into our framework. Comparing to prior approaches that can generate support-free inner structures, our hollowing operation can reduce more volume of material and thus provide a larger solution space for physical optimization. Experimental tests are taken on a number of 3D models to demonstrate the effectiveness of this framework.","Authors":"W. Wang; Y. Liu; J. Wu; S. Tian; C. C. L. Wang; L. Liu; X. Liu","DOI":"10.1109\/TVCG.2017.2764462","Keywords":"Shape optimization;support-free;hollowing;topology variation;3D printing","Title":"Support-Free Hollowing","Keywords_Processed":"3d printing;topology variation;support free;shape optimization;hollow","Keyword_Vector":[0.0296170652,-0.0004562444,-0.0028812131,0.0062566405,0.0241861828,-0.0120499332,0.0065343118,-0.0060551517,-0.0094195697,-0.0151760946,0.0181254407,0.0312517006,0.0134878123,0.0216494644,0.0088545782,-0.031374195,0.0322465328,-0.0195947607,0.0152410857,-0.0139232922,0.0141180638,0.0361581209,-0.0505153735,-0.0173193791,0.0276881441,0.0176557459,-0.0198710771,-0.0136569025,0.0253620884,0.0240489867,-0.0272519442,-0.0293421185,-0.0157270104,0.0516626447,0.0160116901,0.0221520741,0.0259795056,0.0144368457,-0.0174760995,0.0335758565,0.041663372,0.0451145127,-0.0477578458,-0.0067326553,-0.025889026,-0.060032872,-0.0290057145,0.0034477603,0.0188806693,-0.0077440063,0.0390480671,0.0387222551],"Abstract_Vector":[0.1466599128,-0.0564207555,0.0017510155,0.0564541457,0.0057843526,0.0711985475,-0.0021432054,-0.0108133379,0.013094129,-0.0047175909,-0.0556011602,0.0432759317,-0.0155475016,-0.0249180637,0.0357904583,0.0649480699,-0.0549899525,0.0049341952,-0.0159571984,0.0141281749,-0.0400499629,-0.0185179869,0.0230287898,0.0864179389,-0.0117795477,-0.0373880431,-0.0043889355,0.0854791451,-0.007652645,0.0335829027,0.0213822603,0.0696749217,0.0451448726,0.0483002777,-0.0059159191,0.0319298742,0.0031750728,-0.0559639239,0.0164651443,0.0361235578,-0.0023345996,0.028262161,0.019709613,-0.0107560375,0.0344079327,-0.0133687526,0.0133675276,0.0284142473,-0.0246864148,0.0226185503,-0.048276189,0.0687952713,0.0256350831,-0.0344020396,-0.0073110007,-0.034382081,-0.0089360287,-0.0030514564,-0.0442958514,0.0371985493,0.0237821077,-0.0647486182,-0.0051447673,0.0338662826,-0.0508251409,0.0260477495,-0.0404194421,0.0214681477,0.0065930416,0.0009139631,-0.0155004048,0.0257231482,-0.0200359473,-0.0047505204,-0.0347297679,0.0103033626,-0.01012427,0.0502294013,0.0596473525,0.0042536531,-0.0279747625,0.0034721132,0.0062259447,0.010889767,-0.0204540052,-0.0467103438,0.0386273868,-0.0020880965,-0.0164040136,-0.0221689994,-0.0334754647,-0.0018445678,-0.0128581956,0.0140607948,-0.0019324021,0.0325011455,0.00816827,-0.0070805843,-0.0092762277,-0.0101968638,-0.0004146816,0.026612691,-0.0086915012,-0.0381875014,-0.0078237768,-0.0082387046,-0.0347473236,0.00969624,0.0139852681,0.0313865022,0.0310520088,-0.0118640411,-0.0047807163,0.0292634124,-0.0235597072,-0.0002787221]},"420":{"Abstract":"Preattentive visual features such as hue or flickering can effectively draw attention to an object of interest - for instance, an important feature in a scientific visualization. These features appear to pop out and can be recognized by our visual system, independently from the number of distractors. Most cues do not take advantage of the fact that most humans have two eyes. In cases where binocular vision is applied, it is almost exclusively used to convey depth by exposing stereo pairs. We present Deadeye, a novel preattentive visualization technique based on presenting different stimuli to each eye. The target object is rendered for one eye only and is instantly detected by our visual system. In contrast to existing cues, Deadeye does not modify any visual properties of the target and, thus, is particularly suited for visualization applications. Our evaluation confirms that Deadeye is indeed perceived preattentively. We also explore a conjunction search based on our technique and show that, in contrast to 3D depth, the task cannot be processed in parallel.","Authors":"A. Krekhov; J. Kr\u00fcger","DOI":"10.1109\/TVCG.2018.2864498","Keywords":"Popout;preattentive vision;comparative visualization;dichoptic presentation","Title":"Deadeye: A Novel Preattentive Visualization Technique Based on Dichoptic Presentation","Keywords_Processed":"dichoptic presentation;popout;comparative visualization;preattentive vision","Keyword_Vector":[0.1187261603,-0.0809266274,0.0140210308,-0.1157736269,0.023476879,-0.0383925922,-0.0538104664,-0.0091681057,-0.026528169,0.0398769449,-0.0075629768,-0.077015299,-0.0165045631,0.0091578273,-0.0225765566,0.0027833891,-0.0052122529,-0.0265306036,-0.0471854214,-0.0163340569,0.0374608422,0.0082171481,0.0542471664,-0.0040766338,-0.0370045829,-0.009855639,0.0526126869,-0.0027582445,-0.0428050502,-0.0130782511,-0.0192407299,0.0369969134,0.0274924831,0.0199013334,0.0049739249,-0.0586521048,0.0706073459,-0.0637867692,0.0057205195,0.0483580971,0.004455337,0.0142757405,0.0104040465,0.0097702357,0.0189224231,-0.0480246306,0.0354518623,0.0138601876,0.010812128,0.0589799824,0.0038914113,0.0256083885],"Abstract_Vector":[0.168618299,-0.1141104885,0.0262454844,-0.0032663522,-0.0543102493,0.0337191731,0.0149145358,0.0130820626,-0.0051270353,-0.0206067091,-0.0338820343,0.0572568643,0.0672024055,0.0152842522,-0.0113439076,-0.0447006737,-0.0772255347,-0.0650176066,-0.0013617084,-0.0305564838,0.0045101408,0.0336890967,-0.0166686077,-0.0440646752,0.0171377652,0.0525328087,0.0126405736,-0.0017789078,0.0326800012,0.0292358248,-0.034150433,-0.0298634233,0.037168496,-0.031145118,-0.0032386428,-0.0009949765,0.0141518786,0.0473822115,0.0060112638,0.0198060242,-0.0246407566,0.0053702592,0.0338749418,0.0078968819,-0.0100797617,0.0119346015,0.0192097662,0.0032285116,-0.0015021845,-0.0010351482,-0.0019942292,-0.0164702803,-0.0205094668,-0.0088425064,-0.0504132266,-0.0119516437,0.0175471758,0.0526626608,0.0049281292,-0.004925206,-0.0039522126,-0.0292923741,-0.0284272301,0.0055699417,0.000694743,0.0208994161,0.0168481876,0.0050299162,-0.0258982741,0.0160001275,-0.0038143408,0.0034044737,-0.0006530201,-0.0279646198,-0.0249464953,0.0132049864,-0.007054925,0.0128218657,-0.0413553144,-0.0579428344,-0.0057839539,-0.0096488209,-0.004885188,-0.0198813729,0.0042016202,0.012394782,0.0352848208,-0.0046317181,0.0219854837,0.0053740049,-0.0133091488,0.0007403344,0.0268138818,-0.0161838784,-0.015551712,-0.0006351781,-0.0220127426,0.0232734959,-0.013195391,-0.0025434015,-0.0303465929,-0.0107297769,0.0284653791,-0.0006180118,0.0100657337,0.019204836,-0.0234067797,-0.0122893087,-0.0245384923,0.0075766272,-0.0063714878,0.0360229749,-0.002894859,0.0144856914,-0.0226880844,-0.0078815925]},"421":{"Abstract":"When analyzing a visualized network, users need to explore different sections of the network to gain insight. However, effective exploration of large networks is often a challenge. While various tools are available for users to explore the global and local features of a network, these tools usually require significant interaction activities, such as repetitive navigation actions to follow network nodes and edges. In this paper, we propose a structure-based suggestive exploration approach to support effective exploration of large networks by suggesting appropriate structures upon user request. Encoding nodes with vectorized representations by transforming information of surrounding structures of nodes into a high dimensional space, our approach can identify similar structures within a large network, enable user interaction with multiple similar structures simultaneously, and guide the exploration of unexplored structures. We develop a web-based visual exploration system to incorporate this suggestive exploration approach and compare performances of our approach under different vectorizing methods and networks. We also present the usability and effectiveness of our approach through a controlled user study with two datasets.","Authors":"W. Chen; F. Guo; D. Han; J. Pan; X. Nie; J. Xia; X. Zhang","DOI":"10.1109\/TVCG.2018.2865139","Keywords":"Large Network Exploration;Structure-Based Exploration;Suggestive Exploration","Title":"Structure-Based Suggestive Exploration: A New Approach for Effective Exploration of Large Networks","Keywords_Processed":"structure base Exploration;Suggestive Exploration;Large Network Exploration","Keyword_Vector":[0.1072236191,0.005601233,0.1201500317,-0.0185644051,-0.0402001571,-0.0082935235,-0.0203456429,-0.0443739474,0.017156461,-0.1523315683,-0.023755615,0.0449466257,0.006863109,-0.0375284984,0.0204567484,-0.0241582765,0.0629988946,-0.1154956684,0.0406920688,-0.0774902057,0.1467815358,0.0650389646,-0.1308251955,0.0796307529,-0.0410937686,0.0129406904,-0.0282414125,0.0090874447,-0.0075089841,-0.0110556419,-0.0833365248,-0.0225670067,0.1294932784,-0.1500146173,-0.0225457537,-0.0315449068,-0.0670254317,-0.1519641166,0.0090490253,0.0525912426,-0.0406681935,0.0010056807,-0.1395458255,0.2049698367,-0.1107005565,0.0068990348,-0.0747901968,0.0296631867,-0.0464128764,0.0359208162,-0.0219320279,0.042528891],"Abstract_Vector":[0.1639783732,0.0201520851,0.0319740776,0.0010016652,-0.0251103213,0.0218917391,0.0682061801,-0.0027354586,0.010128592,-0.0183565486,0.0256211601,0.0135922312,-0.0346482476,-0.0107187517,-0.0018406211,0.0346608375,0.0031955272,0.0596097541,-0.0102610967,-0.0954799695,0.078896742,-0.0610016966,0.0353681382,0.010555687,0.0290400124,0.0038003289,0.0514500462,-0.0067127297,0.0071927844,0.0122819651,-0.0133610304,0.0013242483,-0.0067728877,0.0191978717,0.0224529028,-0.0289503292,0.0367731809,-0.0237732371,-0.0137647727,-0.0061588413,-0.0021052884,-0.0022969202,-0.011286509,0.0048816928,-0.0210325079,0.0115534207,0.0044268292,0.0544701016,0.0046295976,-0.020012222,0.0230498261,0.0055631725,-0.0358221179,0.006661448,-0.0115507329,0.0159578445,0.0242243391,-0.0174978057,0.0671411863,0.0412580493,0.0180765357,0.0004395505,0.0024499756,-0.0120215656,-0.0450427605,-0.0314550058,-0.0073340717,0.0024431738,-0.0360253378,0.022507407,-0.0176020059,0.0019057068,0.0007956144,0.0342092985,-0.0035265848,-0.0036614882,0.0145202403,0.0024617772,0.0146800444,-0.0084297178,0.0000499203,-0.0121114298,-0.0075759513,-0.0233703128,-0.0263078045,0.0152669799,0.0201271552,0.0109552496,0.0013281009,-0.0011578038,0.0221589644,-0.0062194222,-0.0026042103,0.0312198262,-0.0095569576,-0.0270208003,0.0025886816,0.0009437975,0.0208274195,0.0315181516,-0.0230443402,0.0155502556,-0.0133333272,0.010582918,-0.0253485012,0.0051822634,-0.0103034864,-0.016618042,0.0290845956,-0.0145558702,0.0076456874,-0.0268598072,0.0060274933,-0.0224083909,0.0234654097,-0.0186144698]},"422":{"Abstract":"Branched covering spaces are a mathematical concept which originates from complex analysis and topology and has applications in tensor field topology and geometry remeshing. Given a manifold surface and an$N$-way rotational symmetry field, a branched covering space is a manifold surface that has an$N$-to-1 map to the original surface except at theramification points, which correspond to the singularities in the rotational symmetry field. Understanding the notion and mathematical properties of branched covering spaces is important to researchers in tensor field visualization and geometry processing, and their application areas. In this paper, we provide a framework to interactively design and visualize the branched covering space (BCS) of an input mesh surface and a rotational symmetry field defined on it. In our framework, the user can visualize not only the BCSs but also their construction process. In addition, our system allows the user to design the geometric realization of the BCS using mesh deformation techniques as well as connecting tubes. This enables the user to verify important facts about BCSs such as that they are manifold surfaces around singularities, as well as theRiemann-Hurwitz formulawhich relates the Euler characteristic of the BCS to that of the original mesh. Our system is evaluated by student researchers in scientific visualization and geometry processing as well as faculty members in mathematics at our university who teach topology. We include their evaluations and feedback in the paper.","Authors":"L. Roy; P. Kumar; S. Golbabaei; Y. Zhang; E. Zhang","DOI":"10.1109\/TVCG.2017.2744038","Keywords":"Tensor field topology;math visualization;branched covering spaces visualization;rotational symmetries;ramification points","Title":"Interactive Design and Visualization of Branched Covering Spaces","Keywords_Processed":"branch cover space visualization;math visualization;rotational symmetry;ramification point;tensor field topology","Keyword_Vector":[0.1267814225,-0.1482933579,-0.1468736998,0.1327702595,-0.0555545158,-0.17232376,0.0422334925,0.0446214737,0.063010386,0.0195283267,0.0165947532,0.0778892013,-0.0050995664,-0.0405075235,0.0597494141,0.0460277865,0.0369654563,-0.0892821533,0.0240200598,0.0952709062,0.0304982018,0.0368381168,0.0156324149,-0.0527295899,0.0215181756,0.0377553335,-0.0132618318,-0.0371939107,0.0335781039,-0.024021189,0.0270285417,0.0236932728,0.0256234482,-0.0730352209,0.0656542287,0.072336585,0.0079460855,-0.0323621963,0.0170893797,-0.0437511622,0.0564909601,-0.0085764764,-0.0226985879,-0.0000696108,0.0553107556,-0.0263441652,-0.1011050505,-0.0444477679,-0.0687566156,-0.0076236901,0.0867240787,-0.0314928355],"Abstract_Vector":[0.180313056,-0.0956910687,0.0018281044,-0.0156715099,-0.0548378052,0.0103114512,0.0167954359,0.019425,-0.0003620934,-0.0099714038,-0.0167887969,-0.0238534684,-0.0652504116,-0.0111088478,-0.0103632862,0.0163351324,0.0105931704,0.0391788404,-0.0026610809,0.0290293136,0.0202765689,-0.0442460602,0.0186194865,0.0071366052,-0.0144751655,-0.0113175332,-0.0104629437,0.0381972277,0.0054628436,-0.0448917214,-0.0144320697,0.050800569,-0.0055656035,0.0249759176,-0.068515551,0.0403015915,0.0288572822,-0.0189815467,-0.029461995,0.0001078342,-0.0435955951,0.0175672052,-0.0008852062,-0.0164791013,0.0344151802,0.0038419756,-0.0355611908,0.0086219833,-0.0192667501,-0.0001868794,-0.0200081825,-0.0222227123,-0.0170722519,-0.0143013591,0.0179649331,-0.0410065624,0.013260708,0.004172801,-0.0139278247,0.0250674613,-0.0134569261,-0.0006198633,-0.0226347767,-0.0140901212,0.0082567359,-0.0302237325,-0.025469201,-0.0328279832,-0.0038209449,-0.0286268327,0.0193737402,0.021745748,-0.0032083691,0.0177223243,0.0009076584,0.0174326518,-0.0027298901,-0.0226341274,-0.0152433787,0.0031664233,0.0228159182,0.0163204693,-0.0282853973,0.0354932324,-0.0048966497,0.0018106752,0.002754994,-0.0218389931,-0.0058880039,0.0007857853,-0.0138198599,0.0116517797,-0.0059540036,-0.0180252941,0.0012959144,0.0499775182,0.0073603809,-0.0148318933,-0.0380881884,-0.0007684856,-0.0388437973,0.0687714747,0.0179564013,0.0137927312,0.0065570809,-0.0321220013,0.0140233295,-0.0158876114,-0.01706213,-0.0411896117,-0.0175505819,-0.0214360101,0.0096684343,-0.0025894222,0.0449145188,0.0203055318]},"423":{"Abstract":"We present a method for realtime reconstruction of an animating human body,which produces a sequence of deforming meshes representing a given performance captured by a single commodity depth camera. We achieve realtime single-view mesh completion by enhancing the parameterized SCAPE model.Our method, which we call Realtime SCAPE, performs full-body reconstruction without the use of markers.In Realtime SCAPE, estimations of body shape parameters and pose parameters, needed for reconstruction, are decoupled. Intrinsic body shape is first precomputed for a given subject, by determining shape parameters with the aid of a body shape database. Subsequently, per-frame pose parameter estimation is performed by means of linear blending skinning (LBS); the problem is decomposed into separately finding skinning weights and transformations. The skinning weights are also determined offline from the body shape database,reducing online reconstruction to simply finding the transformations in LBS. Doing so is formulated as a linear variational problem;carefully designed constraints are used to impose temporal coherence and alleviate artifacts. Experiments demonstrate that our method can produce full-body mesh sequences with high fidelity.","Authors":"Y. Chen; Z. Cheng; C. Lai; R. R. Martin; G. Dang","DOI":"10.1109\/TVCG.2015.2478779","Keywords":"Realtime reconstruction;human animation;depth camera;SCAPE","Title":"Realtime Reconstruction of an Animating Human Body from a Single Depth Camera","Keywords_Processed":"SCAPE;depth camera;human animation;realtime reconstruction","Keyword_Vector":[0.0640558726,0.0078345818,-0.0189482445,0.0218332088,0.0925128836,-0.0666307562,-0.0531983865,-0.041695364,-0.0562940407,0.0441101793,-0.0071221299,0.0843772361,0.0309087262,0.0239402729,-0.0373150199,0.0080127088,-0.0297231881,-0.029471638,-0.011383188,-0.0044749462,0.0180999063,0.0513918833,-0.0131247538,0.0396686395,-0.0539141498,0.0024027883,-0.0322234266,0.050472977,0.0427846619,-0.0027996633,0.0032887968,-0.0520623287,-0.0866318666,0.0003256298,-0.0359965751,-0.0699012158,0.0665671777,-0.0631753752,-0.0438672293,-0.0785127057,0.0281586934,0.0680974586,-0.0140981084,-0.0200797966,-0.0128757811,0.0242605001,0.000539242,0.020476068,0.0712399952,-0.033367066,-0.0118517739,0.0049240846],"Abstract_Vector":[0.1688469174,-0.0920681689,-0.0111319811,0.0417572295,-0.0407024022,0.0741740879,-0.0004913706,0.0217808408,0.002359082,-0.0042326067,-0.0655309339,0.0724648624,0.0245022581,-0.0495722249,0.0525077828,0.0584413909,-0.0415125447,0.0405379232,-0.0542084952,-0.0360014814,-0.0287193207,-0.0434232849,0.0109332076,-0.0260015031,-0.0432996302,0.0113310144,-0.0177267045,0.0017030838,-0.0262444478,-0.0233355363,-0.0580102645,0.0299786418,0.0090404383,0.0240725524,0.017042085,0.0214227701,-0.0258839379,-0.0088894766,-0.0663866462,0.0277254103,0.0348002194,0.0314304059,0.056793355,-0.0952169091,0.0458125517,-0.0028515942,0.0150504119,0.0323312662,0.0052335327,0.0752844991,-0.0179858781,0.0580372564,0.0616781169,0.0071853769,0.0524833105,0.0332070652,0.0131203486,-0.0258661246,-0.0323860787,-0.04888465,-0.0103020192,-0.0603296009,-0.0193340577,-0.0638042044,-0.0587981691,-0.0111232822,0.0049055622,-0.0196344384,-0.009372187,-0.0129277282,-0.0461858227,-0.0073844644,0.0025146774,0.0595190169,0.0117201055,-0.0040739971,0.0219034298,0.0635181607,0.0047161892,-0.0411555346,0.0053759988,-0.0135428935,-0.0515149096,0.034309415,0.0158762759,0.0337684237,0.0258363019,-0.0113670682,-0.0627346304,-0.0092760883,0.0457678517,0.0031052525,0.0045698308,0.0477233283,0.0175528395,0.0157582219,0.0174273334,-0.0460217736,-0.0023151128,-0.0370269749,0.0009113698,0.0281139728,0.0185589574,-0.007378431,0.0440509676,-0.0224070509,-0.0078445613,0.0147653804,-0.0320451735,0.0139114937,-0.0387626357,-0.015565697,0.0091584909,-0.0256752057,0.0016513777,0.0269678445]},"424":{"Abstract":"Multi-destination maps are a kind of navigation maps aimed to guide visitors to multiple destinations within a region, which can be of great help to urban visitors. However, they have not been developed in the current online map service. To address this issue, we introduce a novel layout model designed especially for generating multi-destination maps, which considers the global and local layout of a multi-destination map. We model the layout problem as a graph drawing that satisfies a set of hard and soft constraints. In the global layout phase, we balance the scale factor between ROIs. In the local layout phase, we make all edges have good visibility and optimize the map layout to preserve the relative length and angle of roads. We also propose a perturbation-based optimization method to find an optimal layout in the complex solution space. The multi-destination maps generated by our system are potential feasible on the modern mobile devices and our result can show an overview and a detail view of the whole map at the same time. In addition, we perform a user study to evaluate the effectiveness of our method, and the results prove that the multi-destination maps achieve our goals well.","Authors":"J. Zhang; J. Fan; Z. Luo","DOI":"10.1109\/TVCG.2016.2597827","Keywords":"Multi-destination maps;visualization;layout optimization;urban network;traffic visualization;geographic\/geospatial visualization","Title":"Generating Multi-Destination Maps","Keywords_Processed":"layout optimization;traffic visualization;visualization;Multi destination map;urban network;geographic geospatial visualization","Keyword_Vector":[0.2192269047,-0.2671175403,-0.2553279673,0.0802707694,-0.0829768154,-0.2947637203,0.0789807581,-0.0556621902,-0.0367721455,0.1063511775,-0.0562442703,0.2255155236,0.0160992761,-0.172579031,0.0955008198,0.0091214641,0.0776143057,0.0080805212,-0.0712447112,-0.0526037013,0.0065940956,0.0968311947,0.0233566324,-0.0324435018,0.2786513663,-0.0803723311,0.0855555309,-0.0554289993,-0.0871919506,-0.1145260227,-0.0535871788,-0.0210337824,-0.0222096036,-0.0229264871,-0.0533619013,-0.032831763,-0.0188941929,-0.0075942363,0.1294939701,-0.0400450938,-0.0369172304,-0.0395458832,0.0660431118,-0.0420993386,-0.0555094032,0.0550847737,-0.1165817865,0.0233663898,0.1296065011,-0.0040235198,-0.0236611244,0.0421924937],"Abstract_Vector":[0.2730877854,-0.1930674146,-0.0150985422,0.1866693873,0.0967546811,0.1634042753,-0.0144170466,-0.0575244855,-0.1817489499,0.0856587943,0.1103073917,-0.1883513441,-0.1142565058,0.1145436216,0.2040549588,-0.0172582749,0.0285542028,0.0762079699,0.065411053,-0.0851733543,-0.0178937925,0.0198809626,-0.0199783643,0.0043874875,0.0182432742,-0.0633195737,0.0435130728,-0.0558530419,0.1195409217,0.0779278686,-0.0603934009,-0.0347489413,0.0519772725,-0.0367745148,0.0599233162,-0.0647184408,0.0390217208,-0.0817049055,0.0115277309,-0.0372184501,-0.0139394406,0.05214108,-0.0252709374,-0.0178504634,0.0392850245,-0.0108526882,0.0057905806,0.0005064506,-0.0204102539,-0.0434386239,0.0126826998,-0.0160822036,0.0392735968,-0.009215933,0.0392201036,-0.0142060268,0.0570966258,0.0360065764,0.0357758396,0.0240031958,-0.0101585656,0.0165098682,0.0384535351,0.0148207059,0.0283212133,0.0621246091,0.0137000714,-0.0058928416,-0.0297124071,0.0189397654,0.0192664361,0.0084248727,0.0127994358,0.0219779195,0.0586602038,0.0383057431,-0.000304129,-0.0216075781,0.0461945652,0.0007366486,-0.0418644784,-0.0252487499,-0.0236084119,0.0267226975,0.0393538046,-0.0685283962,-0.0369439978,-0.0083338679,-0.0460478052,0.0043535711,-0.0051767308,0.0153272349,0.0202838015,0.0027062893,-0.0614260232,-0.0040016997,0.0099352898,0.0091516143,0.0133000659,-0.0087725267,0.0169432755,0.0061437475,0.0388473166,-0.0218859415,0.0042485503,-0.0090373888,-0.0210010848,-0.0040787093,-0.027764582,-0.0082092437,-0.0087063904,-0.0002080472,-0.0141909239,0.001171803,0.0283554561,-0.031479575]},"425":{"Abstract":"Modal sound synthesis has been used to create realistic sounds from rigid-body objects, but requires accurate real-world material parameters. These material parameters can be estimated from recorded sounds of an impacted object, but external factors can interfere with accurate parameter estimation. We present a novel technique for estimating the damping parameters of materials from recorded impact sounds that probabilistically models these external factors. We represent the combined effects of material damping, support damping, and sampling inaccuracies with a probabilistic generative model, then use maximum likelihood estimation to fit a damping model to recorded data. This technique greatly reduces the human effort needed and does not require the precise object geometry or the exact hit location. We validate the effectiveness of this technique with a comprehensive analysis of a synthetic dataset and a perceptual study on object identification. We also present a study establishing human performance on the same parameter estimation task for comparison.","Authors":"A. Sterling; N. Rewkowski; R. L. Klatzky; M. C. Lin","DOI":"10.1109\/TVCG.2019.2898822","Keywords":"Damping modeling;sound synthesis;modal analysis;statistical modeling","Title":"Audio-Material Reconstruction for Virtualized Reality Using a Probabilistic Damping Model","Keywords_Processed":"statistical modeling;modal analysis;sound synthesis;damp modeling","Keyword_Vector":[0.1175528625,-0.1385164559,-0.1482162952,0.0758755754,-0.0637706424,-0.1778235883,0.0220181517,0.0083879132,-0.0071127633,-0.0096501451,-0.025103395,0.0663734182,-0.0354166775,-0.0116466047,0.0033150293,0.0089103201,-0.1299992942,-0.0161601703,-0.0155696848,-0.0288392297,0.0413640353,-0.0808559697,0.0663388032,-0.0273230932,-0.0966769468,0.0195932307,-0.0057979255,0.0454904817,-0.0099267063,0.0505538431,0.0799833562,0.0624078228,0.0635707782,0.1574114736,-0.0548443842,0.1464140188,-0.1865439184,-0.027057154,0.0425827967,-0.0989205302,-0.008742831,0.0481749511,-0.1573581925,-0.0626919063,0.0388759577,-0.0169972451,0.098136299,0.0295915037,-0.056926719,0.033645244,-0.0016838708,0.0355164619],"Abstract_Vector":[0.2323743466,-0.1072282104,-0.0268125238,0.0366469537,0.0308741921,-0.0205165181,-0.062606421,0.005477541,0.072492803,-0.0495658154,-0.0338333865,0.0590390952,0.0129560993,0.0021932629,0.0828287666,-0.0288449109,-0.0504296623,-0.0177571881,-0.0331095409,-0.0238598952,0.0408862416,-0.0157919435,0.0016070625,0.0282340392,-0.0145588264,-0.0189057791,-0.0008163323,-0.0689435501,-0.0827558562,-0.0847112043,0.0907622633,-0.0272677896,-0.0026940242,-0.0301516364,-0.0346374325,-0.0624666363,0.0757365051,0.013004822,-0.0593584588,-0.0502825567,-0.0027441106,0.0070981196,-0.0287300005,-0.0050049155,-0.0120649147,-0.0097582022,-0.0010794153,-0.0167342993,-0.0415738042,0.0412588902,-0.0164558452,0.0481850584,-0.0188963526,-0.004576943,-0.022467063,0.002676192,-0.0326577245,-0.0084749926,-0.0277260437,0.015626753,-0.0173294735,0.0361780164,0.0280745443,0.00076399,-0.0569375522,0.0052287038,0.0448091888,0.0188075362,0.0106735593,0.0337912965,0.0124884247,0.0402307068,-0.0429482909,0.0216643988,-0.0468323331,-0.0212733228,0.0466343184,-0.0073809561,0.0293023462,-0.0202906247,0.0079824623,-0.0137769208,-0.0504540937,0.0491935337,-0.0069806685,0.0049442553,0.0451328846,0.007380326,0.0071723086,-0.0336112701,-0.0444726424,0.022856144,-0.0492125939,-0.0164587733,0.0137671102,-0.0118008739,-0.0214024227,0.0113595404,-0.0229941726,0.1049627048,0.0498299991,-0.007383747,0.0033208723,-0.0273345374,-0.0039518194,-0.0000559167,0.0354091111,0.031483213,-0.0214002646,0.0305020721,-0.0688119556,0.0547024596,0.01484987,-0.0059387144,0.0142702227,-0.0363543244]},"426":{"Abstract":"This paper presents a novel projected pixel localization principle for online geometric registration in dynamic projection mapping applications. We propose applying a time measurement of a laser projector raster-scanning beam using a photosensor to estimate its position while the projector displays meaningful visual information to human observers. Based on this principle, we develop two types of position estimation techniques. One estimates the position of a projected beam when it directly illuminates a photosensor. The other localizes a beam by measuring the reflection from a retro-reflective marker with the photosensor placed in the optical path of the projector. We conduct system evaluations using prototypes to validate this method as well as to confirm the applicability of our principle. In addition, we discuss the technical limitations of the prototypes based on the evaluation results. Finally, we build several dynamic projection mapping applications to demonstrate the feasibility of our principle.","Authors":"Y. Kitajima; D. Iwai; K. Sato","DOI":"10.1109\/TVCG.2017.2734478","Keywords":"Dynamic projection mapping;spatial augmented reality;laser projector;light pen;geometric registration","Title":"Simultaneous Projection and Positioning of Laser Projector Pixels","Keywords_Processed":"spatial augmented reality;laser projector;dynamic projection mapping;light pen;geometric registration","Keyword_Vector":[0.1273787187,-0.0130801355,-0.0107987319,-0.0311636479,0.0118699697,-0.0259243363,-0.0577995139,0.0006204036,-0.0225348733,-0.0744731743,0.0194029315,-0.0742781092,0.0112085414,0.0400640868,0.0398282076,0.0128840973,0.0592268025,0.1368086042,0.0442841052,-0.0140409176,0.0018401837,0.0367940959,-0.0954283086,-0.0068045059,0.0379886755,0.0377679275,-0.032164113,0.0212289248,0.0032523381,0.0797092617,0.0404712989,0.0815773957,-0.01332731,0.0512617347,0.0374792742,0.0304377919,0.0075967013,0.0447805357,-0.0854431127,0.0061602294,0.0232512095,0.0110646854,0.0903899945,-0.0448915898,-0.1114439084,0.0163907818,0.0350365476,0.0071293541,0.087314117,0.0194221269,0.1125365533,-0.0162311006],"Abstract_Vector":[0.1778300629,-0.0915879474,0.0025715064,0.0160771607,-0.0297373577,0.0244794806,0.0277789783,0.026474178,-0.0213309459,-0.014669063,-0.0114879239,-0.0009770026,0.0438639611,-0.0201975139,-0.0149421734,-0.0197161259,-0.034546856,-0.0776717543,0.0021162773,0.0071389185,0.0610136585,0.0626574839,-0.0075771524,-0.0109603985,0.0406639383,0.0847976696,0.0329563167,0.0322727731,0.0023433478,0.0664121508,-0.0435832916,-0.0527595383,0.0374399603,0.0139160458,0.0180310083,-0.0080801675,0.0411854068,0.0618235448,0.0190736702,-0.0378836794,-0.0046122688,0.008951461,0.0475941544,-0.0099648882,-0.0090234593,-0.0366067317,0.0297824035,0.0155439157,0.0102934755,-0.0061285219,0.0102066157,-0.0223040526,-0.0184593523,-0.0234757521,-0.0220807118,-0.0260407158,0.0428519245,0.0009778752,-0.0092598245,0.0043006979,0.020247789,0.025365775,-0.013461181,0.0139566245,0.0062855924,-0.0387912565,0.0205330312,0.0709487498,-0.0465818036,-0.001105672,0.0435138584,-0.0011536233,-0.0336758974,0.0079036037,-0.0227878124,-0.0161199397,0.0105258581,0.0443894616,-0.0344782725,-0.0555056677,-0.0546520613,-0.0085369982,-0.0348435949,-0.0237035614,0.0030973985,0.0171760055,0.0045197848,-0.020614869,0.0164259331,-0.0067984837,-0.0317813059,0.0372631048,0.0160392786,-0.0110885836,0.0076825722,-0.0219014128,-0.0408127433,-0.0063008887,0.004050185,-0.0295901094,-0.0047062633,0.0140358532,-0.0107038541,-0.0052581324,-0.0182588618,0.0502892254,-0.001227002,0.0270713462,-0.0128751805,0.0162739229,0.0214473272,0.0253468234,0.0054510574,-0.0068075052,-0.0141806803,0.015995016]},"427":{"Abstract":"We define the concept of Dynamic Passive Haptic Feedback (DPHF) for virtual reality by introducing the weight-shifting physical DPHF proxy object Shifty. This concept combines actuators known from active haptics and physical proxies known from passive haptics to construct proxies that automatically adapt their passive haptic feedback. We describe the concept behind our ungrounded weight-shifting DPHF proxy Shifty and the implementation of our prototype. We then investigate how Shifty can, by automatically changing its internal weight distribution, enhance the user's perception of virtual objects interacted with in two experiments. In a first experiment, we show that Shifty can enhance the perception of virtual objects changing in shape, especially in length and thickness. Here, Shifty was shown to increase the user's fun and perceived realism significantly, compared to an equivalent passive haptic proxy. In a second experiment, Shifty is used to pick up virtual objects of different virtual weights. The results show that Shifty enhances the perception of weight and thus the perceived realism by adapting its kinesthetic feedback to the picked-up virtual object. In the same experiment, we additionally show that specific combinations of haptic, visual and auditory feedback during the pick-up interaction help to compensate for visual-haptic mismatch perceived during the shifting process.","Authors":"A. Zenner; A. Kr\u00fcger","DOI":"10.1109\/TVCG.2017.2656978","Keywords":"Dynamic passive haptic feedback;input devices;virtual reality;haptics;perception","Title":"Shifty: A Weight-Shifting Dynamic Passive Haptic Proxy to Enhance Object Perception in Virtual Reality","Keywords_Processed":"virtual reality;dynamic passive haptic feedback;input device;haptic;perception","Keyword_Vector":[0.073648766,-0.0601294546,-0.0233487942,-0.0071017128,0.0195178138,0.0321879389,0.0802137467,-0.0154172611,-0.0178900015,-0.0592911783,-0.0020138378,0.0163924407,-0.0010286524,-0.0388980869,0.0214566225,-0.0294489635,0.0419145429,0.0468182289,0.0105363309,-0.0915578258,0.0239855514,0.0618250284,-0.0545376452,0.0005747797,0.1779105851,-0.0035971605,-0.0299649327,0.0831155198,-0.0934490071,-0.0331582126,-0.0302917695,-0.0924499862,0.060824721,-0.0249078119,-0.104915356,0.0300959876,-0.0182350458,-0.1397019087,0.1348295006,0.0301205441,0.0872018853,0.0585133466,-0.0817244137,0.1213639122,-0.1032849314,-0.0194282077,-0.010322046,-0.0192402376,0.0063696354,0.0204831419,-0.0785705237,0.0930939664],"Abstract_Vector":[0.2198286943,-0.1282601703,0.0233217095,0.098109186,0.0689824082,0.0544403825,0.0066073544,-0.0536693066,-0.0957689444,0.0901871161,0.0744118956,-0.1671331999,-0.1108521173,0.0568097363,0.171564658,-0.0623790967,-0.0516675541,0.0369592738,0.0560955489,-0.0798261861,-0.0082471222,0.0136810175,0.0175153649,-0.050874373,0.0095258568,-0.0551526931,0.006540839,-0.0497384077,0.0368612689,0.0466278189,-0.0586372449,-0.0120336247,-0.0548454952,-0.0585044584,0.0810846024,0.0101916735,-0.0146619937,-0.0070517791,0.0318474334,0.0120369713,-0.0023357937,-0.0141547692,0.0175822608,0.0661095689,0.0013261593,-0.0606619698,0.0686493645,0.0596992426,0.003781749,0.0316593865,0.0380299467,-0.0074092764,-0.0070193342,-0.0104210989,0.0121365723,0.0468993776,0.0290177121,-0.0387317334,0.0012611885,0.0646736821,-0.0379186749,-0.0224351881,0.015599366,-0.0452890628,0.0044342257,-0.03534608,0.0191202378,0.0195068668,0.0144058372,0.0793858864,0.0366753178,0.0413883347,-0.0165495602,-0.0217591366,-0.0342985626,-0.0176433952,0.0089915199,-0.0294431609,0.0065008886,-0.032964947,-0.0281418869,0.0605230266,-0.0527130169,0.0350533395,0.0452927842,0.0421239419,-0.0131769453,0.0241958059,0.0174522366,0.0246518969,-0.0381454096,-0.0452550723,-0.0536898996,0.0650831433,-0.0352228162,-0.0033212048,-0.0156490834,-0.0428446224,-0.0596612191,0.0292760207,0.0013001534,0.0223579795,-0.0106062989,0.0216362185,-0.0212800845,-0.0157336958,0.0218317958,-0.0082545162,0.0384178918,0.0270391184,-0.0641459732,0.0158428487,-0.040736731,-0.0310737584,0.069864756,0.0106827231]},"428":{"Abstract":"Reconstructed building models using stereo-based methods inevitably suffer from noise, leading to the lack of regularity which is characterized by straightness of structural linear features and smoothness of homogeneous regions. We leverage the structural linear features embedded in the mesh to construct a novel surface scaffold structure for model regularization. The regularization comprises two iterative stages: (1) the linear features are semi-automatically proposed from images by exploiting photometric and geometric clues jointly; (2) the scaffold topology represented by spatial relations among the linear features is optimized according to data fidelity and topological rules, then the mesh is refined by adjusting itself to the consolidated scaffold. Our method has two advantages. First, the proposed scaffold representation is able to concisely describe semantic building structures. Second, the scaffold structure is embedded in the mesh, which can preserve the mesh connectivity and avoid stitching or intersecting surfaces in challenging cases. We demonstrate that our method can enhance structural characteristics and suppress irregularities in the building models robustly in some challenging datasets. Moreover, the regularization can significantly improve the results of general applications such as simplification and non-photorealistic rendering.","Authors":"J. Wang; T. Fang; Q. Su; S. Zhu; J. Liu; S. Cai; C. Tai; L. Quan","DOI":"10.1109\/TVCG.2015.2461163","Keywords":"Modeling packages;Reconstruction.;Modeling packages;reconstruction","Title":"Image-Based Building Regularization Using Structural Linear Features","Keywords_Processed":"reconstruction;model package;","Keyword_Vector":[0.1345330584,-0.0838935524,-0.0213143799,0.0205056558,-0.0171509874,0.0808715578,-0.0711769481,-0.0396264528,-0.0392947994,0.0419926412,-0.0241614286,0.0960217812,0.0477965873,-0.0992934214,-0.0096912059,-0.0388155529,0.0345172036,-0.0322860183,-0.020565433,0.0410304867,-0.0690057146,-0.0505512359,-0.0435859907,-0.0221471286,0.0400813397,-0.0084040644,0.0147021734,-0.0471259685,-0.0024131044,0.006864093,-0.0070763541,0.0099272431,0.0030435021,-0.0498986977,-0.0473644705,0.0437050602,0.0007948873,0.0097157717,-0.0060277156,0.0040982276,-0.0078622024,0.0045359386,0.0333270447,0.0017598335,0.0241844074,0.035691517,0.0133213714,0.0255202581,-0.0088829714,0.0002070198,-0.0280505962,-0.0202086327],"Abstract_Vector":[0.2177591864,-0.1121958874,0.0010618732,-0.0352049124,-0.0868130639,0.0041829232,-0.0055268357,-0.0169002699,-0.0268225697,-0.0103590208,-0.0081471886,0.0096819534,0.1020774361,-0.0048665343,-0.0034657836,-0.0022744638,-0.0213254035,-0.0041324295,0.0038301756,0.0198851726,-0.0331798508,-0.0065402905,-0.0073701469,0.0257408997,0.018252112,-0.0177952346,-0.0438314406,-0.0292674169,0.0074734721,0.0395286646,0.0239099984,0.0094471581,0.0817519195,-0.0113124981,-0.0113954675,-0.0253802985,0.0226933745,-0.0086271693,0.0343469991,0.0033508254,-0.0178438761,0.009190607,-0.0003039644,0.0020810731,0.0094203001,0.009228403,0.0103329474,-0.0061921981,-0.0196645874,-0.0471010649,0.0013748142,-0.0453832328,-0.0226638232,0.0100653418,-0.0514418562,-0.0235582608,0.0134627132,0.0135389466,-0.0255949261,-0.0060566849,0.0140170092,-0.0112631748,0.0015033278,-0.0018560148,-0.0128317252,0.021157966,0.0282723866,0.0167002074,-0.0318054579,-0.0202372853,-0.0131684583,0.0076815379,-0.0073893708,-0.0112251641,-0.004440921,-0.0041399786,0.0018343281,-0.0099704913,-0.0197297726,-0.0030049864,-0.0150443286,0.0069662933,-0.0010099466,0.0051121666,0.0195176872,-0.0168844608,0.0095350902,-0.0161326056,-0.0166478001,-0.0142899192,-0.0032848359,-0.0205661708,0.0226700597,-0.0013287886,0.0067680208,-0.0056984911,0.0059958472,0.038050713,-0.0090534794,-0.0168580839,0.0099781094,-0.0207037964,0.013178342,-0.0419866071,-0.0023743281,-0.0139918785,-0.0095701957,0.0023633159,0.0162002544,0.0193116196,-0.0199768315,-0.0026452296,-0.0130278568,0.0380419884,0.0088546676,0.0058147373]},"429":{"Abstract":"Visualizations often appear in multiples, either in a single display (e.g., small multiples, dashboard) or across time or space (e.g., slideshow, set of dashboards). However, existing visualization design guidelines typically focus on single rather than multiple views. Solely following these guidelines can lead to effective yet inconsistent views (e.g., the same field has different axes domains across charts), making interpretation slow and error-prone. Moreover, little is known how consistency balances with other design considerations, making it difficult to incorporate consistency mechanisms in visualization authoring software. We present a wizard-of-oz study in which we observed how Tableau users achieve and sacrifice consistency in an exploration-to-presentation visualization design scenario. We extend (from our prior work) a set of encoding-specific constraints defining consistency across multiple views. Using the constraints as a checklist in our study, we observed cases where participants spontaneously maintained consistent encodings and warned cases where consistency was overlooked. In response to the warnings, participants either revised views for consistency or stated why they thought consistency should be overwritten. We categorize participants' actions and responses as constraint validations and exceptions, depicting the relative importance of consistency and other design considerations under various circumstances (e.g., data cardinality, available encoding resources, chart layout). We discuss automatic consistency checking as a constraint-satisfaction problem and provide design implications for communicating inconsistencies to users.","Authors":"Z. Qu; J. Hullman","DOI":"10.1109\/TVCG.2017.2744198","Keywords":"Visualization Design;Qualitative Study;Evaluation","Title":"Keeping Multiple Views Consistent: Constraints, Validations, and Exceptions in Visualization Authoring","Keywords_Processed":"Qualitative Study;evaluation;visualization Design","Keyword_Vector":[0.1887572815,0.0030363667,0.2032453851,0.0664407156,0.0532701074,-0.0556448156,0.0224513171,0.1788334679,0.0206047213,0.0914850516,-0.0818374619,-0.0480229593,-0.0049164474,0.0429452994,0.0395803816,0.0582317495,-0.0470498325,0.0679129917,0.0153322316,0.0084556155,-0.0627834685,0.010057767,0.0003137552,-0.0385043376,0.0316462017,-0.0089725935,0.005833136,0.018506312,-0.0138744036,0.0065718833,-0.013412819,0.0290493408,-0.0564002905,-0.085350329,0.000809932,0.0214010731,-0.0202761498,0.0082490532,-0.0458082209,-0.0231588068,0.017867544,0.065779272,-0.0267432161,0.065709715,0.0581020841,0.0613298959,-0.0364158429,-0.0133997515,-0.0192054277,0.0220547032,0.0079757746,-0.0430132695],"Abstract_Vector":[0.2814671367,0.0716112846,0.1558636684,-0.0880960616,0.1212784212,-0.1183273203,-0.0554683238,0.032990096,-0.0424555496,0.0633668732,-0.1250499621,-0.0396401576,0.0277766126,-0.0441826863,0.0726006186,-0.031901099,0.1481578781,-0.0016489301,-0.0365657319,0.0252295373,0.0115860047,0.0165064214,-0.0560873758,-0.010280763,-0.0756216621,0.0133720825,-0.0393336515,0.0790456112,0.0317058307,-0.0105014356,0.0960619915,0.0202246571,-0.0335328125,0.0287526838,-0.0291072274,-0.0218794463,0.0308375593,0.0231628606,-0.064047943,0.0010508576,0.0202215835,-0.0034240913,0.0230294144,-0.0843837407,-0.0076337555,-0.0201393693,-0.0339465247,0.0091659143,-0.0397437475,-0.051861296,0.0472176725,-0.0677816698,0.0028392842,0.0372008494,-0.0186293182,-0.0359225868,0.0580430626,-0.0191845402,0.0151119277,0.0349793814,0.002677991,-0.0327220722,-0.0682368281,-0.0301374401,0.0430075767,-0.009386778,0.0021415098,-0.0138669001,0.0148087242,-0.0802002894,0.0259578383,-0.0165842422,-0.0114408546,0.0082471925,-0.0300774809,-0.0349606689,-0.0129981833,-0.0355610897,0.029716148,0.0225925717,0.0435279446,-0.028152383,0.0006689905,-0.0245812922,-0.0016935104,0.0402815683,-0.0166944259,-0.0334717715,-0.0057165069,-0.022608255,-0.030847004,-0.0094939633,-0.0210601265,-0.0345944358,-0.0528277304,0.0123607234,-0.0414018772,-0.019839956,0.0098074744,0.0097023359,0.0571493129,-0.018816832,0.0076026795,-0.0366507366,-0.0436572293,0.0336033716,-0.0290637341,-0.026881629,-0.0452152542,0.0305997274,-0.0260985265,-0.009156229,0.0167760794,-0.0234365976,0.022057766,0.0082029222]},"43":{"Abstract":"We present a volume exploration framework, FeatureLego, that uses a novel voxel clustering approach for efficient selection of semantic features. We partition the input volume into a set of compact super-voxels that represent the finest selection granularity. We then perform an exhaustive clustering of these super-voxels using a graph-based clustering method. Unlike the prevalent brute-force parameter sampling approaches, we propose an efficient algorithm to perform this exhaustive clustering. By computing an exhaustive set of clusters, we aim to capture as many boundaries as possible and ensure that the user has sufficient options for efficiently selecting semantically relevant features. Furthermore, we merge all the computed clusters into a single tree of meta-clusters that can be used for hierarchical exploration. We implement an intuitive user-interface to interactively explore volumes using our clustering approach. Finally, we show the effectiveness of our framework on multiple real-world datasets of different modalities.","Authors":"S. Jadhav; S. Nadeem; A. Kaufman","DOI":"10.1109\/TVCG.2018.2856744","Keywords":"Volume visualization;hierarchical exploration;voxel clustering","Title":"FeatureLego: Volume Exploration Using Exhaustive Clustering of Super-Voxels","Keywords_Processed":"hierarchical exploration;voxel clustering;volume visualization","Keyword_Vector":[0.1122184678,0.1890222987,-0.093288241,-0.0131037352,-0.0298255747,0.0001176796,0.0179465506,-0.023720156,-0.0113209131,-0.0013715313,0.0012954734,-0.0063793924,-0.0221778219,0.0080704906,0.0089112167,-0.0015016953,-0.0381661258,-0.0406459344,0.0119778937,-0.0201723888,-0.0305098098,0.0108891682,0.0125492054,-0.0171847895,-0.0069165596,0.0032442886,0.0112199564,-0.0392360454,-0.040260507,-0.0220490682,-0.0385367751,0.0093274781,-0.0480660302,-0.0248338197,0.0123508938,-0.0237726753,0.0384535679,0.0348655411,-0.0338871953,0.019864979,0.0039462455,0.0111812118,-0.0687600353,-0.0159745352,-0.0018397038,-0.0457090357,-0.0281756426,0.0505628952,-0.0003582363,-0.0052218549,-0.0251889356,-0.0497256642],"Abstract_Vector":[0.1903345178,0.0455292304,-0.0565796211,-0.0565120554,-0.0227470767,0.0338475222,0.0327949228,0.005052474,0.0094254463,0.0397696389,0.0119838787,0.026396455,-0.0011143037,-0.0111453004,-0.0122716533,-0.0411921418,0.0124706983,0.019277818,-0.0156902433,0.0062102661,-0.057392824,-0.0272480082,0.0966024918,0.0191548165,-0.0333689824,-0.0111106013,0.0816331994,-0.0156610115,0.045868559,0.0008293067,0.0014101036,0.0052708274,-0.0601030162,-0.0026264073,-0.0398971905,-0.0333661061,0.0132050422,-0.0009120639,-0.0146151198,-0.0182295174,0.0311867111,-0.0087972849,0.0449990266,0.0201684155,0.0167249124,-0.0187543878,-0.0298488229,0.0005675402,0.0319235463,0.0011739658,0.0182655166,0.030560381,-0.0237042203,0.0307518981,0.0328907686,0.0639225595,0.0104847181,-0.0332414435,-0.0406543344,-0.032754351,0.0019543973,-0.0076240035,0.0130306342,0.0292940357,0.0105597962,-0.0270988416,-0.0206593223,-0.0208197501,-0.0135121787,0.0068818445,-0.0206262175,0.0209082147,0.0280163896,0.0256961477,-0.0086593747,-0.011999487,-0.0283180008,0.0438884434,0.0137080351,-0.0511204867,-0.0041704416,0.0084803822,-0.0431771542,0.0266508876,-0.0109279625,0.0214036553,0.0049112283,-0.0026435185,-0.0348673895,0.0037934639,0.0212175154,-0.0036840176,0.005319358,0.0146210625,-0.0284261078,-0.0577866404,-0.0063195419,-0.0083938571,0.0260627247,0.0076587924,0.0143349854,0.030213399,0.0097356796,0.0128266683,-0.0082947815,-0.0002273308,0.0143174084,0.014028612,-0.0179668039,-0.0075855619,-0.0092158976,0.0199353484,-0.0364072692,0.0090978305,0.0329495054,-0.0023697571]},"430":{"Abstract":"Computing spherical parameterizations for genus-zero closed surfaces is a fundamental task for geometric processing and computer graphics. Existing methods usually suffer from a lack of practical robustness or poor quality. In this paper, we present a practically robust method to compute high-quality spherical parameterizations with bijection and low isometric distortion. Our method is based on the hierarchical scheme containing mesh decimation and parameterization refinement. The practical robustness of our method relies on two novel techniques. The first one is a flat-to-extrusive decimation strategy, which contains two decimation error metrics to alleviate the difficulty of further mesh refinement. The second is a flexible group refinement technique that consists of flexible vertex insertion and efficient volumetric distortion minimization to control the maximum distortion. We convert the task of volumetric distortion minimization to one of tetrahedral mesh improvement to make the vertices distribute uniformly for efficient refinement. Compared with state-of-the-art methods, our method is more practically robust and possesses better mapping qualities. We demonstrate the efficacy of our method in spherical parameterization computations on a data set containing over five thousand complex models.","Authors":"X. Hu; X. Fu; L. Liu","DOI":"10.1109\/TVCG.2017.2704119","Keywords":"Spherical parameterizations;bijection;low isometric distortion;flat-to-extrusive decimation;volumetric distortion optimization;bijective surface correspondence","Title":"Advanced Hierarchical Spherical Parameterizations","Keywords_Processed":"bijective surface correspondence;volumetric distortion optimization;low isometric distortion;flat to extrusive decimation;spherical parameterization;bijection","Keyword_Vector":[0.0423681015,-0.0108272319,-0.0232829388,-0.0025634074,-0.0075788097,0.0058652057,0.0010659626,-0.0131381306,-0.0311207901,-0.0093922558,0.0007991096,0.0144029394,0.0189140744,0.0132925062,-0.0279245001,0.0136060304,0.0099313416,-0.0283980795,-0.0034692393,-0.006248614,-0.0429625305,0.0004534036,0.0379111662,0.0754359478,-0.0067831531,-0.0271187402,0.061389479,0.0409795647,-0.0082478011,-0.0231728368,0.0143202009,0.0614894212,-0.1019161311,-0.0589572745,0.0794140382,-0.0298405665,-0.0005855218,-0.0185698018,-0.0135214901,-0.0192013683,0.061223126,0.1230861383,0.0306181375,-0.0134931943,-0.0530566758,-0.0143883193,-0.0103980243,0.0083981063,-0.0299521837,-0.099683694,0.0117023486,-0.0450500585],"Abstract_Vector":[0.1932877287,-0.0166802385,-0.0092301107,0.0026718378,-0.058700964,0.0495845395,0.0402638859,0.0263895176,0.0411325599,0.060357924,-0.090943224,0.0548235691,-0.042662645,-0.0012160671,-0.0164518473,-0.0422836845,0.0487129866,0.0049901887,-0.0039777407,-0.0035559077,-0.1017032844,-0.0137507954,0.2128908418,0.0150337972,0.0036441463,-0.0864734634,-0.0030267176,-0.01121041,0.0770757054,0.014009564,0.0261265669,0.0009009148,-0.0148781381,0.0194550492,-0.0287792565,-0.01681678,0.0394981322,-0.0245986541,-0.0471403869,0.0338519215,-0.0253137187,0.0183394572,0.0667759115,-0.0033668789,-0.0340116868,-0.0385242258,-0.0270303235,0.0664985614,-0.0300246545,0.0303591234,0.0101534138,0.0045690185,0.0247220453,-0.0109399706,0.0588518126,-0.0273285033,0.0106417152,-0.0152450488,-0.0128782909,-0.0659130176,0.0543693779,-0.0381144868,0.0208484169,-0.01552312,-0.0108532103,0.034228617,0.0045904553,-0.0568843529,0.0211665511,0.0307746579,0.0219487909,-0.0047057343,0.0078837996,-0.0083895447,0.0062141233,-0.001810195,-0.0104110429,0.008132739,0.0182896621,-0.0366003965,-0.009456447,-0.0050223245,0.0221333534,-0.0094478613,-0.0177136721,-0.0047111025,-0.0219107504,-0.0023349029,0.0157406092,-0.0328135039,0.0136426936,-0.0089326388,-0.0035224135,-0.0304282073,0.0145096386,0.0055270587,-0.0344997669,-0.0225758961,0.0093112787,-0.0115180174,-0.0045350254,0.0192324539,-0.0059646904,-0.01239579,-0.0072585122,-0.0032175061,0.0107370439,-0.028866804,-0.019913604,-0.0358990096,-0.0048858663,-0.0504173895,0.0078860696,0.028300806,0.0277116724,-0.0238728552]},"431":{"Abstract":"Multivariate generalized Gaussian distributions (MGGDs) have aroused a great interest in the image processing community thanks to their ability to describe accurately various image features, such as image gradient fields. However, so far their applicability has been limited by the lack of a transformation between two of these parametric distributions. In this paper, we propose a novel transformation between MGGDs, consisting of an optimal transportation of the second-order statistics and a stochastic-based shape parameter transformation. We employ the proposed transformation between MGGDs for a color transfer and a gradient transfer between images. We also propose a new simultaneous transfer of color and gradient, which we apply for image color correction.","Authors":"H. Hristova; O. Le Meur; R. Cozot; K. Bouatouch","DOI":"10.1109\/TVCG.2017.2769050","Keywords":null,"Title":"Transformation of the Multivariate Generalized Gaussian Distribution for Image Editing","Keywords_Processed":"","Keyword_Vector":[0.078309748,0.0516919823,0.1237701588,0.0841200772,0.0235277572,-0.0753588158,0.010505323,0.0665564325,-0.0485697247,0.0929209304,0.0255001825,-0.0098260701,0.017754339,0.0104604519,-0.0028340408,0.0539391401,-0.0145304123,0.0295619856,0.0232031452,0.0029288822,-0.0471434465,0.0406499343,-0.0046809781,-0.0043723075,-0.0037063349,-0.0131399228,-0.0492478643,0.0349358124,-0.015652361,0.0362801612,-0.0134424637,0.0108523684,0.0092783867,-0.0078572347,-0.0286092063,-0.0529089898,0.0430680337,-0.0956316443,0.0242463309,0.0736412377,0.0809908536,0.0192021275,-0.0683579233,0.0194737268,0.0403599212,0.0244888281,-0.0199779207,0.0032111761,0.0231548575,-0.0270077576,0.0094297904,-0.0400164972],"Abstract_Vector":[0.1985487487,0.0907436322,-0.0092236377,-0.0405036899,0.0003287523,0.0676239376,-0.0020626611,0.0817198087,0.0910965327,-0.0283267409,-0.0782260151,-0.0144200086,-0.0754913,-0.080696785,0.0065830312,-0.0945446931,0.015533153,0.067870582,0.0384807518,0.0350000007,-0.034150738,0.0252118158,-0.0321550697,0.0074038457,-0.0148703243,-0.0469531392,0.0484993866,-0.0058366316,0.006675014,-0.0971791897,-0.0219478104,-0.0200762966,-0.0410123382,-0.0213233612,0.045600142,0.0042190044,0.0060955466,0.0226541789,0.05250845,0.0401344477,0.0183139066,0.0449317724,0.0783928305,0.0849602113,0.0094004648,0.0023691259,-0.024414831,-0.0251096557,-0.1026374129,-0.0126362455,-0.0090130417,0.0459299281,0.0245689455,0.0306385534,0.0041368024,-0.025091509,-0.0105648814,-0.0240767054,-0.013544882,-0.0304360773,-0.0349008561,-0.0567732324,-0.0071825916,-0.0227520252,-0.0147609542,0.0155249767,0.0140470532,-0.0075796611,-0.0654102856,-0.0235197597,0.042857044,0.0430201778,-0.030435204,0.0328726809,-0.0089721461,-0.02140696,0.0483757542,0.0506265209,-0.0396111798,0.0226079446,0.0552997805,0.0201518466,0.0537802448,-0.0296787147,-0.0078501821,-0.0119295336,-0.0159038224,-0.0891359924,-0.0399315885,-0.0354206261,0.0232183372,0.0166825359,-0.0336265801,-0.0292726016,-0.0533846968,-0.0188221829,0.0415675791,0.0353545229,0.041488735,-0.0070782465,-0.0298694086,-0.0357482305,0.0474954679,-0.0237885238,-0.0313287662,0.0071987478,0.0056073439,0.0346470616,0.0153331886,-0.0664759025,-0.0109374515,0.0273657554,0.0098289705,-0.044959589,0.0196801251,0.0144910909]},"432":{"Abstract":"We present the design and evaluation of an integrated problem solving environment for cancer therapy analysis. The environment intertwines a statistical martingale model and a K Nearest Neighbor approach with visual encodings, including novel interactive nomograms, in order to compute and explain a patient's probability of survival as a function of similar patient results. A coordinated views paradigm enables exploration of the multivariate, heterogeneous and few-valued data from a large head and neck cancer repository. A visual scaffolding approach further enables users to build from familiar representations to unfamiliar ones. Evaluation with domain experts show how this visualization approach and set of streamlined workflows enable the systematic and precise analysis of a patient prognosis in the context of cohorts of similar patients. We describe the design lessons learned from this successful, multi-site remote collaboration.","Authors":"G. E. Marai; C. Ma; A. T. Burks; F. Pellolio; G. Canahuate; D. M. Vock; A. S. R. Mohamed; C. D. Fuller","DOI":"10.1109\/TVCG.2018.2817557","Keywords":"Visual analytics;precision medicine;design studies;nomograms;parallel coordinate plots;activity-centered design","Title":"Precision Risk Analysis of Cancer Therapy with Interactive Nomograms and Survival Plots","Keywords_Processed":"precision medicine;nomogram;design study;activity center design;parallel coordinate plot;visual analytic","Keyword_Vector":[0.1050595292,-0.0862364387,-0.097160871,0.1129307627,-0.0887935538,-0.1397785382,0.0383669541,0.0120138162,-0.0224576367,-0.0128883103,0.0305068651,0.0505264041,-0.0266113039,-0.0365651004,0.069098903,-0.0054360871,0.0212434176,0.0256438506,0.0516794184,-0.0049153997,-0.0282504623,0.0445444366,0.0509234876,0.0475154352,-0.0459156487,0.0038564391,0.039309409,-0.0213657649,0.0325486793,-0.0589405819,0.0552045867,-0.016398071,0.0105581035,0.0466834829,-0.0029493376,-0.0065040625,0.0689419263,0.0341779417,-0.028509816,-0.0195505006,0.0071515204,0.0182959557,-0.1340812395,-0.0818004389,0.0228638772,-0.0443849021,0.0353640912,0.1316891576,-0.0633408346,0.0307635065,-0.0295225708,0.0121327749],"Abstract_Vector":[0.1896196597,-0.004654983,0.011905462,0.0129168175,-0.0237965841,0.0034371585,0.0189536621,-0.006013151,0.034138649,-0.0803602366,0.0571839399,0.0712432907,-0.1316467731,-0.0147002541,0.0367649341,-0.025381846,0.023629,-0.0145006721,0.0140966914,0.0107409067,-0.012475563,-0.0297953606,0.0263778266,-0.0145137084,-0.0186031608,0.0040285693,0.0164524976,0.0287663821,-0.0081813969,0.0358539304,-0.0126156165,0.0270282125,-0.0921502939,0.0185100702,-0.0217676064,0.0248791178,0.0066907838,0.0282018097,0.0370729428,-0.0142304164,0.0190557311,0.013829491,0.0362481492,0.0326633378,0.0385690767,0.015939741,-0.0071223073,0.0184031428,0.0271021323,0.0131365376,0.0139467485,0.07353855,-0.037394966,-0.0618260351,-0.0109021844,-0.0183279033,0.0034045867,0.0066172403,-0.0117114094,0.0040503982,0.0139366326,-0.0759655898,0.02095515,-0.0068586716,-0.0527492529,-0.0006081754,0.0190880907,0.0019626989,-0.0142134218,-0.011373298,0.0396125697,0.0339562574,-0.0113484791,0.023678558,-0.0308468629,-0.0102808725,-0.0157000229,0.0286653727,0.0259676218,0.0018712486,-0.0139063828,0.0142210902,-0.0275053084,-0.0261077394,-0.0036307565,0.0268823985,-0.0018050475,-0.0048249923,0.015287675,0.006186149,0.0079284922,-0.0105411194,-0.0175592455,0.0601066723,0.0038956726,-0.0143668633,0.0221018959,0.0022117024,-0.0057763522,-0.0114294562,-0.0074464764,-0.0039124403,-0.0346049595,0.0127415997,-0.0393951312,0.004523942,0.0049908561,0.0542592463,0.0090565,-0.0251536596,-0.0241309862,0.0001779215,-0.0125772714,0.0354187627,0.0264532738,-0.0231841438]},"433":{"Abstract":"With the rapid increase in raw volume data sizes, such as terabyte-sized microscopy volumes, the corresponding segmentation label volumes have become extremely large as well. We focus on integer label data, whose efficient representation in memory, as well as fast random data access, pose an even greater challenge than the raw image data. Often, it is crucial to be able to rapidly identify which segments are located where, whether for empty space skipping for fast rendering, or for spatial proximity queries. We refer to this process as culling. In order to enable efficient culling of millions of labeled segments, we present a novel hybrid approach that combines deterministic and probabilistic representations of label data in a data-adaptive hierarchical data structure that we call the label list tree. In each node, we adaptively encode label data using either a probabilistic constant-time access representation for fast conservative culling, or a deterministic logarithmic-time access representation for exact queries. We choose the best data structures for representing the labels of each spatial region while building the label list tree. At run time, we further employ a novel query-adaptive culling strategy. While filtering a query down the tree, we prune it successively, and in each node adaptively select the representation that is best suited for evaluating the pruned query, depending on its size. We show an analysis of the efficiency of our approach with several large data sets from connectomics, including a brain scan with more than 13 million labeled segments, and compare our method to conventional culling approaches. Our approach achieves significant reductions in storage size as well as faster query times.","Authors":"J. Beyer; H. Mohammed; M. Agus; A. K. Al-Awami; H. Pfister; M. Hadwiger","DOI":"10.1109\/TVCG.2018.2864847","Keywords":"Hierarchical Culling;Segmented Volume Data;Bloom Filter;Volume Rendering;Spatial Queries","Title":"Culling for Extreme-Scale Segmentation Volumes: A Hybrid Deterministic and Probabilistic Approach","Keywords_Processed":"volume Rendering;bloom filter;spatial query;Segmented Volume Data;Hierarchical culling","Keyword_Vector":[0.1604642868,-0.1763733324,-0.1649217471,0.1585294532,-0.1324579633,-0.1158646229,0.0933973471,0.0022266756,-0.0004547467,-0.0740980281,0.0691947345,-0.0741774823,0.0167891676,0.1963071939,-0.102233121,-0.0108353671,0.0846682618,0.0805629448,0.0414162057,-0.0616265431,-0.0756510592,-0.0469315492,-0.0103446702,0.0255804119,-0.050690878,0.0681035633,0.0042112471,0.0076888648,0.0155566858,0.0823491296,-0.0249640008,0.0827617995,-0.0757814027,0.0728334473,0.0030241398,-0.0181303627,0.0033713222,-0.0386667333,0.063219565,-0.0129840735,-0.0499298466,-0.0706447284,-0.0246448545,-0.030632913,-0.05558657,0.0494232349,-0.0720143325,0.0253499865,-0.0084072512,0.0094681443,-0.0341254332,-0.0305952654],"Abstract_Vector":[0.2175954951,-0.1418920366,0.0087990336,-0.0019620659,-0.0094518847,-0.006561407,0.0081178707,-0.0223214891,-0.0434395944,0.0606881241,0.0153832677,-0.0643868771,-0.0375796639,0.0217493898,-0.0346422798,0.0226027279,0.0030463182,-0.0402607486,-0.0046635804,0.0111687088,0.016524077,0.0348614606,-0.0170389463,-0.0296929823,0.0495171128,0.0688053054,0.0335908296,0.0437808938,0.0345605025,0.02429516,-0.0175108371,-0.0617222601,-0.0090194949,-0.0306569037,0.0061672474,-0.0250241093,0.0169362836,0.0517431016,0.0263878291,-0.0553230381,-0.0023101322,0.0195681596,0.0429615402,0.016620526,0.0486936399,-0.0511393697,0.029923526,0.0451410192,-0.0306517852,0.0278765172,0.0488983649,0.0086377196,-0.015687323,0.0030665657,-0.006289789,-0.0012391716,0.0159900544,-0.0227782491,0.025521154,-0.0157330287,-0.0009189332,-0.0062960188,-0.0278324877,-0.0481400983,0.0017028631,0.0175500782,-0.0095712891,-0.0095545706,-0.0171519765,-0.0361675443,0.0405454314,0.0253613233,0.0579073816,0.0157168811,-0.0342307489,0.028602268,-0.0632045171,-0.0074171373,-0.0227783173,0.0117729546,-0.0388361299,0.0066334712,-0.0265819184,-0.0116641952,0.0171405292,0.02667781,-0.0021243717,-0.0425229333,-0.0285886511,-0.0025758345,-0.0087097154,-0.0014307973,-0.0155570825,-0.0030163336,0.0489500001,-0.015580016,0.0190261182,-0.0209602439,0.0016419642,-0.0136408556,-0.0075524636,0.0227578889,-0.0177969789,-0.0090017099,0.0256937321,-0.0439174469,0.0400086416,0.0075358401,-0.0115149272,-0.0183435519,0.0103445011,0.0259744265,0.0083767572,-0.0222989013,0.0000910525,0.005622421]},"434":{"Abstract":"Prewriting is the process of generating and organizing ideas before drafting a document. Although often overlooked by novice writers and writing tool developers, prewriting is a critical process that improves the quality of a final document. To better understand current prewriting practices, we first conducted interviews with writing learners and experts. Based on the learners' needs and experts' recommendations, we then designed and developed InkPlanner, a novel pen and touch visualization tool that allows writers to utilize visual diagramming for ideation during prewriting. InkPlanner further allows writers to sort their ideas into a logical and sequential narrative by using a novel widget- NarrativeLine. Using a NarrativeLine, InkPlanner can automatically generate a document outline to guide later drafting exercises. Inkplanner is powered by machine-generated semantic and structural suggestions that are curated from various texts. To qualitatively review the tool and understand how writers use InkPlanner for prewriting, two writing experts were interviewed and a user study was conducted with university students. The results demonstrated that InkPlanner encouraged writers to generate more diverse ideas and also enabled them to think more strategically about how to organize their ideas for later drafting.","Authors":"Z. Lu; M. Fan; Y. Wang; J. Zhao; M. Annett; D. Wigdor","DOI":"10.1109\/TVCG.2018.2864887","Keywords":"Writing;prewriting;diagraming;content and structure recommendation;pen and touch interfaces","Title":"InkPlanner: Supporting Prewriting via Intelligent Visual Diagramming","Keywords_Processed":"writing;pen and touch interface;content and structure recommendation;diagraming;prewriting","Keyword_Vector":[0.1801067176,-0.0969894123,0.0008646674,0.0901219708,0.0890775825,0.0818957482,0.0143720944,0.157371383,-0.0451891018,-0.05882927,-0.0660792208,-0.028819547,-0.0356237601,0.1908801836,-0.1093756932,-0.0936031018,-0.0430211746,0.0372062819,0.0632379412,0.1839970799,0.0306807334,0.0018445282,0.0778511106,0.0069474366,0.018594666,0.0177312071,0.0679469982,0.087070848,-0.0085575533,-0.0218251079,-0.1429698347,0.0927287996,-0.0264154816,0.1813892241,-0.0760782475,-0.0217747579,-0.069076823,0.0771902536,0.0951056598,-0.0755182854,-0.0921122116,0.0102594149,-0.050566218,0.0775822429,-0.0668436911,0.025829949,-0.042800011,-0.0140366582,0.0266126235,-0.0618846881,-0.002954467,-0.0109262806],"Abstract_Vector":[0.2048864894,-0.1129806804,0.005427475,0.0072785883,0.0130918636,-0.0245147004,-0.0392116377,-0.0115473072,-0.0126703479,-0.0403378689,0.0195302643,-0.0034660509,-0.0191552822,-0.0256834811,-0.0107365855,-0.0357444267,-0.0398902901,-0.0487336866,-0.03730138,-0.0039737071,0.026808009,-0.0290482192,0.0134473844,-0.0042498556,-0.0490935689,0.0791381086,-0.0642471544,-0.0142993214,0.063130979,-0.0220734423,0.0663345979,-0.0408182056,0.0525300612,-0.115569589,0.0446093596,-0.0399745068,0.0538620391,-0.0459617628,0.0082814992,0.0423921322,-0.0323414868,-0.0235879651,-0.0481943574,-0.0694680256,-0.0414902694,0.0068361481,-0.0192451366,0.0285483965,0.0418752599,-0.0317337042,-0.0876661205,0.0775852318,0.0150115628,0.0210723098,0.0104127498,0.0313636803,0.0326695712,-0.0054102522,0.0212313384,-0.0278670432,-0.0160356202,-0.070684902,-0.0314582956,0.0005337154,0.016040447,-0.0330602638,-0.0773938009,-0.0198063197,0.0077217636,0.0280198362,0.0571261247,0.00458332,-0.0073016651,-0.0272056801,0.0178920111,0.0019263626,0.0156547136,-0.0060065263,0.0247623416,-0.0700557866,0.0409051926,0.0084089125,0.0153762817,0.0127046839,0.0330513977,0.0170843534,0.031768175,-0.0107900431,-0.0027289711,-0.0058890212,0.050394676,-0.0003753484,-0.0063243737,-0.0000033452,0.003045788,0.0267605489,0.0404128767,0.0051826022,0.0069553592,-0.0068131653,-0.0086069705,-0.0215009996,-0.0036865647,-0.0024897871,0.0013412774,-0.0147814161,0.0182585474,-0.0138218194,-0.0062237468,0.0036519505,-0.0077769779,-0.0203600914,0.0032299723,-0.0106356212,-0.0095917411,0.0045731947]},"435":{"Abstract":"In see-through systems an observer watches a (background) scene partially occluded by a display. In this display, usually positioned close to the observer, a region of the background scene is shown, yielding the sensation that the display is transparent. To achieve the transparency effect, it is very important to compensate the parallax error and other distortions caused by the image acquisition system. In this paper a detailed study of a video see-through methodology with parallax correction is performed. In a system composed by two cameras-one directed to the user and another to the background scene-and a display, the relative position between the user, the display and the scene is estimated using a feature detection algorithm and the parallax error is compensated assuming a planar scene model. The application of the proposed methodology on Driver Assistance Systems (DAS) is proposed. A theoretical assessment of the algorithm shows that although approximations are proposed to simplify the methodology and reduce the computational cost, such as the planar scene model and fixed working distance, on some practical situations their effects can be neglected without noticeable impact on the perceptual quality of the solution.","Authors":"R. A. Borsoi; G. H. Costa","DOI":"10.1109\/TVCG.2017.2705184","Keywords":"See-through;virtual transparency;DAS;parallax;augmented reality;user-perspective rendering;dual-view","Title":"On the Performance and Implementation of Parallax Free Video See-Through Displays","Keywords_Processed":"parallax;user perspective render;virtual transparency;augmented reality;see through;dual view;DAS","Keyword_Vector":[0.0869667816,0.0751321702,-0.0311837323,-0.0091015719,0.0039146995,0.0418025269,0.0299025541,0.0055897272,-0.0005319549,-0.0076520751,-0.0087681842,0.0053334802,-0.0027726102,0.0022568099,-0.02163727,-0.0211705271,-0.0079388066,-0.0215001577,0.0572678984,0.0778031799,0.0180612915,0.044928886,0.0036242003,0.023903895,0.0255395035,0.0230737381,0.0350103758,0.0006198063,-0.0996933354,0.0098777332,0.0484303611,-0.0431960068,0.0684566309,0.1037387535,0.0360894544,-0.0662445011,0.0542946245,0.0119520869,0.044579974,0.0050656293,-0.0223794782,0.0421066115,-0.0145356626,0.0295244693,0.0129018477,0.0165142951,0.0882661092,-0.0448375169,-0.014415603,-0.039441914,0.0641476935,-0.0336282388],"Abstract_Vector":[0.2081532343,0.1296140194,-0.0631760256,0.0913728193,-0.0216738774,-0.0623990284,0.0791352352,0.0817914648,-0.0157769205,-0.0324173649,-0.0073717382,-0.0142539493,0.0220449923,0.0934539469,0.0089501398,0.0401775792,-0.0452876299,-0.0226594382,-0.0271886376,-0.0149620164,-0.0419358906,-0.0378543005,-0.0394191479,-0.0091192528,0.0235372633,0.0076734017,-0.0156506297,0.0114489973,0.046533495,0.0198256488,0.0126239846,-0.0159887525,-0.0407118461,0.0260034406,-0.0223050258,-0.0084398288,-0.0020198317,0.0591940883,0.0092154055,-0.0096751428,0.0285732754,0.0197143669,-0.0112547171,0.0146742192,-0.0153577625,0.0074589731,0.0249916321,0.013804716,-0.0219665721,0.0152131014,-0.0188254105,0.0089455157,-0.0194820729,0.0209005858,-0.0157926528,-0.0122690483,0.0417614725,-0.0265936188,0.0018876177,-0.0146042091,-0.0277092399,0.0067599201,-0.0251960631,0.0116981167,-0.0170910629,0.0133004024,-0.0055818218,-0.029037021,0.0115519916,0.0030474655,-0.0041618591,-0.0262756374,-0.0276054429,0.0293105861,0.0264736868,-0.0006283487,-0.0067543243,-0.0078586589,0.0158374307,-0.034071937,-0.0286594526,0.0368999979,0.0041244049,0.0003040132,-0.0194571407,0.0048991722,0.0001973862,-0.0021415039,0.0065233829,-0.0225559861,-0.0173175044,0.0054834929,-0.0120672569,0.0338402116,-0.0196049351,-0.020344305,0.0023332983,0.0016115648,-0.0417359427,-0.0021904222,-0.0011295427,-0.0193656656,0.0402110138,-0.0123270131,-0.0062998381,0.018890103,-0.0155858981,0.0160531823,0.0622033351,0.0231908027,-0.0211373389,0.0037573944,-0.0277642593,-0.0274053879,0.0218856393,0.0024957912]},"436":{"Abstract":"Occlusion is an issue in volumetric visualization as it prevents direct visualization of the region of interest. While many techniques such as transfer functions, volume segmentation or view distortion have been developed to address this, there is still room for improvement to better support the understanding of objects' vicinity. However, most existing Focus+Context fail to solve partial occlusion in datasets where the target and the occluder are very similar density-wise. For these reasons, we investigate a new technique which maintains the general structure of the investigated volumetric dataset while addressing occlusion issues. With our technique, the user interactively defines an area of interest where an occluded region or object is partially visible. Then our lens starts pushing at its border occluding objects, thus revealing hidden volumetric data. Next, the lens is modified with an extended field of view (fish-eye deformation) to better see the vicinity of the selected region. Finally, the user can freely explore the surroundings of the area under investigation within the lens. To provide real-time exploration, we implemented our lens using a GPU accelerated ray-casting framework to handle ray deformations, local lighting, and local viewpoint manipulation. We illustrate our technique with five application scenarios in baggage inspection, 3D fluid flow visualization, chest radiology, air traffic planning, and DTI fiber exploration.","Authors":"M. Traor\u00e9; C. Hurter; A. Telea","DOI":"10.1109\/TVCG.2018.2864690","Keywords":"Interaction techniques;focus + context;volume visualization;volume rendering;raycasting","Title":"Interactive obstruction-free lensing for volumetric data visualization","Keywords_Processed":"volume render;focus context;raycaste;interaction technique;volume visualization","Keyword_Vector":[0.1176214684,-0.0640448061,0.0059132175,-0.051108628,0.008148319,0.0453839161,0.0370410068,-0.0137475848,0.0108469182,-0.0657683019,0.0131453024,0.0269978272,-0.0048755711,-0.0386528266,-0.0328083089,0.0284490285,-0.0364652404,0.0723031394,-0.0303905819,0.0007348131,-0.0195820169,0.0373801549,0.0066942811,-0.0465950261,0.1804998862,-0.0061121539,0.0035589107,0.0491632407,-0.0889239292,-0.0221897932,-0.0614655077,-0.103997496,0.00885068,0.034700585,-0.0436833032,0.0125806204,-0.0122539354,-0.0514109522,0.0740305073,0.006330525,-0.0227056279,0.0294105363,0.0059837625,0.0143325308,0.0435201425,0.0464304853,-0.080942785,0.0127704218,-0.0458570843,0.0543841512,-0.0522857863,0.0518405596],"Abstract_Vector":[0.2072575224,-0.1605709036,0.0012541348,0.0641402323,0.0501393021,0.0190846745,-0.0284704494,-0.0249380911,-0.0234367878,-0.0264059723,0.0261110618,0.0166157473,0.0340367448,-0.0175326621,-0.0573314525,-0.0291228022,-0.0193574718,-0.012367787,-0.0230773831,0.0174623245,0.034509012,-0.0471441106,-0.0102620049,-0.0148868949,-0.0565152907,-0.0031296298,-0.0218349385,0.032597152,-0.097436813,-0.0481579813,0.0460014475,0.0128399043,-0.0427517346,0.0281460432,-0.0714352019,-0.01041456,-0.0060967094,0.021860579,-0.0516917707,0.0222163802,-0.0425225282,-0.0042819871,0.0067704204,-0.0026218415,0.0146534212,0.0271532448,0.0105868765,-0.0163122206,-0.0115407139,0.0354141303,0.0078090551,0.0288283192,0.0618435533,0.0318368958,0.0529457983,-0.0766311794,-0.0306828153,0.0179094542,-0.0370733818,0.0767878837,-0.0103209882,-0.0473461912,-0.0503975559,-0.0626916832,-0.0095231529,-0.0052636724,-0.0348470622,0.0168852775,-0.0380438263,-0.0071933076,-0.0209497466,0.0125323495,0.0044748405,0.0420290625,-0.0026893378,0.0064466729,-0.0785976532,-0.0083751614,-0.0233767373,-0.0004876831,0.0168845987,0.0042102901,0.0296668504,-0.0388282292,-0.0075238848,0.0069621874,-0.0242686474,0.0242791583,-0.0066754356,-0.0229936722,0.0133553327,0.0043979512,-0.0377903783,-0.0291533057,-0.0199394552,0.0372291431,0.023448885,-0.0314996614,-0.0014821625,0.0052196116,-0.0154836908,0.0063454695,-0.0093669644,0.0612675399,0.0104253895,-0.0034221945,-0.0121525011,0.0308837682,0.0434173607,-0.0387733619,0.0059790934,-0.0272227254,-0.0203560213,-0.0114542925,-0.0256235351,0.002470287]},"437":{"Abstract":"When data categories have strong color associations, it is useful to use these semantically meaningful concept-color associations in data visualizations. In this paper, we explore how linguistic information about the terms defining the data can be used to generate semantically meaningful colors. To do this effectively, we need first to establish that a term has a strong semantic color association, then discover which color or colors express it. Using co-occurrence measures of color name frequencies from Google n-grams, we define a measure for colorability that describes how strongly associated a given term is to any of a set of basic color terms. We then show how this colorability score can be used with additional semantic analysis to rank and retrieve a representative color from Google Images. Alternatively, we use symbolic relationships defined by WordNet to select identity colors for categories such as countries or brands. To create visually distinct color palettes, we use k-means clustering to create visually distinct sets, iteratively reassigning terms with multiple basic color associations as needed. This can be additionally constrained to use colors only in a predefined palette.","Authors":"V. Setlur; M. C. Stone","DOI":"10.1109\/TVCG.2015.2467471","Keywords":"linguistics;natural language processing;semantics;color names;categorical color;Google n-grams;WordNet;XKCD;Linguistics;natural language processing;semantics;color names;categorical color;Google n-grams;WordNet;XKCD","Title":"A Linguistic Approach to Categorical Color Assignment for Data Visualization","Keywords_Processed":"natural language processing;XKCD;color name;semantic;WordNet;categorical color;linguistic;Google gram","Keyword_Vector":[0.0534555834,-0.0456676848,-0.0098991642,0.0161018389,0.0096966353,0.001215321,0.0729071286,-0.017439843,-0.0364631491,-0.0249267273,-0.002981631,0.0114146443,0.0238034539,0.1330856112,-0.038401816,-0.021354127,0.0530829836,-0.001929504,0.0000922831,-0.0288963266,0.0108996115,-0.0243017813,-0.0074985016,0.0175486034,-0.0151581829,-0.0241525266,0.0186163135,-0.0344365966,-0.0628213335,0.0056627549,-0.0101188596,-0.0281236071,0.0357238896,0.0046330151,-0.0002683547,-0.0241003808,-0.0043779227,-0.0020120326,0.0170918034,0.0130396608,0.0076601966,0.0054654904,0.0092547063,-0.0187514319,0.0339446545,0.0136601948,0.0163515589,-0.0114038026,-0.0473460815,-0.0155322886,0.0158842433,0.0369137287],"Abstract_Vector":[0.1557023934,-0.0269223578,-0.0230136966,0.1429545998,0.1000006158,0.0656044592,-0.0663682276,0.0007598739,0.0727799247,-0.0206373507,-0.0417631909,0.0884541763,-0.0259788602,-0.1021872269,0.0419827647,0.1273670149,-0.0746356351,0.0654238603,-0.1438099588,-0.0538597382,-0.0846781023,0.0850780314,-0.0152572769,0.0568195999,-0.0240844859,-0.0577525326,-0.0218107759,0.0338791948,0.0273281695,-0.0750540663,0.0360160291,0.0093440139,-0.0140653312,0.0439530684,0.0446258083,-0.0171844917,0.013626757,0.0872155003,-0.0397493036,-0.0150502517,0.0577959011,-0.052231243,0.0178061428,0.0253964983,0.0134782908,0.0186710872,0.0784616127,0.0437710369,-0.0657381098,0.1002294907,-0.0472760419,0.0588092406,-0.0561662224,0.028500982,-0.0251223695,0.0321294081,0.0797255582,-0.02473912,-0.0555446232,-0.0093502912,0.0260397358,-0.0575491689,0.0068840568,-0.0054059078,-0.0392013311,0.0064271271,0.0090821582,0.0266845871,-0.005301639,-0.0506255396,0.001923536,-0.0150226487,0.0223944307,0.0207165238,0.0015720769,0.0108966961,0.011738575,0.0383828912,0.0964161341,-0.056021051,0.0492446234,-0.0154584219,-0.0214752023,0.0348835853,0.0244290166,-0.0239061292,0.0252921823,0.0151288351,-0.0240120985,0.0071035035,-0.0074558564,0.0022329167,-0.0387082121,0.0048582891,0.0202431593,-0.0291074768,-0.0069633051,-0.0107582094,0.0328786209,0.0031974012,-0.0294310645,0.0476910788,-0.0282345377,-0.0193914257,-0.0110450993,0.0220197747,-0.0149549787,-0.0349821693,-0.0157565118,0.0416234854,-0.0216976832,-0.0086946076,-0.0075905668,-0.0144731652,-0.0411541373,-0.0521646]},"438":{"Abstract":"We present TimeSpan, an exploratory visualization tool designed to gain a better understanding of the temporal aspects of the stroke treatment process. Working with stroke experts, we seek to provide a tool to help improve outcomes for stroke victims. Time is of critical importance in the treatment of acute ischemic stroke patients. Every minute that the artery stays blocked, an estimated 1.9 million neurons and 12 km of myelinated axons are destroyed. Consequently, there is a critical need for efficiency of stroke treatment processes. Optimizing time to treatment requires a deep understanding of interval times. Stroke health care professionals must analyze the impact of procedures, events, and patient attributes on time-ultimately, to save lives and improve quality of life after stroke. First, we interviewed eight domain experts, and closely collaborated with two of them to inform the design of TimeSpan. We classify the analytical tasks which a visualization tool should support and extract design goals from the interviews and field observations. Based on these tasks and the understanding gained from the collaboration, we designed TimeSpan, a web-based tool for exploring multi-dimensional and temporal stroke data. We describe how TimeSpan incorporates factors from stacked bar graphs, line charts, histograms, and a matrix visualization to create an interactive hybrid view of temporal data. From feedback collected from domain experts in a focus group session, we reflect on the lessons we learned from abstracting the tasks and iteratively designing TimeSpan.","Authors":"M. H. Loorak; C. Perin; N. Kamal; M. Hill; S. Carpendale","DOI":"10.1109\/TVCG.2015.2467325","Keywords":"Multi-dimensional data,;Temporal event sequences;Electronic health records;Multi-dimensional data;Temporal event sequences;Electronic health records","Title":"TimeSpan: Using Visualization to Explore Temporal Multi-dimensional Data of Stroke Patients","Keywords_Processed":"electronic health record;multi dimensional datum;temporal event sequence","Keyword_Vector":[0.0127372533,-0.0079388703,-0.0029396918,-0.0028453513,0.0076413196,-0.0064510513,-0.0126498276,0.0084158761,-0.007580948,-0.0121327218,-0.0028680201,-0.0010579892,0.0031503684,-0.00191635,0.0054142602,0.0127443216,-0.0002351535,0.0127755623,0.0241940014,-0.006096873,-0.004796027,-0.0022981914,0.0090441678,0.0064222863,0.0045482158,0.0028794984,-0.0110941611,-0.0074024187,0.0022119457,-0.0066873792,-0.0000491493,0.001387091,-0.0074635074,-0.0211995187,0.0038069472,0.0030221725,0.0035902511,-0.0046562626,-0.0239987032,-0.008549361,-0.0055035555,-0.0095216797,0.0131435302,0.014731034,-0.0045572806,-0.0217942294,0.0036294268,0.001648147,-0.0029346792,0.0176223012,0.0188727802,0.003558449],"Abstract_Vector":[0.188520235,-0.090069492,-0.0213197881,-0.0145965251,-0.0512348748,0.071732703,0.0551538693,0.0548226475,0.0093837327,0.0399577541,-0.080916247,0.0004712372,0.0097129324,-0.0212485553,0.0422725272,0.0651008286,0.045623938,0.0212969006,0.0672665723,0.0626144729,-0.0159944947,-0.067102993,-0.0054419584,0.0859598655,0.0417404745,-0.0580535645,-0.0467265494,0.0201432924,-0.0267947347,0.0210893114,0.0113572461,0.0203866658,0.0066709591,-0.0567047887,0.0269144206,0.007578664,0.0033772712,-0.0360458997,-0.0480802761,-0.0008699935,-0.0013255572,0.0334606836,0.0359140086,0.0296554747,-0.0058962337,-0.020178904,-0.0382181858,-0.0225325751,-0.0195882809,-0.0108166615,0.0085309172,-0.0197263083,0.0010202842,-0.0045933279,0.013841834,-0.002866621,0.0168029292,-0.0155564951,-0.0363385312,0.0015261072,0.0110317443,0.0111587368,-0.0217191338,0.0350685436,-0.0129707502,-0.0136836074,-0.0127474014,-0.0280714405,-0.0509741917,0.003339609,0.001796861,-0.0206282148,0.0123432986,0.0456404591,-0.0158411982,-0.0002811218,0.0140300865,0.0049065667,0.0057043992,-0.0253303667,-0.020955028,0.0268604268,-0.0090940915,0.0309702063,-0.0020978505,0.0089916222,-0.0138892768,-0.0114804601,-0.0058792744,-0.0176417426,-0.0238462256,0.0183272494,-0.0062588722,0.0167325943,-0.0258430969,-0.0301974404,-0.0041905428,-0.0108574897,-0.0275151162,0.0101142192,0.0084832751,0.0013631293,0.0005234812,0.0318926302,-0.0283707049,-0.0483732059,0.003946101,0.0349754553,-0.0042254508,-0.0046527852,-0.0139346566,-0.0067514594,-0.0279405534,0.0039764698,-0.0018183459,0.0287534112]},"439":{"Abstract":"In this paper we present a novel Smoothed Particle Hydrodynamics (SPH) method for the efficient and stable simulation of incompressible fluids. The most efficient SPH-based approaches enforce incompressibility either on position or velocity level. However, the continuity equation for incompressible flow demands to maintain a constant density and a divergence-free velocity field. We propose a combination of two novel implicit pressure solvers enforcing both a low volume compression as well as a divergence-free velocity field. While a compression-free fluid is essential for realistic physical behavior, a divergence-free velocity field drastically reduces the number of required solver iterations and increases the stability of the simulation significantly. Thanks to the improved stability, our method can handle larger time steps than previous approaches. This results in a substantial performance gain since the computationally expensive neighborhood search has to be performed less frequently. Moreover, we introduce a third optional implicit solver to simulate highly viscous fluids which seamlessly integrates into our solver framework. Our implicit viscosity solver produces realistic results while introducing almost no numerical damping. We demonstrate the efficiency, robustness and scalability of our method in a variety of complex simulations including scenarios with millions of turbulent particles or highly viscous materials.","Authors":"J. Bender; D. Koschier","DOI":"10.1109\/TVCG.2016.2578335","Keywords":"Fluid simulation;smoothed particle hydrodynamics;divergence-free fluids;incompressibility;viscous fluids;implicit integration","Title":"Divergence-Free SPH for Incompressible and Viscous Fluids","Keywords_Processed":"viscous fluid;fluid simulation;incompressibility;divergence free fluid;implicit integration;smoothed particle hydrodynamic","Keyword_Vector":[0.0832204159,-0.0155771219,-0.038452659,0.0170676992,0.0568087798,-0.0037593925,0.0527673947,0.0275282351,-0.047704035,-0.0664581699,-0.0259664739,0.01736708,-0.0041099568,-0.0015580157,0.0402248655,0.0298460039,-0.0430095738,-0.0647902935,0.0469130191,-0.1236379426,0.0143514377,-0.0643221252,0.0020343436,0.0093084218,0.2297062438,-0.0571447497,0.1033320832,0.0718000298,-0.0503581855,-0.0264966901,-0.0297268078,-0.012167572,-0.0434948946,-0.0217325588,0.0034894674,0.0776804065,0.0338443356,-0.0435629339,0.0370112823,0.0177907969,0.0681032799,-0.0194554598,0.0501779262,0.0894783162,-0.0951334612,-0.0730799188,-0.054707897,-0.0728417718,0.0095369993,0.0262854402,0.0484497495,-0.0552872718],"Abstract_Vector":[0.1535987356,-0.0361191667,-0.0097201112,0.0276940599,0.0098352315,0.0106783423,-0.0068921185,0.0026294246,-0.0421632133,-0.00698477,-0.0061502345,-0.0159528607,-0.0616026015,-0.0136790557,-0.000804766,-0.0205646098,-0.0054771792,0.0124051642,-0.0193638919,-0.000007959,0.0116364436,0.0345028975,0.0174946906,-0.0070567233,-0.0173021293,0.0474197917,0.0083367472,-0.0171974321,0.016223624,-0.0576466804,0.024505811,0.0240004909,-0.0009262706,-0.0342532954,-0.0352992113,-0.0079915494,0.0026077248,0.0000046348,0.0185105319,0.0234028224,-0.023837273,-0.0181810481,-0.011460601,-0.0254951589,-0.0229258141,0.0064193221,0.0468787088,-0.0018851521,0.0242179894,-0.0293409304,0.0106781908,-0.033904193,0.0114121827,0.0160452726,-0.0295592147,0.0075907186,0.0084320888,0.0023794663,0.0069554377,-0.0132824601,-0.0127191759,-0.0179063731,-0.0180086074,-0.0247244568,0.0021858594,-0.0613919959,0.0166213153,-0.0311404521,-0.0280045012,-0.0337962369,-0.0375887921,-0.0229323538,0.0001914489,-0.069278787,-0.0326914882,-0.0438822039,0.0121922431,-0.0343721344,0.0110254284,0.0439085541,0.0560793006,0.053936492,-0.0011547468,-0.0209051011,0.007850096,-0.0037343783,0.0132989837,-0.0075676418,-0.0233738117,0.0287711769,-0.0272068058,0.0248414209,-0.0071209084,-0.0211880656,0.0187623964,-0.0306617163,0.0092875025,0.0163445605,-0.0274856515,-0.003981098,0.0146594833,0.0264379163,-0.0078798602,-0.016060649,-0.0052196718,0.0122620978,-0.0095033979,-0.0099862159,0.0114806535,0.0140906136,0.007437066,0.0222646214,0.0156127176,-0.010582246,0.0302857105,0.0535381931]},"44":{"Abstract":"In clinical cardiology, both anatomy and physiology are needed to diagnose cardiac pathologies. CT imaging and computer simulations provide valuable and complementary data for this purpose. However, it remains challenging to gain useful information from the large amount of high-dimensional diverse data. The current tools are not adequately integrated to visualize anatomic and physiologic data from a complete yet focused perspective. We introduce a new computer-aided diagnosis framework, which allows for comprehensive modeling and visualization of cardiac anatomy and physiology from CT imaging data and computer simulations, with a primary focus on ischemic heart disease. The following visual information is presented: (1) Anatomy from CT imaging: geometric modeling and visualization of cardiac anatomy, including four heart chambers, left and right ventricular outflow tracts, and coronary arteries; (2) Function from CT imaging: motion modeling, strain calculation, and visualization of four heart chambers; (3) Physiology from CT imaging: quantification and visualization of myocardial perfusion and contextual integration with coronary artery anatomy; (4) Physiology from computer simulation: computation and visualization of hemodynamics (e.g., coronary blood velocity, pressure, shear stress, and fluid forces on the vessel wall). Substantially, feedback from cardiologists have confirmed the practical utility of integrating these features for the purpose of computer-aided diagnosis of ischemic heart disease.","Authors":"G. Xiong; P. Sun; H. Zhou; S. Ha; B. \u00f3. Hartaigh; Q. A. Truong; J. K. Min","DOI":"10.1109\/TVCG.2016.2520946","Keywords":"Cardiac chamber;coronary artery;imaging;anatomy;physiology;perfusion;motion;strain;shear stress;bull\u2019s-eye plot;image analysis;geometric modeling;computational fluid dynamics;visualization.","Title":"Comprehensive Modeling and Visualization of Cardiac Anatomy and Physiology from CT Imaging and Computer Simulations","Keywords_Processed":";image;computational fluid dynamic;anatomy;bull eye plot;image analysis;physiology;geometric modeling;shear stress;motion;coronary artery;cardiac chamber;strain;perfusion;visualization","Keyword_Vector":[0.0380569391,-0.0136840091,-0.0011255442,-0.0128772357,0.0311818914,0.0026834621,0.0563067184,-0.0293054612,-0.0338036859,-0.0295202668,0.0165551967,0.0123878769,-0.0242235448,-0.0234551713,0.0136986554,-0.0097604984,-0.0048026123,0.0160310779,-0.0335063485,-0.067902671,-0.0091047833,0.0419222134,-0.0232738818,-0.0359827034,0.1532964262,-0.008392416,0.0198856071,0.0346113263,-0.0804552182,-0.1068394219,-0.0449434792,-0.0594289271,-0.0329800488,0.0254790828,-0.0478718562,-0.0212095391,-0.0156600248,0.0311296199,0.0225887128,-0.0211275966,0.0330547843,0.005462328,-0.051829998,-0.0095690047,0.0111637811,-0.0681977905,-0.0516621952,0.0517925235,-0.0272652301,-0.0026046667,-0.023591403,-0.0110801556],"Abstract_Vector":[0.164538097,0.0309172834,-0.0422738127,0.0213439881,0.0074209256,0.0419506433,0.0078639804,0.0305134132,-0.0077432093,-0.0183688188,0.0260260345,-0.0416258832,-0.1027061989,-0.0284418712,0.020580654,-0.0622613768,-0.0712422799,0.0264708348,0.0120414305,0.0097947551,-0.0030406085,0.0327867826,-0.0005862302,0.0365145629,0.0119639712,-0.0181649263,0.0296980317,-0.0254340319,0.0052086995,-0.0096988784,-0.0289360506,0.0050253834,-0.0476074104,0.0047608032,0.0059681475,-0.0376209156,0.0226847178,0.0070747104,0.00658848,0.0071821016,0.0074264623,0.0066700947,0.036918187,0.0433077148,0.0417706413,-0.0074641153,0.0120166573,-0.0023690873,0.032125688,-0.0097699869,0.0118041264,-0.0155113056,0.0028287679,-0.0008311954,0.0077697053,-0.0266332701,0.0027034223,-0.0122290251,-0.0293741585,-0.0385559216,0.0130279569,0.0102266768,0.0286201707,-0.0001859027,0.0181138019,-0.0126267433,0.0160702481,0.0144517872,0.0034119493,0.0128494077,0.0300037062,-0.033906028,-0.0096983561,-0.0098037866,-0.0278493847,-0.0175379474,0.0158355943,0.0368518803,0.0093514031,-0.0007368945,0.0263626602,-0.0184278619,0.0471011942,0.0023064744,0.0046453353,-0.0049916043,0.0135723091,0.0072000465,0.0106801509,0.0074211118,-0.0130349235,0.0035429953,0.0044517348,-0.0122397857,0.0017043669,-0.0249312847,0.0066348929,-0.0033343316,0.0122606007,0.0166610467,-0.0006962931,-0.0366810712,-0.0230002436,-0.0101215224,-0.012599797,-0.0051463081,-0.0088291792,0.0445695937,-0.0230415235,-0.0161211758,-0.0272834422,-0.0192551203,-0.0134973067,-0.0061650772,-0.0066564237,0.0063305757]},"440":{"Abstract":"We introduce a shape synthesis approach especially for functional hybrid creation that can be potentially used by a human operator under a certain pose. Shape synthesis by reusing parts in existing models has been an active research topic in recent years. However, how to combine models across different categories to design multi-function objects remains challenging, since there is no natural correspondence between models across different categories. We tackle this problem by introducing a human pose to describe object affordance which establishes a bridge between cross-class objects for composite design. Specifically, our approach first identifies groups of candidate shapes which provide affordances desired by an input human pose, and then recombines them as well-connected composite models. Users may control the design process by manipulating the input pose, or optionally specifying one or more desired categories. We also extend our approach to be used by a single operator with multiple poses or by multiple human operators. We show that our approach enables easy creation of nontrivial, interesting synthesized models.","Authors":"Q. Fu; X. Chen; X. Su; H. Fu","DOI":"10.1109\/TVCG.2017.2739159","Keywords":"3D modeling;shape synthesis;pose-inspired;functional hybrid","Title":"Pose-Inspired Shape Synthesis and Functional Hybrid","Keywords_Processed":"3d modeling;pose inspire;functional hybrid;shape synthesis","Keyword_Vector":[0.077455825,-0.0942317097,-0.1091863689,0.0706152336,-0.0510961579,-0.1167854907,0.067166781,-0.0110585925,0.0215115962,-0.0149134129,0.0233630116,-0.0160644511,-0.0244483922,-0.0163770512,0.0581655872,0.0032816867,-0.01450349,0.0003556411,-0.0089272385,-0.0248215717,0.013968671,0.0150112766,0.0167641898,0.0161267277,0.0318094532,-0.0094303397,-0.0060587911,0.0236509889,0.0061280186,-0.0423972049,0.0017138838,-0.0191619698,-0.0085242273,-0.0637779027,-0.0168056066,-0.0264496949,0.0002293379,-0.0154368719,-0.0451787805,-0.0230228705,0.0122931857,0.0286932123,-0.0004164814,-0.0324903293,-0.0051332303,-0.0005148297,-0.0275464629,0.0168043044,0.0641460438,-0.0205811434,0.0846029404,-0.0471049802],"Abstract_Vector":[0.1957382515,-0.1207942804,0.0169321275,-0.0057507052,-0.0105632845,-0.0494054466,0.0127476735,-0.0221418734,-0.005738245,0.0507091883,0.0009201421,-0.001256107,-0.065146401,0.038319872,-0.0277330953,0.0073615402,0.0516873438,-0.0224160452,-0.0497637657,0.0168770035,-0.0135348558,0.0400808399,0.0312292755,-0.0062420576,0.0049958308,0.056807119,0.0821094942,0.0318373618,-0.0193387863,-0.0177276906,-0.0588417622,-0.0050938557,0.0290839573,0.013857674,0.0129468897,0.0388095671,-0.0412529434,0.0375231504,-0.0657087086,-0.0566098391,0.0051717937,0.0072117832,0.0468929117,-0.0187035533,0.003278204,0.0172210062,0.014249788,-0.0585821476,0.0440861815,-0.0656233979,-0.063945054,-0.0377869337,-0.0088048273,0.0812462715,-0.0224213801,-0.0098413616,-0.0219350886,0.0424256873,0.0043895544,-0.0308318127,-0.0401107411,0.0276935512,0.0084901365,-0.0160521015,-0.0267608083,-0.0266213067,0.0568391853,0.0594438706,-0.0385992674,-0.017438424,-0.0046036888,-0.0592892558,-0.0269287454,0.049395493,0.0159810023,0.0317660011,-0.0042882227,0.0320034595,-0.0384163717,-0.0090390512,0.000676873,-0.008353525,-0.0060927369,0.0008612276,0.0141857784,-0.0540306382,0.0266622671,-0.000163565,0.046904296,0.0194716704,0.0310811074,0.006198242,-0.0393987768,0.0213029421,0.0321646214,0.0271147335,0.0005074648,0.0116618996,-0.0026666445,0.0054935965,-0.0471996233,-0.0592138444,0.0344066956,-0.0217122801,-0.0270612588,0.0043955386,-0.0074848421,-0.0051049759,0.0381343033,-0.03110015,0.0007742937,-0.0158790258,-0.0033104827,-0.0247682667,-0.0714830349,-0.017391619]},"441":{"Abstract":"System schematics, such as those used for electrical or hydraulic systems, can be large and complex. Fisheye techniques can help navigate such large documents by maintaining the context around a focus region, but the distortion introduced by traditional fisheye techniques can impair the readability of the diagram. We present SchemeLens, a vector-based, topology-aware fisheye technique which aims to maintain the readability of the diagram. Vector-based scaling reduces distortion to components, but distorts layout. We present several strategies to reduce this distortion by using the structure of the topology, including orthogonality and alignment, and a model of user intention to foster smooth and predictable navigation. We evaluate this approach through two user studies: Results show that (1) SchemeLens is 16-27% faster than both round and rectangular flat-top fisheye lenses at finding and identifying a target along one or several paths in a network diagram; (2) augmenting SchemeLens with a model of user intentions aids in learning the network topology.","Authors":"A. Coh\u00e9; B. Liutkus; G. Bailly; J. Eagan; E. Lecolinet","DOI":"10.1109\/TVCG.2015.2467035","Keywords":"Fisheye;vector-scaling;content-aware;network schematics;interactive zoom;navigation;information visualization;Fisheye;vector-scaling;content-aware;network schematics;interactive zoom;navigation;information visualization","Title":"SchemeLens: A Content-Aware Vector-Based Fisheye Technique for Navigating Large Systems Diagrams","Keywords_Processed":"network schematic;vector scale;interactive zoom;content aware;navigation;Fisheye;information visualization","Keyword_Vector":[0.0322089335,0.0177110757,0.0194459256,0.0405393935,0.0341013388,-0.0083447275,0.0022604666,-0.0461019973,-0.0159831737,-0.0064140209,0.0162684348,0.0039258595,-0.0348151136,0.0061294758,0.0065765801,-0.0236266868,-0.0296977953,-0.0320483047,0.0015006693,-0.0232510606,0.0086695339,-0.0098596192,-0.031385448,0.014134257,0.0193529891,-0.0154450662,0.0220622852,0.0213759042,0.0032450199,0.0003171527,0.0078477023,-0.0027228005,-0.0134130408,-0.0110104234,0.0305329738,-0.0165758445,0.0758986497,0.0286296889,-0.0356149608,-0.0458507045,-0.0120116096,-0.0368109971,-0.046302246,0.0260330067,0.0272965945,-0.0154358847,-0.0038597003,0.0330456844,-0.0781509828,-0.0027001596,-0.0097351247,-0.0147359938],"Abstract_Vector":[0.1628881351,0.0739485712,0.087164543,0.0461945219,-0.0845186341,0.0089852704,-0.1000481046,-0.0399856406,0.0065257739,0.0327358219,-0.0137790932,-0.0345600947,-0.0157521471,-0.0126644799,-0.0111484029,0.0030605028,-0.005038551,-0.0115322427,-0.0194753422,-0.0147800014,-0.0497602182,-0.0028662359,0.0191304466,-0.0459543923,-0.0115552399,0.0771807741,-0.0104704617,0.0878420439,-0.0472024184,-0.008228103,0.047786823,0.0285349077,0.0275522915,0.069446775,-0.0427510606,0.0200528998,0.0853863421,0.0575844999,0.0249104861,0.0983113992,0.1188448711,0.0421565253,0.0098240277,-0.0152342541,-0.0059341958,-0.0372838008,-0.0282776996,0.1251444501,-0.0296883759,-0.0490216832,-0.0120965114,0.0043566818,-0.0155277901,-0.0025995857,-0.0010230878,0.0071154266,0.0635210129,-0.0318710591,-0.0736067125,0.080161279,0.0178629208,0.0653542379,0.0070074342,-0.0671240664,0.029078527,-0.0412945269,0.0443548796,-0.0645974363,0.073481073,0.1304616803,-0.0290460418,0.0153826781,0.0735632357,0.0670558145,0.0552475306,0.0353558134,-0.057245479,-0.0062473773,0.0082424982,0.0470008324,-0.0019089451,-0.1292903779,0.0145043543,-0.0486248214,0.0459637862,-0.0498968154,-0.0208009903,-0.0456645715,0.0214512923,-0.0234234186,0.0049058401,-0.0479584781,0.0091561736,-0.0430194807,0.0649092345,-0.0468338261,-0.0449362692,0.0656308844,0.0599650066,-0.0004425296,0.0149659641,0.0329109434,0.0328006048,0.0544216695,0.0515870147,-0.0727295925,0.0312532826,-0.0061868622,-0.0022045514,-0.036477955,-0.0774694924,0.0446705473,-0.0354512594,-0.011481886,-0.0036662659,0.0437691356]},"442":{"Abstract":"The energy performance of large building portfolios is challenging to analyze and monitor, as current analysis tools are not scalable or they present derived and aggregated data at too coarse of a level. We conducted a visualization design study, beginning with a thorough work domain analysis and a characterization of data and task abstractions. We describe generalizable visual encoding design choices for time-oriented data framed in terms of matches and mismatches, as well as considerations for workflow design. Our designs address several research questions pertaining to scalability, view coordination, and the inappropriateness of line charts for derived and aggregated data due to a combination of data semantics and domain convention. We also present guidelines relating to familiarity and trust, as well as methodological considerations for visualization design studies. Our designs were adopted by our collaborators and incorporated into the design of an energy analysis software application that will be deployed to tens of thousands of energy workers in their client base.","Authors":"M. Brehmer; J. Ng; K. Tate; T. Munzner","DOI":"10.1109\/TVCG.2015.2466971","Keywords":"Design study;design methodologies;time series data;task and requirements analysis;coordinated and multiple views;Design study;design methodologies;time series data;task and requirements analysis;coordinated and multiple views","Title":"Matches, Mismatches, and Methods: Multiple-View Workflows for Energy Portfolio Analysis","Keywords_Processed":"task and requirement analysis;design study;design methodology;coordinated and multiple view;time series datum","Keyword_Vector":[0.0503594326,-0.0361066119,-0.0253514742,-0.047009349,0.0204688357,-0.0332759354,-0.080296565,-0.014092088,-0.0932798194,0.04803567,-0.02346399,0.0220177352,0.0226618266,-0.0013048194,0.0224722595,-0.0198591665,0.0536704094,0.0653454134,0.0812183557,-0.0842044905,-0.012192118,0.1207385418,0.1237543412,0.0590536324,-0.0030966259,-0.0640861838,0.0442844466,-0.0641151496,0.0297564508,-0.0395446918,-0.0003367796,0.0102087,0.0019579071,-0.0061860088,0.0353535331,0.0366328084,-0.0042242664,0.0328438321,0.0110022114,0.0035340984,-0.0594512274,-0.055617292,0.0539464727,-0.0038505041,-0.0307761243,0.0159195713,-0.0052599275,0.0424305519,0.005176609,-0.0419440084,0.0353844909,0.0359536104],"Abstract_Vector":[0.162026493,-0.0797960067,-0.0024038237,0.0015018588,-0.0518732331,0.0121148533,-0.0063990777,0.0026187125,-0.0168738801,0.0303741035,-0.0692078493,0.0338211406,-0.0444489925,-0.0073670942,-0.017333465,0.0465921409,0.0100827658,-0.0108689754,0.0039409765,0.0327953506,-0.011616377,0.0063435315,-0.011658793,0.0403362364,0.0304668945,0.0324109356,0.0198121419,0.0280699725,0.0115922582,0.0416944125,-0.0101517211,0.0059220484,-0.0008310888,-0.0091073123,0.0507238138,-0.0658889213,0.0021989406,-0.0186791269,-0.0186976612,-0.0467731002,0.0090968383,0.0407703706,0.0326165116,0.0144172694,0.0008910978,0.0176018039,-0.0074168523,-0.0145064726,0.0051974428,-0.0018655713,-0.0042980116,0.0128804818,-0.0022055392,-0.0352025912,0.0226339968,0.0486963048,0.0177663223,0.0009730206,-0.0370366386,-0.0301186877,-0.0000632073,0.0815262384,0.0145382479,-0.0144432475,-0.0408723075,0.0138681644,-0.0085338268,0.0208177709,-0.0375980481,0.0243317989,0.0241950579,0.0487829326,0.0203903462,0.0055829943,-0.0398767447,-0.0241064462,0.0166618838,-0.0184209466,0.0093046746,0.0230126982,0.0196123893,-0.0379786758,0.0044411177,-0.0020534116,-0.0424064961,0.0224005937,-0.0140470693,-0.0074317142,-0.0201213929,0.0023877141,0.0238419661,-0.008594363,-0.0065424474,-0.0421237557,-0.01960323,-0.0283022056,0.0071094924,0.0226222205,-0.0054244855,0.0019816534,-0.0133436801,-0.0063678175,-0.0314249472,0.0049563251,0.0101554856,-0.0446037102,-0.0051852713,0.0180737466,-0.0180885408,-0.0193015526,0.0128455883,0.0328998715,-0.0831048085,0.0101783231,0.0280336034,-0.0085534559]},"443":{"Abstract":"Clustering, the process of grouping together similar items into distinct partitions, is a common type of unsupervised machine learning that can be useful for summarizing and aggregating complex multi-dimensional data. However, data can be clustered in many ways, and there exist a large body of algorithms designed to reveal different patterns. While having access to a wide variety of algorithms is helpful, in practice, it is quite difficult for data scientists to choose and parameterize algorithms to get the clustering results relevant for their dataset and analytical tasks. To alleviate this problem, we built Clustervision, a visual analytics tool that helps ensure data scientists find the right clustering among the large amount of techniques and parameters available. Our system clusters data using a variety of clustering techniques and parameters and then ranks clustering results utilizing five quality metrics. In addition, users can guide the system to produce more relevant results by providing task-relevant constraints on the data. Our visual user interface allows users to find high quality clustering results, explore the clusters using several coordinated visualization techniques, and select the cluster result that best suits their task. We demonstrate this novel approach using a case study with a team of researchers in the medical domain and showcase that our system empowers users to choose an effective representation of their complex data.","Authors":"B. C. Kwon; B. Eysenbach; J. Verma; K. Ng; C. De Filippi; W. F. Stewart; A. Perer","DOI":"10.1109\/TVCG.2017.2745085","Keywords":"Unsupervised Clustering;Visual Analytics;Quality Metrics;Interactive Visual Clustering","Title":"Clustervision: Visual Supervision of Unsupervised Clustering","Keywords_Processed":"Quality metric;visual analytic;unsupervised Clustering;Interactive Visual Clustering","Keyword_Vector":[0.1782979992,0.2874228378,-0.1628809838,-0.0143097939,-0.0527195512,0.0056353194,0.0452633269,0.0135640511,-0.0051057299,-0.0066924713,-0.0040055682,-0.0227293203,0.0392685527,0.0478105224,0.000388707,0.0034027668,-0.0008106276,-0.0320782546,0.0321889672,-0.0737990552,0.0349972937,0.0840684045,-0.0902941757,-0.0751545338,0.0376717478,0.0249338142,-0.0393436945,-0.0745795036,0.0379461993,0.0360005518,-0.0482527126,0.0246776275,-0.0114439176,0.0923494197,-0.0096123772,0.0857746026,0.0888859483,-0.0212186396,0.0277297155,0.0403301737,0.0325234098,0.116222807,-0.0091023034,-0.0235031428,-0.0753548878,0.0001120848,0.0580829926,0.0520211139,0.067515425,0.0674274802,0.0740484816,-0.0922149327],"Abstract_Vector":[0.1698976104,0.0768882383,-0.1167155024,-0.0803347189,-0.0007120223,0.0938481465,0.0141033405,-0.010546052,-0.0394674344,0.0027470049,-0.0320931999,0.0300868712,-0.0294631403,-0.0022776526,-0.003828214,0.0069042026,-0.0310990357,-0.0147584843,-0.032191645,-0.0699922787,-0.0152506025,-0.0467535767,-0.0174550405,-0.0258667567,0.0045347724,-0.0001540741,0.0471578312,0.0180217329,-0.0036613638,-0.0361499276,-0.0221598214,-0.0141379342,0.0178078182,-0.0192698835,0.0059885764,0.0335149279,0.0306298426,0.0817716076,0.0443517361,0.055204122,0.1140967319,0.1361663569,0.009003922,0.073855647,-0.0336395719,0.0429007326,-0.0731073096,0.0456301417,-0.0683288549,-0.0920258061,-0.001190369,0.0442430364,-0.0042376344,0.0980511834,-0.0036877771,0.0070655138,-0.0431850289,-0.0266147119,-0.0218156584,0.0249445243,-0.077988912,-0.0533158604,-0.0035434565,-0.0359803085,-0.0066270602,0.0701390888,-0.0104779099,0.021230966,-0.0020677717,-0.0211500708,0.084717057,-0.0346260726,-0.0209274668,-0.021697447,-0.0548768099,0.0060871355,0.0555758932,0.0231705649,-0.0295310093,0.0086407969,-0.0015180813,0.010251454,0.0287177433,-0.0482116266,-0.0299240889,0.0382990562,-0.0389514095,-0.0103043953,0.0196105585,0.0362304695,0.0097785159,0.0040867896,0.0045128066,0.0393284555,0.0035603428,-0.0172705162,0.0170566993,-0.0226534822,0.0190191444,-0.0325582125,-0.0061158445,-0.0058401123,0.0245262214,0.0094670386,-0.0570975916,0.0091253901,-0.0126371506,0.0407254779,0.0269102928,0.0094209346,-0.0063287633,-0.0244850865,0.0270223861,-0.0485667437,0.0172804429,-0.022747275]},"444":{"Abstract":"Mass spectrometry imaging (MSI) is a transformative imaging method that supports the untargeted, quantitative measurement of the chemical composition and spatial heterogeneity of complex samples with broad applications in life sciences, bioenergy, and health. While MSI data can be routinely collected, its broad application is currently limited by the lack of easily accessible analysis methods that can process data of the size, volume, diversity, and complexity generated by MSI experiments. The development and application of cutting-edge analytical methods is a core driver in MSI research for new scientific discoveries, medical diagnostics, and commercial-innovation. However, the lack of means to share, apply, and reproduce analyses hinders the broad application, validation, and use of novel MSI analysis methods. To address this central challenge, we introduce the Berkeley Analysis and Storage Toolkit (BASTet), a novel framework for shareable and reproducible data analysis that supports standardized data and analysis interfaces, integrated data storage, data provenance, workflow management, and a broad set of integrated tools. Based on BASTet, we describe the extension of the OpenMSI mass spectrometry imaging science gateway to enable web-based sharing, reuse, analysis, and visualization of data analyses and derived data products. We demonstrate the application of BASTet and OpenMSI in practice to identify and compare characteristic substructures in the mouse brain based on their chemical composition measured via MSI.","Authors":"O. R\u00fcbel; B. P. Bowen","DOI":"10.1109\/TVCG.2017.2744479","Keywords":"Mass spectrometry imaging;Data provenance;Visualization;Data management;Analysis Workflows;Data sharing","Title":"BASTet: Shareable and Reproducible Analysis and Visualization of Mass Spectrometry Imaging Data via OpenMSI","Keywords_Processed":"data provenance;datum sharing;mass spectrometry imaging;visualization;analysis workflow;data management","Keyword_Vector":[0.0855798304,-0.0442478172,-0.0072296098,0.0065055738,-0.0098471349,0.0478174967,-0.0141224867,-0.0187263562,-0.0196618223,-0.0002578812,0.0105767173,-0.0018242273,0.0281851625,-0.03182261,-0.0206885417,-0.0087269312,0.0141434708,-0.0360455506,-0.018697217,0.0156474933,-0.0597518276,0.0142820341,0.0249030556,0.043043096,0.0086596567,0.023445239,0.0171576923,0.0284078082,0.0438450739,-0.0437647071,0.041551693,0.1062692398,-0.119727467,-0.0293724132,0.1426786931,-0.0860965145,-0.0384498687,-0.0195917672,0.0054988149,-0.0614280553,-0.0638380821,0.2934453462,0.0030812282,-0.0256516777,-0.0226330642,0.0248251158,-0.0140359422,0.005997331,-0.0487520224,-0.0886193461,0.0800844173,0.006076663],"Abstract_Vector":[0.1890223749,-0.075319363,0.0521829917,-0.0208357627,0.0392175896,-0.070978285,0.0107759173,-0.0233507383,0.0413053727,0.0965062632,-0.0230038769,0.0095774831,-0.0853894332,0.0098486049,-0.0027320743,-0.0250682915,-0.0021932766,-0.0175779723,-0.0337368504,-0.0428132122,0.050005847,-0.0374960077,0.0469029033,-0.0130758512,-0.0026212231,0.0153505016,-0.0090130298,0.0444304257,-0.0133291755,-0.0294718266,0.0158201103,-0.0156271735,0.0192613467,-0.0120308038,0.0359390214,-0.0231537597,-0.0696842793,-0.0335546593,-0.0072196364,0.0554337801,-0.0132138827,0.009545873,0.0338109863,-0.0147839959,0.0923672258,-0.0104344617,-0.0063190051,0.0189851556,0.0107624187,0.0128492783,-0.0316430036,-0.0080501019,-0.0194794324,0.0267480765,-0.0337477702,0.0185081117,-0.0137246908,-0.0239140245,0.0015199594,0.0956144469,0.0224280422,-0.0303009959,-0.065117997,-0.0530679431,0.0093710237,-0.0444215435,-0.0242055568,-0.0473652415,-0.0401369273,-0.010620535,-0.0442936549,-0.0366380563,-0.0513205009,0.0127727859,-0.0118691462,-0.0343920593,-0.0264771268,0.006118632,0.0622012545,-0.0059324978,0.0064858342,-0.0058877792,-0.0159241148,-0.0069811815,-0.0498240241,0.0060878933,-0.0040810591,-0.0100168586,-0.0006051477,0.0295559251,-0.0026289745,0.0160798556,0.000136976,-0.0254350302,0.0081520875,0.01161051,-0.0461922803,0.0515088448,-0.0603630306,-0.0349492369,0.0238385034,-0.0126613094,-0.0136150208,0.0264400757,-0.0519553436,0.061409471,0.0215929577,-0.029120324,-0.0077461509,0.0027511808,0.0265645775,0.0536003463,0.0114959754,-0.0183448958,-0.0249781603,-0.0152578091]},"445":{"Abstract":"This paper introduces ScreenX, which is a novel movie viewing platform that enables ordinary movie theatres to become multi-projection movie theatres. This enables the general public to enjoy immersive viewing experiences. The left and right side walls are used to form surrounding screens. This surrounding display environment delivers a strong sense of immersion in general movie viewing. However, na\u00efve display of the content on the side walls results in the appearance of distorted images according to the location of the viewer. In addition, the different dimensions in width, height, and depth among theatres may lead to different viewing experiences. Therefore, for successful deployment of this novel platform, an approach to providing similar movie viewing experiences across target theatres is presented. The proposed image representation model ensures minimum average distortion of the images displayed on the side walls when viewed from different locations. Furthermore, the proposed model assists with determining the appropriate variation of the content according to the diverse viewing environments of different theatres. The theatre suitability estimation method excludes outlier theatres that have extraordinary dimensions. In addition, the content production guidelines indicate appropriate regions to place scene elements for the side wall, depending on their importance. The experiments demonstrate that the proposed method improves the movie viewing experiences in ScreenX theatres. Finally, ScreenX and the proposed techniques are discussed with regard to various aspects and the research issues that are relevant to this movie viewing platform are summarized.","Authors":"J. Lee; S. Lee; Y. Kim; J. Noh","DOI":"10.1109\/TVCG.2016.2532327","Keywords":"Multi-projection;immersive display system;image representation;immersive theatres","Title":"ScreenX: Public Immersive Theatres with Uniform Movie Viewing Experiences","Keywords_Processed":"image representation;Multi projection;immersive display system;immersive theatre","Keyword_Vector":[0.0380178195,0.0164798639,-0.0099126514,-0.0220996079,0.0127704656,-0.0273836751,-0.037372133,-0.007108065,-0.0925253211,-0.0269682654,0.0369378143,0.0626038534,0.0051202951,0.0178027408,0.0673604693,0.0653424968,0.0637457644,0.0702094953,0.183736345,0.0035536636,-0.0807640237,0.0739444947,0.0854849225,0.0670116606,-0.092920689,-0.028357804,-0.0213825578,-0.0219918965,0.100537651,-0.1130035002,-0.0300385422,-0.1139564883,0.0584595858,0.0445048707,-0.0422583165,-0.0052978176,-0.0259595732,-0.000450178,-0.0431134902,-0.0441180703,-0.0131134848,0.0487625054,0.0206664148,0.0244335512,0.0007228844,-0.0134814314,0.0355030758,0.043979555,0.0002583247,0.0473905504,-0.0331745339,0.0087170232],"Abstract_Vector":[0.1172950192,-0.0256830459,0.0469374812,-0.012990863,-0.0178956957,0.0266755277,0.0241323703,0.03483046,0.0342685916,0.0324754906,-0.0591638734,0.06927034,-0.0245925877,0.0061166656,0.0393265649,-0.0396783564,-0.0004362064,-0.0054767951,0.0122471493,-0.0455234396,0.021146882,-0.0565365817,0.0084366463,0.0015833414,-0.0314189496,0.0098897444,-0.0244569336,-0.0120119444,-0.0186036216,-0.0074916873,0.052882256,-0.0005314708,-0.0295574592,0.003850265,0.0092138836,-0.0462725847,-0.0091210307,-0.027071517,-0.0126798509,-0.0040358714,0.0069101815,0.0478081259,-0.0055652513,-0.0278242723,0.0093061167,0.0267754182,-0.0350409413,0.0155858104,0.0250730789,0.0099311568,-0.0082543714,-0.0100440002,-0.0066124897,-0.0270538069,0.0134593673,-0.0095995331,0.0150498018,-0.0055603598,0.0080430119,-0.0106108598,-0.0055692935,-0.0199179623,-0.0270200431,0.0102420821,-0.0329957756,-0.0189893373,-0.0066455289,-0.0409877198,-0.0286131489,0.0183416574,0.0133819647,0.0174734553,0.0022695528,0.015571658,-0.0264903529,-0.0451540043,0.0372871159,0.03525886,0.0169233622,-0.0194496571,0.0072193379,-0.0012312334,-0.0379678875,0.0519881723,-0.0332867928,0.0088281741,-0.0171289646,-0.0073760131,-0.0595684407,0.0127577676,0.0111166111,-0.0070638319,0.0156630719,0.0078726481,-0.0053354031,-0.0286745733,-0.0057383765,-0.0191409477,-0.0162618128,-0.0279450147,0.0168384751,-0.0159056986,-0.0259687062,-0.0130587602,0.0208355808,-0.0111462472,0.0244555283,0.0291688682,0.006959227,-0.0146566133,-0.0130159661,0.0013388448,0.0440459809,-0.0146870271,0.0082857006,0.0176610536]},"446":{"Abstract":"Spatial time series is a common type of data dealt with in many domains, such as economic statistics and environmental science. There have been many studies focusing on finding and analyzing various kinds of events in time series; the term `event' refers to significant changes or occurrences of particular patterns formed by consecutive attribute values. We focus on a further step in event analysis: discover temporal relationship patterns between event locations, i.e., repeated cases when there is a specific temporal relationship (same time, before, or after) between events occurring at two locations. This can provide important clues for understanding the formation and spreading mechanisms of events and interdependencies among spatial locations. We propose a visual exploration framework COPE (Co-Occurrence Pattern Exploration), which allows users to extract events of interest from data and detect various co-occurrence patterns among them. Case studies and expert reviews were conducted to verify the effectiveness and scalability of COPE using two real-world datasets.","Authors":"J. Li; S. Chen; K. Zhang; G. Andrienko; N. Andrienko","DOI":"10.1109\/TVCG.2018.2851227","Keywords":"Co-occurrence patterns;spatiotemporal visualization;spatial time series;visual analytics","Title":"COPE: Interactive Exploration of Co-Occurrence Patterns in Spatial Time Series","Keywords_Processed":"co occurrence pattern;spatial time series;spatiotemporal visualization;visual analytic","Keyword_Vector":[0.0820572303,0.0324991169,-0.0249167528,0.0205775629,0.1124309094,-0.1161456599,-0.11512636,-0.0672870431,-0.1310843345,0.0306438825,0.0345983295,0.1374871466,0.0277797902,0.0623015009,0.0042407379,0.0880144253,0.0118220764,0.046436035,0.1260001311,-0.0328911504,-0.0607674309,0.1448450623,0.0638568036,0.0975768037,-0.1400383276,-0.043730986,-0.0609459973,0.0509067382,0.0929449602,-0.1225151468,-0.0209531206,-0.1795774785,0.0000610879,-0.0038258509,-0.0850865202,-0.1263520068,0.0296096073,-0.0655429942,-0.090318379,-0.1539310355,-0.0002446199,0.050690982,0.0164618399,0.0174294267,-0.0042358671,0.0072493548,0.0651133132,0.0652241436,0.0685574901,0.0218335289,-0.069279757,-0.0354886087],"Abstract_Vector":[0.1221212586,-0.024937783,-0.0163321992,0.0138392809,-0.026279938,0.0150781999,0.0304705876,0.0312887104,-0.0056691354,-0.0239164689,-0.0339852734,0.0122749465,0.0085157217,-0.0040706776,0.0007132106,-0.0077499433,0.0299077488,0.0144319937,0.0069241853,0.0090855688,0.0160786272,0.0029126479,0.0863357877,0.021417553,0.0243207365,0.0140279922,-0.003686827,-0.0209495518,-0.059265047,-0.0227715878,0.0043515276,0.0263606086,-0.047616374,-0.0376582967,0.0134044786,0.0107155302,0.0509122089,-0.013442905,-0.0093113802,0.0453490777,-0.0073255012,0.0322023251,0.0408861227,-0.0241082438,0.0573034548,-0.0672015019,0.0423109631,0.0260800982,-0.0610818034,0.0279266118,0.0272397526,0.0278326461,0.0351764686,0.0195824248,0.0006766963,0.000903492,0.0774952121,0.0054807908,-0.010159114,0.011446726,-0.0445409917,0.037431334,-0.0324111133,0.0209977958,-0.0413132087,-0.0092791275,0.0167984685,0.0464849699,0.024381642,0.0428269612,0.0001807918,-0.0524358207,-0.0029525022,-0.0042399102,0.0307057537,-0.0926254146,0.0704344694,-0.0342722607,0.0021849958,-0.0944235144,-0.0203429893,-0.0245919424,-0.0137280126,0.0159740675,-0.0168537981,-0.0225757567,-0.0113127361,-0.0193193945,-0.0037574182,-0.0480704846,-0.0307760237,0.0393272707,-0.0203103758,0.0231266509,0.0389453573,0.0415268535,-0.0136006952,0.012667272,-0.010459341,-0.0087087031,0.025163613,-0.0097779465,-0.0387994641,-0.0109730044,0.0033531032,0.0236471861,0.0494606745,0.0062234347,-0.0017960477,0.0046335156,0.0138479519,-0.0042768592,0.0056820523,-0.0149896514,0.0248940754,0.0385861035]},"447":{"Abstract":"We propose Average Vector Field (AVF) integration for simulation of deformable solids in physics-based animation. Our method achieves exact energy conservation for the St. Venant-Kirchhoff material without any correction steps or extra parameters. Exact energy conservation implies that our resulting animations 1) cannot explode and 2) do not suffer from numerical damping, which are two common problems with previous numerical integration techniques. Our method produces lively motion even with large time steps as typically used in physics-based animation. Our implicit update rules can be formulated as a minimization problem and solved in a similar way as optimization-based backward Euler, with only a mild computing overhead. Our approach also supports damping and collision response models, making it easy to deploy in practical computer animation pipelines.","Authors":"J. Rojas; T. Liu; L. Kavan","DOI":"10.1109\/TVCG.2018.2851233","Keywords":"Animation;three-dimensional graphics and realism","Title":"Average Vector Field Integration for St. Venant-Kirchhoff Deformable Models","Keywords_Processed":"animation;three dimensional graphic and realism","Keyword_Vector":[0.1119031917,-0.0491331374,0.0233896006,-0.0645229103,0.0102807238,-0.0133793264,-0.0315218143,0.0164072046,0.0318253872,-0.0461062143,0.0089216626,-0.0749267683,-0.0326460953,-0.0057417383,0.0020569343,0.0216068252,-0.0168552734,0.0475725256,-0.0214690302,0.0057285795,-0.0534979093,-0.019960555,-0.016817334,-0.0129292274,0.0166750155,0.0075315657,0.0078241491,0.0098489748,0.0093882394,0.0028268596,0.01680315,0.0263820306,-0.0271914899,-0.0574105441,0.0143571012,-0.0240239534,0.0363633968,0.0164897398,-0.0531121732,0.014753413,-0.0684068404,0.0150154604,0.0671774655,0.0609438981,-0.0197098457,-0.0023206209,0.0717255992,0.0585330217,0.0869514689,-0.0431922306,0.1557731851,0.0554954506],"Abstract_Vector":[0.1179684935,-0.0235034917,0.0127370251,0.0593404699,0.0213669971,0.0168824189,-0.034751694,-0.0072746016,0.0201100138,0.0065660777,-0.0239595604,0.039334109,-0.00447231,-0.0423484648,0.0137145643,0.0676036271,-0.0281491058,-0.0056484985,-0.02682049,-0.0150827109,-0.0120155075,0.0211434598,-0.0029730942,0.0597733506,0.0560819639,0.003248709,0.0228712301,0.0511363056,0.0451184266,-0.0101727428,-0.0011209132,-0.0396230534,0.0388971371,0.0186158291,0.0567355078,-0.0429786823,-0.0133641448,0.0471807877,-0.0350267759,-0.0610794201,0.015752953,0.0051338473,0.0150950714,0.0206556277,-0.0130913035,-0.0196722453,0.0385392274,-0.0036168839,-0.0318118645,0.0033362885,0.0099195977,0.0214117966,0.0031858788,0.0374932554,0.0130154337,-0.0499362356,0.0651015479,0.0237226812,-0.0366146735,-0.0135911823,-0.0067894979,-0.0399919282,0.0273571163,0.0048340817,-0.0012878909,0.0190979523,-0.044412972,0.0113377634,0.0126227865,0.0085065483,0.0030147778,0.035483234,0.0185158723,0.0267616125,0.0308158576,0.0204521511,-0.0552539474,0.0540915251,-0.0136362537,-0.0357384364,-0.0182715851,0.0043922728,-0.024752746,0.0134663127,-0.0098129802,-0.0064521771,0.0407784588,0.0139942639,0.0011648075,0.0021938628,-0.0600108267,0.0088968438,-0.0158644732,0.0248133159,-0.012906808,-0.026654987,0.0045299638,0.0653497177,-0.017119933,-0.0367233278,0.0748682845,0.037854777,-0.0564054221,0.0310368071,-0.0029976303,0.0292664038,0.0272981594,0.0368185437,-0.0491004989,-0.0351621903,0.0016220496,0.0155470752,0.0225895206,-0.0407673313,0.0415486519,-0.0242205298]},"448":{"Abstract":"Tree boosting, which combines weak learners (typically decision trees) to generate a strong learner, is a highly effective and widely used machine learning method. However, the development of a high performance tree boosting model is a time-consuming process that requires numerous trial-and-error experiments. To tackle this issue, we have developed a visual diagnosis tool, BOOSTVis, to help experts quickly analyze and diagnose the training process of tree boosting. In particular, we have designed a temporal confusion matrix visualization, and combined it with a t-SNE projection and a tree visualization. These visualization components work together to provide a comprehensive overview of a tree boosting model, and enable an effective diagnosis of an unsatisfactory training process. Two case studies that were conducted on the Otto Group Product Classification Challenge dataset demonstrate that BOOSTVis can provide informative feedback and guidance to improve understanding and diagnosis of tree boosting algorithms.","Authors":"S. Liu; J. Xiao; J. Liu; X. Wang; J. Wu; J. Zhu","DOI":"10.1109\/TVCG.2017.2744378","Keywords":"tree boosting;model analysis;temporal confusion matrix;tree visualization","Title":"Visual Diagnosis of Tree Boosting Methods","Keywords_Processed":"model analysis;tree boost;temporal confusion matrix;tree visualization","Keyword_Vector":[0.2332482223,-0.2171858854,-0.1519461332,0.24570597,-0.1314917025,0.1748174035,0.0604385588,-0.061828789,-0.0706625124,-0.0510439846,0.0900554184,-0.0681413727,0.1289991672,0.2721951292,-0.139255163,-0.1017365664,0.0851301986,0.0072370456,0.0574277428,-0.1330657339,-0.0619394735,-0.0658882427,-0.0478875911,0.035337141,0.0055419028,0.0450960676,-0.0273480182,0.007887985,0.1002578234,0.0504300456,-0.0678102514,0.0824246903,-0.0800976004,0.1155002546,-0.0482314756,0.0079680003,0.0521236833,-0.0362896913,0.0769956765,-0.0267525284,-0.0059352724,-0.0036060717,-0.05083796,0.0602353432,-0.0436497858,0.0072711259,-0.0085582158,-0.0135109796,0.0189176303,0.0197255636,-0.0541774976,-0.0353993441],"Abstract_Vector":[0.2439401565,-0.1252529164,-0.0090048534,0.0018239623,0.0158306652,-0.0561095071,-0.0736328572,-0.0206697359,-0.0342262736,-0.0178445056,0.0025954228,-0.0500600847,-0.1060445324,-0.0198343596,-0.1042752827,0.0476982316,0.0001028565,-0.0070893827,-0.011226115,0.0188977986,-0.0514112157,-0.0353036627,-0.0218036052,0.0574244906,-0.0390406627,0.0574034964,-0.1000463207,-0.0158247516,0.0726910792,-0.0416990817,0.079788342,0.040820999,0.0350750402,-0.0856627549,0.1001287282,-0.0439615249,-0.0130739229,-0.0554122669,0.0138889893,0.0210900343,-0.0498493196,0.0598618684,0.0425739854,-0.0914707501,-0.0193837786,0.0458532004,-0.0365924135,0.0456949318,0.0339278683,-0.0374788028,-0.0547166069,0.0741852325,0.0461761876,0.0589621054,0.01086266,0.0244495308,0.0426449318,0.0019501373,0.0027267756,-0.0288089626,-0.0033891582,-0.0370189484,-0.0064503782,-0.0393371451,0.0812416487,0.0373467464,-0.0500910284,0.0191148998,-0.0190796914,0.0156515124,0.0178583709,0.0367355747,-0.003681222,-0.0216890475,0.0109254892,0.0553564588,0.0382222148,0.017765181,-0.0365423901,0.0069023901,-0.0573270936,-0.0000574535,0.0103354888,-0.0394712343,0.0380188208,-0.0198938054,0.0391648304,0.0219261571,0.0026652949,0.052905383,0.0021158552,0.0116610164,0.0098323143,0.0292663878,-0.012779111,0.0609045009,0.020585228,0.0191322618,-0.0228676009,-0.0245257544,-0.0035130052,-0.0247360166,0.0330150619,-0.0645878559,0.0557968538,0.0197580591,-0.053881948,-0.024614269,-0.0067175955,-0.0201004834,-0.0182527483,0.0348516032,0.0097673721,-0.0233320213,-0.0156936526,0.0367158016]},"449":{"Abstract":"Visualizing distributions from data samples as well as spatial and temporal trends of multiple variables is fundamental to analyzing the output of today's scientific simulations. However, traditional visualization techniques are often subject to a trade-off between visual clutter and loss of detail, especially in a large-scale setting. In this work, we extend the use of spatially organized histograms into a sophisticated visualization system that can more effectively study trends between multiple variables throughout a spatial domain. Furthermore, we exploit the use of isosurfaces to visualize time-varying trends found within histogram distributions. This technique is adapted into both an on-the-fly scheme as well as an in situ scheme to maintain real-time interactivity at a variety of data scales.","Authors":"T. Neuroth; F. Sauer; W. Wang; S. Ethier; C. Chang; K. Ma","DOI":"10.1109\/TVCG.2016.2642103","Keywords":"Histograms;particle data;large-scale data;in situ processing;time-varying data;isosurfaces;scientific visualization","Title":"Scalable Visualization of Time-varying Multi-parameter Distributions Using Spatially Organized Histograms","Keywords_Processed":"histogram;scientific visualization;isosurface;large scale datum;time vary datum;particle datum;in situ processing","Keyword_Vector":[0.0543086872,0.0585281372,0.0125792095,-0.0024359214,0.0005167223,-0.0488659135,-0.0531268985,-0.0218141036,-0.0516277494,-0.0650893792,0.0646799684,0.0742795815,0.1430482663,0.0228920219,0.1195336981,0.0169635375,0.080140735,0.0951948453,0.0346932416,0.0345312352,0.0089429828,-0.0862730578,0.1407921989,-0.0005974341,-0.0547062171,-0.0178487678,0.0539553873,0.1173948027,0.0991741285,-0.0597738421,0.0306828067,-0.1413694373,0.0141073741,0.0088535784,-0.0083935221,0.0339137783,0.011236341,0.0069751159,0.0333258633,0.0488558762,-0.0518154087,0.021715458,0.0235367865,-0.0333435513,-0.0138921588,0.0010365862,0.02524118,0.0604257319,0.008429822,-0.017469242,-0.0169658471,-0.0295644373],"Abstract_Vector":[0.1214274231,0.0641625507,0.0253489839,0.0037924076,-0.0491038644,0.038534043,0.0023388768,-0.0258671347,0.0858942212,0.0263171532,-0.0174274686,-0.0121650458,-0.0416508579,0.0296632961,-0.0065332564,-0.0633091851,0.0106705933,-0.0146168085,-0.0231858203,0.0180042026,-0.0141055091,0.0255715481,0.0381592997,0.006712944,0.0051923529,-0.0258845256,-0.0234134545,0.0109581083,0.0032081706,-0.0196235897,-0.0141175522,0.0393149382,-0.0144394356,0.0204142707,-0.0001417171,-0.0309951624,-0.0111985604,-0.0249848663,0.0058289058,0.0030328438,0.021573989,-0.0199997515,0.0121942087,-0.0019429744,-0.0243381092,-0.0265382335,0.035974115,0.0029708928,0.0083164508,0.040676655,0.0015972181,-0.0048737522,0.0240622977,-0.0066043535,-0.0073407277,0.0276786741,0.0102782042,0.0139515326,-0.0004012492,-0.018717463,0.0207600653,-0.0108067922,0.0098823859,-0.0232441035,-0.0004278505,0.015235764,-0.0015825702,-0.0102476255,-0.0057182441,0.0311719543,0.025446225,0.0102709618,0.0311909637,0.0402051817,0.0124198374,0.0116176205,0.0047662906,-0.013413526,0.013214868,-0.0049017283,-0.0281744576,-0.0150410738,-0.0085209696,-0.0118479059,-0.0568385294,0.0075643783,-0.0132027088,-0.0046558478,0.0265639393,-0.0208837682,0.0156960984,0.0179831588,0.0093902598,-0.0231202217,0.0049004518,0.0243406639,0.0067311636,-0.0034146495,-0.0165006157,-0.0147214048,-0.0002250801,0.0065558294,0.0138973358,-0.0047042594,-0.0128427189,-0.0266201919,-0.0012413705,0.0191651455,-0.0155090944,-0.0215506462,-0.0138471282,0.0070749992,-0.0159172035,-0.0209955576,0.0195142527,-0.0164591283]},"45":{"Abstract":"We present an approach to represent DNA nanostructures in varying forms of semantic abstraction, describe ways to smoothly transition between them, and thus create a continuous multiscale visualization and interaction space for applications in DNA nanotechnology. This new way of observing, interacting with, and creating DNA nanostructures enables domain experts to approach their work in any of the semantic abstraction levels, supporting both low-level manipulations and high-level visualization and modifications. Our approach allows them to deal with the increasingly complex DNA objects that they are designing, to improve their features, and to add novel functions in a way that no existing single-scale approach offers today. For this purpose we collaborated with DNA nanotechnology experts to design a set of ten semantic scales. These scales take the DNA's chemical and structural behavior into account and depict it from atoms to the targeted architecture with increasing levels of abstraction. To create coherence between the discrete scales, we seamlessly transition between them in a well-defined manner. We use special encodings to allow experts to estimate the nanoscale object's stability. We also add scale-adaptive interactions that facilitate the intuitive modification of complex structures at multiple scales. We demonstrate the applicability of our approach on an experimental use case. Moreover, feedback from our collaborating domain experts confirmed an increased time efficiency and certainty for analysis and modification tasks on complex DNA structures. Our method thus offers exciting new opportunities with promising applications in medicine and biotechnology.","Authors":"H. Miao; E. De Llano; J. Sorger; Y. Ahmadi; T. Kekic; T. Isenberg; M. E. Gr\u00f6ller; I. Bari\u0161i\u0107; I. Viola","DOI":"10.1109\/TVCG.2017.2743981","Keywords":"Nano;nanotechnology;assembly;multiscale;abstraction;DNA;origami;scale-adaptive modification","Title":"Multiscale Visualization and Scale-Adaptive Modification of DNA Nanostructures","Keywords_Processed":"multiscale;dna;scale adaptive modification;nanotechnology;origami;assembly;nano;abstraction","Keyword_Vector":[0.0356503047,0.0110663886,-0.0150044664,-0.0116043288,0.0017546114,-0.0436189255,-0.0428672989,-0.000920271,-0.0848380051,-0.0236039144,0.0451792292,0.0746134309,0.0017524449,0.0158686168,0.0592112883,0.0665015874,0.0536641134,0.0676818447,0.1843271254,-0.002416177,-0.0841829376,0.085055504,0.0975765278,0.0645176007,-0.093371447,-0.0335145379,-0.0240085029,-0.0108422023,0.1042488963,-0.1300057763,-0.0284880703,-0.1191984598,0.0578701071,0.0319381048,-0.0442313493,-0.001906417,-0.0222611426,0.0109992563,-0.0478998185,-0.0548877525,-0.0351103512,0.0597662584,0.0262526447,0.0392038358,-0.0153116863,-0.0295249213,0.0397572035,0.0556898978,-0.00647037,0.0527181119,-0.0168483493,0.0094574683],"Abstract_Vector":[0.1027860976,-0.0415351003,-0.0063877665,-0.0118515108,-0.0635009507,0.0534896814,0.0334325935,0.0314543923,0.0178969945,0.0207637515,-0.042111886,0.0210868493,0.0026675928,-0.0078868209,0.0095104007,0.0178013373,0.0199663417,0.0282495828,0.0436431906,-0.0142686238,-0.0165152882,-0.0543793992,0.0840727537,0.0208944045,-0.0011524556,-0.0041221601,-0.0126487186,0.0211224946,0.0278456101,-0.0035890519,-0.0030549691,0.0258591117,0.0075237936,-0.0146565984,0.0168750305,-0.0289550062,-0.0523811795,-0.0194229026,-0.0397729684,-0.0448112487,0.0166068588,0.0005215798,0.0017277196,-0.0011630715,-0.0162107179,0.012003945,-0.0169531546,-0.0033521611,0.0157168492,-0.0137710738,0.0404752869,0.0130170543,0.033156881,-0.0297020917,0.0115981373,-0.0101189076,0.0074315207,0.0174519116,-0.0269868552,-0.0398035669,0.008025092,-0.0159442074,-0.008854265,0.0267700262,0.0114872332,-0.0280260147,-0.0163745745,-0.0360188721,-0.0234619097,0.0445054321,-0.0151000392,0.0159232881,0.0192801215,0.045585578,0.0047645075,-0.075175872,0.0443695369,0.0359484981,0.0217587883,-0.0549845493,-0.0096431297,-0.0261714441,-0.0073922821,0.0529407841,-0.0015726123,0.0083132816,-0.0379263977,-0.0067437578,-0.0286758745,-0.0272477885,0.0075824025,0.0082179649,0.0042918305,0.0165049011,0.0135805512,-0.0255833861,0.0062544808,-0.0323194662,0.0072262025,-0.0081010421,0.0279161257,0.0082449599,-0.0031115932,0.0452116394,0.0148606374,0.0156312779,0.002055062,0.0259046452,0.0056704851,-0.0269245049,0.0023420944,-0.0166596086,0.0319431729,-0.0246012255,0.0027374423,0.0343976266]},"450":{"Abstract":"Analyzing high-dimensional data and finding hidden patterns is a difficult problem and has attracted numerous research efforts. Automated methods can be useful to some extent but bringing the data analyst into the loop via interactive visual tools can help the discovery process tremendously. An inherent problem in this effort is that humans lack the mental capacity to truly understand spaces exceeding three spatial dimensions. To keep within this limitation, we describe a framework that decomposes a high-dimensional data space into a continuum of generalized 3D subspaces. Analysts can then explore these 3D subspaces individually via the familiar trackball interface while using additional facilities to smoothly transition to adjacent subspaces for expanded space comprehension. Since the number of such subspaces suffers from combinatorial explosion, we provide a set of data-driven subspace selection and navigation tools which can guide users to interesting subspaces and views. A subspace trail map allows users to manage the explored subspaces, keep their bearings, and return to interesting subspaces and views. Both trackball and trail map are each embedded into a word cloud of attribute labels which aid in navigation. We demonstrate our system via several use cases in a diverse set of application areas-cluster analysis and refinement, information discovery, and supervised training of classifiers. We also report on a user study that evaluates the usability of the various interactions our system provides.","Authors":"B. Wang; K. Mueller","DOI":"10.1109\/TVCG.2017.2672987","Keywords":"High-dimensional data;subspace navigation;trackball;PCA;ant colony optimization1","Title":"The Subspace Voyager: Exploring High-Dimensional Data along a Continuum of Salient 3D Subspaces","Keywords_Processed":"pca;ant colony optimization1;trackball;subspace navigation;high dimensional datum","Keyword_Vector":[0.1632851761,-0.0742973024,-0.02113945,0.082767756,0.01546651,0.1773020256,-0.0671991841,-0.0016568936,0.0578761911,0.0015623408,0.1116714915,0.0009395597,0.0012715153,0.0774818603,0.0374537729,0.0275445831,-0.0413445556,-0.1082112321,0.0393832167,-0.0197517758,0.1396946979,0.084944148,-0.1345160684,-0.1234280151,-0.0536536045,0.0131809003,0.0908636441,-0.011778309,0.1398789321,-0.132335055,0.0607581428,0.0252497101,0.0341434234,-0.1410219037,0.0372765245,-0.0556848748,-0.0563771562,0.086232042,0.0212537715,0.0873311008,-0.0263463538,-0.1034946331,-0.0526077068,0.0448278818,0.0374634661,0.095214707,-0.0224759392,-0.031852972,0.0645927167,-0.0691063278,0.132443051,0.0868340591],"Abstract_Vector":[0.1794040262,-0.0992183645,0.0257565464,-0.0060993632,-0.0547318089,0.0235897343,0.0201650892,0.0051675707,-0.0332125439,0.0218049792,-0.0360944001,0.0249929976,0.0021065155,-0.0085094443,-0.0475577199,0.0066638606,-0.0411749128,-0.0544369112,-0.0249642661,0.0253768563,0.0227261074,0.0798900081,0.0048840081,-0.0328420879,0.0452560652,0.1270741383,0.0481357163,0.072691079,0.0143317041,0.0346947855,-0.0362852967,-0.0777968067,0.0440313627,-0.0084676705,0.0160711526,-0.0134967209,0.0659994331,0.0403426291,0.0223244637,-0.0382337725,-0.0177811565,0.012621476,0.0220531035,-0.0133747089,-0.0048303961,-0.0090105781,0.0588133342,-0.0030611095,-0.0086048,0.0180286762,0.0534871193,0.0307809882,-0.0237152119,0.0402848073,0.022313512,-0.0124507371,-0.0052695516,0.0131502681,0.0067285614,-0.0033289957,0.006194511,-0.0357694357,-0.0440741725,0.0143112225,-0.0119067763,-0.0435318241,-0.0207488871,-0.0060991255,-0.0532425351,-0.0306473673,0.0051165359,0.0364520111,0.0250169947,0.0381389736,0.010022234,0.0165962101,-0.0085669434,0.000832832,-0.0353085032,0.0012157416,-0.0377510425,0.0070312517,-0.0054554391,0.0027896849,-0.0221313819,-0.009911533,-0.004200339,-0.0411810903,0.0011366497,-0.0031251519,0.0348041739,0.001623749,0.0172269921,0.0003750756,0.0351570068,-0.0110522983,-0.0235137759,-0.029025387,-0.0138484347,0.0256925301,-0.019632226,-0.0108603036,-0.0328683946,0.0035089536,-0.0315299192,0.0102347694,0.0091217401,0.0019587082,0.0522141394,0.0084669289,0.0154079705,0.0142111992,0.0247563294,0.039679976,0.008031185,-0.0286613415]},"451":{"Abstract":"Scatterplot matrices (SPLOMs) are widely used for exploring multidimensional data. Scatterplot diagnostics (scagnostics) approaches measure characteristics of scatterplots to automatically find potentially interesting plots, thereby making SPLOMs more scalable with the dimension count. While statistical measures such as regression lines can capture orientation, and graph-theoretic scagnostics measures can capture shape, there is no scatterplot characterization measure that uses both descriptors. Based on well-known results in shape analysis, we propose a scagnostics approach that captures both scatterplot shape and orientation using skeletons (or medial axes). Our representation can handle complex spatial distributions, helps discovery of principal trends in a multiscale way, scales visually well with the number of samples, is robust to noise, and is automatic and fast to compute. We define skeleton-based similarity metrics for the visual exploration and analysis of SPLOMs. We perform a user study to measure the human perception of scatterplot similarity and compare the outcome to our results as well as to graph-based scagnostics and other visual quality metrics. Our skeleton-based metrics outperform previously defined measures both in terms of closeness to perceptually-based similarity and computation time efficiency.","Authors":"J. Matute; A. C. Telea; L. Linsen","DOI":"10.1109\/TVCG.2017.2744339","Keywords":"Multidimensional Data (primary keyword);High-Dimensional Data","Title":"Skeleton-Based Scagnostics","Keywords_Processed":"Multidimensional Data primary keyword;high Dimensional Data","Keyword_Vector":[0.0300432059,0.0259915592,-0.0134977051,0.006446154,0.0051869732,0.000059149,0.0213462587,0.0053241428,-0.0147844474,0.0115071372,-0.0408781345,-0.0029641407,-0.0538476324,0.0149984264,-0.0425802532,-0.0201840681,0.0447622063,-0.025547674,-0.0121920046,-0.0016753408,0.0012872123,0.0183671475,-0.0011257661,0.0463574834,0.0678124816,0.0608100577,0.0100980724,0.035935287,0.0495560154,-0.0136701886,0.0100858546,-0.0396752254,-0.001768603,0.0197618092,-0.0151263479,0.0346868768,-0.0174200926,0.0176336918,-0.0689127182,-0.0093882854,0.0001638075,-0.0092972359,-0.0042481781,-0.0135117075,0.0493050426,-0.0546853396,-0.0150393428,0.0479587005,-0.0080210908,-0.0344888692,-0.0308939647,-0.0084546446],"Abstract_Vector":[0.1531544239,0.0671251519,-0.0002534947,0.0127836199,0.0174127127,0.0254852993,0.0149584235,0.0165790947,0.035298803,0.000404486,-0.033718722,0.0062838898,-0.0513025253,-0.0564540724,0.0079609404,-0.0044981295,-0.0124147531,0.0175241895,0.001105187,-0.069114282,-0.005837533,-0.0516199165,0.0305038771,0.0161270823,0.0366006272,-0.0081096596,-0.0131350525,0.025233154,-0.0034618194,0.0100233529,0.0108180108,0.024553501,-0.0123948007,0.0073337254,0.0226482288,0.0031699256,0.0167875431,0.0322888165,0.0007548727,-0.0040394309,0.0529946564,0.0057457505,0.0199401235,0.0415007409,0.0401595646,0.0036992906,0.0011063938,0.0049926493,0.0090836488,-0.0038589318,-0.0314391613,0.0405069295,-0.0167321906,0.0095450804,-0.0390990612,0.0151416873,0.0031788869,-0.0104865134,-0.035078865,-0.0169201535,-0.0082268555,-0.0356427989,-0.00784282,-0.0143181948,-0.0198773857,0.0455101105,-0.0197161852,-0.0128963488,-0.019456565,-0.0264126998,0.0051042078,0.0285633955,0.0002533728,0.0214074501,0.0020742279,-0.0274627814,0.0443852693,0.0170432726,-0.054682755,0.008034761,-0.0031520556,-0.0373220085,0.0000109532,-0.0403848111,-0.0282848903,-0.0367958192,-0.0268217321,-0.034399006,-0.021214693,0.0529530606,-0.018893183,-0.0120175873,-0.0271329328,-0.0443807125,-0.0127822108,-0.0209994083,0.030802953,-0.0166930857,0.0009731073,0.0000703384,-0.0027128999,-0.0169440154,0.0153436574,0.0046667864,-0.0427772172,0.0096123833,0.0057896002,0.0260524963,-0.001084665,-0.0051906429,-0.0035455236,0.0108161843,-0.0514788794,0.0000070301,0.0397885309,0.0276958218]},"452":{"Abstract":"A myriad of design rules for what constitutes a \u201cgood\u201d colormap can be found in the literature. Some common rules include order, uniformity, and high discriminative power. However, the meaning of many of these terms is often ambiguous or open to interpretation. At times, different authors may use the same term to describe different concepts or the same rule is described by varying nomenclature. These ambiguities stand in the way of collaborative work, the design of experiments to assess the characteristics of colormaps, and automated colormap generation. In this paper, we review current and historical guidelines for colormap design. We propose a specified taxonomy and provide unambiguous mathematical definitions for the most common design rules.","Authors":"R. Bujack; T. L. Turton; F. Samsel; C. Ware; D. H. Rogers; J. Ahrens","DOI":"10.1109\/TVCG.2017.2743978","Keywords":"colormap;survey;taxonomy;order;uniformity;discriminative power;smoothness;monotonicity;linearity;speed","Title":"The Good, the Bad, and the Ugly: A Theoretical Framework for the Assessment of Continuous Colormaps","Keywords_Processed":"uniformity;linearity;monotonicity;survey;speed;discriminative power;order;taxonomy;colormap;smoothness","Keyword_Vector":[0.0959030007,0.0870553694,0.0506839325,0.0271668543,-0.0094206094,-0.0865514136,-0.0541217824,-0.0438319628,-0.0144867795,-0.1020722694,0.0012656603,0.0506607469,0.244154734,0.040812123,0.1727262649,-0.0330116889,0.0814853647,0.0557734217,-0.080628935,0.0799756338,0.0859083022,-0.1989892484,0.1271761942,-0.0670983305,-0.0055750774,0.0140511816,0.1345796604,0.2593247214,0.1181764965,-0.0055272897,0.0863318439,-0.170903818,-0.0426901295,0.02132385,0.019587741,0.0833467797,0.1673988562,0.0694282324,0.0558965856,0.1292999621,-0.0225242948,0.0073434155,-0.1825533152,-0.1062622113,-0.0967699885,-0.0168597637,0.0800380132,0.105375139,-0.0452010449,-0.0271768858,-0.0047159461,0.0316076789],"Abstract_Vector":[0.1599691298,0.0627596686,0.0049274692,0.0012704101,-0.0693646527,0.0913223222,-0.040351725,-0.0270908141,0.0518243352,-0.0410471215,-0.0080156933,-0.0074404374,0.008630955,0.0885272675,0.0527937957,0.0117737396,0.0278899099,0.015054189,-0.0615947962,-0.0451553532,0.1194959008,0.0005696294,0.0037528188,0.0199753079,-0.037742729,0.0074997583,-0.0000499122,0.026134233,0.0528234266,0.0370213577,0.0428052776,0.0594610298,0.0376190289,0.0406845836,-0.0204474698,0.0182402492,-0.045825161,-0.0106490393,-0.0542330012,-0.067631001,-0.0568601434,-0.086788767,0.0017889483,0.0712953507,0.0714348394,0.0716105983,0.0192258141,0.0857888883,-0.0309933582,-0.0509039132,0.0606937823,0.0049538189,-0.0107555428,0.008733261,0.0210586326,0.0222423472,0.0234473451,-0.003966962,-0.0397143408,-0.0337121017,-0.0148025238,-0.0036049129,-0.0342649291,-0.0125038157,0.0218517804,0.0029663582,0.005017491,-0.0068807086,0.0002355783,-0.0355369101,-0.050544718,-0.0395999885,-0.0397389753,0.0007473899,0.0386270567,-0.0021234189,-0.0018944198,0.0078949061,0.0016691741,-0.0198496182,0.0038090006,-0.0156130089,0.013133369,-0.0351146311,0.0034795206,-0.0092862004,-0.015947758,0.0065267087,0.0030812203,-0.0040686202,0.01431642,-0.0208167989,0.0000387258,0.0035369565,-0.0188235296,0.0091279885,0.0160586512,0.0040929708,-0.0014706257,0.0531973386,0.027646404,-0.0064323342,-0.0006241276,0.0202139804,0.0164355979,-0.0175096149,0.0095057451,-0.0086431205,0.0108692443,-0.0149920354,0.0155984442,-0.0032478373,0.0063654657,0.0022683473,0.0359060053,0.0364816342]},"453":{"Abstract":"The results of anomaly detection are sensitive to the choice of detection algorithms as they are specialized for different properties of data, especially for multidimensional data. Thus, it is vital to select the algorithm appropriately. To systematically select the algorithms, ensemble analysis techniques have been developed to support the assembly and comparison of heterogeneous algorithms. However, challenges remain due to the absence of the ground truth, interpretation, or evaluation of these anomaly detectors. In this paper, we present a visual analytics system named EnsembleLens that evaluates anomaly detection algorithms based on the ensemble analysis process. The system visualizes the ensemble processes and results by a set of novel visual designs and multiple coordinated contextual views to meet the requirements of correlation analysis, assessment and reasoning of anomaly detection algorithms. We also introduce an interactive analysis workflow that dynamically produces contextualized and interpretable data summaries that allow further refinements of exploration results based on user feedback. We demonstrate the effectiveness of EnsembleLens through a quantitative evaluation, three case studies with real-world data and interviews with two domain experts.","Authors":"K. Xu; M. Xia; X. Mu; Y. Wang; N. Cao","DOI":"10.1109\/TVCG.2018.2864825","Keywords":"Algorithm Evaluation;Ensemble Analysis;Anomaly Detection;Visual Analysis;Multidimensional Data","Title":"EnsembleLens: Ensemble-based Visual Exploration of Anomaly Detection Algorithms with Multidimensional Data","Keywords_Processed":"Algorithm Evaluation;ensemble Analysis;Multidimensional Data;Visual Analysis;Anomaly Detection","Keyword_Vector":[0.0921215654,-0.0508703982,-0.0113485582,-0.0135754933,-0.0125037447,0.0251377372,-0.0452581396,-0.0227575645,-0.0111865627,0.0208832314,-0.0010556459,0.0655248973,0.0229373616,-0.0366104499,-0.0234409985,0.0138481785,0.017308163,-0.0290890902,-0.0061316509,0.086287627,-0.0416302111,-0.0485029864,-0.0331600621,0.006557838,0.0533689903,0.0189896117,0.0155552228,-0.0185980024,0.0033742876,0.0332665229,-0.0268774109,0.0066670543,-0.0017562991,-0.0153416186,0.0018494972,0.0089389565,0.0370397432,-0.0039695491,0.0081500721,0.0150365522,-0.0625147516,0.0577265084,0.0139819611,0.0301639604,0.0224403016,0.027350515,-0.0398421647,0.0295880953,-0.0337787291,0.0377821843,-0.0149003442,0.0339798688],"Abstract_Vector":[0.1975827896,-0.1068319548,0.0056338999,-0.0240374167,0.0094847412,-0.1235702978,-0.032544132,-0.0290416778,-0.0036042549,0.0172622936,0.0395660871,-0.0611441042,-0.0699155525,0.0769458169,-0.0576583736,-0.0546903609,0.0401224331,-0.0487412813,-0.0811499715,0.0313521016,-0.0252296244,-0.0177178136,0.0859364669,0.0128151974,-0.0270866656,-0.0352477207,0.0374124945,0.0303933363,-0.0299413137,-0.0073796948,0.060100737,-0.0087231663,-0.0274246244,-0.0275514025,-0.0451762309,0.0460483176,-0.0653908457,0.0458667621,-0.0078751376,-0.0093787778,0.0216883998,0.004894787,0.0559605194,-0.0279606818,0.1018960133,0.022400803,0.0158537979,-0.0719086003,0.0452310853,-0.069797703,-0.044422764,-0.0132660942,0.0233197628,0.1145316408,-0.0403802563,-0.053489951,0.0413477491,0.0437366317,0.0441011614,0.0913265529,-0.0149281391,0.0117227902,0.1102345149,-0.0741659018,-0.0251794496,0.0258316352,-0.0012695312,0.0728375658,0.0110731405,-0.092109897,-0.0033598193,0.0254444967,0.1082067682,0.0431548112,0.1054938997,-0.048754924,0.0150997266,0.0389389309,0.0301609135,-0.0383986437,-0.0312773739,0.011663809,-0.0069554163,-0.0881508106,0.1271534863,0.0132858661,0.0084938547,-0.0292891383,-0.02913228,0.0473973829,-0.0115294989,-0.0118601216,-0.0189123838,0.0133055961,-0.0352987845,0.0414069726,0.0429395277,0.0269015312,0.003200275,0.0066164035,0.0056057848,0.0121372677,-0.0347817785,0.0318415359,0.0309047352,-0.0080640757,-0.0098851783,-0.0068007699,0.0496291573,-0.0734096544,0.0035178585,0.001460465,0.03211845,-0.0393409398,0.0139208483,-0.004436024]},"454":{"Abstract":"Interactive visual data analysis is most productive when users can focus on answering the questions they have about their data, rather than focusing on how to operate the interface to the analysis tool. One viable approach to engaging users in interactive conversations with their data is a natural language interface to visualizations. These interfaces have the potential to be both more expressive and more accessible than other interaction paradigms. We explore how principles from language pragmatics can be applied to the flow of visual analytical conversations, using natural language as an input modality. We evaluate the effectiveness of pragmatics support in our system Evizeon, and present design considerations for conversation interfaces to visual analytics tools.","Authors":"E. Hoque; V. Setlur; M. Tory; I. Dykeman","DOI":"10.1109\/TVCG.2017.2744684","Keywords":"natural language;interaction;language pragmatics;visual analytics;ambiguity;feedback","Title":"Applying Pragmatics Principles for Interaction with Visual Analytics","Keywords_Processed":"feedback;interaction;natural language;language pragmatic;visual analytic;ambiguity","Keyword_Vector":[0.147837888,0.0223269931,0.1353970656,-0.0683247596,-0.0804876016,-0.0329086032,-0.0484387357,0.0236610089,0.0477750159,-0.1661496492,-0.0042097075,0.0360297419,0.0123965741,-0.0296282646,-0.0702267917,0.0100877753,0.0114298763,-0.0359432859,0.0052408507,0.0065026254,-0.0895588557,-0.0128521708,0.0131634049,-0.0226993566,-0.0558557187,-0.0095933822,0.0121559756,-0.0697093133,0.0268544245,-0.0918795063,-0.0160908183,0.0040420064,0.0324395126,-0.0120790607,0.0329054029,0.0078127046,0.0191780923,-0.0379127819,0.0310783942,-0.0531180697,0.076885075,-0.0715570939,0.0412384795,0.0136115332,0.0312297746,0.0087480298,0.0038902426,-0.0954188501,0.0032918957,-0.1040434966,-0.0784296593,0.0138995543],"Abstract_Vector":[0.1706299648,-0.0564010037,0.045728963,-0.0503798476,-0.0031722149,-0.0167582187,0.001472888,-0.006737985,0.00562992,0.0126100469,-0.0034352228,-0.0082048165,-0.0019405209,0.0286463403,-0.0078859591,0.0050877615,0.0006760554,-0.0043429915,0.023537787,-0.0145795016,0.0003606777,-0.0203459818,-0.0225490125,0.0071852088,-0.0226562967,0.0107103069,-0.0025549585,0.0275385298,-0.0012378534,-0.0314765892,0.0161292764,-0.0076241131,-0.0606939172,-0.0475508185,-0.0705348179,-0.0382374985,-0.0037002362,-0.0125988042,-0.0017144241,-0.0243331237,-0.0130629177,-0.0230903215,-0.0245167458,-0.0213866638,0.0247444258,-0.0062886484,-0.0010832875,-0.0403734002,0.0220172242,0.0160490302,0.018315371,0.0666469962,0.0126698195,0.0012579608,0.0615171241,-0.015348628,0.0210644033,0.0089188798,-0.0378377376,0.0115951457,0.0091561992,-0.0092427684,-0.0561493226,-0.0468101402,0.0071809621,-0.0131916121,0.0231984439,0.0330480645,0.0051380508,0.0358407893,0.068769243,-0.0202256002,0.0201019991,0.0578330304,0.0344278671,0.040685238,0.0032500997,0.0207182326,-0.0059015233,-0.0062843714,0.008965187,-0.0345976926,0.0055909234,-0.0237053041,0.0000610308,-0.0130430433,-0.0098563709,0.0072506506,-0.0142562465,0.0258848322,0.0092479396,0.0190558441,0.0100516344,0.0168038942,0.0023216934,0.0140423061,0.0127994838,-0.0038207621,-0.0129208717,0.0028512913,-0.0313410794,0.0180458668,0.0057424705,0.0199001847,0.008529705,0.0255353144,-0.0104051264,0.0171979704,0.0069945037,0.0121740795,0.0155146912,-0.0168936617,0.0168970638,-0.0010467875,-0.000456921,0.019168165]},"455":{"Abstract":"We propose the first real-time system for the egocentric estimation of 3D human body pose in a wide range of unconstrained everyday activities. This setting has a unique set of challenges, such as mobility of the hardware setup, and robustness to long capture sessions with fast recovery from tracking failures. We tackle these challenges based on a novel lightweight setup that converts a standard baseball cap to a device for high-quality pose estimation based on a single cap-mounted fisheye camera. From the captured egocentric live stream, our CNN based 3D pose estimation approach runs at 60 Hz on a consumer-level GPU. In addition to the lightweight hardware setup, our other main contributions are: 1) a large ground truth training corpus of top-down fisheye images and 2) a disentangled 3D pose estimation approach that takes the unique properties of the egocentric viewpoint into account. As shown by our evaluation, we achieve lower 3D joint error as well as better 2D overlay than the existing baselines.","Authors":"W. Xu; A. Chatterjee; M. Zollh\u00f6fer; H. Rhodin; P. Fua; H. Seidel; C. Theobalt","DOI":"10.1109\/TVCG.2019.2898650","Keywords":"Egocentric;Monocular;Mobile motion capture","Title":"Mo2Cap2: Real-time Mobile 3D Motion Capture with a Cap-mounted Fisheye Camera","Keywords_Processed":"Egocentric;mobile motion capture;monocular","Keyword_Vector":[0.0148768502,-0.0076892963,0.0052341125,-0.0173004735,0.0196771472,-0.0057532694,0.0016145942,0.0046761987,-0.003443438,0.0056910454,0.0089643368,-0.021883835,-0.00854888,0.0012080011,0.0143391909,-0.0125145154,0.0169841068,-0.0103662501,-0.0119731037,-0.004237099,-0.0085547845,-0.0177373636,-0.0081657057,-0.016861842,0.0261009563,-0.007105357,0.0025049061,-0.0160352195,-0.0281086706,-0.0257823293,0.0082750778,-0.0270005047,-0.0069460794,0.0864515048,0.0645044618,-0.0221359336,-0.009983848,-0.0280356792,-0.0563495325,0.0322044176,0.0280759124,0.014012898,-0.0386233549,0.046458108,0.0255749667,0.0238213325,-0.0086034981,-0.007944696,0.009508262,-0.002322666,-0.0155857383,-0.0132541688],"Abstract_Vector":[0.205609357,-0.0059417985,-0.0409123787,0.1228123946,0.0814245297,-0.0513703806,-0.0553260785,-0.0111152656,0.1027905918,0.002245315,-0.0151120194,0.1345726997,-0.0320316939,-0.1333848967,0.0992782149,0.1833829742,-0.0821291979,-0.0158535185,-0.1785516382,-0.0836767379,-0.0972280407,0.107816811,-0.0323445562,0.0729556262,-0.0320137752,-0.0178982532,0.0296246311,-0.0471015701,0.0285926639,-0.1279471668,0.0574996054,0.0109503483,0.016490285,-0.0289058681,0.0227127724,0.0984601474,0.0029445378,0.0187702857,0.0743862863,-0.0447193129,0.0504799913,0.0154279756,-0.0320494305,0.041503952,0.0918996393,-0.0906878417,-0.0495513514,-0.0505721448,0.0823397926,-0.027306248,0.1248691976,-0.0185517592,0.0218599663,-0.0491282751,0.0403371128,0.0156519269,-0.0219674456,0.041559526,0.0483227287,0.0189746993,0.0079916591,0.0819340967,-0.0190436793,0.0038070553,-0.0108379627,0.0170726631,0.0154838699,0.0072251441,0.0232158812,0.0755498492,0.0314856329,0.0199689655,-0.0450194779,-0.0399188416,-0.0566243797,0.0017811794,0.0066806173,-0.0105346103,-0.0751039211,0.0934849856,-0.038735839,0.0250304804,0.0108699081,-0.0704963747,-0.0110125251,0.0200255463,-0.0359275917,-0.0085352428,-0.0163362827,-0.0092786638,0.03544026,-0.0348031406,0.0260870454,-0.0214738106,-0.0580977769,0.0439716148,0.0360987837,-0.0167608699,-0.0197194915,-0.0117215293,0.0406584636,0.0121830915,-0.0025532314,-0.0193554926,-0.0002165974,0.0400746609,0.0306092666,0.011460956,0.0168264532,-0.0386522402,-0.026108541,0.0006775555,0.0051465298,-0.0047903803,-0.0003664543,0.016749237]},"456":{"Abstract":"Geographic visualization research has focused on a variety of techniques to represent and explore spatiotemporal data. The goal of those techniques is to enable users to explore events and interactions over space and time in order to facilitate the discovery of patterns, anomalies and relationships within the data. However, it is difficult to extract and visualize data flow patterns over time for non-directional statistical data without trajectory information. In this work, we develop a novel flow analysis technique to extract, represent, and analyze flow maps of non-directional spatiotemporal data unaccompanied by trajectory information. We estimate a continuous distribution of these events over space and time, and extract flow fields for spatial and temporal changes utilizing a gravity model. Then, we visualize the spatiotemporal patterns in the data by employing flow visualization techniques. The user is presented with temporal trends of geo-referenced discrete events on a map. As such, overall spatiotemporal data flow patterns help users analyze geo-referenced temporal events, such as disease outbreaks, crime patterns, etc. To validate our model, we discard the trajectory information in an origin-destination dataset and apply our technique to the data and compare the derived trajectories and the original. Finally, we present spatiotemporal trend analysis for statistical datasets including twitter data, maritime search and rescue events, and syndromic surveillance.","Authors":"S. Kim; S. Jeong; I. Woo; Y. Jang; R. Maciejewski; D. S. Ebert","DOI":"10.1109\/TVCG.2017.2666146","Keywords":"Spatiotemporal data visualization;kernel density estimation;flow map;gravity model","Title":"Data Flow Analysis and Visualization for Spatiotemporal Statistical Data without Trajectory Information","Keywords_Processed":"spatiotemporal datum visualization;flow map;kernel density estimation;gravity model","Keyword_Vector":[0.1183283946,-0.0175892463,0.0177510631,-0.0794316055,0.0139925503,-0.0202208134,-0.0825632647,-0.0031896045,-0.0882865034,-0.0401444673,0.042759,0.0541401639,0.0009502446,-0.0029942693,0.0567012427,0.0915037326,0.0521297574,0.1111959969,0.1944397633,-0.0096784156,-0.1146998543,0.0876311064,0.1100696877,0.0765138101,-0.0712845076,-0.0184272812,-0.0106340319,-0.0242504086,0.0981744283,-0.1146382362,-0.0060858803,-0.0863027276,0.0465729213,0.0159880663,-0.0205955836,-0.025821164,-0.0350447293,0.0229579502,-0.0458598596,-0.0774688295,-0.0305964592,0.021690738,0.0588439408,-0.0049079494,-0.0043500412,0.015335587,0.0384543678,0.069759939,-0.0203428497,0.027659195,-0.0157774052,0.0082387848],"Abstract_Vector":[0.19599056,-0.0964987053,-0.0113126852,-0.0274214734,-0.0629702656,0.0044451946,0.0292120033,0.0363809305,-0.0022276279,-0.0287409625,0.0063138845,-0.014542479,0.1416575387,-0.0071048833,0.008961215,-0.0072447796,-0.008375311,0.0208735147,0.0043086692,-0.0146062542,-0.0162001771,0.0333698623,-0.0139456859,-0.0080563187,-0.0360544086,-0.0087813486,0.0072313398,-0.0061719092,-0.000918046,0.0323744189,-0.0214646806,-0.0074678909,0.013441523,-0.0247754871,0.0242749581,0.0268841895,-0.006200199,0.0143677472,0.0325987664,-0.0029221936,0.0248458694,-0.0172023944,0.0327192623,-0.0589603951,0.04960728,0.01396493,0.0074517732,-0.0156371365,0.0430345414,-0.0295865775,0.0138662442,0.0076868185,-0.0270758527,-0.0273351353,0.0023094163,-0.0359887804,0.0161806055,0.0294245462,-0.0292893907,0.0096939702,-0.0361602054,0.0212628535,0.0216864524,0.0022870732,0.0077483351,-0.0134829755,0.0256056975,0.0065466908,-0.0371802632,-0.001761361,-0.0027700382,-0.0003133321,-0.0135264531,0.0092314277,0.0084794863,-0.0158006371,0.0282954909,0.007989754,0.0131828781,-0.0237437944,-0.0172960246,-0.038278689,0.0005781088,0.0029111774,0.0214709364,-0.007710522,0.0211358676,-0.0077546968,-0.0146613013,-0.0125436763,0.0054089362,0.0246075661,-0.0216875791,-0.00325349,0.0003616827,-0.0221398566,0.0016993998,-0.0012856593,-0.0053400717,0.0003599632,0.0138116778,0.0015022005,-0.0201353584,0.016114251,-0.0019847008,0.0058397009,-0.0019618937,0.019955936,-0.0014304685,-0.0074748646,0.0109882778,0.0119519682,0.0146243986,0.0080163354,-0.0000330823,0.0049898777]},"457":{"Abstract":"Many real-world datasets are large, multivariate, and relational in nature and relevant associated decisions frequently require a simultaneous consideration of both attributes and connections. Existing visualization systems and approaches, however, often make an explicit trade-off between either affording rich exploration of individual data units and their attributes or exploration of the underlying network structure. In doing so, important analysis opportunities and insights are potentially missed. In this study, we aim to address this gap by (1) considering visualizations and interaction techniques that blend the spectrum between unit and network visualizations, (2) discussing the nature of different forms of contexts and the challenges in implementing them, and (3) demonstrating the value of our approach for visual exploration of multivariate, relational data for a real-world use case. Specifically, we demonstrate through a system called Graphicle how network structure can be layered on top of unit visualization techniques to create new opportunities for visual exploration of physician characteristics and referral data. We report on the design, implementation, and evaluation of the system and effectiveness of our blended approach.","Authors":"T. Major; R. C. Basole","DOI":"10.1109\/TVCG.2018.2865151","Keywords":"Unit visualization;network visualization;context","Title":"Graphicle: Exploring Units, Networks, and Context in a Blended Visualization Approach","Keywords_Processed":"context;unit visualization;network visualization","Keyword_Vector":[0.0514133,-0.0096574856,0.0041228688,-0.0219631566,0.1346598184,-0.0082389058,0.1450963186,-0.0577866869,-0.0192845948,-0.0053019516,0.0265561977,0.0471879637,0.0781995433,0.0037600527,-0.0171091835,0.0181837663,-0.0059951704,-0.0274114201,-0.0174073643,-0.0106531158,-0.0079699704,0.0354619863,0.0372999087,-0.0016328704,-0.0463416565,-0.0212191954,-0.0208661005,0.006489081,0.0072939942,0.0829495498,0.0267851506,0.0089493729,0.0100385265,-0.0027327447,-0.0539900952,0.0166431143,-0.0217779192,-0.0006660119,-0.0307043512,-0.0260614993,-0.0037856146,-0.0318329793,-0.0033890212,-0.024977016,-0.001354855,0.0041814587,0.0436435755,0.0094015962,-0.0036652188,-0.0147900707,0.0201581766,0.0059828092],"Abstract_Vector":[0.2014675942,-0.0934159655,-0.0170249256,0.0280528816,-0.0294364246,-0.0243114606,0.0018759358,0.0198668725,0.0350353021,0.0045794992,-0.0322653459,-0.0039469463,-0.0204091158,0.0121068883,-0.0084203902,0.0733037142,0.0102882478,-0.0381467994,0.0118146847,0.0395871347,0.0491273348,0.0036288766,-0.0577037035,0.0186809149,0.057297358,-0.0991524478,0.0260778507,-0.0851946587,-0.0117826892,0.0935292945,0.0209821864,-0.0203171361,-0.0352728723,0.0168157304,-0.0444993168,0.0578003849,0.0675011959,0.0755881679,0.0038934802,-0.010393137,-0.013910433,0.0724459822,-0.0392146571,-0.0655434769,-0.0170253383,-0.0396998573,0.0416834447,-0.0243643485,-0.0033745344,-0.0385321313,0.0083639098,0.0183621782,0.0072041097,-0.0014419356,0.0121427787,0.0428761283,-0.0435295446,0.0045319289,0.0112213589,-0.0536419396,-0.0371404989,0.0175157267,0.0299699089,0.0067593049,0.049379008,-0.0191975191,-0.013614066,0.0391054157,0.0553816089,0.0346278229,0.0078077736,-0.0125627017,-0.0032736167,-0.0270724886,0.0251471448,-0.0018375193,0.0533825638,-0.0097557972,0.0512253613,-0.0113743806,-0.0151859721,0.0006231854,0.0056149863,0.05196633,-0.0410127419,-0.0598098173,0.0136843788,0.0233119813,0.0155637959,-0.0077076073,-0.0315224707,0.0162289308,-0.0103692799,-0.0379558676,-0.0196293071,0.030884577,-0.0393064953,0.0424548314,0.0254504615,0.0169114455,-0.0003021359,-0.046357565,0.0557330317,0.0184110535,0.0089179193,0.0207175732,0.0154063116,0.0095179566,-0.0282542372,0.020919538,-0.0126761022,0.0433971111,0.0202238843,0.0159666833,-0.0093379145,0.000842506]},"458":{"Abstract":"In team-based workplaces, reviewing and reflecting on the content from a previously held meeting can lead to better planning and preparation. However, ineffective meeting summaries can impair this process, especially when participants have difficulty remembering what was said and what its context was. To assist with this process, we introduce MeetingVis, a visual narrative-based approach to meeting summarization. MeetingVis is composed of two primary components: (1) a data pipeline that processes the spoken audio from a group discussion, and (2) a visual-based interface that efficiently displays the summarized content. To design MeetingVis, we create a taxonomy of relevant meeting data points, identifying salient elements to promote recall and reflection. These are mapped to an augmented storyline visualization, which combines the display of participant activities, topic evolutions, and task assignments. For evaluation, we conduct a qualitative user study with five groups. Feedback from the study indicates that MeetingVis effectively triggers the recall of subtle details from prior meetings: all study participants were able to remember new details, points, and tasks compared to an unaided, memory-only baseline. This visual-based approaches can also potentially enhance the productivity of both individuals and the whole team.","Authors":"Y. Shi; C. Bryan; S. Bhamidipati; Y. Zhao; Y. Zhang; K. Ma","DOI":"10.1109\/TVCG.2018.2816203","Keywords":"Design study;information visualization;meeting summarization;natural language processing;visual narrative;voice recognition","Title":"MeetingVis: Visual Narratives to Assist in Recalling Meeting Context and Content","Keywords_Processed":"natural language processing;meeting summarization;design study;voice recognition;visual narrative;information visualization","Keyword_Vector":[0.1903083427,-0.2250398424,-0.1674213848,0.2228534638,-0.1859032698,-0.0947754679,0.1234937941,-0.0353108272,0.1019219323,0.0516515947,0.0849074931,-0.0144440261,-0.0242195324,0.141090531,0.0736972406,0.2153950396,0.0824050218,-0.0161499927,-0.117823859,0.0292210076,0.0243157163,-0.0225434971,0.0483443935,0.0590476231,-0.0493098598,0.0391414288,-0.0049804989,-0.1145315262,-0.0581411598,-0.0087506308,-0.0374175754,0.0198220605,0.0369864881,0.0490028533,0.0438452382,0.0458957409,0.0300984364,0.004231359,0.0111582572,-0.0331627988,-0.0309195387,-0.0574669615,-0.0077641089,-0.0389133046,-0.0224095366,-0.102524916,-0.0240975603,0.0198114979,0.0069348224,-0.0639278464,-0.0506053386,0.0788335681],"Abstract_Vector":[0.2027306002,-0.0871326872,0.0055379325,-0.004149664,-0.0369794733,-0.011172283,0.0398525881,0.0216479479,-0.0423985727,0.0491963565,0.0059794947,0.0199750342,-0.0484928462,0.0076471629,-0.0459563368,0.0183218709,0.0009119302,0.0078477698,-0.0243899955,-0.056702998,0.0315415375,-0.0426679718,-0.0070885168,0.0272798875,-0.0160542947,0.0307268552,0.0138430245,-0.0180162862,-0.0028799529,-0.0338712408,-0.0237906867,-0.0156972201,-0.0756592216,-0.02468482,-0.0466325948,-0.0371659821,-0.0264198573,-0.0164672506,0.0216556894,0.0159030287,-0.0063765896,-0.0326799979,0.0163908762,-0.0049276558,0.0525047431,-0.0023776043,0.0642590442,0.0342788851,0.0304087209,0.0099081102,0.0443225141,0.0572202212,-0.0217391145,0.0190606212,0.0349854486,0.0010362967,-0.0142271527,-0.03825649,0.0232246524,-0.0067162192,0.0146686747,-0.0198532025,-0.0394754192,0.008828349,-0.0473965758,0.0157609537,-0.0118485767,-0.0055208596,-0.0187282401,0.0105348508,-0.0116717062,0.0392432985,0.0188147331,0.0207066017,-0.0357467948,0.0711651797,-0.0528666905,0.0253936583,-0.0086926827,-0.0041852294,-0.0090529794,-0.0071523399,0.0195222614,0.0025354696,-0.0056966685,-0.0085924027,0.0072506211,-0.042688644,0.0153228164,-0.0018109314,-0.0159481608,-0.0587234732,-0.0229282794,0.0253840478,0.0008372996,-0.0116019173,0.0303511987,-0.0301656609,-0.0186288646,-0.0017516524,-0.0298451655,-0.0331670271,-0.0317541891,0.0216515296,-0.002533955,-0.0408068875,0.0592870833,-0.0015170294,0.011771795,0.0064350414,-0.0305193097,-0.0209855333,-0.0077346261,0.0234629728,-0.0390112095,0.0346822084]},"459":{"Abstract":"We propose a novel 360\u00b0 scene representation for converting real scenes into stereoscopic 3D virtual reality content with head-motion parallax. Our image-based scene representation enables efficient synthesis of novel views with six degrees-of-freedom (6-DoF) by fusing motion fields at two scales: (1) disparity motion fields carry implicit depth information and are robustly estimated from multiple laterally displaced auxiliary viewpoints, and (2) pairwise motion fields enable real-time flow-based blending, which improves the visual fidelity of results by minimizing ghosting and view transition artifacts. Based on our scene representation, we present an end-to-end system that captures real scenes with a robotic camera arm, processes the recorded data, and finally renders the scene in a head-mounted display in real time (more than 40 Hz). Our approach is the first to support head-motion parallax when viewing real 360\u00b0 scenes. We demonstrate compelling results that illustrate the enhanced visual experience - and hence sense of immersion-achieved with our approach compared to widely-used stereoscopic panoramas.","Authors":"B. Luo; F. Xu; C. Richardt; J. Yong","DOI":"10.1109\/TVCG.2018.2794071","Keywords":"360\u00b0 scene capture;scene representation;head-motion parallax;6 degrees-of-freedom (6-DoF);image-based rendering","Title":"Parallax360: Stereoscopic 360\u00b0 Scene Representation for Head-Motion Parallax","Keywords_Processed":"image base render;head motion parallax;360 scene capture;scene representation;degree of freedom dof","Keyword_Vector":[0.1172193106,-0.1143272182,-0.0164247037,-0.1548172092,0.1093510565,-0.0110869884,0.1771034486,-0.0999003892,-0.149867193,0.0314471636,-0.0931852289,0.1473361635,0.0596687787,0.0331605104,-0.0994628393,0.0179893949,-0.0927516143,0.0344830423,-0.0725474231,-0.105178314,0.1175215769,0.1230028285,0.1429081634,-0.1542659264,0.0651072443,-0.0578772874,0.1053776124,0.0267543748,-0.0727686327,-0.1280053513,0.001147005,0.1320183785,0.0090909272,0.0991168083,-0.0186076167,0.0415960855,-0.0404084761,0.047269258,0.0588224929,0.029190058,0.0216871179,0.0144954495,0.0136541917,0.0789004651,0.030146425,-0.077734963,-0.0493560118,0.0553979133,0.0338928248,0.122203901,0.0039227908,0.0042761628],"Abstract_Vector":[0.2227072688,-0.1597098468,0.0076967867,0.1296480641,0.0895079001,0.1034710843,-0.0233950589,-0.0123437458,-0.0667024857,0.0631020707,0.0980940148,-0.0583512827,-0.0562795669,0.0684836124,0.1279304354,-0.1024451967,-0.0961731963,0.0100400274,0.0341235046,-0.0448228174,0.0122288419,-0.0143174359,-0.013312535,-0.058994188,-0.0149807521,0.0061260813,-0.0162101603,-0.052357327,0.0670108289,-0.0107842601,0.0107306601,-0.0366746271,-0.0306704942,-0.0166845443,-0.0310975051,-0.0910979881,-0.0029975809,-0.0033969203,-0.0491729636,-0.0140152047,0.0157130883,0.0849716865,-0.0007138246,-0.0007286125,-0.021576791,-0.0065808662,0.0451412535,0.0434949076,0.019270978,-0.0041144142,-0.0323461992,-0.0048332662,0.0276573878,-0.0471020985,-0.0097260368,0.0017102427,-0.0090135979,0.0380004297,0.0648446134,-0.021542326,-0.032788057,0.0074263797,-0.0251689131,0.0319826413,-0.0108858284,-0.0295251943,0.0131071931,0.0304892777,-0.0722900127,-0.0091517281,0.0030097792,0.0162697207,0.0071740154,0.0121722126,0.013672579,-0.0012077066,-0.0251064707,0.0031861963,-0.0251866226,-0.0019734135,-0.0403499643,0.0061556249,-0.0167500872,0.0250489308,-0.0010516246,-0.0168550672,-0.0254097683,0.0430275479,0.0351933503,-0.0596447592,0.0188476641,-0.0277524702,0.0026757318,0.0013698936,0.0145275543,-0.0127631587,0.047579808,-0.0328830603,0.0164576718,-0.0113930745,0.0021427865,-0.0065449161,0.0136151076,0.0019455797,0.0039784821,0.0013201957,-0.0119717895,0.0125272585,0.0222356394,-0.0074562337,0.0023258441,0.0131345988,0.0106587523,-0.0103860514,-0.0153696484,-0.0569706568]},"46":{"Abstract":"Line graphs are usually considered to be the best choice for visualizing time series data, whereas sometimes also scatter plots are used for showing main trends. So far there are no guidelines that indicate which of these visualization methods better display trends in time series for a given canvas. Assuming that the main information in a time series is its overall trend, we propose an algorithm that automatically picks the visualization method that reveals this trend best. This is achieved by measuring the visual consistency between the trend curve represented by a LOESS fit and the trend described by a scatter plot or a line graph. To measure the consistency between our algorithm and user choices, we performed an empirical study with a series of controlled experiments that show a large correspondence. In a factor analysis we furthermore demonstrate that various visual and data factors have effects on the preference for a certain type of visualization.","Authors":"Y. Wang; F. Han; L. Zhu; O. Deussen; B. Chen","DOI":"10.1109\/TVCG.2017.2653106","Keywords":"Line graph;scatter plot;time series;trend","Title":"Line Graph or Scatter Plot? Automatic Selection of Methods for Visualizing Trends in Time Series","Keywords_Processed":"trend;scatter plot;time series;line graph","Keyword_Vector":[0.0768018045,-0.0525721951,-0.0235028815,0.0517536415,-0.0209205544,0.1138038559,-0.0380808251,-0.0381336107,-0.029636663,0.0012749908,0.0098245953,-0.0209327148,0.0451267177,-0.0354505925,0.0116422603,-0.0346216178,-0.0303136452,-0.0075686046,0.019684985,-0.0325850336,-0.0239798021,-0.0151724439,0.0136791047,-0.0126254498,-0.0150514462,-0.0137685849,-0.032323653,0.0149117648,0.0002511429,-0.0475382468,0.0195034148,-0.0346020242,-0.008087802,-0.0508559241,-0.0121587087,0.0121424509,0.0052933409,-0.0003076292,-0.0387229788,-0.0258164514,0.0497119613,-0.0327840037,-0.0237728594,-0.0146113171,-0.0033331039,-0.0714287397,-0.0558633213,0.0041562383,0.0151063077,0.0078137778,0.069981769,-0.0084295761],"Abstract_Vector":[0.2219552875,-0.1501703832,-0.0162033809,-0.0109131951,-0.0888784109,0.0211889655,0.0662302619,0.0301148582,-0.0637549728,0.0264272206,-0.0247849762,0.001231792,-0.0120723988,-0.0416293005,-0.0462107557,0.0486098042,0.0128051686,-0.0073733766,0.0131823695,0.0585735947,-0.0111620544,0.0024392055,-0.0223655933,0.0586347128,-0.002539533,-0.0512869381,-0.0180357427,0.0213353481,-0.0031386571,-0.0191878336,-0.039754554,0.0462102902,-0.0174581311,0.0042454244,0.0109323598,0.0275523701,-0.011478092,-0.0496987808,0.0118408535,-0.0168865488,0.0233687585,-0.0173835865,0.0031858359,0.046326225,0.0147378161,0.0249667142,0.000898447,0.0364424675,-0.0047572468,-0.0343419596,-0.0002064762,-0.0169437177,0.0106526503,0.0209380478,0.0037930926,0.0131273734,0.0019295588,-0.0066301699,0.0011113107,-0.0158616365,0.044134724,0.0211436337,-0.0040113699,0.0474728451,0.0378123052,0.0118047199,0.0278084423,0.0049909284,-0.0076852819,0.0274681543,0.007957221,-0.0077994143,0.0230881335,-0.008729548,0.0022181617,0.0083084708,0.0065162559,0.0180159477,0.0229701496,-0.0102007739,-0.0062808939,0.0268457195,0.0179876613,0.0435118177,-0.0325091314,-0.0360053357,-0.012408041,-0.004472116,0.0339772627,0.0032812619,0.0177177652,0.0058366483,-0.0081410714,0.0522098742,0.0016892338,-0.024152433,0.0012358179,-0.0055287758,0.0387964608,0.0295124822,-0.0044349497,-0.0107840789,0.0026949376,0.0406114525,0.0036500483,-0.007751248,0.0267844774,0.0008811953,-0.0197926724,0.0731716016,0.0148231251,0.0163802022,-0.0046181409,-0.0398971214,0.0246338761,0.0003073256]},"460":{"Abstract":"We present a visualization application that enables effective interactive visual analysis of large-scale 3D histopathology, that is, high-resolution 3D microscopy data of human tissue. Clinical work flows and research based on pathology have, until now, largely been dominated by 2D imaging. As we will show in the paper, studying volumetric histology data will open up novel and useful opportunities for both research and clinical practice. Our starting point is the current lack of appropriate visualization tools in histopathology, which has been a limiting factor in the uptake of digital pathology. Visualization of 3D histology data does pose difficult challenges in several aspects. The full-color datasets are dense and large in scale, on the order of 100,000 \u00d7 100,000 \u00d7 100 voxels. This entails serious demands on both rendering performance and user experience design. Despite this, our developed application supports interactive study of 3D histology datasets at native resolution. Our application is based on tailoring and tuning of existing methods, system integration work, as well as a careful study of domain specific demands emanating from a close participatory design process with domain experts as team members. Results from a user evaluation employing the tool demonstrate a strong agreement among the 14 participating pathologists that 3D histopathology will be a valuable and enabling tool for their work.","Authors":"M. Falk; A. Ynnerman; D. Treanor; C. Lundstr\u00f6m","DOI":"10.1109\/TVCG.2018.2864816","Keywords":"Histology;Pathology;Volume Rendering;Expert Evaluation","Title":"Interactive Visualization of 3D Histopathology in Native Resolution","Keywords_Processed":"volume Rendering;histology;Expert Evaluation;pathology","Keyword_Vector":[0.1101308075,-0.1249547667,-0.0999431085,0.0269418047,-0.0009663821,-0.0819285623,0.2224644901,-0.039590375,0.0145092947,-0.0424005283,0.0237999716,0.038697102,-0.0130909887,-0.0628357261,0.0639504935,0.0571853939,-0.0374996247,-0.0225912324,0.0013027896,-0.0553598782,0.0586359856,0.0362487408,-0.0109715433,-0.0240225381,0.0714547254,-0.0383928568,0.0266008645,0.0331842622,0.000262956,-0.069522845,-0.0436025628,-0.0160426596,0.027383074,-0.0337669975,-0.0274951156,-0.0264539625,-0.0243513923,0.0103227225,0.0288579867,-0.0288134114,-0.0082895914,-0.0098328845,-0.0231114734,-0.043561518,0.0114015156,0.0518587139,-0.053478179,-0.0336325485,-0.0180971394,-0.0055607145,0.0041117686,-0.0294628521],"Abstract_Vector":[0.2068779691,-0.1298892235,0.0015487607,0.1258614556,0.0979633674,0.0765486735,0.0082546596,-0.0260186679,-0.0212666525,0.0342339638,-0.0057250905,0.0172241597,-0.0514689847,0.0189540987,-0.1065255482,-0.0267030796,0.0800785596,-0.0126163534,-0.0141223255,0.0163516742,0.0291365645,-0.0721681092,-0.0235140074,0.0697727089,-0.0469280153,-0.0446884765,-0.005786768,0.0685446753,-0.0985345485,0.0008088855,0.0318841679,0.0406252924,-0.0624132486,-0.0275768202,-0.0186992512,0.0091539251,0.0049744071,0.0135298877,-0.0748245475,0.014011769,-0.0616672032,-0.0338817316,-0.0251004303,0.0442962184,-0.0412310601,-0.0028716271,-0.024546559,-0.0079699016,0.0014801671,0.0082906571,0.0083124046,0.0298423838,0.0036896452,0.0104015664,-0.0091369518,-0.0716430738,-0.0174087216,0.0460631619,0.0082804782,0.0350061331,-0.0316226973,0.0086868146,-0.0188300579,-0.042889359,0.0064170096,-0.0499945083,0.0134898519,-0.0247395067,-0.0238092585,0.0163531363,0.0105038298,-0.0365366094,0.0088957941,-0.0411287678,-0.0271182094,-0.0012432275,-0.042077394,-0.0348560261,-0.012460505,0.0138589565,-0.0234374142,-0.0284443807,-0.0029685345,-0.0212966361,-0.0106227797,0.0092583949,-0.0596239856,-0.0071678787,0.0166206802,-0.0053034504,-0.0146446282,0.0223713344,0.0317459233,-0.0419752564,0.0019549204,-0.0037671594,0.0098464087,-0.0207407121,-0.0696105994,-0.0082853858,-0.0490983368,0.0349099568,0.0296071968,0.0437433945,0.0043968043,0.0043836169,0.0321465763,0.0333954019,-0.0327797659,-0.0418362328,0.0032196742,0.0086408636,-0.0430980853,0.021608143,0.0071087896,-0.0181721341]},"461":{"Abstract":"We introduce a new motion blur computation method for ray tracing that provides an analytical approximation of motion blurred visibility per ray. Rather than relying on timestamped rays and Monte Carlo sampling to resolve the motion blur, we associate a time interval with rays and directly evaluate when and where each ray intersects with animated object faces. Based on our simplifications, the volume swept by each animated face is represented using a triangulation of the surface of this volume. Thus, we can resolve motion blur through ray intersections with stationary triangles, and we can use any standard ray tracing acceleration structure without modifications to account for the time dimension. Rays are intersected with these triangles to analytically determine the time interval and positions of the intersections with the moving objects. Furthermore, we explain an adaptive strategy to efficiently shade the intersection intervals. As a result, we can produce noise-free motion blur for both primary and secondary rays. We also provide a general framework for emulating various camera shutter mechanisms and an artistic modification that amplifies the visibility of moving objects for emphasizing the motion in videos or static images.","Authors":"K. Shkurko; C. Yuksel; D. Kopta; I. Mallett; E. Brunvand","DOI":"10.1109\/TVCG.2017.2775241","Keywords":"Motion blur;ray tracing;sampling","Title":"Time Interval Ray Tracing for Motion Blur","Keywords_Processed":"sample;motion blur;ray trace","Keyword_Vector":[0.143174588,-0.0575297174,-0.004044578,-0.1025854602,0.0631458209,0.0647077106,0.2253698371,-0.0392723536,-0.0464282901,-0.0723827052,-0.020915124,0.0620947439,0.0149813263,-0.0168983391,-0.0216789168,0.0164247132,-0.0072995214,0.0528975642,-0.0286490962,0.0169127945,0.0142237434,0.054467766,0.0108971181,-0.0342130079,0.1218766853,0.0179562917,-0.0388590174,0.0206051779,-0.0481775253,0.0057711668,0.0037016878,-0.0783611636,0.0429969183,-0.0371324789,-0.0706305823,0.0455579602,-0.0106185635,-0.0142921735,0.0421529719,0.0057377769,-0.0382692837,0.0226152372,-0.0000527567,0.0302672532,0.0327725068,0.051962127,-0.056906948,0.0164814497,-0.0365993796,-0.0083169327,-0.0318529625,0.0937488155],"Abstract_Vector":[0.2158449961,-0.077757764,-0.0066579184,0.0030310664,-0.0215184762,-0.0439627238,-0.007779052,-0.0014886637,-0.0360308995,-0.0104027881,0.0407813882,-0.0375731976,-0.0771018678,0.0411326454,-0.0385432915,-0.0361561726,-0.0504840466,0.0110723137,0.0082745445,0.0199695282,0.005581923,-0.0246196734,-0.0243268945,-0.0060626111,-0.0577662773,0.0430817335,-0.0131243379,0.0529438926,-0.018945044,-0.0651706134,0.0163819212,0.0171657397,-0.0164978113,-0.0065861883,-0.0246360748,0.0441923219,-0.0373911056,0.0337375889,-0.0068194467,-0.020710734,-0.0812666167,0.0466624219,-0.0092324946,0.0082537929,0.0030979527,-0.0274021741,-0.0484436301,-0.0153184847,-0.035278331,-0.0483117222,-0.0034066389,0.0280632749,0.0173687645,-0.0368664736,0.0202913508,0.0529967214,0.0249328906,0.1048505761,0.0131786453,-0.0027912343,0.0927247105,0.0068096696,-0.0089248089,0.0175795074,0.0001549097,0.0719687115,0.0352242141,-0.034719131,-0.0132897614,-0.0090920034,-0.0236936224,-0.0174844411,-0.0562242433,-0.0396512143,0.0234976011,0.0197667733,-0.0232616701,0.0347730435,0.032996669,0.0230565392,-0.0004416976,-0.0395307867,-0.0257410376,-0.0125374083,-0.0117954618,-0.0182718541,0.0342868968,0.0019398693,-0.0272890803,0.0230761199,0.0267250065,0.0186149384,-0.0004325192,-0.0663390605,-0.0326314473,-0.0385150553,-0.0186132471,0.0027173184,0.0368416838,0.0232510367,0.0611771057,0.0515083305,-0.0175909852,-0.0030382968,-0.0357752992,0.0214774778,-0.0086808886,-0.0455710433,-0.0160914213,-0.0157011657,0.0043764314,-0.0238287371,0.0052295054,-0.0163602915,-0.0177913708,0.0263509086]},"462":{"Abstract":"Shape provides significant discriminating power in time series matching of visual or geometric data as required in many important applications in graphics and vision. The well established dynamic time warping (DTW) algorithm and its variants do this matching by determining a non-linear time mapping to minimise euclidean distances between corresponding time-warped points. However the shape of curves is not considered. In this paper, we present a new shape-aware algorithm which uses time and shape correspondence (TSC) at increasing levels of detail to define a similarity measure with an L0 norm to aggregate the results, making it robust to noise and missing data. The L0 norm is implicitly regularised using a shape-based error. Through extensive experiments we empirically show that our algorithm outperforms existing state of the art algorithms, works more effectively with high dimensional data, and handles noise and missing data better. We demonstrate its versatile applicability and comparative performance using a large in-house created gait data base, an action data base from Microsoft, exercise action data from a local company, a large public time series data base from University of California, Riverside and hand movement in quaternion stream data format.","Authors":"K. Mendhurwar; Q. Gu; S. Mudur; T. Popa","DOI":"10.1109\/TVCG.2017.2691322","Keywords":"Action recognition;gait authentication;shape awareness;time and shape correspondence;time series matching","Title":"The Discriminative Power of Shape an Empirical Study in Time Series Matching","Keywords_Processed":"action recognition;time and shape correspondence;shape awareness;gait authentication;time series matching","Keyword_Vector":[0.0614917204,0.0119814515,0.0089824846,0.0275343895,0.0485308487,0.0077826071,-0.0157908612,-0.0725226426,-0.0152796076,-0.0035090893,0.0382770043,0.0327923953,-0.0289171114,0.0610760814,0.0378581812,-0.0690161462,0.0306979933,-0.061997626,0.0270010404,0.0764408968,0.0452091724,0.1080732716,-0.1022393687,-0.0265934284,0.0128808199,0.0295958652,0.0000032069,-0.0132692767,-0.0372607878,-0.0547316873,0.0456853513,-0.0061558499,0.0062725313,-0.1384517662,0.014416876,-0.0038626197,-0.0182327547,-0.0289177151,-0.0445839385,0.0585879542,0.038655786,-0.0102964274,-0.1932614277,0.0742281578,0.0310353844,-0.2062813017,-0.1141415082,0.1663707262,-0.0534316826,0.0107989094,-0.00657226,0.0083842817],"Abstract_Vector":[0.1461951778,0.0245175117,-0.0280876061,0.0163685974,-0.027615661,0.07951502,0.0597241101,0.0140868451,0.1203381674,0.0649511953,-0.0276845773,0.009484022,-0.0347713547,0.0133558408,-0.022254527,-0.0578232338,0.0870361279,0.0160899008,-0.0004549602,-0.0853427566,-0.0126587777,-0.0272725656,0.074102956,-0.0144521088,-0.0097219851,-0.0146056744,0.0247195916,0.0051155479,-0.0771930653,0.0492978658,0.0707275007,-0.0213370189,-0.0178774383,-0.0358970828,0.0281251818,0.0081442282,0.012559387,0.0027196613,0.0126778415,0.0498837734,0.0797051033,0.0173362208,0.0273765439,0.1008588832,0.0203296983,-0.0266822118,0.0815640473,0.07128521,0.019527657,0.0295578152,-0.0272104633,-0.0171130218,-0.0387745273,0.0061103778,0.0107307366,0.0443463774,-0.0122535457,0.0316592608,-0.0163017094,-0.0571165998,0.0428928011,-0.0529847071,0.0212776295,-0.0406914855,-0.0106963532,0.0531155044,-0.0158289785,-0.0109039557,-0.0219176731,-0.0255271619,0.0195379725,0.0286939091,0.0305779034,0.0896446587,-0.0005912187,-0.0378621238,0.0285200728,0.064213726,-0.0517947549,-0.0490517255,-0.019365153,0.0039721606,0.0250978953,-0.0803306885,-0.0527794381,-0.033381185,0.0057609042,-0.0447053459,0.0273786617,-0.0368000392,0.0406477925,-0.0180520767,-0.0024354438,-0.0302370229,-0.0427463724,-0.0439757597,0.0787492168,-0.0397448189,-0.0407404122,0.018011401,0.0242714465,-0.0292653178,-0.0033912013,0.0168546006,-0.1256809472,-0.0092306653,-0.0738102953,0.0435023736,0.0091509063,0.0108998733,-0.0204196133,0.0048778424,-0.0173758837,-0.0082119784,0.1225901848,0.0045799342]},"463":{"Abstract":"A recently developed light projection technique can add dynamic impressions to static real objects without changing their original visual attributes such as surface colors and textures. It produces illusory motion impressions in the projection target by projecting gray-scale motion-inducer patterns that selectively drive the motion detectors in the human visual system. Since a compelling illusory motion can be produced by an inducer pattern weaker than necessary to perfectly reproduce the shift of the original pattern on an object's surface, the technique works well under bright environmental light conditions. However, determining the best deformation sizes is often difficult: When users try to add a large deformation, the deviation in the projected patterns from the original surface pattern on the target object becomes apparent. Therefore, to obtain satisfactory results, they have to spend much time and effort to manually adjust the shift sizes. Here, to overcome this limitation, we propose an optimization framework that adaptively retargets the displacement vectors based on a perceptual model. The perceptual model predicts the subjective inconsistency between a projected pattern and an original one by simulating responses in the human visual system. The displacement vectors are adaptively optimized so that the projection effect is maximized within the tolerable range predicted by the model. We extensively evaluated the perceptual model and optimization method through a psychophysical experiment as well as user studies.","Authors":"T. Fukiage; T. Kawabe; S. Nishida","DOI":"10.1109\/TVCG.2019.2898738","Keywords":"Spatial augmented reality;human visual system;perceptual model","Title":"Perceptually Based Adaptive Motion Retargeting to Animate Real Objects by Light Projection","Keywords_Processed":"spatial augmented reality;human visual system;perceptual model","Keyword_Vector":[0.144853993,0.2006441678,-0.0723157875,0.0191287588,-0.0431872327,0.0441016995,0.0248390526,-0.0266351467,0.020991763,0.0574264735,0.0305487032,-0.013496604,0.0046712828,0.0273270552,0.0861447805,0.1235426027,-0.0281926991,-0.0655395779,-0.0936644567,-0.0233257614,0.003801369,0.0300568676,0.0016442675,0.0491978992,-0.0746223937,0.0253463116,0.028406616,-0.0707359924,-0.0559475605,0.0286718684,-0.0813019137,-0.0235108249,0.0262611971,0.0289914231,0.0128890371,-0.0151934425,-0.0163959494,0.0389270756,0.0603884995,0.0167011864,-0.0373863971,0.0518101843,0.0203550205,-0.0093947775,-0.0088576328,0.0236686578,-0.0589817769,0.0384702953,0.0471052237,0.0079548883,0.0107437982,0.0365310144],"Abstract_Vector":[0.1987960479,0.0989568201,-0.0491402749,0.0023052082,-0.0008595769,0.0678209861,-0.0143782335,0.1290576733,0.0539377921,-0.0290202014,0.0369571848,-0.0164627481,-0.0899756461,0.0308785665,-0.0555465701,0.0385136751,0.0217563225,-0.0097247033,0.1098566739,-0.0842101137,0.002752095,0.0175788601,0.0594155231,0.2036287997,-0.0628414674,0.1555812093,-0.0654835947,-0.1818299332,-0.0269047573,0.0257739608,-0.054975036,-0.0458963512,-0.0120661241,0.0478101121,0.0033938027,0.054400833,0.0236026404,0.0168316176,-0.0040402299,0.0267247687,0.0379241873,-0.006038391,0.0664420176,0.0231050278,0.0175871337,-0.0641803031,-0.0381114141,0.0206970707,-0.0438773563,0.0468904774,0.0484460296,0.0310614029,0.0055027955,0.0595976148,-0.0456625756,-0.0733642244,-0.0070658228,-0.0104629901,-0.0351950073,0.0741353844,-0.1005360868,0.0416095792,0.0037758396,0.0214004843,0.0109490592,0.0195571813,-0.0421580941,0.0358824829,0.0033454565,-0.0507198471,0.0011188593,-0.0112938202,-0.000823074,-0.0549190009,0.0297290788,-0.037092802,0.0173528704,-0.0393102528,0.0099032804,-0.0374481914,0.0175417403,-0.0213567561,0.0275619485,0.0144537785,-0.0095124396,0.0279409375,-0.0193584815,0.0412579497,-0.0492624474,-0.0557302311,-0.0067236281,0.0155061331,-0.0040119297,0.0543846956,0.017446659,-0.0082953485,-0.0347474128,-0.0057953928,-0.0273567483,0.0162465609,-0.0306626288,-0.0471834726,-0.00881915,-0.0000798428,-0.023198461,0.0108757385,0.0157464313,-0.0012159646,-0.0116716279,-0.0265183024,-0.0049156982,0.0039779703,0.0027968454,-0.0219089012,0.0587372419,-0.0221961731]},"464":{"Abstract":"We present DrawFromDrawings, an interactive drawing system that provides users with visual feedback for assistance in 2D drawing using a database of sketch images. Following the traditional imitation and emulation training from art education, DrawFromDrawings enables users to retrieve and refer to a sketch image stored in a database and provides them with various novel strokes as suggestive or deformation feedback. Given regions of interest (ROIs) in the user and reference sketches, DrawFromDrawings detects as-long-as-possible (ALAP) stroke segments and the correspondences between user and reference sketches that are the key to computing seamless interpolations. The stroke-level interpolations are parametrized with the user strokes, the reference strokes, and new strokes created by warping the reference strokes based on the user and reference ROI shapes, and the user study indicated that the interpolation could produce various reasonable strokes varying in shapes and complexity. DrawFromDrawings allows users to either replace their strokes with interpolated strokes (deformation feedback) or overlays interpolated strokes onto their strokes (suggestive feedback). The other user studies on the feedback modes indicated that the suggestive feedback enabled drawers to develop and render their ideas using their own stroke style, whereas the deformation feedback enabled them to finish the sketch composition quickly.","Authors":"Y. Matsui; T. Shiratori; K. Aizawa","DOI":"10.1109\/TVCG.2016.2554113","Keywords":"interactive drawing;2D shape interpolation","Title":"DrawFromDrawings: 2D Drawing Assistance via Stroke Interpolation with a Sketch Database","Keywords_Processed":"interactive drawing;2d shape interpolation","Keyword_Vector":[0.0830132933,0.0174877051,-0.017449247,-0.0210219496,0.0506189465,-0.0467952246,-0.0378737263,-0.077495359,-0.0881591061,-0.0054421724,-0.0021719772,0.0509332085,0.0073656648,0.0087565385,0.1417441874,0.0433059768,0.0741716946,0.0436112532,0.0741327094,0.0011054945,0.0932636584,-0.0273164934,-0.1606452586,0.0771772471,0.0776609209,-0.0151805431,0.0278494234,0.0086901935,0.0612786994,-0.0429896255,-0.0643099521,0.0052700734,0.0234892548,0.1138238633,0.1348588833,-0.116496718,-0.0800058177,-0.0219264544,-0.1408795225,-0.0022265773,-0.0342873297,-0.0544667406,-0.0991276446,0.0205867218,0.0719680774,0.1226538869,-0.0313431753,-0.0630911664,0.0392845588,-0.0472579232,0.0552331847,-0.0645486782],"Abstract_Vector":[0.1783499846,0.0299563888,-0.0340597752,0.0665951941,0.0525284262,-0.0006856818,0.0595271001,0.0187480795,0.0598298705,0.0464617785,0.0027772611,0.0568116382,0.0190509582,-0.005029172,-0.0073630245,0.0492139926,0.030245848,0.0575093672,0.0085019695,-0.0243994598,0.0644703514,0.0242421239,0.0429891979,0.0354366507,0.072404675,0.0702052944,0.0039021216,-0.0300664888,-0.0126823346,0.0163134216,-0.0342217844,0.0011225856,-0.0527815365,0.0084639421,-0.0457468404,-0.053354795,-0.06162602,-0.0286594858,0.024115873,0.024027976,0.0210348411,0.0107496958,0.0498611733,0.0068873428,0.0061755842,0.0478906469,0.0351631012,0.0242625961,-0.0148774437,-0.0527309737,0.0218091612,-0.0089216555,-0.0001005576,-0.0188470178,0.024327818,-0.0215625855,-0.0012231168,-0.002515609,-0.0327384354,0.03117154,0.0029478644,-0.0587921013,0.0414460472,-0.0006974404,0.0119127478,0.0389048825,-0.0200261095,0.0051679277,0.0386237519,0.0429925149,-0.0141075907,0.010696547,0.0050067437,-0.005022619,0.0340919437,-0.0124247357,-0.0147129835,-0.0017727491,0.0194808079,-0.0102322022,0.0222952369,0.0247773895,0.001831235,-0.0139818263,0.0067015101,0.0194990841,-0.0026580888,0.0135180107,0.0246567127,-0.0205167231,0.0349053924,-0.0207056569,-0.0193129616,0.0020239943,-0.0343314306,0.0045712675,-0.0059483886,-0.0049565407,-0.030832767,-0.0013269664,0.0309390732,0.0031119193,0.0528667911,0.0047595266,-0.0550929311,-0.0209636811,-0.0107202452,0.008421218,0.0026739044,-0.0223401757,-0.0340420393,-0.0062213131,0.014523983,-0.0316916667,0.009034046,0.0120105882]},"465":{"Abstract":"Visual designs can be complex in modern data visualization systems, which poses special challenges for explaining them to the non-experts. However, few if any presentation tools are tailored for this purpose. In this study, we present Narvis, a slideshow authoring tool designed for introducing data visualizations to non-experts. Narvis targets two types of end users: teachers, experts in data visualization who produce tutorials for explaining a data visualization, and students, non-experts who try to understand visualization designs through tutorials. We present an analysis of requirements through close discussions with the two types of end users. The resulting considerations guide the design and implementation of Narvis. Additionally, to help teachers better organize their introduction slideshows, we specify a data visualization as a hierarchical combination of components, which are automatically detected and extracted by Narvis. The teachers craft an introduction slideshow through first organizing these components, and then explaining them sequentially. A series of templates are provided for adding annotations and animations to improve efficiency during the authoring process. We evaluate Narvis through a qualitative analysis of the authoring experience, and a preliminary evaluation of the generated slideshows.","Authors":"Q. Wang; Z. Li; S. Fu; W. Cui; H. Qu","DOI":"10.1109\/TVCG.2018.2865232","Keywords":"Education;Narrative Visualization;Authoring Tools","Title":"Narvis: Authoring Narrative Slideshows for Introducing Data Visualization Designs","Keywords_Processed":"education;Narrative visualization;author tool","Keyword_Vector":[0.0564283699,0.0454623174,0.0492104412,0.0497423305,-0.002903012,-0.0243706294,-0.0016320715,-0.0552459404,-0.0237908829,-0.0285919221,-0.0603796181,0.0271925161,-0.1379114803,0.0510217914,0.0426616629,-0.0842290281,0.0551311394,-0.0838902366,-0.0484080458,-0.006803048,-0.050566663,0.0409134396,0.0474877825,0.0735814145,0.0532777896,0.0382670191,-0.0029764912,0.0895868708,0.0073222084,0.0210472642,0.0496309288,0.0407667909,0.0572761161,0.0118034783,-0.022283348,-0.0390766747,0.0058252968,0.0005737008,-0.0189881871,0.0276540274,-0.0047576277,-0.017887052,0.0292536448,-0.0459507317,0.0644698112,-0.0259940156,-0.0308360249,-0.0248689618,0.0072464831,0.0514927261,0.0090695788,0.0055911428],"Abstract_Vector":[0.1730711911,0.1116432406,-0.0106848768,0.1472450255,-0.0822195472,-0.0630386791,0.0501639687,0.0720478804,-0.063113587,-0.103180281,-0.0638416494,0.009979144,-0.0197475729,0.0986952106,0.0109721863,0.035569432,-0.0084046694,0.020439923,-0.0294352996,-0.0156565017,-0.0374063115,-0.0181717997,-0.0163086331,-0.047277443,-0.0483235523,-0.0195428754,0.013835619,0.0143373663,-0.0098042383,0.011273794,0.0147830985,-0.0001642118,-0.015527784,0.0031708809,-0.0118081479,-0.0623944174,0.0287668826,0.0101613778,0.0189651929,0.0205090241,-0.003576489,0.0044556383,0.0883799186,0.008484557,0.0011577661,0.0083382159,0.0010466229,0.0102469478,0.0079950142,0.0079429219,0.0063796153,0.0150983301,0.002150683,-0.0033396535,-0.0256303218,-0.0010354138,-0.0040399338,-0.0101479769,0.0074927018,-0.0145341529,0.0391974742,-0.0067444134,0.017785363,-0.0050538237,-0.0142180888,-0.0330846097,0.023219729,-0.0044569044,-0.0221479063,-0.0114458301,0.0265375727,0.0039019608,0.0253302703,0.0056940959,0.0326487386,-0.012347675,0.0071038243,-0.0207452866,0.0146059657,0.0313630546,0.0420681088,-0.0017620089,-0.0122796971,-0.0339656045,0.0147884595,-0.0032076464,-0.002579928,0.0333780524,0.0030551743,0.0417061536,-0.0052378356,-0.0201927923,-0.0358724213,0.0410326437,0.0101599232,0.0133596983,0.0093514343,-0.0180136324,0.0367268365,-0.0460932302,-0.0102635303,-0.0652067614,0.0028609185,-0.0460085608,-0.0098475805,-0.0220611553,0.0068675412,-0.0128644833,0.0011030268,0.0429401137,-0.012849214,0.0088060949,0.0118262376,0.0243008129,0.0032816145,0.0200852239]},"466":{"Abstract":"We present a novel framework for visualizing routes on mobile devices. Our framework is suitable for helping users explore their environment. First, given a starting point and a maximum route length, the system retrieves nearby points of interest (POIs). Second, we automatically compute an attractive walking path through the environment trying to pass by as many highly ranked POIs as possible. Third, we automatically compute a route visualization that shows the current user position, POI locations via pins, and detail lenses for more information about the POIs. The visualization is an animation of an orthographic map view that follows the current user position. We propose an optimization based on a binary integer program (BIP) that models multiple requirements for an effective placement of detail lenses. We show that our path computation method outperforms recently proposed methods and we evaluate the overall impact of our framework in two user studies.","Authors":"M. Birsak; P. Musialski; P. Wonka; M. Wimmer","DOI":"10.1109\/TVCG.2017.2690294","Keywords":"Tourist guide;OpenStreetMap;exploration;binary integer program","Title":"Dynamic Path Exploration on Mobile Devices","Keywords_Processed":"exploration;binary integer program;openstreetmap;tourist guide","Keyword_Vector":[0.0337106291,0.0125358602,-0.0028237219,0.0206534599,0.0200033254,-0.0035421635,-0.0085196366,-0.0038406096,-0.0028993228,-0.010513624,0.034807056,0.0068425214,0.0115786114,0.0425901665,0.0078823426,0.0098987064,0.0077499785,-0.0758360937,0.0568885307,0.0250009642,0.0427547427,-0.0020445886,-0.0373749064,-0.011321788,0.00116157,-0.0184356866,0.112081971,0.0156522408,-0.0004186423,-0.0170726671,0.0121799011,-0.0049343181,-0.0224948338,-0.0653177419,0.0260404154,0.0453100293,-0.0656486809,-0.0163586131,-0.00334668,0.0091232927,0.0567518751,-0.0013905227,-0.0081410567,0.0154098051,0.0365393803,-0.0844133012,-0.0259810543,0.0240241671,0.027330177,-0.0196079513,-0.0082736017,-0.0046198096],"Abstract_Vector":[0.1158000828,0.0512708828,-0.0065845772,0.0534323835,-0.0090083795,-0.0048124686,0.0008134586,0.0303263809,-0.0168509785,-0.0196363808,-0.0113487881,0.0117759755,-0.0504773562,0.0241789889,0.0088259156,-0.0173025994,-0.0522984985,0.0282966062,0.0295835668,-0.0011508358,-0.0276763862,0.0224032697,-0.0188577159,0.0039295555,0.0076852956,-0.0272333055,0.0251110643,0.0219318864,0.0206017063,-0.000704561,-0.0036894871,-0.0032722044,-0.0335300049,-0.0156075979,-0.0194294134,0.0002063705,0.0010157751,0.0072509565,-0.0240859742,-0.0085956601,0.0192606185,-0.0091860043,0.0198654684,-0.0032796119,-0.0037873501,-0.0058662965,0.014047209,0.019568302,0.0220781948,0.0053355581,-0.0113713145,-0.0237871155,-0.0133536111,0.0203219879,0.0129443415,0.0178728461,0.021674455,0.0035850943,0.0055315077,-0.0041860356,-0.001985371,-0.0117950208,0.0252794106,0.0015235969,0.0158264166,-0.0067706704,0.0076293652,-0.0189531221,0.012468602,-0.0128041249,-0.0213733757,0.0167767821,0.0022333825,0.0198005831,-0.0030527297,0.024019857,-0.0099569469,0.0000951146,0.0293400798,-0.0168403575,-0.0025076745,0.0221553754,0.0009088034,0.0155552873,-0.0007135106,0.0132850776,-0.0019151426,0.0054923266,-0.0095500644,0.0079359574,-0.0040926409,-0.0011451892,-0.019181358,-0.0037671667,-0.0110834525,-0.013656749,-0.004929949,-0.0023393665,-0.0030541063,-0.0123105338,-0.0135032991,0.0037921793,0.0110913491,-0.0068063052,-0.0019386687,-0.0227292348,0.0106073303,0.0122103817,0.0146149275,0.0084497423,-0.0198701108,-0.0083745607,-0.0042362016,0.0000979339,0.0091717421,0.0173536453]},"467":{"Abstract":"We propose a method for the vortex extraction and tracking of superconducting magnetic flux vortices for both structured and unstructured mesh data. In the Ginzburg-Landau theory, magnetic flux vortices are well-defined features in a complex-valued order parameter field, and their dynamics determine electromagnetic properties in type-II superconductors. Our method represents each vortex line (a 1D curve embedded in 3D space) as a connected graph extracted from the discretized field in both space and time. For a time-varying discrete dataset, our vortex extraction and tracking method is as accurate as the data discretization. We then apply 3D visualization and 2D event diagrams to the extraction and tracking results to help scientists understand vortex dynamics and macroscale superconductor behavior in greater detail than previously possible.","Authors":"H. Guo; C. L. Phillips; T. Peterka; D. Karpeyev; A. Glatz","DOI":"10.1109\/TVCG.2015.2466838","Keywords":"Superconductor;Vortex extraction;Feature tracking;Unstructured grid;Superconductor;Vortex extraction;Feature tracking;Unstructured grid","Title":"Extracting, Tracking, and Visualizing Magnetic Flux Vortices in 3D Complex-Valued Superconductor Simulation Data","Keywords_Processed":"superconductor;feature tracking;unstructured grid;vortex extraction","Keyword_Vector":[0.0923595367,-0.016777338,0.0155942398,-0.0317535816,0.0013350032,0.0035149688,-0.0281015861,-0.00183503,-0.0123138812,-0.0632766038,0.008332399,-0.002700717,-0.0430418431,0.0384070705,-0.0630825079,0.0218915261,-0.0283654048,0.0350148563,-0.0074980206,-0.0043075605,0.0182090996,0.0342836847,-0.0210286466,0.0230508779,0.1235348792,0.0016710841,0.0728178024,-0.0870518339,0.1410586735,0.0998700955,0.0405422023,-0.0291337979,0.0540730784,0.0345033862,-0.0319002895,0.0169832344,-0.0081606373,0.0088074331,-0.0234688234,0.0378867209,0.0654008088,0.0992756533,0.0022054209,-0.0372502855,-0.0239751594,-0.0228741607,0.0365845433,0.0054042045,0.0325426018,0.0197111538,0.0061918443,-0.0257482961],"Abstract_Vector":[0.1707652785,-0.0726890884,0.0277181253,-0.0304726545,-0.044258436,0.000314855,0.0091584777,-0.0417971167,-0.0488506197,-0.2002591726,0.1974608406,0.2864528852,-0.1147076448,0.0191625844,0.1330183407,-0.0333393728,0.1086093884,-0.039020587,0.046184817,0.0124840789,-0.05571142,-0.0012681184,-0.014482948,-0.0153892479,0.0159403541,-0.0186205235,-0.1297910191,0.0437290973,0.0190612356,0.0540461119,-0.0091950374,-0.0363698157,-0.0287052914,-0.0005221327,0.0238789384,-0.0262860677,0.0004041209,0.0235283034,0.0013410723,-0.0026694638,0.0188344928,0.0579985037,0.0063396225,0.0092926714,-0.0292981819,-0.0020508862,-0.016904189,0.0009024146,-0.0154049503,-0.0421491789,0.0218105095,0.0098663641,0.0141446522,0.0135377679,-0.0208759842,0.0428373663,0.0327925474,0.008926222,0.0034614501,-0.0533105499,0.030359742,-0.0221169973,-0.0201707544,0.0422163535,0.0053067634,-0.0216582431,0.0348832392,-0.0636335581,0.0070838765,0.0128482942,0.0346911822,0.0189215752,-0.0069259067,0.003804946,-0.0057224828,-0.022366717,-0.0099164264,0.0175136656,0.0358451504,0.0142631895,-0.0047028374,0.0232893414,-0.0169496028,0.0036704532,-0.002104049,-0.0439940238,0.0011216293,0.0264999382,-0.0416831881,-0.0112862335,0.0133162226,0.0056097924,0.0017719463,0.0250529178,0.0272448817,0.0075467413,-0.0180399379,-0.0205782136,-0.0257646849,-0.0382848411,0.0091964605,0.0263681694,-0.00186147,-0.009428433,-0.0128690887,-0.0076652174,0.0190666782,-0.0117907126,-0.0344352909,0.039400519,0.0157116457,0.0078539984,-0.013450319,-0.0405468597,0.0073478007,-0.0147710213]},"468":{"Abstract":"Light scattering in participating media is a natural phenomenon that is increasingly featured in movies and games, as it is visually pleasing and lends realism to a scene. In art, it may further be used to express a certain mood or emphasize objects. Here, artists often rely on stylization when creating scattering effects, not only because of the complexity of physically correct scattering, but also to increase expressiveness. Little research, however, focuses on artistically influencing the simulation of the scattering process in a virtual 3D scene. We propose novel stylization techniques, enabling artists to change the appearance of single scattering effects such as light shafts. Users can add, remove, or enhance light shafts using occluder manipulation. The colors of the light shafts can be stylized and animated using easily modifiable transfer functions. Alternatively, our system can optimize a light map given a simple user input for a number of desired views in the 3D world. Finally, we enable artists to control the heterogeneity of the underlying medium. Our stylized scattering solution is easy to use and compatible with standard rendering pipelines. It works for animated scenes and can be executed in real time to provide the artist with quick feedback.","Authors":"T. R. Kol; O. Klehm; H. Seidel; E. Eisemann","DOI":"10.1109\/TVCG.2016.2554114","Keywords":"Interactive stylization;artist control;single scattering","Title":"Expressive Single Scattering for Light Shaft Stylization","Keywords_Processed":"interactive stylization;artist control;single scattering","Keyword_Vector":[0.0556918306,-0.0321393604,-0.0207192121,0.0072681006,-0.005408591,-0.0054514645,0.0057020644,-0.0256077036,-0.0119647359,-0.0165493753,0.0150283522,-0.0421603622,-0.0047194717,0.0814298502,0.0043068341,0.0206691926,0.0381448117,-0.0376392877,0.0240884679,-0.0477334309,0.0259153325,-0.0496921311,-0.0344317523,0.0001204094,0.0144768674,-0.0124584033,0.0494935053,-0.0103184436,-0.0177984839,-0.0092737606,-0.0423430194,-0.0177171711,0.0133039515,0.0577300253,0.0624210453,-0.0215159925,-0.0284481024,-0.0102463517,-0.030816039,0.0068561971,-0.0381716822,-0.0119819888,-0.0539001888,0.009359906,0.0270710777,0.0705888461,0.0008637914,-0.0148165496,-0.0179428382,-0.0114400028,0.011172318,0.0162014329],"Abstract_Vector":[0.168049298,-0.005702355,-0.0125123218,0.0053582726,-0.0062222944,-0.059881453,0.0043419053,-0.0201922439,0.0515308908,0.0274672376,-0.0129071438,0.0024609114,-0.0304992799,0.0063283886,-0.0414855788,-0.0537300688,-0.0088787618,-0.0111177447,-0.0287376718,-0.0226134086,-0.0195813804,0.0424842596,0.0677855337,0.0111084398,-0.0052145211,-0.0594624485,-0.0132696069,-0.0001474419,-0.0463438499,-0.0163028591,0.1028997296,-0.0254411615,-0.0447170917,0.0306582712,0.008499478,-0.0372929,0.0360496802,0.0479852302,0.0324651959,0.021456808,-0.0290781521,0.0476317927,0.0664872165,-0.0009343231,-0.0324188102,-0.0027117906,0.0340986039,0.0860059507,-0.0289548002,0.0696077058,0.016964943,-0.0095840803,-0.0446243704,0.0012118777,0.0394561667,0.0470830048,0.0052181011,0.0147474249,0.0167769521,0.0209786392,0.0592187709,-0.0100833047,0.0395645178,-0.0422359198,-0.0300872612,-0.0196127718,0.048881582,-0.0387849907,-0.0325288887,0.0890217963,0.0013986382,0.0303584954,0.0900186898,-0.0341481614,0.0759318396,-0.0309050173,0.0674128653,-0.0153062192,-0.0100021242,-0.0415381322,-0.0397647133,0.056322417,0.0336890158,-0.0058055505,-0.0815967345,-0.0147362901,-0.0351164154,0.030852032,0.0609056287,-0.0435721915,0.0390941558,0.0281023277,0.0254572745,-0.0719294866,-0.0069091486,0.0418593809,-0.0414140177,0.0320926049,-0.024937846,-0.0382530399,-0.0165845867,-0.0331126647,0.0167294014,-0.0282139324,-0.0663801005,-0.0329625151,-0.0110942007,0.0038413194,0.0001038374,0.0112705834,-0.0222544897,-0.0057947709,0.0125033641,-0.0440566701,0.0442759283,-0.0183544321]},"469":{"Abstract":"When motion capture data is applied to virtual characters, the applied motion often exhibits geometric and physical errors, which necessitates a cumbersome refinement process. We present a novel framework to efficiently obtain a corrected motion as well as its supporting contact information from multi-contact motion capture data. To this end, first, we present a projective dynamics-based method for optimizing character motions. By carefully defining objective functions and constraints using differential representation of motions, we develop a highly efficient motion optimizer that can create geometrically and dynamically adjusted motions given reference motion data and contact information. Second, we develop a contact optimizer that finds a set of contacts that allows the motion optimizer to generate a motion that best follows the reference motion under dynamic and geometric constraints. This is achieved by iteratively improving the hypothesis on the best set of contacts by getting feedback from the motion optimizer. We demonstrate that our method significantly improves the naturalness of a wide range of motion capture data, from walking to rolling.","Authors":"S. Lee; S. Lee","DOI":"10.1109\/TVCG.2018.2818721","Keywords":"Character animation;motion capture;motion retargeting;multi-contact","Title":"Projective Motion Correction with Contact Optimization","Keywords_Processed":"multi contact;character animation;motion capture;motion retargete","Keyword_Vector":[0.0152334107,0.0113553217,0.0010109494,-0.0002083508,0.0033898223,-0.0018923497,-0.0100334299,-0.0093977162,-0.0045008961,-0.014692763,0.0040753916,0.0088773739,-0.0227864622,0.0107941921,0.0108980759,-0.0131663181,0.0002841851,-0.0037149075,-0.0184402186,0.01014605,0.0016355678,0.0056137201,-0.0028836096,0.0288988883,-0.0010000635,-0.0065816307,0.00667504,-0.0131717607,-0.0015372639,0.0076978084,0.0061423965,0.0114474254,0.0082522628,0.0049514251,0.0159417516,-0.0279200111,-0.002814453,-0.018801586,0.0233233005,-0.0493223581,0.0887531052,-0.030628647,0.0313741245,0.0038928959,0.0073439489,0.0725289132,-0.0267457825,0.1019728841,-0.0212871592,-0.0066450476,0.0448491759,-0.0035196163],"Abstract_Vector":[0.1025632647,-0.004741228,0.014634237,0.0111512278,-0.0287073393,0.0366100277,0.0268921805,-0.0058575081,0.0594526737,0.0822748085,-0.052341021,0.0768975594,-0.01935643,0.0229605303,-0.0447391594,-0.0819334619,-0.0174164089,-0.0268133146,-0.0475690902,-0.0307640827,-0.0990188029,0.0030820789,0.2849630087,-0.022890028,-0.0980024493,-0.0361140041,-0.0258989002,-0.0105551046,0.0968690234,0.0006164628,0.0464284885,-0.0288513489,-0.0778645208,0.0013974013,-0.0182605839,0.0041070387,0.0142422107,-0.0145569283,-0.0132173151,0.0002726921,-0.0235652298,0.014611656,0.0302230615,-0.0012805404,-0.0624006825,-0.034285808,-0.0608604213,0.0367774425,-0.0147541664,0.0634357514,0.0168235152,0.0035745022,-0.0018172959,-0.0516695441,0.0042361774,0.0156485083,0.0228437665,0.0315187734,-0.002320862,-0.0262805206,0.0587586533,-0.0058370635,0.0288545162,-0.030552846,0.0335643277,0.0073723191,0.0186228826,-0.0221741867,-0.0370598016,0.1083364225,-0.0023208304,0.0332486038,0.054912943,0.0141921731,0.0042010445,-0.0184795722,0.0046146481,0.0045712769,0.009717358,-0.0057259174,-0.0158602587,0.0143770106,0.0485518619,-0.0138553323,-0.1037542865,0.0058632555,-0.0338586443,0.0117341628,0.0605046475,-0.0485517908,0.020125445,-0.0184924352,0.0148743017,-0.0640404989,0.0170436754,0.0308816313,-0.0108218769,0.0236371854,-0.0119342186,-0.0496830568,-0.0255833944,0.0003430015,0.00912559,-0.0108275306,-0.0198093852,-0.0010141787,0.0119000529,0.0256558423,0.0067719483,-0.0271696289,0.0241067195,-0.0308190512,-0.014501926,0.0010004711,0.0315599939,-0.0406607093]},"47":{"Abstract":"Generative models bear promising implications to learn data representations in an unsupervised fashion with deep learning. Generative Adversarial Nets (GAN) is one of the most popular frameworks in this arena. Despite the promising results from different types of GANs, in-depth understanding on the adversarial training process of the models remains a challenge to domain experts. The complexity and the potential long-time training process of the models make it hard to evaluate, interpret, and optimize them. In this work, guided by practical needs from domain experts, we design and develop a visual analytics system, GANViz, aiming to help experts understand the adversarial process of GANs in-depth. Specifically, GANViz evaluates the model performance of two subnetworks of GANs, provides evidence and interpretations of the models' performance, and empowers comparative analysis with the evidence. Through our case studies with two real-world datasets, we demonstrate that GANViz can provide useful insight into helping domain experts understand, interpret, evaluate, and potentially improve GAN models.","Authors":"J. Wang; L. Gou; H. Yang; H. Shen","DOI":"10.1109\/TVCG.2018.2816223","Keywords":"Generative adversarial nets;deep learning;model interpretation;visual analytics","Title":"GANViz: A Visual Analytics Approach to Understand the Adversarial Game","Keywords_Processed":"model interpretation;generative adversarial net;visual analytic;deep learning","Keyword_Vector":[0.1116849884,-0.0306469478,0.0035993714,-0.0871086337,0.0787170056,-0.0217727004,0.0709503562,-0.0147127025,-0.1360222905,-0.0598702343,0.0476213068,0.130980692,0.0235409386,0.0245111784,0.0769630039,0.1282851972,0.0194250121,0.0473347846,0.2945614463,-0.0060339441,-0.0801055931,0.0178893035,0.2015471034,0.0784807188,-0.0753114474,0.0353265096,-0.0776638238,-0.0625216158,0.0922158966,-0.1222019859,0.01973076,-0.0823358989,0.0410461533,-0.0241611633,-0.0716653587,-0.0538514372,-0.0453029629,0.0287781456,-0.0606012164,-0.0377863473,-0.0347029278,0.0256066227,0.0442287634,0.0106058289,-0.0142781235,0.0011474002,0.048109363,0.1129583796,-0.0103165432,0.0624871407,-0.0209201071,0.0014999717],"Abstract_Vector":[0.2327589116,-0.1081195832,-0.003596361,0.040486405,0.0263091826,0.1333906539,-0.033707317,0.0316971352,0.1241916696,-0.0425233326,-0.0315092397,0.0792392725,0.1021955885,0.0164893932,0.0037983254,-0.0352414686,-0.0298085977,-0.0176547545,0.0723734476,0.0345229516,0.0194497986,-0.040873554,-0.0427409712,0.0057987945,-0.0406497845,-0.0073502241,0.0113981728,-0.0000423031,0.0078243885,0.0047882437,0.0087709529,-0.0577153876,-0.019787074,0.0032989703,0.0309770608,-0.0320994383,-0.0053248354,0.0694521594,-0.0826923471,-0.0558032922,0.0189412951,0.0066454354,-0.0977213324,0.0193416946,-0.0386216956,0.0360031288,0.0069328994,-0.0086104833,0.0265566093,-0.0451478284,-0.04445856,-0.01616847,0.025546799,-0.0555249736,0.0031620015,-0.0120649594,0.0271160152,-0.0021111853,-0.0048730661,0.0301725961,-0.0084589346,0.0065991682,0.01155636,0.0243968416,0.0681457854,-0.0344963953,-0.0261878052,-0.0054726582,-0.0206166074,-0.006718902,-0.0426095108,-0.0274933818,0.0028219762,-0.0253728011,-0.0311652645,-0.0538514232,0.0087144072,-0.0135524609,0.0154895603,-0.0370688236,-0.0001850089,0.0188802148,-0.0248863146,-0.0070532707,0.0146630903,0.0032269311,-0.0232138541,0.0080571721,-0.0029355106,0.0054998594,-0.000612105,0.0029895498,0.0396624602,0.0215488963,0.0418737563,-0.0160185463,0.0215706654,-0.0239399416,-0.0283316199,-0.0201217989,-0.0099354637,0.0503926827,0.0131097245,-0.0097055801,0.0336751263,0.033726842,-0.0084345716,-0.0168964885,0.0119965014,0.0134225875,-0.0084542792,-0.0407206889,-0.0041254396,0.0154457844,-0.0019366782,0.0048446338]},"470":{"Abstract":"Clustering of trajectories of moving objects by similarity is an important technique in movement analysis. Existing distance functions assess the similarity between trajectories based on properties of the trajectory points or segments. The properties may include the spatial positions, times, and thematic attributes. There may be a need to focus the analysis on certain parts of trajectories, i.e., points and segments that have particular properties. According to the analysis focus, the analyst may need to cluster trajectories by similarity of their relevant parts only. Throughout the analysis process, the focus may change, and different parts of trajectories may become relevant. We propose an analytical workflow in which interactive filtering tools are used to attach relevance flags to elements of trajectories, clustering is done using a distance function that ignores irrelevant elements, and the resulting clusters are summarized for further analysis. We demonstrate how this workflow can be useful for different analysis tasks in three case studies with real data from the domain of air traffic. We propose a suite of generic techniques and visualization guidelines to support movement data analysis by means of relevance-aware trajectory clustering.","Authors":"G. Andrienko; N. Andrienko; G. Fuchs; J. M. C. Garcia","DOI":"10.1109\/TVCG.2017.2744322","Keywords":"Visual analytics;movement data analysis;trajectory clustering;air traffic","Title":"Clustering Trajectories by Relevant Parts for Air Traffic Analysis","Keywords_Processed":"movement datum analysis;visual analytic;air traffic;trajectory clustering","Keyword_Vector":[0.1159035041,-0.1483574081,-0.1583239726,0.0904102846,-0.076056231,-0.2001626592,0.0338871789,-0.0129098504,-0.0016266285,0.0352855741,-0.0160836595,0.0361145284,-0.0239091169,-0.0715221674,0.0645121461,-0.0289205489,-0.023114487,0.0234740458,-0.0133145258,-0.0305167885,0.0617087962,0.1064637789,0.0708923512,0.0584578432,0.0092005973,-0.0368762222,0.0590346927,-0.0326037982,0.0086730011,-0.0236392219,0.0073868892,0.0354139957,-0.0219111732,0.0022026466,0.0363899659,-0.0053111645,0.0122738041,-0.0186504857,-0.0005627289,0.0098079499,-0.0240329105,-0.0324359312,0.0319242109,-0.0105509353,-0.0137799416,0.0137358759,-0.0420074111,0.0335984759,0.0728235159,-0.0299467582,0.0292612611,0.02666067],"Abstract_Vector":[0.1778754853,-0.0667588012,-0.0122767548,0.0357988076,-0.1243782421,0.0407133716,0.0446134315,0.0306090619,-0.0711106501,-0.0238186792,-0.0221301186,0.0194493455,-0.1025067067,-0.0444121324,-0.0280326257,0.110871125,0.1089024989,0.0684426926,-0.0303361972,-0.0930794624,0.0759990871,0.0242850379,0.0306528858,0.0406183539,-0.0216712179,0.0031458292,0.0816343896,-0.0133300563,0.018504124,-0.0007769871,-0.0804634392,-0.0147320131,0.0137149713,0.0066011502,-0.0182356329,-0.0389414701,-0.0003643053,-0.0584985579,0.0165823621,-0.0314684806,-0.0364255354,-0.0191504708,0.0092825668,-0.0150746481,-0.0048122073,0.0503760877,0.0308905005,0.0287927158,-0.0020107791,-0.0514665787,0.0120758422,0.003608321,0.0166719572,0.0602504086,0.0045862404,-0.024923763,0.0634160986,-0.0080315057,0.0396683458,-0.0002920134,0.0660829293,0.0050962107,-0.0047763071,-0.0111605347,-0.0468685146,0.0024823058,0.0232191369,-0.0083107016,-0.0026668148,0.0312906628,-0.0362707364,0.0147874555,-0.0428283652,-0.0170364489,-0.0280442422,0.0612534862,-0.0081888543,-0.0035492114,-0.0174974173,-0.019528896,-0.0069184842,0.0291610287,0.0006922504,0.015590777,-0.0166436854,-0.0621137144,-0.0120681689,0.0174806216,-0.0039777278,-0.0086445753,0.0262058757,-0.0519790245,-0.0084016728,0.0066658243,-0.0046420409,0.0115475447,0.0081470113,-0.0470117598,-0.0375526333,0.0292939705,-0.0309760509,-0.0302797397,0.0173890109,0.011834672,-0.0354651237,-0.0244192592,-0.013697424,-0.0548255657,0.0344475923,-0.011599191,-0.0304252855,-0.0102137174,-0.0238099588,0.0350336046,-0.0235113873,-0.0142896443]},"471":{"Abstract":"Deep neural networks are now rivaling human accuracy in several pattern recognition problems. Compared to traditional classifiers, where features are handcrafted, neural networks learn increasingly complex features directly from the data. Instead of handcrafting the features, it is now the network architecture that is manually engineered. The network architecture parameters such as the number of layers or the number of filters per layer and their interconnections are essential for good performance. Even though basic design guidelines exist, designing a neural network is an iterative trial-and-error process that takes days or even weeks to perform due to the large datasets used for training. In this paper, we present DeepEyes, a Progressive Visual Analytics system that supports the design of neural networks during training. We present novel visualizations, supporting the identification of layers that learned a stable set of patterns and, therefore, are of interest for a detailed analysis. The system facilitates the identification of problems, such as superfluous filters or layers, and information that is not being captured by the network. We demonstrate the effectiveness of our system through multiple use cases, showing how a trained network can be compressed, reshaped and adapted to different problems.","Authors":"N. Pezzotti; T. H\u00f6llt; J. Van Gemert; B. P. F. Lelieveldt; E. Eisemann; A. Vilanova","DOI":"10.1109\/TVCG.2017.2744358","Keywords":"Progressive visual analytics;deep neural networks;machine learning","Title":"DeepEyes: Progressive Visual Analytics for Designing Deep Neural Networks","Keywords_Processed":"progressive visual analytic;deep neural network;machine learning","Keyword_Vector":[0.1175879862,-0.1275306819,-0.1482949192,0.1346410909,-0.0785658204,-0.1089964915,0.0492964737,0.0093440529,0.0101694237,-0.0411938533,0.0444672331,-0.0206225237,0.0217342555,0.1771601062,-0.0287688593,0.0057377514,0.0670121451,0.0292334354,0.0684017147,-0.1048960759,0.0229832441,0.0062251984,-0.0664652788,-0.0417570874,0.0552479594,0.0460497858,-0.0214786931,-0.0601659971,0.0447249692,0.0341211883,-0.0084699712,0.0140038014,0.0255933006,0.065615912,-0.0311589473,0.0665151316,0.0651664373,-0.0141950451,-0.0139702412,-0.0010052586,0.0040351029,0.0284451978,-0.0038734672,-0.0432203011,-0.0306356162,-0.0137340514,0.010077383,0.0431371586,0.005710699,0.0368564724,0.031046394,-0.040125938],"Abstract_Vector":[0.156615473,-0.1146787526,0.0247799594,-0.0082970741,-0.063267623,0.0140982069,0.0307237678,0.0125052177,-0.0641662348,-0.0121779886,0.0136943202,0.0405770876,-0.0714079501,-0.0225660848,0.0055744643,-0.0114580705,-0.0059957998,-0.0685334199,-0.0087945383,0.0503852669,-0.0019158978,0.0228696395,-0.0063004733,-0.000961421,-0.0295803052,0.0325907015,-0.0210997148,0.033879937,-0.0035312805,-0.0500938649,-0.0017493687,-0.0140429371,-0.0018804752,-0.0070065952,-0.0009245133,-0.0273389891,-0.0040415029,0.005600909,0.0075644445,-0.0240586455,0.0124807469,-0.0190216986,-0.0691879409,0.0049700166,0.022035613,0.0154100689,0.0093360618,-0.0108708517,-0.0585828952,-0.0391177922,0.0025824125,0.0292886328,0.0178212502,-0.0005812333,0.0259489432,0.0262631563,-0.0150470875,-0.0095081372,0.073879349,0.0038432552,0.0018025639,-0.0342670481,-0.04018744,0.0423653243,0.0190039513,0.0396128737,0.0005800709,-0.0455666403,0.0162659934,0.0036836861,-0.0355891079,-0.0038778256,-0.0106043397,0.0198289812,0.0150423858,-0.0201484377,-0.0119307591,0.0009139249,0.0051007402,-0.0069333279,-0.0286806454,0.0055507859,0.0588608589,0.0191404177,0.013413868,0.0056679184,-0.002257339,0.0027991667,0.0067622228,0.013620513,0.0225655807,0.0075520413,-0.0048336714,-0.0060387741,0.0359432454,-0.008005913,0.0235065461,-0.0250351893,0.0134534204,-0.0050556735,-0.0087799169,-0.0113698027,0.0133935045,0.0232971949,-0.0023992041,-0.0237464939,0.0207332155,-0.0059006515,-0.0126389389,0.0016820065,0.0034479883,0.0035913911,-0.0056730932,-0.0152761074,-0.0031751476,0.0020619212]},"472":{"Abstract":"We present a novel real-time approach for user-guided intrinsic decomposition of static scenes captured by an RGB-D sensor. In the first step, we acquire a three-dimensional representation of the scene using a dense volumetric reconstruction framework. The obtained reconstruction serves as a proxy to densely fuse reflectance estimates and to store user-provided constraints in three-dimensional space. User constraints, in the form of constant shading and reflectance strokes, can be placed directly on the real-world geometry using an intuitive touch-based interaction metaphor, or using interactive mouse strokes. Fusing the decomposition results and constraints in three-dimensional space allows for robust propagation of this information to novel views by re-projection. We leverage this information to improve on the decomposition quality of existing intrinsic video decomposition techniques by further constraining the ill-posed decomposition problem. In addition to improved decomposition quality, we show a variety of live augmented reality applications such as recoloring of objects, relighting of scenes and editing of material appearance.","Authors":"A. Meka; G. Fox; M. Zollh\u00f6fer; C. Richardt; C. Theobalt","DOI":"10.1109\/TVCG.2017.2734425","Keywords":"Intrinsic video decomposition;reflectance fusion;user-guided shading refinement","Title":"Live User-Guided Intrinsic Video for Static Scenes","Keywords_Processed":"intrinsic video decomposition;reflectance fusion;user guide shading refinement","Keyword_Vector":[0.0599630624,0.0871202121,-0.0274582699,0.040287772,0.0422108808,-0.0230193816,0.0054323507,-0.0545115096,-0.0110700188,-0.0014521295,0.0319220816,-0.0143868443,-0.0355795274,-0.0034685031,-0.0103254228,-0.0216336553,-0.0133055268,-0.0135250018,0.0148143723,0.0039259169,-0.0029838119,-0.045311871,-0.0255003666,0.0610418422,0.0008718845,0.017313926,0.1007400346,-0.0085356805,-0.0362325244,-0.0418774859,-0.0121023253,0.0226600646,0.0261174519,-0.0575063074,0.0079500226,0.04740857,0.0006746564,-0.0015878882,0.0002995376,-0.0026395196,0.061040345,0.0051138223,0.0128424737,0.0142182584,0.0341080485,0.0381322083,0.0065430623,0.0129429014,0.0357679167,0.0035585519,0.0210930301,-0.0000312918],"Abstract_Vector":[0.2414113935,0.1652825412,-0.023771115,0.1090813262,-0.0779699145,-0.0091199344,0.0006782547,0.0684293329,-0.0369661021,-0.0970396161,-0.0307551846,-0.0090345856,-0.0796564688,0.0724206937,-0.0403172214,-0.0300838676,-0.0406442477,-0.0182457507,-0.0155450229,0.0139840042,-0.0700636276,0.0149225628,-0.0190218521,-0.0093952265,0.0495614508,-0.0282778894,0.0105765425,-0.011668422,-0.0223594682,-0.057445708,0.0307655373,-0.0465656055,-0.0241446895,0.0388908235,0.0058976345,-0.0132879487,0.020581004,0.0125005837,-0.0086328634,0.0073107155,0.008875066,-0.0223275948,-0.0003610944,-0.0353961516,-0.0099691141,-0.0066137027,0.000822887,0.0070108654,-0.0031784176,0.0093502618,-0.0309270039,-0.0167490016,-0.0047198488,-0.0483729848,-0.0221775303,-0.0531292883,-0.0002180211,-0.0240928019,0.0063088839,0.0003639409,-0.0036315592,-0.0125083464,-0.0286637457,0.0123379049,-0.0501281095,-0.041742932,-0.0209311276,-0.0212608187,0.0339218363,-0.0092857216,0.025770038,0.015856349,-0.0271871836,0.0310624105,0.0445406405,0.0145550768,0.0168964619,0.0208936088,-0.0140630237,0.0403793429,-0.0184814818,0.0066415744,-0.0286007051,0.0158563357,0.0041375903,0.0378856403,-0.0385082081,0.006706554,0.0086138205,-0.0230995211,0.0103826133,0.0313573995,-0.0111867343,-0.0464428295,-0.0120183011,-0.0348832798,0.00030002,0.0309232305,-0.0127644625,-0.0121813522,0.0122019006,-0.0235014227,0.0031258941,-0.0180558951,-0.0450068022,-0.0064467263,-0.0150950652,0.023557901,0.0134855865,-0.0231610235,-0.0134913441,-0.0025942933,-0.0015503158,0.0347739585,-0.0259643978,0.0187413569]},"473":{"Abstract":"This paper presents a novel approach to content delivery for video streaming services. It exploits information from connected eye-trackers embedded in the next generation of VR Head Mounted Displays (HMDs). The proposed solution aims to deliver high visual quality, in real time, around the users' fixations points while lowering the quality everywhere else. The goal of the proposed approach is to substantially reduce the overall bandwidth requirements for supporting VR video experiences while delivering high levels of user perceived quality. The prerequisites to achieve these results are: (1) mechanisms that can cope with different degrees of latency in the system and (2) solutions that support fast adaptation of video quality in different parts of a frame, without requiring a large increase in bitrate. A novel codec configuration, capable of supporting near-instantaneous video quality adaptation in specific portions of a video frame, is presented. The proposed method exploits in-built properties of HEVC encoders and while it introduces a moderate amount of error, these errors are indetectable by users. Fast adaptation is the key to enable gaze-aware streaming and its reduction in bandwidth. A testbed implementing gaze-aware streaming, together with a prototype HMD with in-built eye tracker, is presented and was used for testing with real users. The studies quantified the bandwidth savings achievable by the proposed approach and characterize the relationships between Quality of Experience (QoE) and network latency. The results showed that up to 83% less bandwidth is required to deliver high QoE levels to the users, as compared to conventional solutions.","Authors":"P. Lungaro; R. Sj\u00f6berg; A. J. F. Valero; A. Mittal; K. Tollmar","DOI":"10.1109\/TVCG.2018.2794119","Keywords":"Eye-tracking;VR;QoE;video streaming;content delivery","Title":"Gaze-Aware Streaming Solutions for the Next Generation of Mobile VR Experiences","Keywords_Processed":"VR;qoe;eye tracking;video streaming;content delivery","Keyword_Vector":[0.0325356806,0.0273060524,0.0330091324,0.0427693436,0.0249474541,-0.0334441602,-0.0133891048,-0.1084371525,-0.0184568759,-0.0221501803,0.020165371,0.0217205891,-0.099933407,0.0592920735,0.1106743234,-0.0973816392,-0.0283652573,-0.0152650041,-0.0499678701,0.0119810258,-0.0566718277,0.01909436,0.0356817983,-0.0339225094,0.0114117847,0.0510355719,0.0039888028,-0.0037103695,0.0065643732,-0.0042862801,-0.0188285839,0.0256074536,-0.0135885845,-0.0113933925,-0.0094744178,0.0081386598,0.0034628353,-0.0073037596,-0.0028130783,0.0100532955,-0.0109699651,-0.0018710564,-0.0044576156,-0.0130175573,0.0076195281,-0.0183128608,-0.0034009634,-0.0216645674,-0.006234735,0.0181538772,0.0037942959,0.0036998663],"Abstract_Vector":[0.1576882613,0.0276111524,-0.028384106,-0.0043575736,-0.0624775472,0.0430253186,-0.0184814224,0.0092673952,-0.024012032,-0.0241431743,-0.0044836237,-0.0040187686,-0.007606731,-0.0242049699,-0.003608813,0.036754482,0.0370680707,0.0507002332,-0.037159714,-0.0432399034,0.0458761553,-0.0384788161,-0.0029118754,0.0269401617,0.0325122205,-0.0164641647,0.0321804983,-0.0350777855,0.0016710703,0.0080667534,0.0114057261,-0.0022152348,-0.0154845063,0.0157176562,0.0107363743,-0.0130916618,0.0048010116,0.0007065944,-0.0211773594,-0.0378676659,0.0705758576,0.0538293414,0.0007465103,-0.0116519451,-0.0292081071,0.0078303438,-0.0210536573,-0.0160424518,-0.0131339402,-0.0549034548,0.0064496892,0.0290461261,0.0025281283,0.0094804266,-0.0086954066,-0.0048517851,-0.0673363725,0.0333832026,-0.0189077308,-0.0169693521,-0.0091881834,0.0680565804,0.0120296322,0.025267481,-0.0163928207,-0.0341816859,0.0148047889,-0.0093181011,-0.0253286444,-0.0165303529,0.0222082186,0.064801224,-0.0574056332,-0.001288896,0.0332765002,0.0036782073,0.0157466007,0.0336726217,-0.0160756969,0.030726949,0.0193792048,-0.0495250719,-0.0056675149,-0.0489431764,-0.0417899896,0.0958137382,0.0018420172,-0.0196930859,0.0090951018,0.0065555291,-0.0206775111,0.0347148275,-0.0187469291,0.0209991275,0.005410864,-0.0091352255,-0.0038988689,0.007596769,-0.0558481455,-0.0381436132,0.00727966,0.0226652335,-0.0147880186,-0.0187537292,-0.0693379966,0.0171764226,0.0304071723,-0.0190127834,0.0400662004,0.0249529482,-0.0263078783,-0.0206179164,-0.0386990504,0.0448955356,0.0078357618,-0.0176673013]},"474":{"Abstract":"Visualization and virtual environments (VEs) have been two interconnected parallel strands in visual computing for decades. Some VEs have been purposely developed for visualization applications, while many visualization applications are exemplary showcases in general-purpose VEs. Because of the development and operation costs of VEs, the majority of visualization applications in practice have yet to benefit from the capacity of VEs. In this paper, we examine this status quo from an information-theoretic perspective. Our objectives are to conduct cost-benefit analysis on typical VE systems (including augmented and mixed reality, theater-based systems, and large powerwalls), to explain why some visualization applications benefit more from VEs than others, and to sketch out pathways for the future development of visualization applications in VEs. We support our theoretical propositions and analysis using theories and discoveries in the literature of cognitive sciences and the practical evidence reported in the literatures of visualization and VEs.","Authors":"M. Chen; K. Gaither; N. W. John; B. Mccann","DOI":"10.1109\/TVCG.2018.2865025","Keywords":"Theory of visualization;virtual environments;virtual reality;augmented reality;mixed reality;cost-benefit analysis;information theory;cognitive sciences;visualization applications;immersive analytics;four levels of visualization","Title":"An Information-Theoretic Approach to the Cost-benefit Analysis of Visualization in Virtual Environments","Keywords_Processed":"cognitive science;theory of visualization;virtual reality;visualization application;mixed reality;immersive analytic;information theory;four level of visualization;augmented reality;virtual environment;cost benefit analysis","Keyword_Vector":[0.0204779227,0.0227706072,0.0023684997,0.0004009835,-0.0060830623,-0.0042284792,0.0106930769,0.0038775455,-0.0132189291,-0.0119559334,-0.0157748177,0.0103961583,-0.0081101766,0.0127350929,0.0035964136,-0.0114147925,0.0283439904,-0.0188644918,-0.0087784663,-0.0059573179,0.0123758034,0.0091557513,-0.0052648386,0.0246273166,-0.0058371832,0.0099271276,-0.0173960898,-0.0125220277,-0.004087967,-0.0079517701,0.0288095619,0.002546204,0.031280406,0.0030744029,-0.0053913377,-0.0037263819,0.0119489885,0.010870649,0.0204534187,-0.008504851,0.0368832786,0.0025661398,0.0080680834,-0.0025847162,0.01323918,0.0344423303,0.0036969934,0.0424928681,0.0051630961,-0.0207352352,0.0396330513,0.0141555281],"Abstract_Vector":[0.0995119918,0.0251546276,0.0316438508,-0.0044234545,-0.0162566649,0.02295347,-0.0024578328,0.00879474,-0.0163996584,-0.0205340127,-0.0121124662,0.0221172652,-0.0315123171,-0.0030164777,-0.0123935671,0.0007826826,-0.0059764874,0.0194159291,0.0059981435,-0.0280027245,0.0213122592,0.0037713909,-0.0019260059,0.0265925979,0.0128679824,0.0293855936,0.0341970157,0.0104827038,0.0409357143,-0.0096853506,0.034804153,0.0200300189,-0.0371816984,-0.0202052275,0.0031000323,0.0098783962,-0.006711118,0.0020417334,-0.0138183688,0.0184736449,0.0093012243,-0.0032025224,0.0192106994,-0.0054430051,-0.0234909055,0.0049635014,0.0662864606,0.0137753847,0.0226410265,0.0036155231,-0.0153768346,-0.0260034735,0.0282834234,-0.0112417653,0.0200298237,0.0251761128,-0.0187474831,-0.0269699589,-0.0100702427,0.0011646335,-0.0134828703,0.0088937803,0.0307591555,0.0250665147,-0.0089056007,0.008899045,-0.0104151816,-0.0142630783,0.006468795,-0.0159322536,0.0219910737,0.0096681016,-0.0007512164,0.0014122952,0.0114344992,-0.0038748513,-0.0305568647,0.050205271,-0.0082938229,-0.013956257,-0.0117471441,0.0122847132,-0.0035051049,-0.0067758313,-0.0109520581,0.021698526,-0.0072506859,-0.006873519,0.001036881,-0.0076427156,0.0207687595,0.0104214596,0.0098055367,-0.0020300529,-0.0257137094,-0.0121743658,0.0254179726,-0.0039872269,0.0013197864,0.0169357095,0.0414102505,0.0106870775,0.044045969,-0.0003640308,-0.0139049565,0.0146723282,0.0130080668,0.0151213049,-0.0181886662,0.029237227,0.0110986716,-0.0375151348,-0.0047413345,0.007563786,-0.0245678433,0.0317697416]},"475":{"Abstract":"In this paper, we present story curves, a visualization technique for exploring and communicating nonlinear narratives in movies. A nonlinear narrative is a storytelling device that portrays events of a story out of chronological order, e.g., in reverse order or going back and forth between past and future events. Many acclaimed movies employ unique narrative patterns which in turn have inspired other movies and contributed to the broader analysis of narrative patterns in movies. However, understanding and communicating nonlinear narratives is a difficult task due to complex temporal disruptions in the order of events as well as no explicit records specifying the actual temporal order of the underlying story. Story curves visualize the nonlinear narrative of a movie by showing the order in which events are told in the movie and comparing them to their actual chronological order, resulting in possibly meandering visual patterns in the curve. We also present Story Explorer, an interactive tool that visualizes a story curve together with complementary information such as characters and settings. Story Explorer further provides a script curation interface that allows users to specify the chronological order of events in movies. We used Story Explorer to analyze 10 popular nonlinear movies and describe the spectrum of narrative patterns that we discovered, including some novel patterns not previously described in the literature. Feedback from experts highlights potential use cases in screenplay writing and analysis, education and film production. A controlled user study shows that users with no expertise are able to understand visual patterns of nonlinear narratives using story curves.","Authors":"N. W. Kim; B. Bach; H. Im; S. Schriber; M. Gross; H. Pfister","DOI":"10.1109\/TVCG.2017.2744118","Keywords":"Nonlinear narrative;storytelling;visualization","Title":"Visualizing Nonlinear Narratives with Story Curves","Keywords_Processed":"visualization;nonlinear narrative;storytelle","Keyword_Vector":[0.0362617459,0.0152698201,-0.0132683794,-0.0025566054,0.0205396477,-0.016935101,-0.0021375697,0.0051391843,-0.0264525548,-0.0385487684,0.0078067135,-0.0003386228,0.0083412897,0.0215598811,0.0234080251,0.01504291,-0.0078738981,-0.0722339184,0.0588658799,-0.030915591,0.0153679719,-0.039846391,-0.0453255215,0.0502766535,0.0097573202,-0.0024521042,0.0932540511,0.0523271777,-0.059905255,0.0814162131,-0.0097915946,0.0227773376,-0.0013694746,-0.0722101928,0.0110055842,0.0698806336,-0.0478860562,-0.031022408,-0.038501853,-0.0265163855,0.0357018992,0.0035646773,0.0037340468,0.0132725194,-0.0383933299,-0.0357778145,0.0032415811,-0.0096464197,0.0516455714,-0.0005285934,-0.0529484145,-0.0015406836],"Abstract_Vector":[0.199040881,-0.0475537058,-0.012533984,-0.0193273752,-0.0358402148,0.0661954812,-0.0132112363,0.007554748,0.0788487679,0.0008985958,-0.0119020856,0.0328246106,-0.0850102759,0.022860846,-0.0096913145,-0.0418108634,0.029677643,0.048144114,-0.0248265525,-0.043671324,-0.0164015975,0.04591561,-0.0202204051,0.0019537005,0.0142691206,-0.0329695406,0.0197270449,0.0013821029,-0.0455464442,-0.0355122518,-0.0208461376,-0.0154014731,0.0131605594,0.0036613984,-0.0282926135,-0.0439494973,-0.004477317,-0.068007414,0.0098665318,-0.0373488865,-0.0114839763,0.0096640317,0.0007048585,0.0015878245,-0.0164072308,-0.0636062791,0.0250475018,0.0215500628,-0.0090866451,-0.0041469445,-0.0098677024,-0.0298007666,0.0032177062,0.0050358781,-0.0208378269,0.0143861944,0.0028659496,0.0638510812,0.0647520182,-0.0311638807,0.0098381128,-0.0069601722,0.0402413697,-0.0496757545,-0.0191459536,-0.0162572067,-0.0081228408,-0.0307911328,-0.0515826766,0.035065026,0.0159248258,0.0453059347,-0.0076250913,-0.0019327743,0.0362635318,0.0285327994,0.0197736689,0.0676386938,0.0052112938,-0.0245341347,-0.0044720274,0.0666504916,0.0477691069,0.0163595614,0.0470059471,0.0134794787,0.0700345361,0.0760382824,0.0081551068,-0.0006825652,-0.0096407343,-0.0006373907,-0.0366433764,0.0067766485,0.0407720703,-0.0120435744,-0.0507244803,0.0025283328,0.0024803185,0.058996883,-0.0096096123,-0.0259590257,-0.0113072709,-0.0115686178,0.03256494,0.0181260877,0.0044569856,-0.0284333281,0.0125427218,0.0210836868,0.0236665393,0.0569206719,-0.0240474463,-0.0049977158,-0.0026644984,-0.0264344922]},"476":{"Abstract":"We address the problem of visualizing multivariate correlations in parallel coordinates. We focus on multivariate correlation in the form of linear relationships between multiple variables. Traditional parallel coordinates are well prepared to show negative correlations between two attributes by distinct visual patterns. However, it is difficult to recognize positive correlations in parallel coordinates. Furthermore, there is no support to highlight multivariate correlations in parallel coordinates. In this paper, we exploit the indexed point representation of p -flats (planes in multidimensional data) to visualize local multivariate correlations in parallel coordinates. Our method yields clear visual signatures for negative and positive correlations alike, and it supports large datasets. All information is shown in a unified parallel coordinates framework, which leads to easy and familiar user interactions for analysts who have experience with traditional parallel coordinates. The usefulness of our method is demonstrated through examples of typical multidimensional datasets.","Authors":"L. Zhou; D. Weiskopf","DOI":"10.1109\/TVCG.2017.2698041","Keywords":"Multidimensional data visualization;multivariate correlations;parallel coordinates","Title":"Indexed-Points Parallel Coordinates Visualization of Multivariate Correlations","Keywords_Processed":"multidimensional datum visualization;multivariate correlation;parallel coordinate","Keyword_Vector":[0.0496142372,0.0369671214,0.0009858454,0.0188386677,0.030401659,-0.0420997479,-0.0057227426,-0.0290310009,-0.0234290214,-0.0473584454,0.0166218609,0.0199771624,0.0849145226,0.016019678,0.0507060273,-0.0187368533,0.003184775,-0.0220550084,0.0093515675,0.0087801799,0.0334691705,-0.100624898,-0.0171563095,0.034868438,-0.0168889433,0.0026655028,0.1161621383,0.0636978316,-0.0396337467,0.0466950099,-0.0182411799,-0.0840289015,0.0109439234,-0.0251225544,0.0759969446,0.0190657891,0.0077877265,0.0412665565,0.0593081021,0.0067550188,0.0291091172,0.0340352084,-0.0197189031,0.0192743402,0.0098545206,-0.1032482104,-0.0607451424,0.0187078559,0.0630975596,-0.0124241634,-0.0192625933,0.0422595763],"Abstract_Vector":[0.2099863618,0.1238739735,-0.064475536,-0.0091771579,-0.0587030739,0.1161005841,0.0561783871,0.0540518384,0.1013841636,-0.0599969696,-0.0145695772,-0.0463736676,-0.0940668722,0.0596777726,0.0455506545,-0.0879596768,-0.0174594458,-0.1477420707,-0.0717100427,0.0010287117,0.1278136013,0.1202256915,-0.0399693624,-0.0186679371,-0.0539050948,-0.0478092877,-0.0756911724,0.0217272787,-0.0104599039,-0.0511045574,0.0120818927,0.1636187045,0.0183998296,0.0539482647,0.0172592977,-0.0605721718,0.0119042359,-0.0089654981,0.030916927,-0.0246657129,0.0801014161,0.0238611517,0.0342546426,0.0646910917,-0.0566707115,-0.0131369399,0.0155095329,-0.0702549089,0.0583717545,0.0378442207,-0.0411016764,0.0603319141,0.0291107384,-0.0245406731,-0.0182596434,0.015485292,-0.036478682,-0.0072892636,0.0419840159,0.0387871677,0.0935359659,-0.0248068545,-0.0197856082,-0.0153873237,0.0130098983,0.0056330815,0.0094643034,-0.012291365,-0.0278548122,0.0150707217,0.0361281462,0.0668111043,0.0065718738,-0.0396531968,-0.0002983771,-0.0049382596,-0.0571615659,-0.0302420515,0.0122185636,0.0742923792,0.0153588453,-0.0250730215,-0.0417711892,-0.0352709662,-0.0128686321,-0.032613985,0.0956980339,-0.0266550202,-0.007856029,-0.0067975874,-0.0660225888,-0.0121701191,0.044514041,0.0145201202,0.0095199842,0.0435533757,0.0311309176,0.0018161878,-0.007025481,-0.0087250296,0.0300921992,-0.0281204675,0.0656066519,0.001319452,0.000012277,0.0056839444,-0.0253611958,-0.0220523317,0.0195069033,-0.0523807853,-0.0273337296,-0.0201507942,-0.0088514891,-0.0131055272,-0.0278426894,0.02941352]},"477":{"Abstract":"In this paper we move beyond memorability and investigate how visualizations are recognized and recalled. For this study we labeled a dataset of 393 visualizations and analyzed the eye movements of 33 participants as well as thousands of participant-generated text descriptions of the visualizations. This allowed us to determine what components of a visualization attract people's attention, and what information is encoded into memory. Our findings quantitatively support many conventional qualitative design guidelines, including that (1) titles and supporting text should convey the message of a visualization, (2) if used appropriately, pictograms do not interfere with understanding and can improve recognition, and (3) redundancy helps effectively communicate the message. Importantly, we show that visualizations memorable \u201cat-a-glance\u201d are also capable of effectively conveying the message of the visualization. Thus, a memorable visualization is often also an effective one.","Authors":"M. A. Borkin; Z. Bylinskii; N. W. Kim; C. M. Bainbridge; C. S. Yeh; D. Borkin; H. Pfister; A. Oliva","DOI":"10.1109\/TVCG.2015.2467732","Keywords":"Information visualization;memorability;recognition;recall;eye-tracking study;Information visualization;memorability;recognition;recall;eye-tracking study","Title":"Beyond Memorability: Visualization Recognition and Recall","Keywords_Processed":"recognition;recall;eye tracking study;memorability;information visualization","Keyword_Vector":[0.0202735505,0.001601934,0.0379648948,-0.0072741319,-0.0181745644,-0.0152162378,-0.0236604465,-0.0055711455,-0.0027674676,-0.074284295,-0.0170128973,0.0157858001,-0.0150181821,-0.0245987888,-0.0133133945,-0.0030625285,0.0608947379,-0.0096273849,0.0139801909,-0.0021058861,0.0353089286,0.0279917929,-0.0231898767,0.0255439849,-0.0378540734,0.0130075596,-0.0391018704,0.0216838577,-0.0513764998,0.0270697695,-0.0237177135,-0.0136004728,0.0420609438,-0.035060377,-0.0128789991,0.0058666297,-0.0184678442,-0.0758306548,-0.0173704134,0.0442568593,-0.0363617462,0.0146610194,-0.0338024777,0.0619530456,-0.044303994,-0.0541539654,-0.0209264816,0.0319345496,0.004163863,0.0225812443,0.0335012998,0.0290762255],"Abstract_Vector":[0.1662949581,0.0483607727,0.1091219345,-0.0295165959,-0.0209452038,0.012218201,0.0746074514,0.0132301753,-0.0297410732,-0.0287814275,0.0111072485,0.0138813449,-0.0242012642,-0.0172565927,-0.0238187838,0.0414409671,0.0234179309,0.1018982414,-0.0240277256,-0.0933956351,0.0476834529,-0.0829723679,0.0323606803,0.0191710482,0.0804439412,0.0274809741,0.0189830177,0.0210100474,0.050935178,-0.0196270938,0.0244048444,0.0414594091,-0.040205252,0.0411519121,0.0481521763,-0.0075486089,0.0311067881,-0.0033955166,-0.014550128,-0.0374459814,0.0128656634,-0.0111697653,0.0166501559,0.0085954956,-0.0827656003,0.0052830007,0.0329013173,0.0072087078,-0.022397672,0.0090342413,0.0082604825,-0.0421537733,0.0257573832,0.0191777234,0.0084175062,0.0234875959,-0.0288382378,0.0117955831,0.0428228328,0.0312038149,-0.0166307617,-0.0002109462,0.016526793,0.0228097211,-0.0299306536,0.0189465828,0.0032934152,0.0118820164,-0.0029640297,0.0423344875,0.0147542566,-0.0073652721,0.0363278462,0.0124756638,0.0023039814,-0.0026674587,0.0108615163,0.0089185805,0.0318998448,-0.0324351783,-0.0010904927,0.0027253339,0.0124571068,-0.0356396423,-0.0456013194,0.022962268,0.0068231077,-0.0078447072,0.0021948332,-0.0018145856,0.0433996739,0.0153015078,-0.0127707626,0.0110331799,0.0043445458,-0.0037499921,0.0083432042,-0.0027039623,0.0061322208,0.0174804819,0.032483424,-0.00213216,0.0133759555,0.0248364773,-0.0089035344,0.0293334426,-0.0156587392,-0.0445320884,-0.0037426137,0.0078055524,-0.0232134817,-0.0175022973,-0.0132586114,-0.0011296743,0.0264242641,-0.015332989]},"478":{"Abstract":"Unified simulation of versatile elastoplastic materials and different dimensions offers many advantages in animation production, contact handling, and hardware acceleration. The unstructured particle representation is particularly suitable for this task, thanks to its simplicity. However, previous meshless techniques either need too much computational cost for addressing stability issues, or lack physical meanings and fail to generate interesting deformation behaviors, such as the Poisson effect. In this paper, we study the development of an elastoplastic model under the state-based peridynamics framework, which uses integrals rather than partial derivatives in its formulation. To model elasticity, we propose a unique constitutive model and an efficient iterative simulator solved in a projective dynamics way. To handle plastic behaviors, we incorporate our simulator with the Drucker-Prager yield criterion and a reference position update scheme, both of which are implemented under peridynamics. Finally, we show how to strengthen the simulator by position-based constraints and spatially varying stiffness models, to achieve incompressibility, particle redistribution, cohesion, and friction effects in viscoelastic and granular flows. Our experiments demonstrate that our unified, meshless simulator is flexible, efficient, robust, and friendly with parallel computing.","Authors":"X. He; H. Wang; E. Wu","DOI":"10.1109\/TVCG.2017.2755646","Keywords":"Peridynamics;projective dynamics;position-based dynamics;elasticity;plasticity;viscoelasticity;granular flows","Title":"Projective Peridynamics for Modeling Versatile Elastoplastic Materials","Keywords_Processed":"granular flow;projective dynamic;viscoelasticity;peridynamic;position base dynamic;elasticity;plasticity","Keyword_Vector":[0.0929017749,-0.0791817772,-0.0501243988,0.0002653353,-0.034620624,-0.0923541907,0.0247179328,0.0034377719,0.0286521443,-0.0092594151,0.0089824206,0.0040429041,-0.0268838901,-0.0269691114,-0.0051313684,0.0400113975,-0.0449027275,0.0139620424,-0.0186944869,0.0195964905,0.0116111699,-0.0074968538,0.0258760916,-0.0323645932,0.0205234609,0.0065194968,0.0118012343,0.0154217147,-0.0134589609,-0.0450027251,0.0153510641,0.019892516,0.0438580631,0.0014599507,0.0030687349,-0.0232621415,0.0287545389,-0.0023366196,-0.0266846149,-0.05020789,-0.0279107708,0.0144612048,0.0356968418,-0.0224808384,-0.0147065744,-0.0337927039,-0.0547734121,0.0008669884,0.0374657625,-0.0269525044,0.0470421455,0.0137173346],"Abstract_Vector":[0.2035315291,-0.1431243034,0.0094994116,0.008486583,-0.0710611765,0.0136242671,0.0384168164,0.0220058563,-0.0774156757,0.033212261,-0.0355944673,0.0093641803,-0.068833558,0.0033994465,-0.009176886,0.0183932519,-0.0064049073,-0.0128554275,-0.0153077298,0.0275049522,0.0096608038,-0.0184149352,0.0167800821,-0.0186325156,-0.0240023993,0.0025381777,-0.0346809588,-0.0066307912,-0.0174183751,-0.093692201,-0.0356130709,0.0130110487,-0.0564597235,-0.003096313,-0.0002361136,0.0168984772,-0.0176131608,-0.007231095,0.0059488616,-0.0090961627,-0.0080022373,-0.0268769079,-0.0355544784,-0.014499571,-0.0108620582,0.0223170737,-0.0128593776,-0.0072051471,0.0033883785,0.0089579817,0.0144320374,0.0158267033,-0.0140992675,0.0417793109,-0.005803873,0.0025979278,0.0082498154,-0.0172887684,0.0155024153,-0.0219466318,0.0330812223,-0.0001123766,-0.0119508249,0.0287617223,0.0080398151,0.0766001338,0.0051275507,-0.0294773644,0.0046621888,0.0385871575,-0.0364391772,0.0144638628,0.0068040979,-0.0040324525,0.0089063305,0.0108280406,-0.0007271488,0.030427074,0.0058223415,-0.0338703815,-0.0079659283,0.0083044861,-0.0057326475,0.0096876783,-0.0397322009,-0.0282892803,-0.0354995453,0.0350786577,0.0213271402,-0.0048206742,-0.0216141657,-0.0396524262,0.0048947998,0.0096675583,-0.0035636609,-0.0195266637,0.0311313849,-0.0372471431,0.0353646872,0.0034429313,0.0198881288,-0.0061011896,0.0116113934,0.0078276318,0.0386636313,-0.0220418611,0.0115054705,0.0122518179,0.0039394684,0.0022120548,0.0066728679,0.0368569453,-0.0163144354,-0.0276456713,0.0133103243,0.0304993618]},"479":{"Abstract":"Urban forms at human-scale, i.e., urban environments that individuals can sense (e.g., sight, smell, and touch) in their daily lives, can provide unprecedented insights on a variety of applications, such as urban planning and environment auditing. The analysis of urban forms can help planners develop high-quality urban spaces through evidence-based design. However, such analysis is complex because of the involvement of spatial, multi-scale (i.e., city, region, and street), and multivariate (e.g., greenery and sky ratios) natures of urban forms. In addition, current methods either lack quantitative measurements or are limited to a small area. The primary contribution of this work is the design of StreetVizor, an interactive visual analytics system that helps planners leverage their domain knowledge in exploring human-scale urban forms based on street view images. Our system presents two-stage visual exploration: 1) an AOI Explorer for the visual comparison of spatial distributions and quantitative measurements in two areas-of-interest (AOIs) at city- and region-scales; 2) and a Street Explorer with a novel parallel coordinate plot for the exploration of the fine-grained details of the urban forms at the street-scale. We integrate visualization techniques with machine learning models to facilitate the detection of street view patterns. We illustrate the applicability of our approach with case studies on the real-world datasets of four cities, i.e., Hong Kong, Singapore, Greater London and New York City. Interviews with domain experts demonstrate the effectiveness of our system in facilitating various analytical tasks.","Authors":"Q. Shen; W. Zeng; Y. Ye; S. M. Arisona; S. Schubiger; R. Burkhard; H. Qu","DOI":"10.1109\/TVCG.2017.2744159","Keywords":"Urban forms;human scale;street view;visual analytics","Title":"StreetVizor: Visual Exploration of Human-Scale Urban Forms Based on Street Views","Keywords_Processed":"urban form;human scale;visual analytic;street view","Keyword_Vector":[0.1770724516,-0.0108964642,0.0059715434,-0.1287391387,0.07120562,0.0664948533,-0.0166725506,0.1310213929,0.2813407425,0.0752692052,0.2066902655,0.2240461053,-0.0924475948,0.1045966366,-0.0303187575,-0.1333707316,0.0864075198,0.1125445038,0.0664033654,-0.159081403,0.0598385165,-0.0114706588,-0.0646589444,0.0379708082,0.038456358,0.0051632076,-0.0150539301,0.0254644498,0.0265211855,0.046169202,0.020845463,0.0179362914,0.00101307,0.0869667079,-0.018403602,0.0638510468,0.0437017938,0.0078099138,-0.0411387073,0.0223626666,0.0306875421,0.1248004651,0.0368587812,-0.0733720008,-0.0461136917,-0.0815532013,0.0252317407,0.0081340072,0.0040023712,0.0756596459,0.0517850147,-0.1069708579],"Abstract_Vector":[0.1167397588,-0.0343460522,0.0142833087,0.0085154931,-0.0461949076,0.0096367779,-0.0067689303,0.0081928649,-0.0154025349,-0.0435969836,0.0145864624,0.0180004885,-0.0181718385,-0.0135507281,-0.0168653687,0.0275170224,0.0096561199,0.012098094,0.0057799546,-0.0661920091,0.0212342537,0.0121366414,0.0116037699,0.0266235608,-0.0027884171,0.0034736906,0.0379682304,0.0223917687,0.0260236436,0.0224210599,-0.0173560691,0.0183174441,-0.0287030939,-0.0034190533,-0.0242021623,-0.0120963798,-0.003483757,-0.0141805271,0.0278380317,0.006112726,-0.0201405791,0.0028276744,-0.0054309555,0.0175951389,-0.01541683,0.0171072446,0.032723471,0.030508541,0.0221640142,-0.0061846499,-0.0277799677,0.007375876,0.0220603927,0.0138147781,0.0075993551,-0.0132208336,0.0155249881,-0.0075437722,0.0305120222,0.0248171435,0.0033933748,-0.0253022674,-0.0421375582,-0.0048409007,-0.0085244649,0.0142544495,0.0263939038,0.0125036481,-0.0138749592,-0.0013562706,-0.0361774468,-0.0001739932,0.0006268319,0.0161239864,-0.0187827787,0.0493045804,-0.0012324317,0.003251917,-0.0421475116,-0.0208973667,0.0161454618,-0.0002632578,0.0177469081,-0.0025109692,0.020466813,0.0415868993,0.0179890681,0.0220505471,-0.0034021275,-0.0193366035,-0.0119232917,-0.017858768,-0.0294343538,-0.0025370773,-0.0120974467,0.0221407751,0.0040899753,-0.0091556937,-0.0085274392,-0.0184505919,0.0167985218,-0.0196532391,0.0290225471,-0.0241406261,0.0499383594,-0.0149004802,-0.0025071833,-0.0297515838,-0.0077842523,0.003816887,0.0124035595,-0.0017092196,0.0186167476,0.0176515806,-0.0113092999,-0.0198600266]},"48":{"Abstract":"Wayfinding signs play an important role in guiding users to navigate in a virtual environment and in helping pedestrians to find their ways in a real-world architectural site. Conventionally, the wayfinding design of a virtual environment is created manually, so as the wayfinding design of a real-world architectural site. The many possible navigation scenarios, as well as the interplay between signs and human navigation, can make the manual design process overwhelming and non-trivial. As a result, creating a wayfinding design for a typical layout can take months to several years. In this paper, we introduce the Way to Go! approach for automatically generating a wayfinding design for a given layout. The designer simply has to specify some navigation scenarios; our approach will automatically generate an optimized wayfinding design with signs properly placed considering human agents' visibility and possibility of making mistakes during a navigation. We demonstrate the effectiveness of our approach in generating wayfinding designs for different layouts such as a train station, a downtown and a canyon. We evaluate our results by comparing different wayfinding designs and show that our optimized wayfinding design can guide pedestrians to their destinations effectively and efficiently. Our approach can also help the designer visualize the accessibility of a destination from different locations, and correct any \u201cblind zone\u201d with additional signs.","Authors":"H. Huang; N. Lin; L. Barrett; D. Springer; H. Wang; M. Pomplun; L. Yu","DOI":"10.1109\/TVCG.2017.2761820","Keywords":"Wayfinding;navigation;procedural modeling;level design;spatial orientation","Title":"Automatic Optimization of Wayfinding Design","Keywords_Processed":"level design;navigation;wayfinde;spatial orientation;procedural modeling","Keyword_Vector":[0.0547545851,0.0023153376,-0.0177985346,-0.0224205323,0.0537878543,-0.0114905823,0.0441854691,-0.0333374714,-0.0608125395,-0.0170058177,-0.0113963029,0.0148942169,0.0545116206,0.025083458,0.0500255239,-0.0264072335,0.0151164305,0.0660803835,0.0181585321,0.0621733364,0.1122385127,0.0360972067,-0.0450493971,0.0858784217,0.0155214548,0.0199569156,0.0080181924,-0.0483375169,-0.1015947776,0.0289618922,0.0759348156,-0.0307667453,0.0649234553,0.08387455,0.0441608476,-0.07956482,-0.0021134937,0.0407481876,-0.014646738,-0.05818499,-0.0205053922,0.0287957711,0.022729642,-0.0315318939,0.0099581014,0.0224865446,0.023788249,-0.1648169353,-0.0229108003,-0.0390355312,0.0411777617,-0.0524018304],"Abstract_Vector":[0.1217366961,-0.001115362,-0.0203120426,0.0216327923,0.0324877037,-0.0325260633,-0.0213523081,0.0003373729,0.0351695728,-0.0328298895,0.0160770412,0.0236317673,-0.0182047714,-0.0418429401,0.0456033024,-0.019588004,-0.0600473118,0.0129119367,-0.0303024799,-0.026349948,0.0038988212,-0.0064496841,0.0013589565,0.0204168427,-0.0188646507,-0.0055297532,0.0294512853,0.0087708546,-0.0139735346,0.0069735917,-0.0113412706,0.0178297654,-0.0060685146,0.037359341,-0.0211443203,0.0004692436,0.03190229,0.0150000572,-0.0127964361,0.002856804,-0.0032234591,0.0206685082,0.0244086131,-0.0091215663,-0.0062417454,-0.0119182918,0.0338913188,-0.0219583639,0.0246400648,-0.0357465243,-0.0199675368,0.0049740927,0.0005001478,-0.0424483996,0.036230656,-0.0227309902,-0.0203404071,-0.0058664433,0.0165994383,-0.0428488639,0.0042414777,0.005050248,-0.0243214213,-0.0023382992,0.0029705341,0.0244931687,-0.0271982991,-0.0313463106,-0.001104297,0.021949781,0.0159273764,-0.0259102232,0.0088661935,0.0568886497,0.0153107316,-0.005295884,0.0278900966,-0.0329604209,-0.0110488416,-0.0594668023,0.0570776322,0.0133586216,-0.019875678,0.0035327748,0.0343606859,-0.0151620062,-0.0370301859,0.0133182667,-0.0078467105,0.0134260353,-0.0244486891,-0.0086057197,-0.0121261712,-0.0210174588,-0.0000874953,0.0270936885,-0.0081963139,-0.0248486524,-0.0045831869,-0.004455504,-0.0264712847,-0.0303999889,-0.0022607519,0.0317565207,-0.0007929766,-0.0084565843,-0.0320949908,-0.0013738812,-0.0083696563,-0.0253588405,0.0215931331,-0.0283591258,-0.0038790077,0.0179204899,0.0039000959,0.0336151403]},"480":{"Abstract":"Network visualizations, often in the form of node-link diagrams, are an effective means to understand relationships between entities, discover entities with interesting characteristics, and to identify clusters. While several existing tools allow users to visualize pre-defined networks, creating these networks from raw data remains a challenging task, often requiring users to program custom scripts or write complex SQL commands. Some existing tools also allow users to both visualize and model networks. Interaction techniques adopted by these tools often assume users know the exact conditions for defining edges in the resulting networks. This assumption may not always hold true, however. In cases where users do not know much about attributes in the dataset or when there are several attributes to choose from, users may not know which attributes they could use to formulate linking conditions. We propose an alternate interaction technique to model networks that allows users to demonstrate to the system a subset of nodes and links they wish to see in the resulting network. The system, in response, recommends conditions that can be used to model networks based on the specified nodes and links. In this paper, we show how such a demonstration-based interaction technique can be used to model networks by employing it in a prototype tool, Graphiti. Through multiple usage scenarios, we show how Graphiti not only allows users to model networks from a tabular dataset but also facilitates updating a pre-defined network with additional edge types.","Authors":"A. Srinivasan; H. Park; A. Endert; R. C. Basole","DOI":"10.1109\/TVCG.2017.2744843","Keywords":"Network modeling;visual analytics;user interaction","Title":"Graphiti: Interactive Specification of Attribute-Based Edges for Network Modeling and Visualization","Keywords_Processed":"user interaction;network modeling;visual analytic","Keyword_Vector":[0.1498287359,-0.035462703,-0.017712486,0.0151453378,-0.0153835713,0.0520922981,-0.0698372635,-0.0470015235,-0.0567579909,-0.0387170574,0.0000540346,0.0095914078,0.0478194768,-0.0153538597,0.0377914024,-0.0051193159,-0.0228887526,0.0811075171,-0.0121630912,0.047086735,0.0473741395,-0.0390291716,-0.0289155488,0.0513484087,0.0327450113,-0.0378252568,-0.0774511912,0.018434964,-0.0181867412,0.0436465134,0.0580007645,0.0723473025,0.0341833367,0.0587009928,-0.0140822781,-0.001962578,-0.0881292276,0.0080441255,-0.0340773831,-0.1141134357,0.0075485548,0.0016222902,-0.0327207823,-0.1056836384,0.0302183332,-0.0051531761,0.0719653858,-0.1032424921,0.0109011472,0.0383247183,0.0351302757,0.0508595731],"Abstract_Vector":[0.134431483,-0.0883652893,0.000147734,-0.003030286,-0.0316923827,0.0176109935,0.0046585302,0.0101128888,-0.0112766421,-0.0192140967,-0.0180608111,0.0278549242,-0.0250461811,-0.0130905582,0.0197015492,-0.0018694963,-0.0175192253,-0.0716633783,0.0124102977,-0.0030735178,0.0432682631,-0.0381646032,-0.018541644,-0.0391407981,-0.012329629,0.0026255313,0.013660257,-0.0362658985,-0.0165497507,-0.0089762794,0.0421094385,-0.0054198632,-0.0248695991,0.0150764401,-0.0282529173,-0.0293212977,0.0251721803,-0.0242245962,0.0258330758,-0.0197747769,0.0283184699,0.0124627913,-0.0486748602,-0.0151528473,-0.0320679162,-0.0181643684,-0.007878684,-0.0346291783,-0.0018384336,0.0149070824,-0.013613138,0.0342646059,-0.0148802007,-0.0174756066,0.0103786947,0.0015502162,0.0258992258,0.0116288796,0.0027354575,-0.0005139906,-0.0078604707,0.049607582,0.0448313938,-0.020947232,-0.0347784288,-0.0328190511,0.0372490968,-0.0139717026,-0.016682995,-0.0222318762,0.0131456369,-0.0163564489,-0.0158451088,0.00020876,-0.0234446392,0.0040994838,0.0037460549,0.0164543849,0.0019120923,-0.0315759848,0.0286901451,0.0205825521,-0.0146739054,-0.0428629357,0.0341667357,-0.0000740436,0.0062973915,0.0327962389,-0.0054033724,-0.0043592536,-0.0052566259,-0.0069736198,0.0306545403,0.0101079674,0.007729409,0.0079012586,-0.0344067755,-0.0009134268,0.0037560498,-0.0074922369,-0.0231064117,0.0085466647,0.0011459418,0.0225176621,0.0095479826,0.0115596862,-0.0193231286,-0.0159051182,-0.0042393562,0.0207596246,-0.0197185574,0.0097873221,0.0308737758,-0.0190748873,-0.0101079463,-0.0255491248]},"481":{"Abstract":"Data are often viewed as a single set of values, but those values frequently must be compared with another set. The existing evaluations of designs that facilitate these comparisons tend to be based on intuitive reasoning, rather than quantifiable measures. We build on this work with a series of crowdsourced experiments that use low-level perceptual comparison tasks that arise frequently in comparisons within data visualizations (e.g., which value changes the most between the two sets of data?). Participants completed these tasks across a variety of layouts: overlaid, two arrangements of juxtaposed small multiples, mirror-symmetric small multiples, and animated transitions. A staircase procedure sought the difficulty level (e.g., value change delta) that led to equivalent accuracy for each layout. Confirming prior intuition, we observe high levels of performance for overlaid versus standard small multiples. However, we also find performance improvements for both mirror symmetric small multiples and animated transitions. While some results are incongruent with common wisdom in data visualization, they align with previous work in perceptual psychology, and thus have potentially strong implications for visual comparison designs.","Authors":"B. Ondov; N. Jardine; N. Elmqvist; S. Franconeri","DOI":"10.1109\/TVCG.2018.2864884","Keywords":"Graphical perception;visual perception;visual comparison;crowdsourced evaluation","Title":"Face to Face: Evaluating Visual Comparison","Keywords_Processed":"visual comparison;visual perception;graphical perception;crowdsource evaluation","Keyword_Vector":[0.1001199379,-0.0810999004,-0.0417201348,0.0209197073,-0.00544395,0.0802854174,-0.0651534926,-0.0355722966,-0.0582679324,0.0109385157,-0.0314973936,-0.0162082416,0.011433456,0.0041159899,-0.0492771701,-0.0148582014,-0.1294299275,0.0083956055,-0.0406170974,-0.05082503,-0.0170545208,-0.0933614362,0.0648393089,-0.0712780292,-0.0573005436,-0.057856075,-0.0380808068,0.0751000217,-0.0008868412,-0.0210918815,0.0407146977,0.1050783095,0.0657478489,0.0780459725,-0.0321922128,0.039556642,-0.0365061532,0.0451690446,-0.0499829242,-0.0082321611,0.0073044477,0.0421049212,0.0348005689,0.1678523195,-0.0010829071,-0.0286756851,-0.0030941934,0.0396612901,0.0972102399,-0.0627448121,0.1530135287,0.0405996654],"Abstract_Vector":[0.1086345578,-0.0375453531,-0.0098840877,0.0137154627,0.011909006,-0.0204363588,-0.0361144158,0.0073753445,0.0388712499,-0.0304203919,-0.0100089242,0.0152588765,-0.000782987,0.0102643828,0.0080503368,-0.013326122,-0.0252096724,-0.0272530415,-0.0070294038,0.0199518112,0.0269307211,0.0104367667,0.0015735279,-0.0384520356,0.0167691344,0.0542369024,0.0192608091,0.0099688792,0.004823244,0.0199064902,-0.0206931672,-0.0124579952,0.0306171191,0.013825991,-0.0100883923,-0.0318049925,0.0263209396,0.0227367257,-0.0065685698,-0.0272742091,0.0015337633,0.038154052,0.0389247787,0.0192218119,-0.018812322,0.0247211645,0.0153837677,0.0002642416,-0.0034070614,0.0231481448,-0.0025123188,-0.0121716703,0.0187862885,-0.0168643378,0.0064782506,-0.0259886414,-0.0153255083,0.0617485941,0.0094670443,0.0141307273,0.0274850694,-0.0113293982,-0.0322680927,0.0041311534,-0.0063723669,0.0013123304,-0.0180198158,0.0055160096,0.0114089656,0.0402185887,-0.0029550746,-0.0142600491,-0.0048915791,-0.0070603551,-0.0105426557,0.0307763017,0.0035626966,-0.0122240788,-0.0048069359,-0.0082309181,-0.0067215032,0.0032921826,0.0252268477,-0.0329196226,0.0121693656,-0.0024802573,0.0164213249,0.0046628396,0.028718525,-0.0158421203,-0.0101266642,-0.0351436895,0.0056230824,0.0182175999,0.0227238336,-0.0246710996,0.0090129349,-0.0026165375,0.0104289433,-0.0204735371,0.0324399898,-0.0206438992,0.014699038,-0.0230533547,0.0250760051,-0.0049155565,0.014806511,0.0349676895,-0.0347212159,-0.000298052,0.0156139519,0.0061664162,0.0368896372,-0.003617047,0.0323379024,0.0001928696]},"482":{"Abstract":"Generating visualizations at the size of a word creates dense information representations often called sparklines. The integration of word-sized graphics into text could avoid additional cognitive load caused by splitting the readers' attention between figures and text. In scientific publications, these graphics make statements easier to understand and verify because additional quantitative information is available where needed. In this work, we perform a literature review to find out how researchers have already applied such word-sized representations. Illustrating the versatility of the approach, we leverage these representations for reporting empirical and bibliographic data in three application examples. For interactive Web-based publications, we explore levels of interactivity and discuss interaction patterns to link visualization and text. We finally call the visualization community to be a pioneer in exploring new visualization-enriched and interactive publication formats.","Authors":"F. Beck; D. Weiskopf","DOI":"10.1109\/TVCG.2017.2674958","Keywords":"Sparklines;word-sized graphics;literature survey;text and visualization;interactive documents;scientific publishing","Title":"Word-Sized Graphics for Scientific Texts","Keywords_Processed":"text and visualization;interactive document;scientific publishing;word sized graphic;sparkline;literature survey","Keyword_Vector":[0.1988820336,-0.0971449029,-0.023660597,0.0023358162,-0.0334957443,0.0811855444,-0.0766125475,-0.0482237376,-0.0853990487,0.0221183814,-0.0333217428,0.0666944825,0.0222932921,0.0946164595,-0.1281948043,0.0052821583,-0.0761003803,0.0440720426,-0.0652376732,-0.0155528484,0.0482395662,0.0085120345,-0.0003042741,-0.0267034313,-0.04944987,0.0713324586,0.0241267008,0.0366330793,0.0315103896,-0.0458663822,-0.0526224145,0.0147166396,-0.0342899101,-0.0191891063,0.0568602258,-0.1087955307,0.0745586859,-0.0503130349,-0.0194600828,-0.0891118644,0.0396460789,0.0161440726,0.0329261877,0.0097063075,0.0080373693,0.0729448716,-0.0199351824,0.0744315183,0.0371163142,-0.0012943589,0.0846455328,0.0177931733],"Abstract_Vector":[0.2172865762,-0.1151747798,0.0185776309,-0.0247118939,-0.090959568,0.0278450351,0.056461952,0.0361486163,-0.0322273765,0.010694433,-0.0072716045,0.0516106087,-0.0023743233,-0.0054280719,0.0016399454,0.0062006357,0.006210821,0.0239659434,0.0038950862,0.0196976399,-0.0060773149,-0.0181650235,-0.0225339199,-0.0338845007,-0.0252104425,0.0154212427,-0.047539532,0.0472606822,0.0122348158,0.0054029894,-0.0213301721,0.0010166918,-0.011136044,0.0023552873,0.0068232991,0.0190168671,-0.0164321777,0.018715636,-0.0048473732,0.0172080084,0.0249961088,-0.0065373464,-0.0058900562,0.0006597664,0.018930147,0.0252640939,0.0257265831,0.0082397429,0.0338676722,0.0031615979,-0.0083217476,0.0061655405,-0.0195776803,-0.0385158893,-0.0260615064,0.0317478058,-0.022919415,0.0100438976,-0.0367908822,0.0021547955,-0.020040913,0.0231829912,-0.0291567154,0.0256274664,0.0024339408,-0.0174528253,0.0138359495,0.025635339,0.008609247,-0.0059556245,-0.0096623783,-0.0004692379,0.0200896784,-0.0052445674,0.0017585165,-0.0249761947,0.0234831538,-0.0061142294,-0.0326666896,-0.0210499597,0.0164418625,0.015521976,0.011901877,0.0063187037,-0.0434602934,-0.0065040945,-0.0084866349,-0.0291616985,-0.009566564,-0.0207315844,0.0257496457,0.0180648171,0.0221787203,-0.017892174,0.0069790336,-0.0043513372,-0.0109087538,0.006504182,0.0038880622,-0.0575096134,-0.0195200712,0.0113106411,-0.0237049744,0.0240070359,0.0193906158,0.0096381485,-0.0147080446,-0.0331267463,0.006873494,-0.0316818883,-0.0043049293,-0.0087556957,0.0527031464,-0.0078821283,0.0199137875,0.0177973425]},"483":{"Abstract":"We propose a visual analytics approach for the exploration and analysis of dynamic networks. We consider snapshots of the network as points in high-dimensional space and project these to two dimensions for visualization and interaction using two juxtaposed views: one for showing a snapshot and one for showing the evolution of the network. With this approach users are enabled to detect stable states, recurring states, outlier topologies, and gain knowledge about the transitions between states and the network evolution in general. The components of our approach are discretization, vectorization and normalization, dimensionality reduction, and visualization and interaction, which are discussed in detail. The effectiveness of the approach is shown by applying it to artificial and real-world dynamic networks.","Authors":"S. van den Elzen; D. Holten; J. Blaas; J. J. van Wijk","DOI":"10.1109\/TVCG.2015.2468078","Keywords":"Dynamic Networks;Exploration;Dimensionality Reduction;Dynamic Networks;Exploration;Dimensionality Reduction","Title":"Reducing Snapshots to Points: A Visual Analytics Approach to Dynamic Network Exploration","Keywords_Processed":"exploration;dimensionality Reduction;dynamic network","Keyword_Vector":[0.2284660532,-0.1164186486,-0.012873064,0.1365929293,-0.0841579329,0.1230849894,-0.027620192,-0.1109899132,0.0154872531,0.0966966476,-0.0067442381,-0.0033877676,-0.0757631401,-0.0949153683,0.0302607253,0.1225077582,0.0546687831,0.0617035239,-0.0288475481,0.0274093734,0.0827431651,0.0394829923,0.1018194169,0.0622396541,-0.0193449909,-0.0317712283,0.0044037542,-0.064201094,0.0240230327,0.0923532147,-0.067587645,-0.087865266,-0.037045448,0.0473733153,0.0511584415,0.1175296906,-0.001626625,-0.0270033579,0.0376783958,0.0122818793,-0.0557343993,-0.019232663,-0.0598692883,0.0275744925,0.011945937,-0.0138154869,-0.0426336569,0.0485402768,-0.0502343123,0.0409966957,-0.0124675854,0.0077161838],"Abstract_Vector":[0.316711123,-0.0274476487,-0.0368867901,-0.0103632733,0.0646257356,-0.2184345206,-0.0119281386,-0.1230155112,0.0400715026,0.1465840169,0.0130249991,0.1172485265,-0.094262298,0.0763547718,-0.0221010592,-0.0418364402,0.0043185486,0.0087453835,0.1172520693,0.0079071346,0.1081705881,0.0271169198,0.0244384449,0.0154511599,0.028619879,-0.0431361182,0.0114906039,-0.0055398485,-0.0317765109,-0.0241996929,0.0223283099,0.0529373139,0.0896407739,-0.0357549625,-0.0292079276,0.0609081638,0.0443819808,-0.0603734293,-0.04291824,-0.0203356346,0.0522552672,-0.0029384867,-0.014213631,0.0051775342,0.1149173685,-0.0591574938,-0.0310648011,0.0510153644,-0.0415360265,-0.0259338412,-0.0489033404,0.078576364,0.0619558808,-0.040844774,-0.05517406,-0.0766479833,-0.0126974288,-0.0931408576,-0.015352653,0.0361861662,0.0642894476,0.0358138842,0.0192295084,-0.0622617785,-0.0340317467,-0.0156355287,0.0411478883,0.0847786818,-0.0486857196,-0.0116622027,0.0001611009,0.055650188,0.0835185043,-0.0195209429,0.0177844908,0.0105156702,-0.0494175221,0.0097352293,-0.0231434059,0.0042966445,-0.001002561,0.0139436585,-0.0110949218,0.0271945836,-0.0327133532,0.0925250449,0.0188628328,0.0205234665,-0.0811009476,-0.0039428537,-0.0025481137,0.0044415261,0.0628153949,-0.0225812224,-0.0018994497,0.0110186023,-0.003600844,-0.0375809063,-0.028252846,-0.0176436166,-0.0165932888,-0.020533097,-0.002826886,0.0002832391,0.0851564062,-0.0003561855,-0.0269805419,0.0409064869,0.0356425524,-0.0555811521,0.0927758802,0.018463201,-0.0490283254,0.0054349891,-0.0373836766,-0.0262297257]},"484":{"Abstract":"Recently, an increasing number of visualization systems have begun to incorporate natural language generation (NLG) capabilities into their interfaces. NLG-based visualization systems typically leverage a suite of statistical functions to automatically extract key facts about the underlying data and surface them as natural language sentences alongside visualizations. With current systems, users are typically required to read the system-generated sentences and mentally map them back to the accompanying visualization. However, depending on the features of the visualization (e.g., visualization type, data density) and the complexity of the data fact, mentally mapping facts to visualizations can be a challenging task. Furthermore, more than one visualization could be used to illustrate a single data fact. Unfortunately, current tools provide little or no support for users to explore such alternatives. In this paper, we explore how system-generated data facts can be treated as interactive widgets to help users interpret visualizations and communicate their findings. We present Voder, a system that lets users interact with automatically-generated data facts to explore both alternative visualizations to convey a data fact as well as a set of embellishments to highlight a fact within a visualization. Leveraging data facts as interactive widgets, Voder also facilitates data fact-based visualization search. To assess Voder's design and features, we conducted a preliminary user study with 12 participants having varying levels of experience with visualization tools. Participant feedback suggested that interactive data facts aided them in interpreting visualizations. Participants also stated that the suggestions surfaced through the facts helped them explore alternative visualizations and embellishments to communicate individual data facts.","Authors":"A. Srinivasan; S. M. Drucker; A. Endert; J. Stasko","DOI":"10.1109\/TVCG.2018.2865145","Keywords":"Natural Language Generation;Mixed-initiative Interaction;Visualization Recommendation;Data-driven Communication","Title":"Augmenting Visualizations with Interactive Data Facts to Facilitate Interpretation and Communication","Keywords_Processed":"datum drive communication;mixed initiative interaction;visualization recommendation;Natural Language Generation","Keyword_Vector":[0.2669798012,-0.211217952,-0.1195483074,-0.0158401473,-0.0637002975,0.0043907552,-0.0627244003,-0.0638408265,-0.076652593,0.0434588036,-0.028348135,0.0115447647,0.0218582795,0.0062638631,-0.1027779694,-0.0067988226,-0.1105741058,0.0464719295,-0.0722859654,-0.0701286266,0.0459933757,0.0398854931,0.1199675576,-0.0842616044,0.005860093,-0.0139782084,0.0487485691,0.0499555856,-0.0487965124,-0.0577063496,-0.0292929486,0.0027043964,0.0538920617,0.022028503,-0.0240807676,-0.0303197212,0.0606508807,-0.1045285315,0.0117255296,-0.0261593644,0.0160492201,0.0009416828,0.0231676044,0.0176706846,0.048426952,-0.004817781,-0.0673905203,-0.0307076023,0.0119723821,-0.0051621707,-0.0473183536,0.0248303589],"Abstract_Vector":[0.1491820563,-0.0792062248,0.004153801,0.0014678046,-0.0114773027,0.0098929214,0.0009779451,0.0046826704,0.0139752771,0.0291426554,0.007350017,0.0083640438,-0.0111658295,0.0494589298,0.0280438225,-0.0384657989,-0.0126307413,-0.0105513177,-0.0090166601,-0.0346266244,-0.0209998741,-0.0018059265,0.0000698383,-0.0068650608,-0.0070193752,0.0318596431,0.0083461082,-0.009732138,-0.0005738994,-0.0223946669,-0.0256193502,0.0066903029,-0.0100075905,-0.03183643,-0.0101785857,0.024240694,-0.012315422,0.014854631,-0.0382773421,-0.0148122842,-0.0064239843,0.0237070077,0.0016849657,-0.0484111859,0.0464614986,-0.011499865,0.0121224622,-0.0151793553,0.0173180606,-0.0058939844,-0.0306282102,0.000682224,0.0263422465,0.0208890168,0.0519769643,-0.014797147,0.0005986259,0.0510219327,-0.0133174491,0.0128534051,-0.0319992959,-0.0184915866,-0.0090547425,-0.0047256929,-0.0353097742,0.0016830116,0.0100735536,0.0459376971,-0.0001477317,0.0432762518,-0.0009307502,-0.0272552367,-0.0047312596,0.025652515,0.0083249981,0.0125031252,0.0053868813,0.0086490439,0.0018496511,0.0280406922,-0.0189586243,0.0127721338,-0.0114235711,0.0144023294,0.0643626426,-0.012404835,-0.0100835861,0.0158684676,0.0254932942,-0.0659140154,-0.0117594621,-0.0013406009,-0.0164982432,-0.02624497,-0.0201203779,0.0038607793,0.0215115223,0.0294954951,-0.0070583937,0.0031242631,0.0249183015,0.0136929922,-0.0174198014,0.0183393825,0.0077319435,0.0143792087,-0.0333134389,0.0023281217,0.016479632,0.0314443564,-0.0005975797,0.0443099467,0.0023771264,0.0021195791,0.0106571912,-0.0085121949]},"485":{"Abstract":"We present an approach to pattern matching in 3D multi-field scalar data. Existing pattern matching algorithms work on single scalar or vector fields only, yet many numerical simulations output multi-field data where only a joint analysis of multiple fields describes the underlying phenomenon fully. Our method takes this into account by bundling information from multiple fields into the description of a pattern. First, we extract a sparse set of features for each 3D scalar field using the 3D SIFT algorithm (Scale-Invariant Feature Transform). This allows for a memory-saving description of prominent features in the data with invariance to translation, rotation, and scaling. Second, the user defines a pattern as a set of SIFT features in multiple fields by e.g. brushing a region of interest. Third, we locate and rank matching patterns in the entire data set. Experiments show that our algorithm is efficient in terms of required memory and computational efforts.","Authors":"Z. Wang; H. Seidel; T. Weinkauf","DOI":"10.1109\/TVCG.2015.2467292","Keywords":"Pattern matching;multi-field visualization;Pattern matching;multi-field visualization","Title":"Multi-field Pattern Matching based on Sparse Feature Sampling","Keywords_Processed":"pattern matching;multi field visualization","Keyword_Vector":[0.1629682036,-0.1599983957,-0.1357098614,0.0273132775,-0.0487256004,-0.1519844451,0.0297408106,0.0093653107,0.0315886892,0.0007638259,-0.0053687354,-0.0435339607,-0.0301874479,-0.0172791782,0.0399076409,-0.0269256639,-0.0463651567,-0.0145415812,-0.0020110147,-0.0278935765,0.0431660698,-0.0042957668,-0.0028624532,-0.029139846,-0.0079538158,0.0148485639,0.0163286832,0.0212860528,0.0666775735,0.0150589456,0.0398120653,0.0460310954,0.0548337676,0.1123229315,-0.0568269085,0.0526030225,-0.009574938,-0.0065322908,0.0164457929,0.0130541982,0.0241660622,0.0494072137,-0.0196579781,0.0336665561,-0.0509442124,-0.0260330844,-0.0087127419,0.0086046295,0.0469907983,0.03080107,0.0088270137,-0.0328455672],"Abstract_Vector":[0.1384989863,-0.0859947348,0.0058761151,0.0076617137,-0.0357819454,0.0012696283,0.0094247024,0.0053810989,-0.0310796939,-0.0016470367,-0.0214056513,0.0166942007,-0.0710896817,-0.025809656,-0.0116907975,0.0242564352,-0.0046516924,-0.0627602105,0.0070786657,-0.0171191176,0.0680532272,-0.0334145852,-0.0076513066,-0.0153073999,-0.0108609424,-0.0179919322,0.0188007605,-0.02270334,-0.004745711,0.0154827314,0.0172763448,0.0164341711,-0.047211936,-0.0026346164,-0.0365887827,-0.0170277105,0.01578039,-0.0081546738,0.050398847,-0.01184763,0.0303239222,0.018777074,-0.0247067855,0.0431554867,-0.0183177327,0.0065261481,-0.0029369395,0.0400791913,0.0141054416,0.0498502588,0.0125074385,0.057692676,0.0049802969,0.0135361948,-0.0102452827,-0.004888632,0.0406558205,-0.0221479563,0.0186697957,-0.0221657011,0.0035415766,-0.0131448516,-0.0308000578,-0.0275472097,-0.0773962211,-0.0236191548,0.0425866265,0.0039950794,0.0324185659,-0.0250797463,-0.0483929328,-0.0150466458,-0.0006400239,-0.0279402216,-0.0401276032,0.070032282,0.0040712237,0.009357404,-0.0287684623,-0.0364318508,0.0029129919,-0.0151372314,0.0269486747,-0.0137448044,0.0095750049,0.0732981385,0.0293032377,0.0299540082,0.0098561245,0.0214728592,-0.0182144961,-0.0618533105,0.0041545186,0.0104027277,-0.0304261227,-0.0224194653,0.0155387969,0.0121286927,-0.0198817744,-0.004641318,0.0139632044,-0.0182924012,0.0558972866,-0.0601433711,0.0550186239,-0.0052983293,-0.0047504883,-0.0170399026,-0.0171782428,-0.0111716067,0.0170474868,-0.0274116594,0.0052464382,0.0344982687,0.0175190235,0.0237352513]},"486":{"Abstract":"We present a system for converting a fully panoramic (360 degree) video into a normal field-of-view (NFOV) hyperlapse for an optimal viewing experience. Our system exploits visual saliency and semantics to non-uniformly sample in space and time for generating hyperlapses. In addition, users can optionally choose objects of interest for customizing the hyperlapses. We first stabilize an input 360 degree video by smoothing the rotation between adjacent frames and then compute regions of interest and saliency scores. An initial hyperlapse is generated by optimizing the saliency and motion smoothness followed by the saliency-aware frame selection. We further smooth the result using an efficient 2D video stabilization approach that adaptively selects the motion model to generate the final hyperlapse. We validate the design of our system by showing results for a variety of scenes and comparing against the state-of-the-art method through a large-scale user study.","Authors":"W. Lai; Y. Huang; N. Joshi; C. Buehler; M. Yang; S. B. Kang","DOI":"10.1109\/TVCG.2017.2750671","Keywords":"360 degree videos;hyperlapse;video stabilization;semantic segmentation;spatial-temporal saliency","Title":"Semantic-Driven Generation of Hyperlapse from 360 Degree Video","Keywords_Processed":"semantic segmentation;spatial temporal saliency;360 degree video;hyperlapse;video stabilization","Keyword_Vector":[0.0786905605,-0.0143740118,0.0360818662,-0.0337866267,-0.0185395075,-0.003209151,-0.0367505312,-0.0282018151,-0.0193985533,-0.0694282172,0.0131570299,0.0015773819,-0.0719411533,0.0481375306,-0.1055899881,0.0276137974,-0.0882889238,0.0517097035,-0.0578296405,0.0582794156,0.0145833592,-0.0315874811,0.0506303293,0.0718968238,0.1287959939,-0.0261182531,0.1306283501,-0.1222416609,0.1677280016,0.1239289276,0.0409944433,-0.0461630433,0.0602229248,-0.0306153372,-0.0532086587,-0.0729293451,-0.0539250589,0.0315197696,-0.0296930695,0.0318540157,0.057960017,0.0619479517,-0.0339742618,-0.0332291145,0.0060183572,-0.0216396269,-0.011361385,-0.029583675,0.0414932857,0.0000179837,0.0077849601,0.0014441296],"Abstract_Vector":[0.1489647497,-0.0394389833,0.0213931346,-0.0218022796,-0.0248384474,-0.025214809,0.0004198066,-0.0045805205,0.0488123804,-0.1883443397,0.1539854481,0.221342731,-0.0764128119,0.0308013442,0.1438678165,-0.033767898,0.1513063665,0.0091626357,0.065391599,0.0675768834,-0.0560222781,-0.0383785184,-0.022934487,-0.043355952,0.0334378723,0.0242218525,-0.040999221,0.0147611004,0.0203625117,0.0352027639,-0.0285725733,-0.0252491901,-0.0289453575,0.0346393677,0.0378697296,0.0093505068,-0.0128112522,0.0697688788,0.0049664065,-0.0206956828,-0.0012196911,0.0485730635,-0.0159724737,-0.017155252,-0.008537329,0.0206308385,-0.0725144215,-0.0515914634,0.0204368857,0.0146201917,-0.0236868016,0.0229569612,-0.028379053,-0.0101263491,0.0077824412,0.0240557673,0.0289356437,-0.0107399214,0.0011842115,0.0335184654,0.0090565673,0.0118404441,0.0028088517,0.0090228483,-0.0177126689,0.0209757893,0.0275825908,-0.0267579845,-0.0098488769,-0.0178761605,-0.0058773806,-0.0211515163,-0.0238243261,0.0097679439,0.0135135132,-0.0082491261,-0.0288273183,-0.0013449761,0.0116862017,-0.0090961086,0.0470139776,0.0519460531,0.038149523,0.0078193881,0.0167739469,-0.0329906781,-0.0165270501,0.005657648,0.0080560955,0.0241917938,-0.0247401189,-0.0269207074,0.0829723424,-0.0225120609,0.1167773955,0.0391506898,0.0142567227,0.0368755815,0.0431345828,0.0718557621,-0.0154287407,-0.0199693355,-0.0296851002,-0.0230234497,0.0304172964,-0.0241333561,-0.041791627,0.0175610121,-0.0378426297,-0.0087936327,-0.0202696872,-0.0061049112,-0.0169337629,0.0249884906,-0.0226729866,0.0094133934]},"487":{"Abstract":"We propose a concept for a lens attachment that turns a standard DSLR camera and lens into a light field camera. The attachment consists of eight low-resolution, low-quality side cameras arranged around the central high-quality SLR lens. Unlike most existing light field camera architectures, this design provides a high-quality 2D image mode, while simultaneously enabling a new high-quality light field mode with a large camera baseline but little added weight, cost, or bulk compared with the base DSLR camera. From an algorithmic point of view, the high-quality light field mode is made possible by a new light field super-resolution method that first improves the spatial resolution and image quality of the side cameras and then interpolates additional views as needed. At the heart of this process is a super-resolution method that we call iterative PatchAnd Depth-based Synthesis (iPADS), which combines patch-based and depth-based synthesis in a novel fashion. Experimental results obtained for both real captured data and synthetic data confirm that our method achieves substantial improvements in super-resolution for side-view images as well as the high-quality and view-coherent rendering of dense and high-resolution light fields.","Authors":"Y. Wang; Y. Liu; W. Heidrich; Q. Dai","DOI":"10.1109\/TVCG.2016.2628743","Keywords":"Light field;super-resolution;computational imaging","Title":"The Light Field Attachment: Turning a DSLR into a Light Field Camera Using a Low Budget Camera Ring","Keywords_Processed":"computational imaging;super resolution;light field","Keyword_Vector":[0.1486602429,-0.0988093872,0.0182105053,-0.1478155352,0.1652757208,0.0669462174,0.3751714646,-0.0802917703,-0.0818735864,-0.0740362626,-0.0159701821,0.0867772124,0.0541284665,-0.0166957141,0.0012752764,0.0469950357,-0.0316508293,0.0077408342,0.0163548226,-0.0390652739,0.0279458019,0.0183224409,0.0556769565,-0.0433215637,0.0728294044,-0.0085912642,-0.0711942109,-0.0102108954,-0.0400065697,0.0043402972,0.003752541,-0.0166948401,0.0296429402,-0.083434402,-0.0715241235,0.0065459536,0.0332333506,0.0253644991,-0.0018110873,-0.0006892445,-0.0096933762,-0.0643583978,0.0257051159,0.0031876744,-0.0136523524,0.0049037334,-0.0381372181,-0.0002972157,-0.0290670089,0.0027570791,0.0164651748,0.0059429993],"Abstract_Vector":[0.1986096187,-0.0676795792,-0.0400679607,0.2682398878,0.2314120791,0.1366779375,-0.0221903233,-0.0317704681,0.0105017134,0.0455652295,0.0627530273,0.0269824367,0.0060726365,-0.0687643379,0.1214746889,0.0429245665,-0.0914402703,0.0706952107,-0.1167807574,-0.0657971761,-0.0856976512,0.0578522302,-0.0132077921,0.0627980905,-0.017314363,-0.0795730483,-0.0100924849,0.0134113717,0.0540356456,-0.0615584724,-0.0228015683,-0.0058833219,-0.0436627036,0.027243801,0.0671315167,-0.0075366839,0.0215164115,0.1657179155,-0.0537307089,-0.006767517,0.0092368698,-0.0299288187,-0.0312982913,0.0943900115,-0.0017046553,0.0670241215,0.0676349949,0.0370820189,-0.0874870034,0.0791106619,-0.0621149941,0.0630376515,-0.0239458883,0.01820634,0.0307999632,-0.0119865271,0.0235530664,-0.0298328869,-0.0414785383,0.0155991115,0.020419432,-0.0416248164,-0.0310088767,-0.0116420605,-0.0183964525,0.0008173871,-0.0220643852,0.0099253962,-0.0008593784,-0.059766777,-0.0279471775,0.0530677271,-0.0247096181,-0.031353676,-0.0146886057,0.0024862067,0.0490767285,0.0409963375,0.0762208589,-0.0153005259,0.0410074882,0.0069993273,-0.0068269332,0.018195767,-0.00253161,-0.0066089297,0.0302210962,-0.0303097912,-0.0355506768,-0.0466144874,-0.0095574256,-0.0068604687,0.0083647517,-0.0205721781,0.0442185981,-0.003200061,-0.0264362453,-0.0072122381,-0.0158268519,0.0076537809,-0.0032027585,0.019871089,-0.0181504542,-0.0045902391,0.0073662753,-0.0060968228,-0.0296455774,-0.030048812,0.007772842,0.0090918676,-0.0010755696,-0.0023682718,0.0111622242,0.0172617193,0.0057574466,-0.0010896845]},"488":{"Abstract":"Visual SLAM is one of the key technologies to align the virtual and real world together in Augmented Reality applications. RGBD dense Visual SLAM approaches have shown their advantages in robustness and accuracy in recent years. However, there are still several challenges such as the inconsistencies in RGBD measurements across multiple frames that could jeopardize the accuracy of both camera trajectory and scene reconstruction. In this paper, we propose a novel map representation called Probabilistic Surfel Map (PSM) for dense visual SLAM. The main idea is to maintain a globally consistent map with both photometric and geometric uncertainties encoded in order to address the inconsistency issue. The key of our PSM is proper modeling and updating of sensor measurement uncertainties, as well as the strategies to apply them for improving both the front-end pose estimation and the back-end optimization. Experimental results on publicly available datasets demonstrate major improvements with our approach over the state-of-the-art methods. Specifically, comparing with \u03c3-DVO, we achieve a 40% reduction in absolute trajectory error and an 18% reduction in relative pose error in visual odometry, as well as an 8.5% reduction in absolute trajectory error in complete SLAM. Moreover, our PSM enables generation of a high quality dense point cloud with comparable accuracy as the state-of-the-art approach.","Authors":"Z. Yan; M. Ye; L. Ren","DOI":"10.1109\/TVCG.2017.2734458","Keywords":"Visual SLAM;Dense Visual Odometry;RGBD 6-DoF Tracking;3D Reconstruction;Augmented Reality","Title":"Dense Visual SLAM with Probabilistic Surfel Map","Keywords_Processed":"3d reconstruction;RGBD dof Tracking;augmented reality;visual SLAM;dense visual Odometry","Keyword_Vector":[0.0306353749,0.0054941518,-0.0198955955,-0.014185346,0.0011731173,-0.0217627819,-0.0330120597,-0.0011064342,-0.0851705231,-0.0142115081,0.0343460292,0.0572784572,0.0082408318,0.0054236927,0.0532238531,0.0503280694,0.0467089211,0.0632759558,0.1682672236,-0.0190105516,-0.1021190052,0.0720445909,0.1014999989,0.0605280428,-0.0831086165,-0.0421658122,0.0050479323,-0.0070201522,0.0940187729,-0.1213637612,-0.0171219829,-0.0943080076,0.0328775968,0.0500209441,-0.0488772824,-0.0230751307,-0.0193127013,0.0101919235,-0.0007053355,-0.0222186717,-0.0057985009,0.030308259,0.0418226644,0.0647502034,-0.0024780427,-0.022081886,0.0188345751,0.0304350111,-0.0159461609,0.038816002,-0.0303826816,0.0104626914],"Abstract_Vector":[0.1642382618,-0.0276533978,0.0097588736,-0.0417566252,-0.0334164734,0.0286827628,0.0231280739,-0.0021257073,0.0476089557,0.0757518575,-0.0701556244,0.0424012782,-0.0458704862,0.0379519535,-0.0165583704,-0.0038121688,0.021353812,0.0195343031,-0.0147047269,-0.0151640151,-0.0451774518,-0.044659282,0.1196931486,-0.0193029827,-0.0533347718,-0.038189879,-0.063833822,0.0067144818,0.0714532131,-0.0288464603,0.0121990103,-0.0112494292,-0.0414743534,-0.0226880164,-0.0015801424,-0.0239991958,0.0061321544,-0.0001816761,-0.0394762327,0.0071594452,-0.0012372605,0.0272901038,0.0475233982,0.0286078216,-0.0219154182,-0.0114483249,-0.0155826795,0.0606039353,-0.0268573527,0.0293952226,0.020277145,-0.0140038127,-0.0251625379,-0.0622469424,0.0430916715,0.0119462672,0.0170900905,0.0754053874,-0.0250530013,-0.0452972006,0.0305094036,-0.0121167303,0.0160918798,-0.0114390575,0.0190485361,-0.002479447,0.0621076182,-0.008325566,-0.0271128754,0.0837036249,-0.0232357827,0.0418690695,0.0643373153,-0.0063949619,0.0242198081,-0.0400874602,0.0407717587,0.0032484996,0.0331462101,-0.0483377595,-0.0328769258,0.0262814544,0.0313028816,-0.0122085895,-0.0612996181,-0.0174361878,-0.0198087967,0.0119853789,0.0832816784,-0.0565726404,0.0324053159,-0.0094391715,0.0412242466,-0.0658710668,0.0193434728,0.0227796096,0.0025106564,0.005620396,-0.0228250412,-0.0677262562,0.0166345704,-0.0308745261,-0.0007783648,-0.0508544358,-0.0554681387,-0.0288618627,-0.0029173144,-0.0111311338,-0.0368428865,-0.0203041577,-0.0203035884,-0.0315618291,0.0484786664,-0.006478826,0.0398042225,-0.0263785565]},"489":{"Abstract":"The exploration and analysis of scientific literature collections is an important task for effective knowledge management. Past interest in such document sets has spurred the development of numerous visualization approaches for their interactive analysis. They either focus on the textual content of publications, or on document metadata including authors and citations. Previously presented approaches for citation analysis aim primarily at the visualization of the structure of citation networks and their exploration. We extend the state-of-the-art by presenting an approach for the interactive visual analysis of the contents of scientific documents, and combine it with a new and flexible technique to analyze their citations. This technique facilitates user-steered aggregation of citations which are linked to the content of the citing publications using a highly interactive visualization approach. Through enriching the approach with additional interactive views of other important aspects of the data, we support the exploration of the dataset over time and enable users to analyze citation patterns, spot trends, and track long-term developments. We demonstrate the strengths of our approach through a use case and discuss it based on expert user feedback.","Authors":"F. Heimerl; Q. Han; S. Koch; T. Ertl","DOI":"10.1109\/TVCG.2015.2467621","Keywords":"scientific literature;visual document analysis;visual citation analysis;streamgraph;clustering;scientific literature;visual document analysis;visual citation analysis;streamgraph;clustering","Title":"CiteRivers: Visual Analytics of Citation Patterns","Keywords_Processed":"scientific literature;clustering;streamgraph;visual citation analysis;visual document analysis","Keyword_Vector":[0.0170845606,-0.0124965547,0.0011687742,0.0066470771,-0.0101653745,-0.0088802683,-0.0026363694,0.0128001894,-0.0177824083,0.0029328989,0.0102192249,0.0008199681,-0.0047039387,0.0170680892,-0.0150789211,0.0116468683,0.0160983351,0.0033335906,-0.0057467121,-0.0030243993,0.0098721978,0.0099611812,0.0267025889,0.008604615,-0.0066155894,-0.0059951478,0.0288760339,-0.0051670696,-0.0307467411,-0.0060477807,-0.0015139137,0.0396015155,0.0079206454,0.0032415004,0.0220406338,-0.0209037287,0.0258180123,-0.0328604188,0.0065429187,0.0117505137,-0.0085676817,0.012744699,-0.0057398204,-0.0062571867,0.0156459495,-0.0262094617,0.0132557583,0.0188933608,-0.0108235424,0.0383312537,0.0350840345,0.0161291549],"Abstract_Vector":[0.1282752492,-0.0311840729,-0.0174712497,-0.0090491455,-0.0160622293,-0.0063798648,-0.0045485277,-0.0098500428,0.0091200418,0.0002320694,-0.0158973356,-0.0006144372,-0.0330336596,0.0200735329,-0.0043016727,0.0039686179,0.0218404407,0.0026672374,0.0020786462,-0.042404727,0.0066140606,-0.0289723821,0.0223857813,0.0519029526,0.0048310279,-0.012843875,0.0169660959,0.0019910063,-0.0065937901,-0.035451686,0.0251933108,-0.0161175149,-0.0251672853,-0.017315074,-0.0019213187,-0.0136092873,-0.030808198,-0.0420818238,0.0085893834,-0.0078332872,0.0159970921,0.0306996842,0.0294843717,-0.0172949925,-0.0311889184,0.0206498101,-0.0168773048,0.0354356576,-0.0143937891,-0.0122653312,-0.0454960541,0.0064633643,-0.0236992874,-0.0005430689,0.0068112093,-0.02736472,0.0146832248,0.0448452012,-0.0136319206,0.0203853038,0.003914116,-0.0122717398,-0.0263449626,-0.0177992528,-0.0085579942,0.0179437647,-0.0241475337,-0.0241473876,-0.0398043794,0.0053440373,-0.0060444923,0.0134345772,0.0095786222,-0.0054869741,0.0019915234,-0.0245603832,-0.0012502783,0.060070816,0.0077654213,-0.0456509529,-0.0210735379,-0.0122380933,-0.0598609915,-0.0361158236,-0.0638244667,-0.016323752,-0.0099208807,-0.0143142725,0.0006505944,0.0318075953,-0.0034241459,-0.0028714166,0.0315951198,-0.0064018072,-0.0073342317,-0.0277441382,0.0227685151,-0.0041670497,-0.0333592817,-0.0057082045,-0.0014902369,-0.0420980299,0.0338909683,0.0204713342,-0.0211875276,0.0260583442,-0.0186777219,0.030950826,-0.0067617845,-0.0026103628,0.0095207998,0.0605800849,0.0076591585,0.0032735254,-0.0068572841,-0.0000278524]},"49":{"Abstract":"Fuzzy clustering assigns a probability of membership for a datum to a cluster, which veritably reflects real-world clustering scenarios but significantly increases the complexity of understanding fuzzy clusters. Many studies have demonstrated that visualization techniques for multi-dimensional data are beneficial to understand fuzzy clusters. However, no empirical evidence exists on the effectiveness and efficiency of these visualization techniques in solving analytical tasks featured by fuzzy clusters. In this paper, we conduct a controlled experiment to evaluate the ability of fuzzy clusters analysis to use four multi-dimensional visualization techniques, namely, parallel coordinate plot, scatterplot matrix, principal component analysis, and Radviz. First, we define the analytical tasks and their representative questions specific to fuzzy clusters analysis. Then, we design objective questionnaires to compare the accuracy, time, and satisfaction in using the four techniques to solve the questions. We also design subjective questionnaires to collect the experience of the volunteers with the four techniques in terms of ease of use, informativeness, and helpfulness. With a complete experiment process and a detailed result analysis, we test against four hypotheses that are formulated on the basis of our experience, and provide instructive guidance for analysts in selecting appropriate and efficient visualization techniques to analyze fuzzy clusters.","Authors":"Y. Zhao; F. Luo; M. Chen; Y. Wang; J. Xia; F. Zhou; Y. Wang; Y. Chen; W. Chen","DOI":"10.1109\/TVCG.2018.2865020","Keywords":"Evaluation;multi-dimensional visualization;fuzzy clustering;parallel coordinate plot;scatterplot matrix;principal component analysis;radviz","Title":"Evaluating Multi-Dimensional Visualizations for Understanding Fuzzy Clusters","Keywords_Processed":"principal component analysis;scatterplot matrix;multi dimensional visualization;fuzzy clustering;evaluation;radviz;parallel coordinate plot","Keyword_Vector":[0.0155117399,0.0165215424,0.0112344574,0.0241413656,0.026089305,-0.0106525916,-0.0015265444,-0.0271472566,-0.0026207745,0.019439607,0.0176560876,-0.0165190455,-0.0019121642,-0.0300671136,-0.0354851479,-0.0150091246,0.0244964841,-0.0010739872,-0.0116358246,0.0074006754,0.0066788733,-0.0047768545,0.0142901599,0.010588808,0.0092716064,0.0105023984,0.021758852,0.0125911523,-0.0018647293,-0.044714137,0.0256022901,-0.0210216716,-0.0187678462,0.0112296485,0.004878048,0.0121506193,0.0064203881,-0.0246302251,-0.0546188925,0.0055233878,0.0110769858,-0.0045972364,-0.0744679181,0.0528395673,0.0067303613,0.0078230782,-0.0052209735,0.0493628446,-0.0109517152,-0.069740519,0.0105284301,-0.0104021546],"Abstract_Vector":[0.2585216892,0.0987697823,-0.0338556105,0.0626739257,-0.0717934767,0.0517038282,0.0456643364,0.029311966,0.0104646003,-0.0203531019,0.0396357271,0.0075856097,0.0171538628,-0.0626242961,-0.0239437613,-0.011451286,-0.0064677808,0.0178855885,0.0105033888,-0.0452210169,0.0414304108,-0.0465641405,0.0240357312,0.0207422661,0.0384053387,0.0143615104,0.0297083498,-0.0114465415,0.0435873936,-0.0177605789,-0.0164748,-0.0057805233,0.0081011098,-0.010832672,-0.0388495075,0.0215600985,-0.0139741537,-0.0230202526,0.0052118352,-0.0013354155,-0.0355814032,0.0347983928,0.0062489022,-0.0626730901,-0.0192194227,0.0310212244,0.0255179963,0.0125467193,0.0253209104,0.0113693587,0.0027136383,-0.0281442048,-0.0118843271,0.0003020916,0.0055946305,-0.0100184453,0.0184874585,-0.0769122159,-0.0006721268,0.0343607967,0.0098273876,0.0108773006,-0.0037472834,-0.0086496058,0.020786279,0.0125836631,-0.0023049666,0.0374208232,-0.0191831692,-0.0429881644,-0.0684084415,0.0201181111,0.0014089802,0.0265861927,0.004503927,0.0054557132,-0.0169801419,0.0048093417,0.0466355509,0.0128902614,0.0096170135,-0.0313305081,-0.036113316,0.0288331636,-0.0219830988,0.0040792139,-0.0033200313,0.0307112388,0.0154055141,-0.0198584446,0.0085567188,0.007415423,0.0110847525,0.0075539546,-0.0232516867,-0.0106742762,-0.0539832053,-0.0265314992,-0.0498321229,0.0080972009,-0.0074699954,-0.0069479654,-0.0094997231,-0.0133350737,-0.0310800333,-0.0444369758,-0.0115917183,0.0020319284,0.0107471656,-0.0290549733,0.0057016709,0.0349427888,0.0163827113,0.0104104048,-0.0022787618,0.0216897725]},"490":{"Abstract":"We present a novel approach to using Bounding Volume Hierarchies (BVHs) for collision detection of volumetric meshes for digital prototyping based on accurate simulation. In general, volumetric meshes contain more primitives than surface meshes, which in turn means larger BVHs. To manage these larger BVHs, we propose an algorithm for splitting meshes into smaller chunks with a limited-size BVH each. Limited-height BVHs make guided, all-pairs testing of two chunked meshes well-suited for GPU implementation. This is because the dynamically generated work during BVH traversal becomes bounded. Chunking is simple to implement compared to dynamic load balancing methods and can result in an overall two orders of magnitude speedup on GPUs. This indicates that dynamic load balancing may not be a well suited scheme for the GPU. The overall application timings showed that data transfers were not the bottleneck. Instead, the conversion to and from OpenCL friendly data structures was causing serious performance impediments. Still, a simple OpenMP acceleration of the conversion allowed the GPU solution to beat the CPU solution by 20 percent. We demonstrate our results using rigid and deformable body scenes of varying complexities on a variety of GPUs.","Authors":"R. Schmidtke; K. Erleben","DOI":"10.1109\/TVCG.2017.2784441","Keywords":"Collision detection;bounding volume hierarchies;parallel tandem traversal;scheduling algorithm","Title":"Chunked Bounding Volume Hierarchies for Fast Digital Prototyping Using Volumetric Meshes","Keywords_Processed":"bound volume hierarchy;scheduling algorithm;collision detection;parallel tandem traversal","Keyword_Vector":[0.2622591081,-0.2478960084,-0.2061348128,0.2916465041,-0.1582324125,0.1534760416,0.0053430874,-0.0183144238,0.0497303835,-0.0158228964,0.1209455902,0.0037995019,0.122653784,0.1968461212,-0.094037748,-0.0746165186,0.0194307085,-0.0463345331,0.0947714524,0.1457076612,0.0760630959,0.1034827081,0.0032913088,-0.1537343185,0.0038903973,0.0511943997,-0.0151617136,-0.0487054319,0.0272387364,0.0280284181,0.0793913743,-0.0524195766,0.0586230898,-0.0363105267,0.0300691016,0.1247783965,0.017507714,-0.0840368349,0.0664129109,0.080721453,0.0832788374,-0.0165702163,-0.0393130469,0.0029561803,0.1259849478,-0.0206070664,-0.0801774844,-0.0677542836,-0.0578432958,0.1051568934,-0.0766892635,0.0487214526],"Abstract_Vector":[0.2103029381,-0.0539684811,0.0453646413,-0.0444981487,0.0528668297,-0.0695533413,-0.0268751701,-0.0396258006,-0.047742566,-0.0666632652,0.0112053137,-0.0369210049,-0.004911833,0.0205338541,-0.0183443886,0.0277709659,0.0270996777,0.0106915778,-0.0085592572,0.0561467124,0.0863798593,0.0040301244,0.0168814966,-0.0222626584,-0.0240325527,-0.0223063948,0.023125748,-0.0229119485,-0.0252055873,0.0102830581,0.0319267669,-0.0047036465,-0.0397878454,0.0113384649,-0.0417978293,0.0157468349,0.0363014333,0.0250853696,-0.0921696422,0.0188240655,0.003701793,-0.0271404286,-0.007654517,0.0353598609,0.0457447005,-0.032045061,-0.0285027725,-0.0366394016,-0.0019351387,0.0060593858,0.0180043566,-0.0092115481,0.0359441487,0.0086482791,-0.0130383809,-0.013177401,0.0765900685,-0.048255423,-0.0089340822,0.041168148,0.0091418374,0.036706025,0.0193027812,-0.1220918028,0.0144718993,0.0083989835,0.003123307,0.0438360171,0.0065637889,-0.0523850884,0.0406207808,0.0381092259,0.0505807786,-0.014939892,-0.0848667146,0.0005673323,-0.0693417659,0.0245225767,-0.0298099341,0.0030909774,0.0641189977,0.001491245,0.0585617641,-0.0169732335,0.0033070858,0.0218294928,0.0109556042,0.0123825768,-0.0373048808,-0.021726222,-0.0451192264,-0.0090270091,-0.0001024161,-0.0202402399,-0.0033272481,0.0133812803,-0.0107253157,-0.0312692011,0.0044627365,-0.0418833295,-0.0059938346,-0.0322289472,0.0064564524,0.0174656355,0.0132967219,0.0269666959,-0.014906729,-0.0148149897,0.0375464816,0.0014353754,0.0614696938,0.002438334,0.0645477389,-0.0362029961,-0.0234265437,0.0060955183]},"491":{"Abstract":"The majority of diseases that are a significant challenge for public and individual heath are caused by a combination of hereditary and environmental factors. In this paper we introduce Lineage, a novel visual analysis tool designed to support domain experts who study such multifactorial diseases in the context of genealogies. Incorporating familial relationships between cases with other data can provide insights into shared genomic variants and shared environmental exposures that may be implicated in such diseases. We introduce a data and task abstraction, and argue that the problem of analyzing such diseases based on genealogical, clinical, and genetic data can be mapped to a multivariate graph visualization problem. The main contribution of our design study is a novel visual representation for tree-like, multivariate graphs, which we apply to genealogies and clinical data about the individuals in these families. We introduce data-driven aggregation methods to scale to multiple families. By designing the genealogy graph layout to align with a tabular view, we are able to incorporate extensive, multivariate attributes in the analysis of the genealogy without cluttering the graph. We validate our designs by conducting case studies with our domain collaborators.","Authors":"C. Nobre; N. Gehlenborg; H. Coon; A. Lex","DOI":"10.1109\/TVCG.2018.2811488","Keywords":"Multivariate networks;biology visualization;genealogies;hereditary genetics;multifactorial diseases","Title":"Lineage: Visualizing Multivariate Clinical Data in Genealogy Graphs","Keywords_Processed":"multivariate network;multifactorial disease;genealogy;biology visualization;hereditary genetic","Keyword_Vector":[0.1054055633,-0.0514237756,0.0418035783,-0.0981063559,0.0879375252,0.0300943212,0.2108279145,-0.0041162911,-0.069764854,-0.0461188617,0.001038836,0.045779723,0.0128594664,0.0016600706,-0.002459745,0.0073125643,-0.0008362477,0.0207586876,-0.0172345767,0.0287984317,0.0102757737,-0.0006071753,0.001752122,-0.0189365305,-0.0700173938,0.0092358365,-0.0231515287,-0.0314512488,0.0108528114,0.0361265345,0.0370343196,-0.0012999587,0.0227043599,-0.0664605895,-0.0291662756,0.0147642824,-0.0042820189,0.0528214926,-0.0255924374,-0.0026185477,-0.0507866765,-0.0002194879,0.0102010471,-0.0072278188,-0.0154228632,0.0253301565,0.0273682532,-0.0062709045,-0.009512814,-0.0253696899,0.026139986,-0.0116210191],"Abstract_Vector":[0.1710173921,0.0049316774,0.0059367642,0.1558447616,0.1237872067,0.0956936857,-0.0515648939,0.0025292856,-0.0191779797,-0.0308841076,-0.0065952319,0.0279641039,0.0294416723,-0.0299064321,-0.0870880711,-0.0204709148,0.0211656022,0.0134180045,-0.0198452605,0.0013893546,0.0052469498,-0.0804270793,-0.0032135771,0.0498617162,-0.0190021085,-0.017322605,0.0378252331,0.042143783,0.0062694533,0.0181856699,-0.0324445822,0.0509312606,-0.0257405735,-0.0226439259,-0.0008139484,0.0113886159,0.0030899995,0.0252934703,0.013021315,0.034176402,0.0376518955,0.0172220599,0.0161730996,0.0032174733,0.0011324687,-0.0027548468,0.0233649252,0.0696991067,-0.0109309131,0.0288151888,0.0002350346,0.0053271959,0.0011037496,0.030177037,-0.0035986776,0.0191143416,0.0503129904,-0.0140228944,0.0070400585,0.0052691431,0.0161240243,0.0068156403,-0.033983758,0.0182211159,0.0083733953,-0.0117785439,-0.0027036981,-0.0109556489,-0.0045564206,0.0131180673,0.0193126276,0.0160852972,0.0174973763,0.0126089884,0.0341551135,0.0057091125,0.0126011313,-0.0011700056,0.0160626056,-0.0007248523,0.0296968148,0.009585231,0.0408099669,-0.0221391821,-0.0261741595,-0.0092982621,-0.0113501184,0.0211534856,-0.0034184657,0.0307807303,-0.0109834204,-0.0002939645,-0.0337126276,0.0148255752,0.0278509777,-0.0207443874,0.0084056626,-0.0075824819,0.0429019225,0.0212594126,-0.0175978518,0.0055256924,0.0346929308,-0.0147190091,-0.0055364695,0.0063110325,-0.0064202216,-0.025448697,0.0186187306,-0.0091786845,-0.0154331128,-0.0117794493,-0.0011986091,-0.0159529271,-0.0231655855,-0.0168256126]},"492":{"Abstract":"In this paper, we address the problem of constraint detection for layout regularization. The layout we consider is a set of two-dimensional elements where each element is represented by its bounding box. Layout regularization is important in digitizing plans or images, such as floor plans and facade images, and in the improvement of user-created contents, such as architectural drawings and slide layouts. To regularize a layout, we aim to improve the input by detecting and subsequently enforcing alignment, size, and distance constraints between layout elements. Similar to previous work, we formulate layout regularization as a quadratic programming problem. In addition, we propose a novel optimization algorithm that automatically detects constraints. We evaluate the proposed framework using a variety of input layouts from different applications. Our results demonstrate that our method has superior performance to the state of the art.","Authors":"H. Jiang; L. Nan; D. Yan; W. Dong; X. Zhang; P. Wonka","DOI":"10.1109\/TVCG.2015.2480059","Keywords":"Layout regularization;constraint detection;constraint analysis;linear integer programming","Title":"Automatic Constraint Detection for 2D Layout Regularization","Keywords_Processed":"constraint detection;linear integer programming;constraint analysis;layout regularization","Keyword_Vector":[0.0952358444,0.0147618277,0.1493503462,-0.0082768351,-0.0769852332,-0.0646903542,-0.0588724713,-0.0209398336,0.0418408649,-0.1160981061,-0.0697694594,0.0385193925,0.075620454,-0.0259548882,-0.044680305,-0.0926100532,0.0488081451,-0.0781785325,-0.003283099,-0.0609956727,0.050552346,0.0430158353,0.0292254595,0.0473504545,-0.0262974702,-0.0278424254,-0.0611199749,-0.1356631907,0.0160352753,-0.0599685458,-0.0452117726,-0.0340238231,-0.0208860829,-0.0032208492,0.0541910347,0.0819914856,-0.0403444132,0.0381986808,0.0010855086,-0.0564509283,0.0079215417,-0.0465970998,0.012217896,0.0083212649,0.0000427531,-0.0161304244,0.0156068798,0.0167595703,-0.0077121997,-0.0350746896,-0.0488251523,-0.039022914],"Abstract_Vector":[0.2303320662,0.0349863307,0.0585390151,0.0223035295,-0.071809513,0.0064432563,0.0006058359,0.0082728586,-0.0128133241,-0.0049319415,0.0397649998,0.0043243754,0.0079930474,-0.0371373956,-0.0319579055,0.0322706345,0.0014521408,0.0513149413,-0.0123560375,-0.0459843558,0.0373225914,-0.0249809605,-0.0066377166,0.0599861919,0.0210262441,0.0065310458,0.0411736948,-0.0182469046,0.0320400045,-0.0012213759,-0.0255025492,0.0636228127,-0.0863627509,-0.0119955165,0.0068553617,-0.026200487,0.0080138055,0.0339250487,-0.0250468927,-0.0216476752,0.0278544765,0.0221120186,0.0586141241,-0.0287850429,0.0159074091,-0.0446423262,0.0267721904,0.0292271091,0.0070706023,-0.0744134198,0.0024965883,0.040104878,-0.0100821785,0.057149905,0.0258220456,0.0464193439,0.0169043287,-0.0222873552,-0.0180127182,-0.0003668005,-0.0045525566,0.06971119,0.042844298,0.0632363265,0.0158531577,-0.0238040573,-0.0547643082,-0.0254411375,-0.0099923779,0.0277927659,-0.0089615074,0.0252101762,-0.0423647183,0.025572023,-0.0112056509,0.0206591078,-0.008340212,0.0049598009,0.0120598882,0.0223286815,0.0229137797,0.0068100949,-0.0030619285,-0.0018718629,-0.0719728601,0.070880326,0.0162582826,-0.0137700784,0.0224365988,0.0260968613,-0.0017990123,-0.0444080237,0.0115902771,0.0350236031,-0.0257476063,-0.0117850844,0.0160323404,0.0035766761,-0.0284874678,0.021406031,0.0248004134,0.0400944699,0.0040811233,0.0686490779,0.0005524244,0.0107022085,0.0391813393,-0.0188796609,-0.0192457091,0.0185350766,-0.00309456,0.0234802866,-0.0275913461,0.0006309627,0.0438391665,-0.022670967]},"493":{"Abstract":"The mobility and ubiquity of mobile head-mounted displays make them a promising platform for telepresence research as they allow for spontaneous and remote use cases not possible with stationary hardware. In this work we present a system that provides immersive telepresence and remote collaboration on mobile and wearable devices by building a live spherical panoramic representation of a user's environment that can be viewed in real time by a remote user who can independently choose the viewing direction. The remote user can then interact with this environment as if they were actually there through intuitive gesture-based interaction. Each user can obtain independent views within this environment by rotating their device, and their current field of view is shared to allow for simple coordination of viewpoints. We present several different approaches to create this shared live environment and discuss their implementation details, individual challenges, and performance on modern mobile hardware; by doing so we provide key insights into the design and implementation of next generation mobile telepresence systems, guiding future research in this domain. The results of a preliminary user study confirm the ability of our system to induce the desired sense of presence in its users.","Authors":"J. Young; T. Langlotz; M. Cook; S. Mills; H. Regenbrecht","DOI":"10.1109\/TVCG.2019.2898737","Keywords":"Telepresence;Remote Collaboration;CSCW","Title":"Immersive Telepresence and Remote Collaboration using Mobile and Wearable Devices","Keywords_Processed":"Remote collaboration;CSCW;telepresence","Keyword_Vector":[0.1873642859,-0.2256223326,-0.2125370787,0.241326938,-0.1615175112,-0.0089727523,0.0395293575,-0.0296176272,0.0184089943,-0.0085219828,0.0611148087,-0.0240498995,0.0627044233,0.0451656295,-0.039932773,-0.0366646047,0.0294886565,-0.0180808602,-0.015325655,-0.0550030825,-0.0499360038,-0.0166391724,0.0317145338,0.0214575063,-0.0087509473,-0.0133137095,-0.0344911812,-0.0036572678,0.0262814436,-0.0297536561,0.0415824663,0.0876925248,-0.0783687352,-0.057425016,0.1034312839,-0.0516589895,-0.0438585436,-0.0262325965,-0.0345500711,-0.0421819153,-0.0431287074,0.1954143936,-0.0513963649,-0.072674653,-0.0087563936,0.049846149,0.0274520672,-0.0030547961,-0.0800738884,-0.0377505935,-0.0450987388,-0.0067692038],"Abstract_Vector":[0.2169606596,-0.1451883728,-0.0075079601,-0.0224837136,-0.0190504463,-0.0749428239,0.0123095093,-0.0225390473,-0.042571878,0.0654309321,0.031228233,-0.0501519228,-0.0458595515,0.0079312356,-0.0322141296,-0.0005216501,0.0473387996,-0.0314039197,-0.0252482844,0.0101028994,0.0056891858,-0.0213119187,-0.0185687787,-0.0409032342,-0.0049254579,-0.0015644729,0.028542639,0.0019990942,-0.0238987333,-0.0198501669,0.0079435746,0.0297894156,-0.0181418239,0.013866971,0.0000404094,0.0575134197,0.0005612063,0.0153420407,-0.0294400962,0.0072002735,0.0441707513,-0.0373806077,0.0262933559,0.0036249454,0.0473876455,0.021423838,-0.0423326876,-0.0277883574,-0.0153825759,0.0401315868,-0.0240965126,0.0034874902,-0.0202581832,0.0240623341,-0.0245140395,0.0236785364,0.0302290252,-0.0053287726,-0.039873685,0.0321240848,-0.0832765744,0.0079753643,-0.0059515666,-0.0310962912,0.0003531205,0.0158886994,0.0349082231,0.0280683905,-0.0242667589,-0.0096149691,-0.0283782058,0.0020153008,-0.0258357456,0.0362671485,-0.0097116412,0.0231409383,-0.0444472362,0.0015545925,-0.0234729965,-0.0051958162,0.0084884383,0.0300624802,-0.0271059611,0.0386921886,-0.0313981197,0.0198881567,-0.0072883567,-0.0134148947,0.0127991335,0.0074242483,0.0026583201,0.0084533701,0.0016419103,0.0466080918,0.0098135402,0.0053206101,0.0025161501,-0.001463437,-0.0010972549,-0.0155197469,-0.0087096512,0.0246696518,0.0891196355,-0.0238405455,-0.0343171508,-0.0634714129,0.0222456411,0.0312333978,-0.0499048429,-0.0654324022,-0.0170169546,-0.012427784,-0.0069658126,0.0136515769,-0.0005350569,0.0134277797]},"494":{"Abstract":"Convolutional Neural Networks (CNNs) currently achieve state-of-the-art accuracy in image classification. With a growing number of classes, the accuracy usually drops as the possibilities of confusion increase. Interestingly, the class confusion patterns follow a hierarchical structure over the classes. We present visual-analytics methods to reveal and analyze this hierarchy of similar classes in relation with CNN-internal data. We found that this hierarchy not only dictates the confusion patterns between the classes, it furthermore dictates the learning behavior of CNNs. In particular, the early layers in these networks develop feature detectors that can separate high-level groups of classes quite well, even after a few training epochs. In contrast, the latter layers require substantially more epochs to develop specialized feature detectors that can separate individual classes. We demonstrate how these insights are key to significant improvement in accuracy by designing hierarchy-aware CNNs that accelerate model convergence and alleviate overfitting. We further demonstrate how our methods help in identifying various quality issues in the training data.","Authors":"A. Bilal; A. Jourabloo; M. Ye; X. Liu; L. Ren","DOI":"10.1109\/TVCG.2017.2744683","Keywords":"Convolutional Neural Networks;deep learning;image classification;large-scale classification;confusion matrix","Title":"Do Convolutional Neural Networks Learn Class Hierarchy?","Keywords_Processed":"large scale classification;image classification;Convolutional Neural network;confusion matrix;deep learning","Keyword_Vector":[0.1812236962,-0.0516091267,0.0335839474,-0.1952637541,0.1717504791,0.1077192485,0.2912614035,0.0232370191,0.1618574443,0.0637990159,0.1448886612,0.286556186,-0.0579841226,0.0444513719,0.0104684532,-0.0828749513,0.0245103851,0.0765374087,0.0026545142,-0.1271189208,0.1014056445,-0.0102979451,-0.0039136836,0.0480042554,-0.1151001145,-0.0281269304,-0.0084484819,-0.0060713385,0.0647060272,0.0358086545,0.0599174402,-0.0110628428,0.0488082915,-0.04121594,0.0164476313,0.0140927266,-0.0101737953,0.0363034812,-0.025634276,0.0059424581,-0.000733325,-0.0558923767,-0.0297030696,-0.0288520116,0.0208868314,0.1029698388,-0.0176562786,-0.0503186647,0.0497304311,-0.0854850149,0.0397344337,0.0243630867],"Abstract_Vector":[0.178772116,-0.073181628,-0.0314550312,0.1485718301,0.136470654,0.1192232584,-0.0142463252,-0.0102441613,0.0366061479,-0.0028129102,-0.0134748561,0.0359340018,0.0346385134,-0.0017116526,-0.053965679,0.0182086468,0.0143017203,0.011787291,0.0151542149,0.0079951474,-0.020814367,-0.0508568925,-0.0084522992,0.0366856097,-0.0388570327,-0.0087951174,-0.0423985562,0.0430146248,-0.0254100854,-0.0426523369,-0.0174061632,0.0288631296,-0.0428216171,0.0007246605,0.0109326846,0.0197524812,0.0284574758,0.044215609,-0.0431523167,0.02864669,-0.0524172046,0.057892656,-0.0138983461,0.0182597879,-0.0239295217,0.0184305529,0.0197231722,-0.0142569062,-0.0120100357,0.0197590431,0.0052954011,0.041750135,0.0076138635,0.070029304,-0.0240966877,-0.0259894673,0.0304590595,-0.053009062,0.0114880883,-0.0304569246,0.0396339544,0.012667077,-0.0038264979,0.0072774098,0.0077416633,-0.0430358918,-0.0032757691,-0.0190198946,-0.0723754824,-0.0139897548,0.0074468254,0.0070105495,-0.0085459396,0.0144523338,-0.0135744633,-0.0177247995,-0.0271709979,0.0062379941,0.028300319,0.0243293952,-0.0018200954,0.0275971548,0.0417328114,0.0146085154,0.0176486085,-0.0151269374,-0.0041874796,0.0242149001,0.0164540826,-0.005221322,0.025689237,-0.0168032838,0.0345888135,-0.0415972145,0.0019427093,-0.0134923705,0.0250971384,0.017810986,0.0029922728,-0.0311630686,0.0063531942,0.0042947251,0.0251742223,0.060107943,0.0362181759,-0.0165090042,-0.0195787699,0.015320052,-0.0018383624,-0.0011276415,-0.0033421364,0.001266057,-0.0110019693,-0.0219696613,-0.0373699328,-0.0028657539]},"495":{"Abstract":"Media data has been the subject of large scale analysis with applications of text mining being used to provide overviews of media themes and information flows. Such information extracted from media articles has also shown its contextual value of being integrated with other data, such as criminal records and stock market pricing. In this work, we explore linking textual media data with curated secondary textual data sources through user-guided semantic lexical matching for identifying relationships and data links. In this manner, critical information can be identified and used to annotate media timelines in order to provide a more detailed overview of events that may be driving media topics and frames. These linked events are further analyzed through an application of causality modeling to model temporal drivers between the data series. Such causal links are then annotated through automatic entity extraction which enables the analyst to explore persons, locations, and organizations that may be pertinent to the media topic of interest. To demonstrate the proposed framework, two media datasets and an armed conflict event dataset are explored.","Authors":"Y. Lu; H. Wang; S. Landis; R. Maciejewski","DOI":"10.1109\/TVCG.2017.2752166","Keywords":"Semantic similarity;media annotation;visual analytics;causality modeling;social media","Title":"A Visual Analytics Framework for Identifying Topic Drivers in Media Events","Keywords_Processed":"social medium;causality modeling;semantic similarity;medium annotation;visual analytic","Keyword_Vector":[0.1023817345,-0.0367761762,0.058799243,-0.0491822479,-0.022364847,-0.0606481054,-0.0321281272,0.0329029044,-0.1453474733,0.06229248,0.0347253328,0.042364561,-0.029424742,-0.0077134686,-0.1290400522,-0.0086906025,-0.0602508546,0.0116830835,-0.0602372609,-0.034424024,0.0784008417,0.0312179351,0.0042812269,-0.0295996575,-0.0178934364,0.0403459564,0.0050492886,0.0472073297,0.0064793145,-0.008146478,-0.0865650216,0.0104390455,0.0176712569,-0.0389505062,0.0965786069,-0.0822115597,0.0705248091,-0.0463294642,0.0218337433,-0.0249762703,0.0320511325,-0.0209601985,-0.0134775789,-0.0211556387,-0.0223303863,-0.0754581658,0.06092113,0.0069858755,0.0538940117,0.0188048796,-0.0461126207,0.0167161784],"Abstract_Vector":[0.172854251,-0.0078622444,0.0228928658,0.0090358045,0.0266844024,0.0103171762,-0.086306186,0.0565519634,-0.0331270237,-0.0169564985,-0.0143102748,-0.0072249424,0.0116465904,0.0055213004,-0.0506500023,0.0117110548,-0.0143815503,0.0478778027,0.0774716679,-0.0222281039,-0.0191813158,-0.0071817749,0.0226418797,0.1257139623,-0.1033878486,0.0686876704,-0.1420809781,-0.0984583489,-0.0491195237,0.0665457827,-0.0140602653,-0.0477102006,0.0051818496,0.0765574317,0.0223819007,-0.0006512112,-0.0475965878,0.0103147187,-0.036860025,-0.0609148507,0.0265749395,0.0057152544,0.0134201927,0.0469682696,-0.0174537665,-0.0491606839,-0.0211263,-0.025625721,0.007230217,0.0137936207,0.0087548594,0.0046084029,-0.0179584549,0.0263002289,-0.0017753981,-0.0722504484,-0.0206672447,-0.0168197034,0.0218514886,0.0336746454,-0.0632615875,0.0249056837,0.0174225744,-0.0185653072,0.028971324,0.0043453929,-0.0316226133,-0.0158754603,-0.0172662978,-0.0119303574,0.0178769966,0.0195160435,0.0149799656,-0.033971312,0.0050555675,0.0203994005,-0.0208976564,-0.057540674,-0.0011615016,-0.0064343888,0.0104816636,-0.0257777377,0.0422400271,-0.0318148382,-0.0371316426,0.0814215593,0.01562601,0.0168610877,-0.0163215306,0.0154393543,0.0260371436,-0.0261493979,0.0155997673,0.0075411928,0.0092093625,0.0382288619,0.0084529346,-0.0264948919,-0.0169370813,0.0067540418,-0.0159008304,-0.0278927095,-0.0258835487,0.0412185819,0.0021586107,-0.0060177908,0.0272476174,-0.0384942546,0.0691625261,-0.0339353218,-0.0078832369,0.0457564363,0.0000561581,0.0129119705,0.0215735554,-0.0200066629]},"496":{"Abstract":"We propose a new shape analysis approach based on the non-local analysis of local shape variations. Our method relies on a novel description of shape variations, called Local Probing Field (LPF), which describes how a local probing operator transforms a pattern onto the shape. By carefully optimizing the position and orientation of each descriptor, we are able to capture shape similarities and gather them into a geometrically relevant dictionary over which the shape decomposes sparsely. This new representation permits to handle shapes with mixed intrinsic dimensionality (e.g., shapes containing both surfaces and curves) and to encode various shape features such as boundaries. Our shape representation has several potential applications; here we demonstrate its efficiency for shape resampling and point set denoising for both synthetic and real data.","Authors":"J. Digne; S. Valette; R. Chaine","DOI":"10.1109\/TVCG.2017.2719024","Keywords":"Shape similarity;local shape descriptor;point set denoising and resampling","Title":"Sparse Geometric Representation Through Local Shape Probing","Keywords_Processed":"point set denoise and resampling;local shape descriptor;shape similarity","Keyword_Vector":[0.0536877239,-0.0129431455,0.0256039035,0.0424787594,0.0569148408,-0.0146168329,0.0028208174,0.0127348488,-0.0243832427,0.0345655367,-0.0259357082,-0.0038264618,-0.0134761217,-0.0210548418,0.004132424,0.0000456445,-0.0641682874,-0.035737737,0.0837783984,-0.0404266908,-0.0136427452,-0.0822899728,0.0052611504,-0.0319005667,0.0680178413,-0.0279230555,-0.1582698985,0.0310153249,0.0717714213,0.0265446314,-0.1107773721,0.021390718,0.1809041224,-0.0244838893,0.1859088335,0.0393670087,0.1012108634,0.0113817575,0.0544517336,0.062085266,0.0690077152,-0.0149469637,0.073891705,-0.0030835629,-0.0148085695,-0.1121069398,-0.07977659,0.013426816,-0.1658308806,-0.0732937198,0.153877537,-0.0376405767],"Abstract_Vector":[0.1570978589,0.0022948201,0.0223343886,0.0210548883,0.0274863821,-0.0405045659,-0.0041959531,0.0047858685,-0.0005397087,-0.0035646874,-0.0061012134,-0.0440921383,-0.0452657843,-0.0267206574,-0.0062277487,-0.0033655697,0.015871492,0.0585632193,-0.0047229279,-0.0059390998,0.007191149,-0.0353056422,0.0045973432,-0.0225077607,0.0467850027,0.0557456836,-0.0211399845,-0.0039975874,0.047992884,0.0270192457,0.007904036,0.0442783594,0.0001669361,0.0569436527,0.0190383737,0.0491033371,-0.0012645818,0.0540670312,-0.0015422287,0.0052819107,-0.0359017533,0.0379985278,-0.045774597,-0.029525136,-0.0196443162,0.0011186681,0.0091102221,-0.008506625,0.0416829634,-0.0330575241,-0.0023676369,0.0401288759,-0.0029956369,-0.0068279665,-0.0064249323,0.0007677944,0.0014335613,0.0005476229,-0.0098553482,-0.0508460902,-0.0142607401,-0.039232768,-0.014396746,0.0021511607,0.0297435746,-0.0362929332,-0.0170602029,0.0024665355,-0.0023275529,0.004064942,0.0174387897,0.0036443128,-0.0018916311,-0.0255551989,0.0460916001,0.0138287342,0.0069625407,-0.0079440965,-0.0147904194,0.004611199,-0.0129069112,0.0084827826,0.0185711817,0.0227814946,0.007098536,0.0088338792,0.0056194515,0.005110864,0.0065092882,0.0036779886,0.012307804,0.0031968258,-0.0124690143,-0.0024698333,-0.0375575978,-0.0009294342,0.0077270553,-0.0043117709,-0.0013692222,0.0237869976,0.0033628009,0.0077022442,-0.0200528731,0.0108614727,-0.0047031268,-0.0031317054,0.0171906362,0.0213972127,0.0043715917,-0.0197046422,0.0066072834,0.0350822779,0.0120686812,-0.0084119005,0.0013927407,-0.0242535513]},"497":{"Abstract":"Paper documents such as passports, visas and banknotes are frequently checked by inspection of security elements. In particular, optically variable devices such as holograms are important, but difficult to inspect. Augmented Reality can provide all relevant information on standard mobile devices. However, hologram verification on mobiles still takes long and provides lower accuracy than inspection by human individuals using appropriate reference information. We aim to address these drawbacks by automatic matching combined with a special parametrization of an efficient goal-oriented user interface which supports constrained navigation. We first evaluate a series of similarity measures for matching hologram patches to provide a sound basis for automatic decisions. Then a re-parametrized user interface is proposed based on observations of typical user behavior during document capture. These measures help to reduce capture time to approximately 15 s with better decisions regarding the evaluated samples than what can be achieved by untrained users.","Authors":"A. D. Hartl; C. Arth; J. Grubert; D. Schmalstieg","DOI":"10.1109\/TVCG.2015.2498612","Keywords":"Document inspection;holograms;augmented reality;user interfaces;mobile devices;Document inspection;holograms;augmented reality;user interfaces;mobile devices","Title":"Efficient Verification of Holograms Using Mobile Augmented Reality","Keywords_Processed":"document inspection;mobile device;user interface;augmented reality;hologram","Keyword_Vector":[0.1611402828,-0.0624094938,0.0608198253,0.165589346,-0.0877505311,0.10618639,-0.0219607087,-0.114459193,-0.0290058002,0.0302856618,-0.0235931412,0.0077259407,-0.0868872519,0.1250610752,-0.0486599474,0.1410402788,0.1049140033,-0.0509021174,-0.0712545092,0.0126137583,0.0248098989,-0.0425624266,0.0225534532,0.1815405596,-0.0329643441,-0.1855621922,-0.0680585598,0.0048764964,-0.1365647901,-0.0099591058,0.0075473326,0.0239080327,-0.0017026667,0.0735914667,-0.0365560392,-0.0173332346,0.0379752734,0.002731987,0.0321167583,0.1125378655,-0.0133260639,-0.0195954365,-0.026365147,-0.0314187865,0.0331464604,-0.0196374628,-0.0229693358,-0.0068346077,0.0419024349,0.0301501643,0.0435054283,-0.0662061031],"Abstract_Vector":[0.2696898948,0.0244958279,0.0634028136,-0.0595713268,0.0626580116,-0.1319234531,-0.0124037236,-0.0729523942,-0.0137624565,0.1973670647,-0.0809356796,0.1008678287,-0.066215682,0.0998091886,0.0014664007,-0.0068880656,0.1178153732,0.0269411557,-0.0409232018,-0.0934930182,-0.0838735552,-0.0541329155,-0.0863240736,-0.0364694135,-0.0505738274,0.0876628249,-0.0652755853,-0.0124948121,0.0096276517,-0.0609583435,0.068332401,0.0686340416,0.0046626873,-0.0196574483,0.0209782195,-0.1128128941,0.0988393532,0.043363835,0.1118908709,0.0937417155,0.0011001778,-0.0904886292,0.0548124675,-0.0446537362,-0.0796265113,-0.0685344907,0.0897428526,0.053239993,0.0290701694,-0.0472279254,0.0205275773,-0.072775201,-0.0470584604,-0.0348261984,0.0233064373,-0.0380215165,0.0154350254,-0.0041101431,-0.0550634283,-0.0082599181,0.0094522087,0.0528759803,-0.0245215183,0.0304132304,0.004720353,0.029320051,-0.0321131153,0.0248544019,-0.0553405197,-0.0270705252,-0.0484747136,-0.0234913594,0.0028403416,-0.0480265655,-0.0251635045,-0.0705583539,0.0197001074,-0.0562103404,-0.0055229664,-0.0327440292,0.0378154274,0.00724708,-0.0039102542,0.0116664409,-0.052403552,0.0432697364,0.0205479688,0.0066094516,0.0266128169,-0.0051844059,0.0005764028,0.0131846035,0.0179893257,-0.0175093266,-0.005873495,0.0224343778,0.0269952461,-0.0444263268,0.0305370733,-0.0561305618,-0.0126487571,-0.0250204232,0.0318257207,-0.0007039098,-0.0140617217,-0.0053282382,0.0019927157,-0.0310433651,0.0260797832,-0.0091486415,0.0172459209,0.0322485921,-0.0088024592,-0.0420902382,0.0795956908,-0.0158760387]},"498":{"Abstract":"In recent years, significant progress has been made in developing high-quality interactive methods for realistic volume illumination. However, refraction - despite being an important aspect of light propagation in participating media - has so far only received little attention. In this paper, we present a novel approach for refractive volume illumination including caustics capable of interactive frame rates. By interleaving light and viewing ray propagation, our technique avoids memory-intensive storage of illumination information and does not require any precomputation. It is fully dynamic and all parameters such as light position and transfer function can be modified interactively without a performance penalty.","Authors":"J. G. Magnus; S. Bruckner","DOI":"10.1109\/TVCG.2017.2744438","Keywords":"Interactive volume rendering;illumination;refraction;shadows;caustics","Title":"Interactive Dynamic Volume Illumination with Refraction and Caustics","Keywords_Processed":"interactive volume render;illumination;refraction;shadow;caustic","Keyword_Vector":[0.0453875985,0.0307650094,-0.0274214093,0.0159300013,0.0070617802,-0.0068688372,0.0152644314,0.0151520509,-0.0418564758,0.0319661805,-0.0453641326,0.0127917507,-0.0477634408,0.0749885286,-0.0478831239,-0.0396345073,-0.0213135208,-0.0476836382,-0.0426506989,0.0408591961,-0.0108972231,-0.0104344042,-0.0445230918,0.1006601687,0.0411452917,-0.0474319086,-0.0610001604,0.042141974,0.0173179952,-0.0509575992,0.0615168217,-0.08182959,-0.015126936,-0.0055715335,0.0047325612,0.0477066444,-0.0438209247,0.0061545117,0.0057032711,0.013928495,-0.0577436113,-0.0339496414,0.026410664,-0.0276546469,0.0031564206,0.0161925475,-0.037468485,0.0211609308,0.0098114003,0.02174015,-0.0236739345,0.0139175091],"Abstract_Vector":[0.2007844761,0.0454137385,-0.0049892422,0.0236131576,-0.0243527953,0.0076088469,0.0187864758,0.033119349,-0.0154245949,-0.0390330352,0.022393699,0.0174321449,-0.0384658895,-0.0406694832,-0.0179962293,0.0488424068,-0.0113225753,0.071636925,-0.0108053219,-0.0733469095,0.0346139084,-0.0284985609,0.0169780654,-0.0055792851,-0.0131256118,0.0162679072,0.0617916464,0.0238651709,0.0137152926,0.016069043,-0.0103979981,0.0125894906,-0.0473732466,-0.0225489874,-0.0105206961,-0.0444083791,0.0039503616,0.0207631254,0.0049056649,0.0554529202,0.0160787493,-0.0035924354,0.005246568,0.0081953664,0.0101873975,-0.0031328281,0.0294098504,0.0510460556,-0.0354852994,-0.0379728238,-0.0350352379,0.0278244648,-0.0257663176,-0.0085064333,0.0240765167,-0.0083042936,-0.0049128686,-0.0187096934,0.0105572625,0.0105953022,0.0360172448,0.0196917677,0.0334134898,-0.0064437938,0.0181342522,-0.0172992109,-0.0086895496,0.0250413288,-0.0031282941,0.0136301599,-0.0155038119,0.0360194605,0.0088772534,0.005685814,0.021777212,0.0130743538,-0.0565064993,-0.017132046,-0.0039286942,0.0287391889,0.0334850937,0.0142735154,0.0724157523,0.0025533637,0.0284411175,-0.0221623975,-0.0144292824,-0.004600239,0.0116455354,-0.0312677761,0.0668125045,-0.0083556731,-0.028512054,-0.0131267703,0.0053281933,0.0458366636,0.0283789103,0.0044184432,0.0013731824,0.0305426337,-0.0543654105,-0.0025407665,-0.0097952268,0.0060987239,0.0083113282,0.0258083802,0.0328104262,-0.0004476378,0.0220704374,0.0114009941,-0.0323831809,0.0172155558,0.024745968,-0.0297590748,0.0235651722,0.0111425038]},"499":{"Abstract":"Understanding the movement patterns of collectives, such as flocks of birds or fish swarms, is an interesting open research question. The collectives are driven by mutual objectives or react to individual direction changes and external influence factors and stimuli. The challenge in visualizing collective movement data is to show space and time of hundreds of movements at the same time to enable the detection of spatiotemporal patterns. In this paper, we propose MotionRugs, a novel space efficient technique for visualizing moving groups of entities. Building upon established space-partitioning strategies, our approach reduces the spatial dimensions in each time step to a one-dimensional ordered representation of the individual entities. By design, MotionRugs provides an overlap-free, compact overview of the development of group movements over time and thus, enables analysts to visually identify and explore group-specific temporal patterns. We demonstrate the usefulness of our approach in the field of fish swarm analysis and report on initial feedback of domain experts from the field of collective behavior.","Authors":"J. Buchm\u00fcller; D. J\u00e4ckle; E. Cakmak; U. Brandes; D. A. Keim","DOI":"10.1109\/TVCG.2018.2865049","Keywords":"Spatio-Temporal Visualization;Spatial Abstraction;Spatial Index Structures;Collective Movement","Title":"MotionRugs: Visualizing Collective Trends in Space and Time","Keywords_Processed":"spatial index structure;spatial Abstraction;collective Movement;spatio Temporal visualization","Keyword_Vector":[0.1942974862,0.3649123318,-0.192203492,-0.05529603,-0.0871442866,0.022977119,0.0907362801,0.0282473041,-0.0288993607,0.0390754901,-0.0357803753,-0.0334013004,0.011550008,-0.0045940792,-0.0191506645,-0.0199636113,-0.0065769806,-0.0365486198,-0.0347934047,0.0021602783,-0.009810998,0.0189580048,-0.0001136486,-0.0511254644,-0.0036485918,0.0131109146,-0.0568537258,-0.0679927427,-0.0292195425,-0.0172569772,0.0146485698,-0.0040097934,-0.0111345015,-0.0178883178,-0.0086214644,0.031073632,0.0591279777,0.0088603541,0.0448796655,0.0311240024,-0.0185359588,0.0256995564,-0.0810017526,0.0361236665,-0.046657988,0.008648624,0.065927375,-0.0153945103,0.0549092512,-0.0129133646,0.0636963461,-0.0130050168],"Abstract_Vector":[0.2256851138,0.1418683067,-0.0423428343,-0.0768082551,0.006746292,0.0490243713,-0.1167248548,-0.0594841758,-0.0701103312,-0.0102325646,-0.0130402711,0.0032369727,-0.0143022129,0.0025721265,0.0138528025,-0.0104957307,-0.0095142069,0.0015406469,-0.0307658177,-0.0478857979,-0.0326566535,-0.0615819528,0.0285571164,0.0283407235,0.0107082714,0.0232298218,0.0222112864,-0.0186123044,-0.0536358986,-0.0330535231,-0.0090438266,-0.0229929427,0.0464394674,0.0167178726,-0.0552843082,0.0282316038,0.0244053841,0.0638058519,0.0329160581,0.0343815067,0.0821317405,0.0575359451,0.0142799048,0.020673287,-0.0736020015,0.0351644421,-0.0201476963,0.0293190725,-0.0354013984,-0.0372460972,-0.0193030371,-0.0036643314,-0.0120364218,0.04508258,-0.0100229405,0.0124145622,-0.0179321238,0.0225774973,-0.0131305095,0.038318964,0.0175614993,0.0138361208,-0.0083121938,-0.0269927493,0.0141125259,0.0044875575,0.0144843271,-0.049166271,0.0151844391,0.028672667,0.0682363013,-0.0021237958,0.0186660034,0.0013005155,-0.0137447671,-0.013021829,0.0027746261,-0.0340793459,-0.0409361692,0.0216704294,-0.0278452988,-0.018612414,0.022435518,-0.0044601629,-0.0063628834,0.0066135013,-0.0284746839,-0.006730142,-0.0031174944,-0.010306324,-0.0134910695,-0.0102120592,-0.0172751887,-0.0004454207,0.0182910731,-0.0011837723,-0.0030792483,0.0303747376,-0.021368279,0.0029658211,0.0038081255,0.0216289768,0.0101007606,-0.0518307049,0.0062639072,-0.0392364897,0.0105607833,-0.0167155618,-0.0363564147,-0.008894995,0.0325596016,0.0236455386,-0.0211585769,0.0024065911,-0.0322398748,0.0263289796]},"5":{"Abstract":"This paper examines how humans adapt to novel physical situations with unknown gravitational acceleration in immersive virtual environments. We designed four virtual reality experiments with different tasks for participants to complete: strike a ball to hit a target, trigger a ball to hit a target, predict the landing location of a projectile, and estimate the flight duration of a projectile. The first two experiments compared human behavior in the virtual environment with real-world performance reported in the literature. The last two experiments aimed to test the human ability to adapt to novel gravity fields by measuring their performance in trajectory prediction and time estimation tasks. The experiment results show that: 1) based on brief observation of a projectile's initial trajectory, humans are accurate at predicting the landing location even under novel gravity fields, and 2) humans' time estimation in a familiar earth environment fluctuates around the ground truth flight duration, although the time estimation in unknown gravity fields indicates a bias toward earth's gravity.","Authors":"T. Ye; S. Qi; J. Kubricht; Y. Zhu; H. Lu; S. Zhu","DOI":"10.1109\/TVCG.2017.2657235","Keywords":"Virtual reality;intuitive physics;mental simulation","Title":"The Martian: Examining Human Physical Judgments across Virtual Gravity Fields","Keywords_Processed":"mental simulation;virtual reality;intuitive physics","Keyword_Vector":[0.0233525066,-0.0096213074,0.0000942407,-0.0130935453,0.025117665,0.0030264605,0.0126066249,-0.0111588877,-0.0026745931,-0.0018835471,-0.001773457,0.0182899307,-0.0065730993,0.0003593933,0.0152342187,0.0099263113,0.0055155622,-0.0101889705,-0.0072441493,0.002993103,-0.0136910674,-0.0240154343,-0.0179552101,-0.01561329,0.004737846,-0.0013769218,0.0255347765,-0.0047944048,-0.0003046589,-0.003616994,0.0292862303,-0.0159255976,-0.0161588427,0.0760755521,0.0700391179,-0.0145975751,-0.0050537734,-0.0334822936,-0.0541946949,0.0049160759,-0.0119387472,-0.0165688482,0.0055723763,0.0017117155,0.0412518251,-0.0014408407,0.010263295,0.014114212,-0.0014474162,0.0247556638,0.0006640726,-0.0141605199],"Abstract_Vector":[0.0951210965,-0.0668533161,0.0170433649,-0.0145611294,-0.0493058333,0.0278827102,0.0019088139,-0.0027298786,-0.0462147886,0.0202649986,-0.0109921973,0.0124270621,-0.0358496954,0.0106129808,-0.0225225534,0.0141061565,-0.0250057948,-0.0182685854,0.0077350084,0.0045401977,-0.0080404441,-0.0011780543,-0.0359215741,0.0178741697,-0.0037059721,0.0537113757,-0.0228468304,0.0174992308,-0.0049511286,-0.0385230108,0.0232181604,-0.0048333883,-0.0174261785,-0.0154941154,-0.0455043139,-0.0180983642,-0.0037529444,-0.01258922,0.0232803893,-0.012676151,-0.0123751066,-0.0125161592,-0.0271067211,-0.0007725832,-0.0011862852,0.0044496934,-0.001628198,-0.0296896163,0.0041421299,0.0043653282,0.0044609945,0.0237146916,-0.0393579667,-0.0031908646,0.0328079197,-0.0125145795,0.0056826259,-0.0037794148,-0.0441597444,0.0016979072,0.0105508762,-0.000818146,-0.0365149069,0.0148211078,-0.0307670526,0.0080255417,0.0187490534,0.0227951711,0.0055578288,0.0094437131,0.0019033906,0.021239148,0.0507601581,0.009816492,-0.0060650758,0.0120153673,-0.002271889,-0.0243125847,-0.0144517913,0.0236665801,0.0010899033,-0.0179345964,-0.0260138366,0.0144607869,0.0077974319,-0.0232520989,0.0062924453,-0.0221645572,0.015438301,0.0309543183,0.0018753179,-0.0096178013,0.0044015509,-0.033046985,-0.0179659177,0.0126282583,0.0105498208,-0.0151082714,0.0401978213,-0.0030997223,-0.0042970821,-0.0007559022,-0.0302597672,-0.0399702255,-0.0020002276,0.0248845378,-0.0153136247,0.024413634,0.0171692732,0.0179835514,0.0197988911,0.0117200566,-0.0005198301,0.0296650483,0.0101353043,0.0060924274]},"50":{"Abstract":"Games and experiences designed for virtual or augmented reality usually require the player to move physically to play. This poses substantial challenge for level designers because the player's physical experience in a level will need to be considered, otherwise the level may turn out to be too exhausting or not challenging enough. This paper presents a novel approach to optimize level designs by considering the physical challenge imposed upon the player in completing a level of motion-based games. A game level is represented as an assembly of chunks characterized by the exercise intensity levels they impose on players. We formulate game level synthesis as an optimization problem, where the chunks are assembled in a way to achieve an optimized level of intensity. To allow the synthesis of game levels of varying lengths, we solve the trans-dimensional optimization problem with a Reversible-jump Markov chain Monte Carlo technique. We demonstrate that our approach can be applied to generate game levels for s of motion-based virtual reality games. A user evaluation validates the effectiveness of our approach in generating levels with the desired amount of physical challenge.","Authors":"B. Xie; Y. Zhang; H. Huang; E. Ogawa; T. You; L. Yu","DOI":"10.1109\/TVCG.2018.2793618","Keywords":"Virtual Reality;Level Design;Procedural Modeling;Exergaming","Title":"Exercise Intensity-Driven Level Design","Keywords_Processed":"Level Design;virtual reality;exergame;procedural modeling","Keyword_Vector":[0.2062603901,-0.1124561437,-0.0248234378,0.1281943595,-0.0470260359,0.3013312218,-0.0758114102,0.000125295,-0.0126810767,-0.0468196178,0.0205487351,0.011973775,0.0329900014,-0.0651904639,-0.0280785713,-0.1078935678,-0.1064805337,-0.0039421666,0.0626923592,0.1557250445,0.0391841829,0.0288194368,0.0727220754,-0.0129232352,0.0339651224,-0.0075716023,-0.0145056611,0.0561918642,-0.0471754538,0.0296106203,-0.0545690907,-0.0560380144,0.0175546222,0.0209558421,-0.0352150193,0.0136282316,0.0029863476,-0.0145523305,-0.0472563387,0.0064332327,-0.0428724049,-0.0090079752,-0.0027468887,-0.0045409869,0.0332686433,0.061444596,-0.0209137063,0.0123309453,0.0053482028,0.0334623674,-0.000164865,0.0513521732],"Abstract_Vector":[0.2000932498,-0.0248032497,0.0009660765,0.0140606984,-0.0233603659,-0.0585334619,-0.0661844746,-0.0256285282,0.042750999,-0.0437929733,0.0198940367,-0.0676059312,-0.0282537863,-0.0515872346,0.0031089437,0.0360541798,-0.060290576,-0.007197727,0.0168565338,0.0082017641,0.0155255577,-0.015001259,0.0277221355,-0.0300534885,-0.0693116701,-0.0249160387,-0.0097867028,0.1268718264,-0.005703533,0.0222587416,0.0256826981,-0.0219890388,0.0127572667,0.0045434445,-0.0087393283,0.0190981545,0.0330179766,-0.0392780647,0.0126874478,0.0508045621,0.0340064907,-0.0117573898,0.0385688558,-0.0177718802,-0.0622385434,-0.0365986401,-0.0196851098,0.0181930277,0.002718376,-0.0081156496,-0.0205339911,0.0141423952,-0.0258433686,0.017104899,0.0253351045,-0.0531465631,0.0373024329,-0.0668293824,-0.0464307567,0.0173128183,-0.0140100089,-0.0153162216,0.0265512435,0.0386612998,0.0050998131,-0.0137115041,-0.0066754583,0.0013056291,0.0669873517,0.0717709653,-0.0082310091,0.0175588881,0.0055913037,0.0271093186,0.0660027231,-0.0193749838,-0.019022059,-0.0420053948,-0.0285755737,0.006660202,-0.0142759034,-0.0204182933,-0.0383211887,-0.1039166058,0.0162301863,0.0107142204,0.0276028812,0.0393542251,-0.0087969189,-0.0297521322,0.0229447465,0.0160686756,0.0358864894,-0.0176645102,0.0213007356,-0.0263549683,-0.0041718712,0.0274711413,0.0157823753,0.0176127726,0.0114012118,-0.0204651011,0.0571056401,0.048423986,0.0357613592,-0.0197391207,-0.0076681343,0.0175178605,-0.0265166466,0.0037368033,-0.0618471666,0.025588555,-0.0587425542,0.0084428615,0.0311146956,-0.0105109911]},"500":{"Abstract":"We present EdWordle, a method for consistently editing word clouds. At its heart, EdWordle allows users to move and edit words while preserving the neighborhoods of other words. To do so, we combine a constrained rigid body simulation with a neighborhood-aware local Wordle algorithm to update the cloud and to create very compact layouts. The consistent and stable behavior of EdWordle enables users to create new forms of word clouds such as storytelling clouds in which the position of words is carefully edited. We compare our approach with state-of-the-art methods and show that we can improve user performance, user satisfaction, as well as the layout itself.","Authors":"Y. Wang; X. Chu; C. Bao; L. Zhu; O. Deussen; B. Chen; M. Sedlmair","DOI":"10.1109\/TVCG.2017.2745859","Keywords":"Wordle;consistency;text visualization","Title":"EdWordle: Consistency-Preserving Word Cloud Editing","Keywords_Processed":"wordle;text visualization;consistency","Keyword_Vector":[0.0347827381,-0.0062947115,-0.0143141005,-0.0234281062,0.0035568333,-0.0374604452,-0.0700390139,-0.0148373473,-0.0947418413,-0.0053310498,-0.002166259,0.1063106871,0.0003375173,0.0103155469,0.0421898333,0.0706919332,0.1124804861,0.0679132571,0.1215336149,0.0159109589,-0.0927741822,0.0250510968,0.0120759135,0.0675638152,-0.0389475134,0.0131507494,-0.0039743019,0.0151582418,0.0079122215,-0.024042188,-0.0396879712,-0.0433468256,0.0575303531,0.0006804094,-0.0106413345,-0.0484978576,-0.0712390818,-0.0287340989,-0.022176161,-0.021742225,-0.014530556,-0.011373275,-0.0104372154,0.0213581137,0.0350000732,-0.0095372625,0.0428780432,0.0266222889,-0.0448869047,-0.0250332715,0.0610205372,-0.0484753255],"Abstract_Vector":[0.2081284973,0.0602276017,-0.0189439381,-0.0073339969,-0.0846553571,0.0926512514,-0.0036489066,0.007026857,0.079919365,0.0323557702,-0.0286554248,-0.0165283521,-0.0723557492,-0.0283292214,-0.0010418445,0.0279964845,0.0777097492,0.0705302379,0.049039793,-0.0031113464,-0.0540417958,-0.0634460944,0.030283984,-0.0362856182,-0.0087485071,0.0413811233,0.0419853511,0.0183836281,0.0055505405,0.0236515543,0.0997172618,-0.0018418148,-0.0287475561,-0.0288953168,0.0405215616,0.010162635,-0.0239504032,0.0057969219,0.0593558373,0.0721855615,0.0755545899,-0.0614963603,0.0259542991,-0.0300411057,0.0033626704,-0.0354529518,0.0529241948,0.0449364692,0.0126867694,-0.0074832593,-0.004616567,-0.0517492456,-0.0342527677,0.0078459422,0.0134776349,-0.0078323746,0.017712005,0.0044570811,-0.0266887796,-0.0027057644,0.0093825711,-0.0078019952,-0.0051199078,0.0237502653,0.0189763146,0.0346328495,0.0328599465,0.0031806968,-0.0242475608,0.0203211831,-0.013703633,0.0012535905,-0.0095603286,0.0173450798,0.0012788494,-0.0665019947,0.0034011113,-0.0282323992,-0.0228998176,-0.0419018792,0.0138229675,-0.0405791971,-0.0659194441,0.0454810594,-0.0027883807,0.0103393782,-0.0149369196,0.0044046406,-0.0682617522,-0.0604738651,0.0187277553,0.0174751451,0.028608319,-0.011270891,0.0156802222,-0.0511784937,0.0339899893,0.0126389408,0.0076440059,-0.0076806239,0.0672119397,-0.0169029807,-0.0119795267,0.0009066791,0.018431194,0.0471000736,-0.0110934951,0.0303774728,-0.0034544715,-0.0207983217,-0.0108701653,0.0568720535,0.0411861066,-0.007819568,0.0016434156,0.0367080681]},"501":{"Abstract":"Data clustering is a common unsupervised learning method frequently used in exploratory data analysis. However, identifying relevant structures in unlabeled, high-dimensional data is nontrivial, requiring iterative experimentation with clustering parameters as well as data features and instances. The number of possible clusterings for a typical dataset is vast, and navigating in this vast space is also challenging. The absence of ground-truth labels makes it impossible to define an optimal solution, thus requiring user judgment to establish what can be considered a satisfiable clustering result. Data scientists need adequate interactive tools to effectively explore and navigate the large clustering space so as to improve the effectiveness of exploratory clustering analysis. We introduce Clustrophile 2, a new interactive tool for guided clustering analysis. Clustrophile 2 guides users in clustering-based exploratory analysis, adapts user feedback to improve user guidance, facilitates the interpretation of clusters, and helps quickly reason about differences between clusterings. To this end, Clustrophile 2 contributes a novel feature, the Clustering Tour, to help users choose clustering parameters and assess the quality of different clustering results in relation to current analysis goals and user expectations. We evaluate Clustrophile 2 through a user study with 12 data scientists, who used our tool to explore and interpret sub-cohorts in a dataset of Parkinson's disease patients. Results suggest that Clustrophile 2 improves the speed and effectiveness of exploratory clustering analysis for both experts and non-experts.","Authors":"M. Cavallo; \u00c7. Demiralp","DOI":"10.1109\/TVCG.2018.2864477","Keywords":"Clustering tour;Guided data analysis;Exploratory data analysis;Interactive clustering analysis;Interpretability;Explainability;Visual data exploration recommendation;Dimensionality reduction;What-if analysis;Clustrophile;Unsupervised learning","Title":"Clustrophile 2: Guided Visual Clustering Analysis","Keywords_Processed":"what if analysis;interactive clustering analysis;clustering tour;exploratory datum analysis;clustrophile;dimensionality reduction;guide datum analysis;visual datum exploration recommendation;interpretability;explainability;unsupervised learning","Keyword_Vector":[0.1524960487,-0.0745975082,0.140812034,-0.1165295501,-0.0643850855,-0.1263201279,-0.0769629044,-0.0336197756,0.0821608179,-0.0663068038,-0.0898357303,-0.0658004183,0.0742605232,0.0348307056,-0.0548929342,-0.1106278093,0.0259087409,-0.1618544005,-0.0836465696,-0.058328076,0.1059063462,-0.013082921,0.1312213742,-0.0287654689,-0.0887807428,-0.0010889335,0.0073189649,-0.1363300132,-0.0106577316,-0.0511766719,-0.081481411,-0.0538266714,0.0392189147,0.0015120116,-0.0217211748,-0.039935964,0.0440609688,-0.0303209309,-0.0065393968,-0.0357838431,0.0287461431,0.0118037284,0.0576055397,0.0432255905,-0.0018929729,-0.0035654113,0.0108283311,-0.04675992,0.0213776047,0.0390829028,-0.0677703117,-0.0427690908],"Abstract_Vector":[0.1675472496,-0.01810692,0.1487186819,-0.0584754808,0.0703166266,-0.0267040531,0.1045276439,-0.0916215908,0.0224801128,-0.02877189,-0.0096244299,-0.0047332776,-0.0649350029,0.0741818648,-0.0346518767,0.0214510032,-0.0386268473,0.0024434314,-0.0223512418,-0.0110069135,0.0397450662,-0.0421003796,-0.0138346933,-0.0484299729,-0.0126564526,-0.042337693,0.0175029678,-0.0160461932,0.0060351345,0.0109404475,0.0208654744,-0.0054026477,0.0104215503,-0.0129678293,-0.0284006411,-0.0302374511,0.0527941863,0.0230021395,-0.0317726374,-0.0114491954,0.0141706959,-0.0414931303,0.0073772922,-0.0087932867,0.0914903576,0.0241140435,-0.0435470323,-0.0542972916,0.0491760261,0.0211904875,0.0112879282,0.0310147743,-0.0288913265,-0.0026335713,0.0035389991,-0.0448982829,0.0622994555,0.0461158999,0.011880706,0.0713483168,-0.0238140182,-0.0214164314,0.0799238153,-0.0159991957,0.0486867537,-0.0087982908,0.002232754,0.0140753513,-0.0532246517,0.0097412781,0.0242180761,-0.0458662484,0.0045453595,0.0172291032,-0.0030546932,0.0217660392,-0.0059740618,0.0215776231,0.0030381459,-0.0396965981,0.032038395,0.0020675527,0.0178872497,0.0183768225,-0.0261708679,-0.0251085615,-0.0162589616,-0.0322989812,-0.080693171,-0.0109419095,0.0381086594,0.0146647991,0.0304609296,0.0193095113,0.015212816,0.0197842735,0.0103482607,-0.0048631593,-0.0111745405,-0.0143401764,0.0399116573,0.007549459,-0.009147435,0.0293956348,0.0369137821,-0.0202892843,-0.0072001497,0.0019751102,0.0108895974,-0.026638193,-0.013684905,-0.0201993244,0.0372535788,0.0107332476,-0.0087142065,0.0193984939]},"502":{"Abstract":"Augmented reality (AR) marker hiding is a technique to visually remove AR markers in a real-time video stream. A conventional approach transforms a background image with a homography matrix calculated on the basis of a camera pose and overlays the transformed image on an AR marker region in a real-time frame, assuming that the AR marker is on a planar surface. However, this approach may cause discontinuities in textures around the boundary between the marker and its surrounding area when the planar surface assumption is not satisfied. This paper proposes a method for AR marker hiding without discontinuities around texture boundaries even under nonplanar background geometry without measuring it. For doing this, our method estimates the dense motion in the marker's background by analyzing the motion of sparse feature points around it, together with a smooth motion assumption, and deforms the background image according to it. Our experiments demonstrate the effectiveness of the proposed method in various environments with different background geometries and textures.","Authors":"N. Kawai; T. Sato; Y. Nakashima; N. Yokoya","DOI":"10.1109\/TVCG.2016.2617325","Keywords":"Marker hiding;diminished reality;texture deformation","Title":"Augmented Reality Marker Hiding with Texture Deformation","Keywords_Processed":"texture deformation;diminish reality;marker hiding","Keyword_Vector":[0.1884357129,0.194737394,-0.0678486868,-0.0572907116,-0.0498701851,0.0047412865,0.0145684943,0.0120300618,0.0116162417,0.0122656205,-0.0189389981,-0.0210390057,0.02422848,-0.0034712702,0.0153060293,0.0183338463,-0.0251667521,-0.0218076569,-0.0144795308,0.0165072102,-0.0592396816,-0.0087742896,0.0079939216,-0.0406795835,0.0090012299,0.002121756,0.0218100048,-0.0064206055,-0.0205227773,-0.0170409659,0.0183815826,0.0149781615,-0.0118306791,-0.0012034222,0.0070298764,-0.00378484,0.0803883228,0.0476243583,0.0085818818,0.0098041466,-0.0063603607,0.0417890308,-0.0639086633,-0.0382247077,-0.0484686063,0.0210776797,0.0305074645,0.0433401519,-0.0446334837,0.0257484186,0.0354033385,0.0591058859],"Abstract_Vector":[0.1411597698,0.00929556,0.0402310131,0.0048510244,-0.0341398868,-0.0111771846,-0.024421395,-0.0277316466,-0.0295695955,0.0342453417,-0.0032005427,-0.0146475544,-0.0105570033,-0.0049059527,-0.0145535506,0.0219429334,0.0230526578,0.0387378295,0.0102902499,-0.025809773,-0.001649555,0.0068005251,0.0032364009,-0.0673319745,-0.0194475808,0.0572441614,-0.0119601668,0.0075348749,0.0074692535,-0.0180640205,0.0318709101,0.011842249,-0.0215171772,0.0334342125,0.0022141329,0.0136439778,-0.0039735955,0.044804903,0.0475228871,0.001580455,-0.000057876,-0.0015003888,-0.0428089232,-0.0147914243,0.0028989552,-0.0102706714,0.0220746905,-0.0327003984,-0.0049260887,-0.0300214647,0.0014221537,0.0086183103,-0.0250420628,0.0369132708,0.0038071427,0.0117509193,-0.0033660751,-0.0173006901,-0.018390607,-0.0135546543,0.0035502462,0.0142486884,0.0053044709,0.0043413308,-0.0113591726,-0.0353809905,-0.0269018877,-0.0087231101,-0.0191385293,0.0280071876,-0.0019498656,0.021446984,0.0161300482,-0.0214915294,-0.0083867323,-0.0002238881,-0.0175148695,-0.0004884565,-0.0190954804,-0.0026128875,-0.0241047449,-0.0242103722,0.0197669121,0.0085639004,-0.0044049039,-0.0184498706,-0.0267119953,0.0118575407,0.0207680804,-0.0116911743,0.0033997345,-0.0093489489,-0.0164890334,0.0225540867,0.0261852721,-0.0031838332,0.0104332917,0.0278171839,-0.0085369653,0.015857926,-0.0360126481,-0.0156153724,0.0099136538,-0.0085251239,0.0125186421,-0.0134603704,0.0219702913,-0.0009999747,0.0144424514,-0.0204154824,-0.0274652926,0.0052604702,0.0024939084,-0.0112073851,-0.0101433651,0.003160324]},"503":{"Abstract":"This paper presents a visualization framework that aids readers in understanding and analyzing the contents of medium-sized text collections that are typical for the opus of a single or few authors. We contribute several document-based visualization techniques to facilitate the exploration of the work of the German author Bazon Brock by depicting various aspects of its texts, such as the TextGenetics that shows the structure of the collection along with its chronology. The ConceptCircuit augments the TextGenetics with entities - persons and locations that were crucial to his work. All visualizations are sensitive to a wildcard-based phrase search that allows complex requests towards the author's work. Further development, as well as expert reviews and discussions with the author Bazon Brock, focused on the assessment and comparison of visualizations based on automatic topic extraction against ones that are based on expert knowledge.","Authors":"P. Riehmann; D. Kiesel; M. Kohlhaas; B. Froehlich","DOI":"10.1109\/TVCG.2018.2824822","Keywords":"Glyph-based techniques;text and document data;coordinated and multiple views","Title":"Visualizing a Thinker's Life","Keywords_Processed":"Glyph base technique;text and document datum;coordinated and multiple view","Keyword_Vector":[0.0884430882,-0.0143049849,0.1017125281,0.0830661075,-0.0824439344,0.0635321579,0.0246321644,0.0554032132,-0.1291553456,0.1041350599,0.1244202058,-0.0169494487,0.0168687737,-0.0529000823,0.0115128756,-0.0379260972,-0.0340552839,-0.0030661041,-0.0317874362,-0.0124172835,0.0017309895,0.0055569422,0.0071746693,-0.0305896496,0.0103144026,-0.0100785095,-0.032656797,0.0259877754,-0.0137005341,0.0063858137,0.0241109127,-0.0334372188,-0.0007686072,-0.0220451077,-0.0211275462,0.0276308562,-0.0156217349,-0.0067012784,-0.0439657202,0.0045278157,-0.0127640708,0.0329122019,-0.0048168589,-0.0119847951,0.0389760396,0.0443510396,0.0036490403,0.0189216561,0.0128179822,0.0249296206,0.0129046175,-0.0494777596],"Abstract_Vector":[0.1514239954,0.0218781049,0.1810101882,-0.0730305055,0.0597001329,0.0510101961,0.1491233525,-0.0795653133,-0.0024688273,-0.0246870903,-0.0095666217,-0.0197043999,-0.0433092122,0.0127544821,-0.031371733,0.0609036293,-0.0122784159,0.0331319649,-0.0235483541,0.0803811338,-0.0056215219,0.0049846365,0.0438393464,0.0161146709,-0.028623869,-0.027126722,0.0185170394,-0.0039771628,-0.0157877123,0.0027411507,0.0319451859,0.0022634634,0.0820090285,0.004275051,-0.0163080065,0.0094294542,0.0024321264,-0.0084034733,-0.0130420888,0.0470178034,-0.0015640136,0.0095358901,0.0085419881,0.0108616973,0.0284435655,-0.0188213723,-0.0182528349,0.0043057404,-0.0077866997,-0.0192977952,-0.0362133755,-0.0373095668,-0.0103738125,0.0164019281,-0.0143411233,-0.0044633629,0.0210354683,0.0171512024,0.0132718221,0.0140173933,0.0132772457,-0.0264330535,0.0161651263,-0.0325203986,-0.0024677417,-0.0432027099,-0.0097640098,-0.0485762853,-0.0320724015,-0.0124495026,0.0026501475,-0.0193462259,-0.0172362928,-0.0125945592,0.0088752468,-0.0058258077,0.0120994101,-0.0122694177,0.0098131805,-0.017744153,-0.0000261839,0.0011991486,0.0013106669,-0.0135608386,0.0107016028,-0.011221213,0.0097618595,0.0059943076,0.0179884961,-0.0272242558,-0.0150174809,0.0097434438,-0.0174635064,-0.0317878791,-0.0051859975,0.0180193423,0.0128271151,0.0098910981,0.0053139288,-0.019371416,0.0159548529,-0.018834273,-0.0111195157,0.001396275,-0.0226565838,0.0273575774,-0.0363192647,-0.0207222669,-0.0309819487,-0.003041607,0.0103777179,0.0332305636,0.0013398357,0.0040535245,0.0260696366,0.0018637483]},"504":{"Abstract":"Precomputed sound propagation samples acoustics at discrete scene probe positions to support dynamic listener locations. An offline 3D numerical simulation is performed at each probe and the resulting field is encoded for runtime rendering with dynamic sources. Prior work place probes on a uniform grid, requiring high density to resolve narrow spaces. Our adaptive sampling approach varies probe density based on a novel \u201clocal diameter\u201d measure of the space surrounding a given point, evaluated by stochastically tracing paths in the scene. We apply this measure to layout probes so as to smoothly adapt resolution and eliminate undersampling in corners, narrow corridors and stairways, while coarsening appropriately in more open areas. Coupled with a new runtime interpolator based on radial weights over geodesic paths, we achieve smooth acoustic effects that respect scene boundaries as both the source or listener move, unlike existing visibility-based solutions. We consistently demonstrate quality improvement over prior work at fixed cost.","Authors":"C. R. A. Chaitanya; J. M. Snyder; K. Godin; D. Nowrouzezahrai; N. Raghuvanshi","DOI":"10.1109\/TVCG.2019.2898765","Keywords":"Diffraction;interpolation;mean free path;radial basis function;ray tracing;reciprocity;room acoustics;wave simulation","Title":"Adaptive Sampling for Sound Propagation","Keywords_Processed":"mean free path;room acoustic;reciprocity;diffraction;wave simulation;radial basis function;ray trace;interpolation","Keyword_Vector":[0.1751140702,-0.0567131176,-0.0206981392,-0.0405556184,-0.006499286,0.1469295098,-0.0162802128,0.0585913973,0.3011439478,0.262539831,0.1959059727,0.3817503228,-0.0770741733,0.0133690236,0.0851162586,0.0383673871,0.1497951659,0.0101562847,-0.0785840257,-0.0680112254,0.0317472213,-0.119049436,-0.0195015304,0.1277698507,-0.0120596447,-0.0090836817,-0.0136731145,-0.0389681038,0.0004738844,0.0222136828,-0.0256541317,-0.0037529476,0.0122409801,0.0388988687,-0.0213875622,0.0935417302,0.0396533686,0.0875260913,0.0842752314,0.0084174043,0.07870239,-0.0218629394,-0.0034965248,0.0030700037,-0.0909196493,-0.0595002428,0.0323997208,-0.079310739,0.0843144807,-0.0462519472,-0.0669510067,0.038456145],"Abstract_Vector":[0.3127908774,-0.0740684369,0.0037823478,-0.0261826392,0.0257410503,-0.0951814964,-0.042849205,0.0086952857,0.088997445,-0.0269194205,0.0212713648,0.008268771,0.0441395213,-0.0051422114,0.0217722731,-0.0461597284,-0.0316063678,-0.0399587087,0.0057143584,-0.0238829995,0.0032363249,0.0235546295,-0.0183227258,0.04979472,0.0008083122,0.0192505696,0.0387512645,-0.0108514737,-0.0075325576,-0.0461993729,-0.0140971041,0.0003801966,-0.0426643001,0.0161878313,0.0154366966,-0.0150845591,0.0016924739,-0.033820914,-0.0450738442,-0.0350790289,0.0093050704,0.0422775426,-0.0375493908,-0.0066742706,0.0002687322,-0.0311332574,0.0490830888,0.0300752008,-0.0314323949,-0.0430204241,-0.0515858527,-0.0288351371,0.0461880477,-0.0263510739,0.0504944523,-0.0321464359,0.0227370085,-0.0283516578,0.0298759639,0.002043353,-0.0239557959,-0.0135044613,0.024536893,0.0417729607,0.0251332621,-0.067340373,-0.026751105,-0.0029680382,0.0215036487,-0.0111392525,0.0172849282,0.0113334871,-0.0132095122,0.0193994312,0.0405621337,0.0113758224,0.0336096419,0.0206672585,0.0061497317,-0.0078047307,0.0377085677,0.0120616965,-0.0346710082,0.0318337739,-0.0267149552,-0.0067104082,-0.067942628,-0.0120040232,0.0098553577,-0.0252458626,-0.0356850417,0.0148600434,-0.022010905,-0.0073692497,0.0314158845,0.0099233081,0.0099968175,-0.026254128,-0.0289288991,-0.0057393811,0.005619123,-0.0537958757,0.0194839614,-0.0090266392,-0.0010638662,0.0299866276,-0.0238978465,0.0232445014,-0.0474153427,0.0221938429,0.0177130884,-0.0018102944,-0.0142742567,-0.0337860644,-0.0141532851,0.0154862175]},"505":{"Abstract":"Decorating the surfaces of 3D printed objects with color textures is still not readily available in most consumer-level or even high-end 3D printers. Existing techniques such as hydrographics color transfer suffer from the issues of air pockets in concave regions and discoloration in overly stretched regions. We propose a novel thermoforming-based coloring technique to alleviate these problems as well as to simplify the overall procedure. Thermoforming is a widely used technique in industry for plastic thin shell product manufacturing by pressing heated plastic sheets onto molds using atmospheric pressure. We attach on the transparent plastic sheet a precomputed color pattern decal prior to heating, and adhere it to 3D printed models treated as the molds in thermoforming. The 3D models are thus decorated with the desired color texture, as well as a thin, polished protective cover. The precomputation involves a physical simulation of the thermoforming process to compute the correct color pattern on the plastic sheet, and the vent hole layout on the 3D model for air pocket elimination. We demonstrate the effectiveness and accuracy of our computational model and our prototype thermoforming surface coloring system through physical experiments.","Authors":"Y. Zhang; Y. Tong; K. Zhou","DOI":"10.1109\/TVCG.2016.2598570","Keywords":"3D printing;thermoforming;thermoplastic sheet simulation;texture mapping","Title":"Coloring 3D Printed Surfaces by Thermoforming","Keywords_Processed":"thermoplastic sheet simulation;texture mapping;3d printing;thermoforme","Keyword_Vector":[0.1381893519,0.0787785929,-0.0120967252,-0.037882358,-0.015043798,0.0046780622,0.0011991144,0.0090748275,0.0108426623,-0.0099324699,-0.0124425031,-0.0046011369,0.0212503441,-0.0081645367,0.0202446532,0.0190826082,-0.0265517967,-0.0195907448,-0.0087702596,0.0265633097,-0.0278816948,0.015781041,0.0057180126,-0.0429721152,0.0001550522,0.0013160905,0.0447181646,0.015639739,-0.0059375869,0.0052885636,0.0159025248,-0.0002428159,-0.0054180093,-0.0173535052,-0.0173062498,0.0006952821,0.0506705802,0.0521818006,-0.02640511,-0.0213585061,-0.0173371049,0.0524875956,-0.056781211,-0.056268548,-0.0422456096,0.0275976465,-0.0062565983,0.0687097809,-0.0720033771,0.0304610059,0.0297659345,0.0902813863],"Abstract_Vector":[0.1623305247,-0.0370224048,-0.0135006673,-0.0255635966,-0.0193454179,-0.00866259,0.0229204337,0.0086372604,0.0332843058,0.006247857,0.0257599598,-0.015894854,0.0423219517,0.0096999088,-0.0050264528,-0.0388448543,-0.0223590027,0.0178695532,-0.0200257694,-0.0113106554,-0.0203188942,-0.0049662893,0.0064123989,-0.0079822623,-0.0059159802,0.0012314655,-0.0130021374,0.0362150915,-0.0061078134,0.031771741,0.0192889441,0.0123960944,0.0139633319,0.0073028217,0.017277576,0.0199059192,-0.034112038,0.011031426,0.0225243513,-0.021233647,-0.0107913569,-0.0146706098,0.0129093936,0.0454244713,0.0404487765,0.0117717301,-0.0022345083,0.0544712275,-0.0146528525,-0.0186256154,0.0388943241,-0.0191201326,-0.0346173656,0.0059016273,0.0102931512,0.0053042204,-0.0207836827,-0.0065132694,-0.0467125143,0.0112713266,-0.0193319463,-0.0105393657,-0.0014678733,0.0104569398,-0.0166470037,0.0210975682,-0.0017995165,-0.0147174539,-0.0135705688,0.0084715937,0.0077781437,0.0163228714,0.031275911,-0.0385252679,0.0401958737,0.0108966765,0.0434761911,0.0190647755,-0.0116751642,0.0206059731,-0.014100582,0.0084512128,0.0143072587,0.000587236,-0.0223371848,0.007648731,0.0012689664,0.0112405974,0.0404542634,0.0108851805,0.0015003704,-0.0079324194,0.0107103024,-0.0054108084,-0.0375185847,-0.0034646681,0.0304311041,-0.0106047567,0.0154085146,-0.0103380131,0.0011294465,-0.0296676782,0.0286120758,-0.0156041515,-0.0430066409,-0.0135285485,0.000593135,-0.0007097802,-0.0001151868,0.0154020716,0.0278425766,0.0076574731,-0.033381192,0.0027700175,0.0326737686,-0.0184740405]},"506":{"Abstract":"Multivariate event sequences are ubiquitous: travel history, telecommunication conversations, and server logs are some examples. Besides standard properties such as type and timestamp, events often have other associated multivariate data. Current exploration and analysis methods either focus on the temporal analysis of a single attribute or the structural analysis of the multivariate data only. We present an approach where users can explore event sequences at multivariate and sequential level simultaneously by interactively defining a set of rewrite rules using multivariate regular expressions. Users can store resulting patterns as new types of events or attributes to interactively enrich or simplify event sequences for further investigation. In Eventpad we provide a bottom-up glyph-oriented approach for multivariate event sequence analysis by searching, clustering, and aligning them according to newly defined domain specific properties. We illustrate the effectiveness of our approach with real-world data sets including telecommunication traffic and hospital treatments.","Authors":"B. C. M. Cappers; J. J. van Wijk","DOI":"10.1109\/TVCG.2017.2745278","Keywords":"Event Visualization;Multivariate Events;Regular Expressions;Sequence Alignment;Interaction","Title":"Exploring Multivariate Event Sequences Using Rules, Aggregations, and Selections","Keywords_Processed":"Multivariate event;regular expression;interaction;sequence Alignment;event visualization","Keyword_Vector":[0.2137274741,-0.108393054,0.0398724096,-0.1603277004,0.1430794001,0.0748075849,0.3340595507,-0.0892148647,-0.0537428997,-0.0931451113,-0.0103409272,0.0706875028,-0.0052737949,-0.0496073334,0.0162889061,0.0693471142,-0.0116963215,0.0191476808,0.0243388564,0.0039159138,0.025929155,-0.0100366244,-0.0267380175,-0.0684482198,0.1216490372,-0.0456530154,0.0569860717,0.0114122292,0.000234592,-0.0403993343,-0.0495479343,-0.1046359422,0.0187070007,0.1337173893,0.122199561,-0.011230373,-0.0451697105,-0.0730052935,-0.0757001782,0.0537176296,-0.0739617228,-0.0369365952,-0.0668939393,0.0351098136,0.0638477312,0.1478097827,-0.1236435885,0.0261686609,-0.0095704651,0.0211785851,0.0093288092,0.0494338518],"Abstract_Vector":[0.227731137,-0.1664323651,-0.013583852,0.2441696672,0.2671335535,0.146122562,-0.0466484854,-0.0781515596,-0.0349700759,0.0515455039,0.1070758929,-0.0279685193,0.0461004979,0.0294232214,-0.0097287443,-0.0697877456,0.0153351818,0.0382066677,-0.010384091,0.030205567,0.0608615849,0.0145962158,0.0243459067,-0.0520413915,0.0466812106,-0.0050554093,-0.0138392328,0.0043795868,0.0155270047,0.0388546414,-0.0397713546,-0.055601872,-0.0600941334,0.0414734701,-0.1141462613,-0.0932264847,-0.0098800254,0.0288165765,-0.0032295506,0.0188801815,0.0414755608,-0.0438532659,-0.004388267,0.0392370463,0.0505455779,0.0411346724,0.014006096,-0.0134851076,-0.0309160908,-0.0463170331,-0.0336594544,-0.013415588,0.0667474827,0.0121027732,-0.0095343622,-0.0613717149,-0.0445933264,0.0269324258,0.0338652012,0.0396465508,-0.0120351705,0.0281505836,0.0192378345,-0.0328039701,0.0532898092,0.029081007,-0.0194900802,-0.0151488797,-0.0314774339,0.0057756492,0.0122780494,-0.0489402267,0.0390110672,-0.0077466848,0.0340866679,-0.0121217435,-0.0452465996,-0.0064008431,0.0004857832,-0.0087453839,0.0181468446,-0.012624934,0.0268839666,-0.0095326728,0.0199155193,0.0251603769,-0.0485947441,0.0004346342,-0.0083039346,0.0133558352,0.0287751757,-0.0239616634,0.0241125722,-0.0379315865,0.013807692,0.0365001581,0.0522636047,-0.0115023474,-0.0182216464,-0.0170659598,0.0222799554,-0.0341602306,0.0013704068,0.0204951588,0.0348574665,-0.0300688253,0.028071557,-0.0169029959,0.0676375246,-0.0141375106,-0.0088090812,0.0227763578,-0.0017172613,0.0191772714,0.0184605054,-0.0218472461]},"507":{"Abstract":"This paper presents a framework to explore multi-field data of aneurysms occurring at intracranial and cardiac arteries by using statistical graphics. The rupture of an aneurysm is often a fatal scenario, whereas during treatment serious complications for the patient can occur. Whether an aneurysm ruptures or whether a treatment is successful depends on the interaction of different morphological such as wall deformation and thickness, and hemodynamic attributes like wall shear stress and pressure. Therefore, medical researchers are very interested in better understanding these relationships. However, the required analysis is a time-consuming process, where suspicious wall regions are difficult to detect due to the time-dependent behavior of the data. Our proposed visualization framework enables medical researchers to efficiently assess aneurysm risk and treatment options. This comprises a powerful set of views including 2D and 3D depictions of the aneurysm morphology as well as statistical plots of different scalar fields. Brushing and linking aids the user to identify interesting wall regions and to understand the influence of different attributes on the aneurysm's state. Moreover, a visual comparison of pre- and post-treatment as well as different treatment options is provided. Our analysis techniques are designed in collaboration with domain experts, e.g., physicians, and we provide details about the evaluation.","Authors":"M. Meuschke; T. G\u00fcnther; P. Berg; R. Wickenh\u00f6fer; B. Preim; K. Lawonn","DOI":"10.1109\/TVCG.2018.2864509","Keywords":"Medical visualizations;aneurysms;blood flow;parametrization","Title":"Visual Analysis of Aneurysm Data using Statistical Graphics","Keywords_Processed":"parametrization;medical visualization;aneurysm;blood flow","Keyword_Vector":[0.1484755559,-0.124158594,-0.0738491937,0.1449364207,-0.0785559016,0.1375796595,-0.0068421527,-0.019987538,-0.0254220979,-0.0314120013,0.0454420279,-0.0520184013,0.0655386099,0.2787584556,-0.0963837858,-0.0524328355,0.0748112163,-0.0204634931,0.0679718337,0.027167953,0.0009416964,-0.0200821321,0.018926045,-0.0275438004,0.0508480105,-0.013214612,0.0220080333,0.0352356513,-0.0662881598,0.0151396291,-0.075051944,0.0081391589,0.0485037156,0.0157544691,-0.0213601155,-0.0343431376,0.0156061269,0.034421807,0.0034819059,0.013221949,-0.0242146,-0.0152967474,-0.0495898772,-0.0068508971,-0.0107782105,0.0269204044,0.0251682819,-0.0174321441,-0.0026109799,0.0047586119,0.0458557077,0.009333658],"Abstract_Vector":[0.1809825609,-0.0761882522,-0.0108392747,0.0180583265,0.0330074138,-0.0631862116,-0.0468824728,-0.0359008217,0.0435191276,-0.0407159054,0.0324487361,-0.0039072693,0.0024773418,-0.0258802256,0.0305554835,0.0018284547,-0.0466489467,-0.0126180501,-0.0351730879,-0.0261002587,-0.0224537183,0.0094720709,0.0147332592,-0.0037113435,-0.0393082598,-0.0480519604,0.001216242,0.0409890168,-0.0187548484,-0.0445887204,0.0414734972,-0.0143298858,0.0106815227,-0.0001572511,-0.0259044723,-0.0122753603,0.0169908625,-0.0107512539,-0.009118563,0.0010919251,0.0221241868,-0.0360227144,0.0075188079,0.0035761572,-0.0109807356,-0.0020035987,0.0058505406,-0.0274572622,-0.0364196636,0.0167165131,-0.022038915,0.0153914864,0.0007062966,0.0248008851,0.0004153219,-0.0268185366,0.0008762491,-0.0389816491,-0.0096939023,0.0301057371,-0.0054033829,-0.0351771873,0.0069136194,-0.0088052676,-0.0092207975,0.0308779318,-0.0377037299,-0.0200270833,-0.0186869393,-0.0319745743,0.0106249311,-0.0074353889,0.0071746011,-0.0133155929,0.0267730559,-0.003107114,-0.0305569618,0.0096390505,0.0003833957,-0.0441607702,-0.0022458542,-0.020986514,-0.0211487667,-0.0024478497,0.0090155578,0.0081757112,0.0270325452,-0.0184794036,-0.0438954813,0.0078407267,-0.0067218925,0.0078395167,0.0232471699,-0.0171092182,0.0170783458,-0.0093183525,-0.0116358681,-0.0025423284,0.0183705236,-0.0252864389,0.0057429193,-0.0195923552,-0.0095425221,0.038085591,0.024076947,-0.0016935844,-0.0306299148,-0.0114540281,0.0084908584,0.0181998578,-0.0183054753,0.0306102606,-0.0171283369,0.011409256,0.0007312717,-0.0330200156]},"508":{"Abstract":"Treemaps are a popular tool to visualize hierarchical data: items are represented by nested rectangles and the area of each rectangle corresponds to the data being visualized for this item. The visual quality of a treemap is commonly measured via the aspect ratio of the rectangles. If the data changes, then a second important quality criterion is the stability of the treemap: how much does the treemap change as the data changes. We present a novel stable treemapping algorithm that has very high visual quality. Whereas existing treemapping algorithms generally recompute the treemap every time the input changes, our algorithm changes the layout of the treemap using only local modifications. This approach not only gives us direct control over stability, but it also allows us to use a larger set of possible layouts, thus provably resulting in treemaps of higher visual quality compared to existing algorithms. We further prove that we can reach all possible treemap layouts using only our local modifications. Furthermore, we introduce a new measure for stability that better captures the relative positions of rectangles. We finally show via experiments on real-world data that our algorithm outperforms existing treemapping algorithms also in practice on either visual quality and\/or stability. Our algorithm scores high on stability regardless of whether we use an existing stability measure or our new measure.","Authors":"M. Sondag; B. Speckmann; K. Verbeek","DOI":"10.1109\/TVCG.2017.2745140","Keywords":"Treemap;Stability;Local Moves","Title":"Stable Treemaps via Local Moves","Keywords_Processed":"local move;stability;treemap","Keyword_Vector":[0.0553763457,0.0097699026,0.0467111943,0.0444736635,0.0479106756,-0.050829058,0.0262497651,-0.0563608746,-0.0449116249,0.0243516241,0.0202464842,0.0258322099,-0.0188165191,0.0247432454,0.0333529384,-0.0719219236,-0.0197844926,-0.0045756279,0.0526400727,0.0077849092,-0.0165077555,-0.0209433465,-0.0349268059,0.0631502011,0.051130646,0.0147448268,-0.0170832081,0.0445393246,0.0138083089,-0.0128647559,-0.0719736722,-0.0708562238,0.0257904122,0.0841614617,0.1786827271,-0.0437975791,-0.0529265929,-0.0372188519,-0.0651259616,0.0180501637,0.0267633282,-0.0141540774,-0.0547639839,0.049365005,0.0068069924,0.0443780095,0.0208057906,0.072588048,0.1723807593,-0.0689306159,-0.0320912158,0.0476440599],"Abstract_Vector":[0.147393251,0.0006918012,-0.0120820083,0.0901333604,0.0836308753,0.010789705,-0.0201436734,-0.0057758832,0.0169681115,0.0056816226,-0.0507587449,0.0551234407,0.0581450823,-0.0115798523,0.0127109965,0.0127910259,0.0155649356,0.0357576693,0.068627014,0.0706385242,0.0713614581,0.0946773319,0.0742964609,-0.0527770268,0.096608579,0.1008235443,-0.0421205093,-0.0738567581,-0.074739161,-0.0312430622,0.0316846889,-0.0369260901,-0.0856354111,0.086631662,-0.109895553,-0.0308615647,-0.0521592145,-0.066279144,0.0812048158,0.0226041146,-0.0222216896,0.0134763128,0.0679641614,-0.0196123424,0.0032411475,0.0703759695,0.0147799329,-0.0142771507,-0.0775268123,-0.0562841955,0.0260178699,-0.0009552154,-0.0339035702,0.0263891941,-0.0006491972,-0.0097957392,-0.0263561585,0.0089648885,-0.0024399159,0.0157025269,-0.0094896037,-0.0540891104,0.0688126803,-0.0092308173,0.0042997707,-0.017438704,-0.0439003448,-0.0073077884,0.0539061875,0.067717671,0.0049824945,0.0411385615,-0.0480603098,0.0074782523,0.0786956601,-0.0267076031,-0.0346292373,0.0060648321,0.0366660745,-0.0156941477,0.0022031676,0.0285181351,-0.0457022793,-0.0300275143,0.0076368801,0.0733416152,0.0411799789,-0.0034204561,0.0327348277,0.036591098,0.0389930956,-0.0250967614,0.0046719096,0.0675246315,-0.0356292718,0.0013350754,-0.0297511684,-0.0105707229,0.0291674708,0.0078666022,0.0439058875,0.0005703925,0.0003569275,-0.0276598367,0.0113374159,-0.0279812409,-0.0287728581,-0.0291346228,-0.0172928252,-0.0113338165,-0.0128747006,-0.0309533212,0.0031680609,-0.0088320973,0.0250420624,-0.0033178598]},"509":{"Abstract":"This paper presents DXR, a toolkit for building immersive data visualizations based on the Unity development platform. Over the past years, immersive data visualizations in augmented and virtual reality (AR, VR) have been emerging as a promising medium for data sense-making beyond the desktop. However, creating immersive visualizations remains challenging, and often require complex low-level programming and tedious manual encoding of data attributes to geometric and visual properties. These can hinder the iterative idea-to-prototype process, especially for developers without experience in 3D graphics, AR, and VR programming. With DXR, developers can efficiently specify visualization designs using a concise declarative visualization grammar inspired by Vega-Lite. DXR further provides a GUI for easy and quick edits and previews of visualization designs in-situ, i.e., while immersed in the virtual world. DXR also provides reusable templates and customizable graphical marks, enabling unique and engaging visualizations. We demonstrate the flexibility of DXR through several examples spanning a wide range of applications.","Authors":"R. Sicat; J. Li; J. Choi; M. Cordeil; W. Jeong; B. Bach; H. Pfister","DOI":"10.1109\/TVCG.2018.2865152","Keywords":"Augmented Reality;Virtual Reality;Immersive Visualization;Immersive Analytics;Visualization Toolkit","Title":"DXR: A Toolkit for Building Immersive Data Visualizations","Keywords_Processed":"virtual reality;immersive Analytics;visualization Toolkit;immersive visualization;augmented reality","Keyword_Vector":[0.1791908351,-0.0746525812,-0.0750468513,-0.0104092014,-0.0010698506,-0.0600201542,-0.1298724236,-0.0433018879,-0.1475149358,0.1493521149,-0.1110056828,0.344695185,0.1061419104,-0.0703507431,0.0148492739,0.0195270393,0.1127141475,-0.1072106211,0.015156601,0.1443494474,-0.1077529569,-0.1885091984,-0.0795044638,-0.0373565705,0.1138734292,0.0100814174,-0.0018032819,-0.0496211024,0.0098501646,-0.0263320942,0.0272916593,-0.0404791483,-0.0526893236,-0.0431858777,-0.0745058179,-0.0243593499,0.0411190729,0.0770232263,0.0421494364,0.0604848946,-0.069344419,-0.0206346275,-0.009639319,0.0222141774,-0.0685839814,0.023972358,0.0165090507,0.0277821464,0.0485848493,0.0206975912,0.0163790296,0.0371219218],"Abstract_Vector":[0.1849445714,-0.1121404883,-0.0098597167,-0.0265260989,-0.0395972917,0.0426781243,0.0045000015,0.0103665159,0.0286207102,0.0063631862,-0.0274461042,-0.0197851108,0.0015761979,0.0004421791,0.002193353,0.0733512239,-0.0047507211,0.0020496129,0.0715982942,0.0246981858,0.0053971353,-0.0620360883,0.0010799702,0.0178951509,-0.0004835987,-0.0512545095,-0.0460884944,0.0041414044,-0.0204775587,-0.0250252558,0.0091217254,-0.0243128442,-0.0085034819,-0.0227419958,0.0028160928,-0.0227890879,0.0011353443,-0.0195007471,-0.0405453707,-0.0243906566,0.0049649867,-0.0124369497,-0.0019771791,-0.0050428242,0.0387322919,-0.0351961262,0.0362227923,0.0196472256,-0.0126956416,-0.0191294027,-0.0125390112,-0.0212541381,0.048995232,-0.020458685,0.0469116698,0.0235289633,0.0228325246,-0.0105598404,-0.0358007798,0.0605357028,0.0138415402,-0.0460924934,0.0203374027,0.0472971249,0.0249241472,-0.0379898504,0.012082043,0.0355530774,-0.0569273081,-0.0279756737,0.0251593852,0.0261087778,-0.0067106205,0.0067310152,-0.0210782052,-0.0224018576,-0.0045552793,-0.0051195558,0.002295553,-0.0060469759,-0.0173059377,0.026516519,0.0251981329,-0.0306569367,0.0062567228,-0.01161737,-0.0298142116,0.0102311356,0.0356679979,0.0424414993,0.0387037339,-0.0084176654,-0.028906172,0.0057003462,0.021300027,0.0372598289,0.0039159521,0.0144983662,0.0111839395,0.0071548001,-0.0215698735,-0.0147944407,0.0033682205,-0.0176763665,0.021518212,0.0159935597,-0.0070747772,-0.0141824114,0.0425313489,0.014428965,-0.0148470708,0.0129564859,0.0182802292,-0.0131830597,-0.0294315559,0.0002925628]},"51":{"Abstract":"Details-on-demand is a crucial feature in the visual information-seeking process but is often only implemented in highly constrained settings. The most common solution, hover queries (i.e., tooltips), are fast and expressive but are usually limited to single mark (e.g., a bar in a bar chart). `Queries' to retrieve details for more complex sets of objects (e.g., comparisons between pairs of elements, averages across multiple items, trend lines, etc.) are difficult for end-users to invoke explicitly. Further, the output of these queries require complex annotations and overlays which need to be displayed and dismissed on demand to avoid clutter. In this work we introduce SmartCues, a library to support details-on-demand through dynamically computed overlays. For end-users, SmartCues provides multitouch interactions to construct complex queries for a variety of details. For designers, SmartCues offers an interaction library that can be used out-of-the-box, and can be extended for new charts and detail types. We demonstrate how SmartCues can be implemented across a wide array of visualization types and, through a lab study, show that end users can effectively use SmartCues.","Authors":"H. Subramonyam; E. Adar","DOI":"10.1109\/TVCG.2018.2865231","Keywords":"Graphical overlays;details-on-demand;graph comprehension","Title":"SmartCues: A Multitouch Query Approach for Details-on-Demand through Dynamically Computed Overlays","Keywords_Processed":"graph comprehension;detail on demand;graphical overlay","Keyword_Vector":[0.172705277,-0.1013602697,-0.0054235203,-0.0313435132,-0.0184565015,0.0779464325,-0.1179209186,-0.0336018939,-0.0680957371,0.0426284177,-0.0550122764,0.0693913065,0.0117049513,-0.0030392021,-0.1145933082,0.009658403,-0.1001557355,0.0539731461,-0.0835812587,-0.0603240452,0.0692821922,0.0263264707,0.0246106214,-0.0287801077,-0.0539147617,0.0615504052,0.0294089112,0.0574098102,0.0258125005,-0.0093478025,-0.05577796,-0.0377608798,-0.0589827766,-0.0486916687,0.0089961212,-0.081761338,0.056367702,-0.0517502029,-0.0703493739,-0.0332822847,0.0211075429,0.0105750477,-0.0068485074,-0.0035022797,0.038180073,-0.0454136661,0.0150377322,-0.0301033944,0.1041453767,-0.0274498803,0.0595958471,-0.0066106471],"Abstract_Vector":[0.1902960368,-0.0986784901,0.0280304452,-0.0299586471,-0.0335121703,-0.0350535088,0.0043445527,-0.0017011597,-0.0404771253,-0.022048233,0.0066929343,0.0337209039,0.02856923,-0.0115419212,0.0186286295,-0.0242156706,0.0320272609,0.0202413513,-0.0302643656,-0.02708205,-0.0088763659,-0.0032939969,-0.0319049736,-0.0382745996,-0.0210741018,0.0269855713,-0.0245068406,0.0275118544,0.0102153325,-0.0254732881,0.0009968091,0.0368855558,0.0019616891,0.0074657642,-0.0361950603,-0.0214656306,0.0138584868,0.029231564,-0.0137146312,-0.0025060049,0.0265440876,-0.0231856774,-0.0009751098,-0.0274501635,0.0051761623,-0.0460515251,0.0110779235,-0.0019734396,-0.0235064842,-0.0117287271,0.0182417578,0.0073616173,-0.0074877359,-0.0050783895,-0.0365015989,0.0129657607,0.0115207579,0.0241157318,0.0224761714,0.0223069604,0.0009351922,-0.0074450088,-0.0459338345,0.0242283854,0.0054296632,0.0020224001,-0.0027000103,0.0297128482,-0.006055876,-0.0096978258,-0.0176098215,0.0078765039,-0.0199794867,-0.0051968857,0.0213150652,-0.0007526083,0.0002972424,-0.0022921349,-0.0534986941,-0.0513846016,0.0539049126,0.0427605221,-0.0025910626,0.0419812073,-0.0033410511,0.0307269321,-0.0054098234,-0.0092205884,0.0289836734,0.0267426176,-0.0082613476,-0.0304481799,0.064244682,-0.0419559764,0.0632252839,0.0314713799,0.0112718338,0.0300967878,0.040793843,0.0515975868,0.004135111,-0.0111590135,0.0151899491,0.0181781129,0.0262236344,0.0056176238,0.0020779922,0.0651031295,-0.024226791,-0.0135212316,-0.0517490971,-0.0115330451,-0.0165706977,-0.0343083439,0.0150130008,-0.0007216272]},"510":{"Abstract":"Order selection of autoregressive processes is an active research topic in time series analysis, and the development and evaluation of automatic order selection criteria remains a challenging task for domain experts. We propose a visual analytics approach, to guide the analysis and development of such criteria. A flexible synthetic model generator-combined with specialized responsive visualizations-allows comprehensive interactive evaluation. Our fast framework allows feedback-driven development and fine-tuning of new order selection criteria in real-time. We demonstrate the applicability of our approach in three use-cases for two general as well as a real-world example.","Authors":"T. L\u00f6we; E. F\u00f6rster; G. Albuquerque; J. Kreiss; M. Magnor","DOI":"10.1109\/TVCG.2015.2467612","Keywords":"Visual analytics;time series analysis;order selection;Visual analytics;time series analysis;order selection","Title":"Visual Analytics for Development and Evaluation of Order Selection Criteria for Autoregressive Processes","Keywords_Processed":"time series analysis;order selection;visual analytic","Keyword_Vector":[0.1845501226,-0.0417448826,-0.0380332886,-0.019500029,0.1204812766,-0.0927291831,-0.1989757489,0.0552191849,-0.2344789783,0.1447898461,-0.0779946705,0.2302471474,0.107702703,0.0288600096,0.1055510235,0.0480013811,0.2013865133,0.039236306,0.2556007448,-0.028366125,-0.1076242693,0.2394221353,0.2693579132,0.0404451577,-0.0411813272,-0.1636104112,0.1054734081,-0.1436333368,0.1369871263,-0.1966829697,-0.0097725726,-0.0511744389,0.018853726,-0.0360752579,0.0379230045,0.1201047508,0.0727152743,0.1335886949,0.0460566758,-0.0324683643,-0.0407494428,0.0005698337,0.030086638,0.0746160004,-0.100111662,0.0207947462,-0.0928098767,0.0567189988,-0.0612377394,0.0920613523,-0.0094056717,0.0876755265],"Abstract_Vector":[0.1704117343,-0.0172332913,-0.0264738129,0.0094890277,-0.0261851476,0.0606022005,0.0211483245,0.0294304027,0.0894214262,-0.0031068229,-0.0353857476,0.0109228544,-0.0171308215,0.0387806561,0.0389032151,0.0293753394,0.056810254,0.0664702709,0.0661729417,-0.0014979121,-0.0051773936,-0.0633375821,0.0051442483,0.0101617725,0.0126141611,0.0035831545,-0.0068794755,0.0184450495,-0.0290542086,-0.0281418782,0.0103468702,-0.0132790163,-0.0117555447,-0.067255223,-0.0092379955,0.022568835,0.0464764292,0.0188838546,-0.0268079694,-0.0328848059,-0.040614325,0.0233751976,-0.0044997572,-0.0101125507,-0.0149452455,0.0101744221,-0.0139934961,0.0193278722,0.0066096811,-0.0284308723,-0.0022901292,-0.0403692712,0.0092177396,-0.0087867225,0.0550627954,-0.0131681033,-0.0216807692,-0.0482868306,0.0085255501,-0.0030719239,-0.0176714103,-0.0291155027,-0.0052640984,0.047706188,-0.01415483,-0.0562758935,0.0128548173,-0.0503594099,-0.0105765784,0.0217486713,-0.0154505797,0.0199711445,-0.0434949653,0.0514466399,-0.0454964413,-0.0316262894,0.0027341389,-0.0048060201,0.041060189,-0.0064300792,0.0263849577,0.0037828044,0.0268304284,0.0522487738,0.0284838172,0.0084739424,0.0130036519,-0.0214780608,0.0215769936,0.0244903106,-0.0070000534,-0.0547781981,-0.0377367112,-0.0051529311,-0.0283932833,0.0073790586,0.0114521371,-0.0287461002,0.0118240496,-0.0328774519,-0.0070118992,0.026462697,-0.034794432,0.0192657694,-0.0006224993,-0.009214955,0.0341112116,0.0219796916,0.0061028997,-0.0275986438,-0.0021049752,0.0165312644,0.0144109321,-0.010853939,0.0188560766,-0.0103874911]},"511":{"Abstract":"Visualizing network data is applicable in domains such as biology, engineering, and social sciences. We report the results of a study comparing the effectiveness of the two primary techniques for showing network data: node-link diagrams and adjacency matrices. Specifically, an evaluation with a large number of online participants revealed statistically significant differences between the two visualizations. Our work adds to existing research in several ways. First, we explore a broad spectrum of network tasks, many of which had not been previously evaluated. Second, our study uses two large datasets, typical of many real-life networks not explored by previous studies. Third, we leverage crowdsourcing to evaluate many tasks with many participants. This paper is an expanded journal version of a Graph Drawing (GD'17) conference paper. We evaluated a second dataset, added a qualitative feedback section, and expanded the procedure, results, discussion, and limitations sections.","Authors":"M. Okoe; R. Jianu; S. Kobourov","DOI":"10.1109\/TVCG.2018.2865940","Keywords":"Evaluation;user study;graphs;networks;node-link;adjacency matrices","Title":"Node-Link or Adjacency Matrices: Old Question, New Insights","Keywords_Processed":"adjacency matrix;user study;graph;evaluation;network;node link","Keyword_Vector":[0.0940613009,0.041310435,0.0261570075,0.1268930212,0.1923289657,-0.0819343804,-0.0453942157,0.0431011907,-0.0135420449,-0.0141418887,-0.0368058014,0.0058405566,-0.004511873,0.021870218,-0.0567062183,0.0228275544,-0.0287958311,0.0134806872,-0.0493428764,0.0773801433,0.0182378386,0.0177999923,-0.050523795,0.0819957408,0.0143980666,-0.1311298151,-0.1397012585,-0.0130600584,-0.0462847401,-0.0626087947,-0.0200759967,-0.0634438469,-0.058956052,-0.0447055908,0.0014937986,0.0753390455,-0.0154579753,-0.0370216747,-0.0490432863,0.0863766297,-0.0111470946,0.0631488717,-0.1003513351,0.0376383551,0.0124240067,-0.0026288536,-0.0246831854,0.0778292448,0.0353572568,-0.0835711135,-0.0121204875,-0.058602129],"Abstract_Vector":[0.1931865182,0.0596582845,0.0725894177,-0.0081867738,0.0011994225,0.0138749981,0.0854581366,0.0063137195,-0.0013121333,-0.0444786151,0.0293590919,-0.0069475073,-0.0433529221,-0.0257824668,-0.0139021829,0.0148890871,-0.0592366947,0.0862881842,-0.0146728931,-0.043714345,0.0061986464,-0.0265575926,0.0045720761,0.0364720893,0.049307417,0.0198573998,0.0849499394,0.0081800392,0.0394926585,-0.0045821043,-0.0028392865,0.0806950721,-0.0536752874,0.0276154624,0.0068494839,-0.0122841907,0.0030008684,0.0557319525,-0.0254604746,-0.0069665423,0.0223641328,0.0318281302,0.0123732977,-0.012724659,-0.0116928655,-0.0284411523,0.0174358137,0.0559421957,0.012828306,-0.0212599426,-0.016876069,-0.021964455,-0.0192145548,0.0249684974,-0.0049481842,-0.0176580349,0.072117957,-0.0221346431,-0.0105599024,-0.0145390964,-0.0176478355,0.0098723707,0.0377875541,0.0467245818,0.0191321647,-0.014768632,-0.0382752647,-0.0498738172,0.0392167938,0.0112735964,-0.0127502215,-0.0014352067,-0.0240757651,0.0361639661,-0.0437768284,-0.0228011098,-0.0048451123,-0.0160679309,-0.0022814479,-0.0120965115,-0.0072021069,0.0420794067,-0.0121793835,-0.0106195188,0.0061844584,0.0441844622,0.0493750673,0.0074384367,0.0230355423,-0.002953809,0.0262695108,0.0073402574,-0.0240339967,0.0025858558,-0.0100599636,0.0108074513,0.0076468995,-0.0039611388,0.0221056347,0.0152163222,0.0201366242,0.0460850103,0.0048546983,0.0406898266,0.0002870682,-0.0154283982,-0.0041432508,-0.0524443946,-0.0134243787,-0.0135344639,-0.022342001,0.0118630037,-0.0243981984,0.0057466815,0.0629626575,-0.0131816987]},"512":{"Abstract":"We propose TrajGraph, a new visual analytics method, for studying urban mobility patterns by integrating graph modeling and visual analysis with taxi trajectory data. A special graph is created to store and manifest real traffic information recorded by taxi trajectories over city streets. It conveys urban transportation dynamics which can be discovered by applying graph analysis algorithms. To support interactive, multiscale visual analytics, a graph partitioning algorithm is applied to create region-level graphs which have smaller size than the original street-level graph. Graph centralities, including Pagerank and betweenness, are computed to characterize the time-varying importance of different urban regions. The centralities are visualized by three coordinated views including a node-link graph view, a map view and a temporal information view. Users can interactively examine the importance of streets to discover and assess city traffic patterns. We have implemented a fully working prototype of this approach and evaluated it using massive taxi trajectories of Shenzhen, China. TrajGraph's capability in revealing the importance of city streets was evaluated by comparing the calculated centralities with the subjective evaluations from a group of drivers in Shenzhen. Feedback from a domain expert was collected. The effectiveness of the visual interface was evaluated through a formal user study. We also present several examples and a case study to demonstrate the usefulness of TrajGraph in urban transportation analysis.","Authors":"X. Huang; Y. Zhao; C. Ma; J. Yang; X. Ye; C. Zhang","DOI":"10.1109\/TVCG.2015.2467771","Keywords":"Graph based visual analytics;Centrality;Taxi trajectories;Urban network;Transportation assessment;Graph based visual analytics;Centrality;Taxi trajectories;Urban network;Transportation assessment","Title":"TrajGraph: A Graph-Based Visual Analytics Approach to Studying Urban Network Centralities Using Taxi Trajectory Data","Keywords_Processed":"taxi trajectorie;transportation assessment;Graph base visual analytic;urban network;centrality","Keyword_Vector":[0.0350860717,-0.0088848906,-0.0034649992,-0.0116363312,0.0073593563,0.0012211237,-0.0145382993,-0.006811686,-0.0423129594,-0.0277208766,0.0073142323,0.0386970801,0.0092946805,-0.0082467971,0.0067871871,-0.0043782439,0.0445786233,0.0104472417,0.0549162109,0.0035064573,0.0055226859,0.066965453,-0.0156494996,0.0491948767,-0.0266244734,0.0182547126,-0.0257491621,0.0260084842,0.028480483,0.009351154,-0.0088309434,-0.0349594395,0.0182052388,-0.0289753518,-0.0310999425,-0.0178284275,-0.0251184011,-0.0625826915,-0.0072751296,0.0017489935,-0.0161749953,0.0923665787,-0.0331813922,0.0958764655,-0.0535494999,-0.0407536006,-0.0456697898,0.070695554,0.0103462318,0.0014386778,0.0624762504,0.0474520397],"Abstract_Vector":[0.1687709514,0.0660829593,0.0018505016,0.0214499377,-0.0056863387,0.0484426512,0.0245517093,0.0400217064,0.0321121076,0.0333369165,-0.0171380403,0.038162638,-0.0573047098,-0.0678894744,-0.0104625774,0.0325788822,0.0337462664,-0.0149520852,0.0128931651,-0.0916903253,0.0098274393,-0.0298226064,0.0279569487,-0.0238340198,-0.0055273527,0.0029394592,0.0497136571,-0.010355815,-0.0001792703,0.0846885334,0.138912855,-0.021100302,-0.046572862,-0.079978131,0.0425060378,0.0337656591,-0.0285157198,0.0590544079,-0.0261661744,0.0492207865,0.041317768,0.0206935438,0.0253232175,-0.0070529614,0.0281392468,-0.0239819811,0.0417450378,0.0342167152,-0.0400964363,-0.0288561758,-0.0030911601,-0.0549360204,-0.0150399526,0.0063019957,-0.0093654142,-0.0188562979,0.0234067003,0.003734386,-0.0064206797,-0.0569027051,0.0286551218,-0.0426196248,0.0333610069,-0.005348324,-0.0055547227,0.0574815556,-0.0179983254,-0.0077725471,0.0095611459,-0.0026194266,0.029088337,0.0207825491,0.002689723,0.0301292204,-0.0088967238,-0.0132653053,-0.0579426092,0.0053494725,-0.0180559987,0.0126110478,0.0002852888,0.0323656728,0.0041628102,-0.0396929839,-0.012121329,0.0061939285,-0.0130990149,-0.021059348,0.0308213811,0.0022366954,-0.0238336859,0.0174464364,-0.0039922163,-0.022533539,-0.0333937271,-0.0302653698,0.0259516025,-0.0404963273,-0.0468945765,-0.0084400388,0.0240496803,-0.0199872503,0.0714025168,-0.0227555785,-0.0373927483,0.0210820812,-0.0357012378,0.0232592136,-0.0427388172,0.0095934833,0.0089391106,-0.0423297854,-0.0662967179,-0.0287927285,0.0319889351,0.0590216762]},"513":{"Abstract":"Alternative splicing is a process by which the same DNA sequence is used to assemble different proteins, called protein isoforms. Alternative splicing works by selectively omitting some of the coding regions (exons) typically associated with a gene. Detection of alternative splicing is difficult and uses a combination of advanced data acquisition methods and statistical inference. Knowledge about the abundance of isoforms is important for understanding both normal processes and diseases and to eventually improve treatment through targeted therapies. The data, however, is complex and current visualizations for isoforms are neither perceptually efficient nor scalable. To remedy this, we developed Vials, a novel visual analysis tool that enables analysts to explore the various datasets that scientists use to make judgments about isoforms: the abundance of reads associated with the coding regions of the gene, evidence for junctions, i.e., edges connecting the coding regions, and predictions of isoform frequencies. Vials is scalable as it allows for the simultaneous analysis of many samples in multiple groups. Our tool thus enables experts to (a) identify patterns of isoform abundance in groups of samples and (b) evaluate the quality of the data. We demonstrate the value of our tool in case studies using publicly available datasets.","Authors":"H. Strobelt; B. Alsallakh; J. Botros; B. Peterson; M. Borowsky; H. Pfister; A. Lex","DOI":"10.1109\/TVCG.2015.2467911","Keywords":"Biology visualization;protein isoforms;mRNA-seq;directed acyclic graphs;multivariate networks;Biology visualization;protein isoforms;mRNA-seq;directed acyclic graphs;multivariate networks","Title":"Vials: Visualizing Alternative Splicing of Genes","Keywords_Processed":"multivariate network;mRNA seq;protein isoform;biology visualization;direct acyclic graph","Keyword_Vector":[0.0663024705,0.0734625633,-0.0458685066,0.0122103875,-0.0002802686,0.0030207287,0.0061842841,0.0052605104,-0.0011822076,-0.003936751,0.0013817009,0.0020238208,0.0114987101,0.0381414816,0.0140995853,-0.004674141,0.0181346341,-0.022677966,0.028507543,-0.0378757242,0.0241381519,0.0769036973,-0.0493970166,-0.0259851046,0.0126428392,0.0244574997,0.0051803695,-0.0361701128,0.0202197969,0.0427791582,-0.020560767,-0.0042900895,0.0325780911,0.0616296598,0.0008099053,0.0464021146,0.0162162271,-0.0222758523,0.0041949781,0.026429452,0.0127548455,0.095376752,0.0079925711,0.0084454073,-0.0099926752,-0.0077235705,0.0008039349,0.0144660354,0.0226670468,0.0420498129,0.0184112063,-0.0007650427],"Abstract_Vector":[0.1714433169,-0.0373818417,0.0162082054,-0.0228909851,0.0003157172,0.0144510802,0.0206701103,0.0092975808,0.0337790502,0.0290552283,0.0075652182,-0.0506197479,0.0795203501,0.0422856771,-0.0228572696,-0.0008145945,-0.007362447,0.0434517275,0.0568318511,-0.0593103397,0.0039088646,0.0354846297,0.0148009825,-0.0615045959,-0.0051207358,0.0622347678,-0.0531049746,0.0583710027,-0.0204663726,-0.0230710153,0.003659402,-0.0525808186,-0.040518964,0.0289621748,0.0226214042,0.0768237623,-0.0281549049,0.0860819793,0.0786814715,0.0478453804,-0.1028230453,0.0367032927,-0.0682690764,0.0582956421,0.0408623712,0.0030664797,0.001403847,-0.0233858973,0.0345147875,-0.0555937933,0.0679309156,0.0561396237,0.0365044508,-0.0001192169,-0.0220399131,0.0266721918,0.111794712,0.0167126862,-0.0132953032,-0.042573559,0.102938616,-0.0100001833,0.0453670907,-0.0083083539,-0.0249118645,0.0169920325,-0.0102814687,-0.0321770031,-0.0144942559,-0.075484898,0.0293437147,-0.0193268561,-0.0060983216,-0.0424090131,0.0028969701,-0.0182573215,0.0109522952,-0.0158256241,-0.0044143728,0.0061467417,-0.0198472415,-0.0658870585,-0.0293565674,-0.0040507764,-0.0359884876,0.0207686591,0.026006313,0.0289123716,0.0102005184,0.0161084116,0.0339269817,0.0011686808,-0.0027645564,0.0098711556,0.050058982,-0.0479200591,0.0260402696,0.0087548832,-0.0003752403,0.0237271216,-0.0034659216,-0.0072813756,-0.010159612,-0.0092688551,0.0278370885,-0.0041767243,0.0240156401,0.0346810099,0.0201575378,0.022935145,0.0125017563,-0.0226794972,0.0119661307,0.0411704891,0.0009536566,0.0156717207]},"514":{"Abstract":"Real-time indoor scene reconstruction aims to recover the 3D geometry of an indoor scene in real time with a sensor scanning the scene. Previous works of this topic consider pure static scenes, but in this paper, we focus on more challenging cases that the scene contains dynamic objects, for example, moving people and floating curtains, which are quite common in reality and thus are eagerly required to be handled. We develop an end-to-end system using a depth sensor to scan a scene on the fly. By proposing a Sigmoid-based Iterative Closest Point (S-ICP) method, we decouple the camera motion and the scene motion from the input sequence and segment the scene into static and dynamic parts accordingly. The static part is used to estimate the camera rigid motion, while for the dynamic part, graph node-based motion representation and model-to-depth fitting are applied to reconstruct the scene motions. With the camera and scene motions reconstructed, we further propose a novel mixed voxel allocation scheme to handle static and dynamic scene parts with different mechanisms, which helps to gradually fuse a large scene with both static and dynamic objects. Experiments show that our technique successfully fuses the geometry of both the static and dynamic objects in a scene in real time, which extends the usage of the current techniques for indoor scene reconstruction.","Authors":"H. Zhang; F. Xu","DOI":"10.1109\/TVCG.2017.2786233","Keywords":"Scene reconstruction;dynamic reconstruction;single view","Title":"MixedFusion: Real-Time Reconstruction of an Indoor Scene with Dynamic Objects","Keywords_Processed":"dynamic reconstruction;single view;scene reconstruction","Keyword_Vector":[0.0644539778,0.020679525,0.001892248,0.0047121567,0.0066198735,-0.018115194,-0.0100702093,-0.0566178675,-0.048517077,-0.02341463,0.0029036468,0.0258836595,-0.0160589818,0.0422518094,0.0386635055,0.0194213795,-0.0019257017,-0.0331823426,0.0181696315,-0.0236146125,0.0338838707,0.005199169,-0.0688696775,0.0377894327,0.0013639709,0.0285618058,0.0567905503,0.041406772,-0.0150917052,0.0025799856,-0.0153326251,0.0046312176,-0.0230980995,-0.0187469493,-0.0053227519,-0.0194364079,0.0700310716,0.0850545804,-0.049578958,0.0215003708,0.0195596289,0.0427053103,-0.0768755144,-0.0623848146,-0.0878866271,0.0456510765,-0.0624796435,0.0539881216,-0.0246524037,-0.0360670285,-0.0883491412,-0.0557217012],"Abstract_Vector":[0.2213196184,0.0658140364,0.0163562098,0.0953286355,-0.0715730329,0.0474719857,-0.0084459049,-0.0180922383,0.052734146,0.032013538,-0.0370709129,0.0146334829,0.0333269725,0.076048272,-0.0359172307,-0.0038074911,0.0584288283,0.0078020149,-0.0086511499,-0.0400015825,-0.056970798,0.0162401866,0.1345709372,-0.0092840709,-0.0469427284,-0.0806182467,-0.000493385,0.0600208693,-0.0314549457,-0.0074879099,0.0305107534,-0.034486496,-0.0199898298,0.0364548112,0.0242445526,-0.0201441183,0.0553910033,0.0059233237,-0.013201621,0.0428494742,0.0375447915,0.0198028651,0.0011637339,0.0632120329,-0.0205912885,-0.0292013234,-0.0170065782,0.0977111276,0.0049123005,-0.0035303948,0.0315633094,0.0018667335,-0.0012525928,0.032631046,-0.0031707963,0.0233315982,0.0235840285,-0.011816128,0.0076568184,-0.0199696563,-0.0215374087,0.0268299663,0.0480945011,-0.0692849857,0.0029472798,-0.0648218836,0.0503306879,-0.0375317993,-0.0125303455,0.0891525132,-0.005708558,0.0732841413,0.0156412797,0.0472724648,0.0279447264,0.0463614537,-0.0091703789,0.0316251951,-0.0055339143,0.0021868244,-0.0440248829,0.0329780703,0.0493982422,-0.0164426229,-0.0091743538,-0.0343032539,0.0227380902,0.0299758897,0.0684210065,-0.0389269255,0.0168026546,-0.0092253346,-0.0116721712,-0.0383755627,0.0151521559,-0.0317383574,-0.0691450779,0.0312658062,0.0432851493,0.0236264363,-0.0045775775,-0.0104574328,0.0236007929,0.0236406609,-0.0332017087,-0.0385907091,0.0158652702,0.0242414101,0.0423814063,0.0004780311,0.0226597665,0.0251610543,-0.0354116466,-0.0022359795,0.0173860194,-0.0051005041]},"515":{"Abstract":"Modeling cloth with fiber-level geometry can produce highly realistic details. However, rendering fiber-level cloth models not only has a high memory cost but it also has a high computation cost even for offline rendering applications. In this paper we present a real-time fiber-level cloth rendering method for current GPUs. Our method procedurally generates fiber-level geometric details on-the-fly using yarn-level control points for minimizing the data transfer to the GPU. We also reduce the rasterization operations by collectively representing the fibers near the center of each ply that form the yarn structure. Moreover, we employ a level-of-detail strategy to minimize or completely eliminate the generation of fiber-level geometry that would have little or no impact on the final rendered image. Furthermore, we introduce a simple self-shadow computation method that allows lighting with self-shadows using relatively low-resolution shadow maps. We also provide a simple distance-based ambient occlusion approximation as well as an ambient illumination precomputation approach, both of which account for fiber-level self-occlusion of yarn. Finally, we discuss how to use a physical-based shading model with our fiber-level cloth rendering method and how to handle cloth animations with temporal coherency. We demonstrate the effectiveness of our approach by comparing our simplified fiber geometry to procedurally generated references and display knitwear containing more than a hundred million individual fiber curves at real-time frame rates with shadows and ambient occlusion.","Authors":"K. Wu; C. Yuksel","DOI":"10.1109\/TVCG.2017.2731949","Keywords":"Cloth rendering;procedural geometry;real-time rendering","Title":"Real-Time Cloth Rendering with Fiber-Level Detail","Keywords_Processed":"real time render;procedural geometry;cloth render","Keyword_Vector":[0.0796826979,-0.0393244918,-0.0991011474,0.0402884778,-0.0481362891,-0.1141405539,-0.0200960925,0.0091847069,-0.1016892588,-0.0382221276,0.0794428542,0.1041259554,0.0012074564,0.0066222878,0.1081495163,0.0958370816,0.0893812571,0.1139139811,0.2824873839,-0.0183040688,-0.1477287436,0.1346084946,0.176537968,0.0976594993,-0.1608185175,-0.0587279285,-0.0120009667,-0.0411311109,0.1696047545,-0.1913493339,-0.0213203458,-0.17120789,0.10898913,0.0609356119,-0.0673249025,-0.0153209992,-0.0246111341,-0.0158176044,-0.0363450732,-0.0927616132,-0.032697701,0.0737239704,0.0244158571,0.0384410984,0.0172355362,-0.0270809961,0.0514256054,0.0557663875,-0.0162835273,0.0936649005,-0.0548656552,-0.0095020865],"Abstract_Vector":[0.2021814608,-0.0991573529,0.0009556834,0.0001468793,-0.0670934831,0.0603723279,-0.0126287944,0.0092569537,0.01487913,0.0004845989,-0.0393568095,0.0606428139,-0.0644375114,0.0108137315,-0.0226993424,0.0191498227,-0.0408996136,-0.0643400447,0.0118295259,-0.0613403502,0.057442627,-0.037594614,-0.016553249,-0.0335671161,-0.0397806389,-0.0021300323,-0.0313084337,-0.0552796424,-0.0040852686,-0.0011605914,0.0417051127,0.0242600998,-0.0455758916,-0.0105112911,0.0294532345,-0.0378887651,-0.0033651002,-0.0102057178,0.0023803409,-0.0359502471,0.0458227347,0.0067037721,-0.0163984496,0.025975253,-0.0010458615,-0.0141208896,0.0251072386,0.0053902895,-0.0141158683,-0.0081274473,0.0208851123,0.0293313784,0.0185277017,0.0033559392,0.0584306432,0.018451391,0.0221895804,-0.0210604174,-0.0046433199,-0.0230497879,0.0353664159,-0.0359940004,0.0019378763,0.0129413165,-0.0310791111,-0.0712936645,0.0098535426,-0.0278841007,0.0187663324,0.0348461874,0.014969539,-0.0187769084,-0.0229644907,-0.0082041146,-0.0095617539,-0.0263671594,0.0461709803,0.0138044754,0.0017532531,0.0059238842,0.001876682,-0.0138441614,0.042238213,-0.0338427503,0.0152320447,-0.0158640234,0.0295200006,0.0491841254,-0.0313410271,-0.0332118436,-0.0087254685,-0.0136411456,-0.0031575445,-0.004639423,-0.0013363839,-0.0529100835,-0.0428467385,-0.0252406934,-0.0205625126,-0.0027830966,-0.0447665261,-0.0139801966,-0.0308040404,-0.0091518971,-0.0114827167,-0.03284803,-0.0030371302,0.0033069827,-0.0167271932,-0.0362167511,0.0000141935,-0.0009866965,0.060834958,-0.0247193968,-0.0252071088,-0.0110396393]},"516":{"Abstract":"We present a novel method for posing and animating botanical tree models interactively in real time. Unlike other state of the art methods which tend to produce trees that are overly flexible, bending and deforming as if they were underwater plants, our approach allows for arbitrarily high stiffness while still maintaining real-time frame rates without spurious artifacts, even on quite large trees with over ten thousand branches. This is accomplished by using an articulated rigid body model with as-stiff-as-desired rotational springs in conjunction with our newly proposed simulation technique, which is motivated both by position based dynamics and the typical O(N) algorithms for articulated rigid bodies. The efficiency of our algorithm allows us to pose and animate trees with millions of branches or alternatively simulate a small forest comprised of many highly detailed trees. Even using only a single CPU core, we can simulate ten thousand branches in real time while still maintaining quite crisp user interactivity. This has allowed us to incorporate our framework into a commodity game engine to run interactively even on a low-budget tablet. We show that our method is amenable to the incorporation of a large variety of desirable effects such as wind, leaves, fictitious forces, collisions, fracture, etc.","Authors":"E. Quigley; Y. Yu; J. Huang; W. Lin; R. Fedkiw","DOI":"10.1109\/TVCG.2017.2661308","Keywords":"Computer graphics;physically-based modeling;botanical tree","Title":"Real-Time Interactive Tree Animation","Keywords_Processed":"botanical tree;computer graphic;physically base modeling","Keyword_Vector":[0.1822017409,-0.177373477,-0.1212389924,-0.0091470243,-0.0260086309,-0.1370665606,0.0752531236,0.005679064,0.0631560744,-0.0130378457,0.0205100556,-0.0716077664,-0.0143812859,-0.0662341437,0.0668265522,0.0073803536,0.0056975553,-0.0045507438,-0.0035386004,-0.0698645602,0.012081951,0.0637633388,-0.0398668804,-0.0443205459,0.12703073,0.0119790035,0.0083140635,0.0181690167,0.0081819089,-0.0490840402,-0.0086379877,-0.0330963372,0.014402052,0.083034949,-0.0696874045,0.0193185469,0.0039749162,-0.0163355714,0.0728294216,-0.0026557117,0.0563848068,0.0376747887,0.0271511777,-0.0076042157,-0.0332598306,-0.0224917974,-0.0045757816,0.0045879999,0.0203189958,0.0295590629,-0.0143827129,-0.028550861],"Abstract_Vector":[0.1613679173,-0.0746274411,-0.0099114871,0.0180657501,-0.0553271452,0.0373025512,0.0326332751,0.0168041042,-0.03990012,0.0060166736,-0.0090931367,-0.0075637495,-0.0053476942,-0.0019894171,-0.0255025025,0.0442335178,0.0738712312,0.052412962,-0.0137458886,-0.0550814304,0.0480768242,0.0008011812,0.0021976132,0.0143207127,-0.005690875,0.0029217003,0.0159136229,-0.0101382635,-0.0077343739,0.0087803694,-0.0338473693,0.0032440777,-0.0085218822,-0.0034175047,-0.0029107183,-0.0025663844,0.0052272323,0.0179543717,0.0037578873,-0.0361720586,-0.0368427634,0.0038997027,-0.0152095932,-0.0050813165,-0.0268715236,-0.009707432,-0.0016672786,0.0139506065,0.0085714182,-0.0217014649,0.0233516631,0.0016581532,0.0117513818,0.0235899274,-0.0253351974,-0.0080972955,0.0197318179,0.0032158263,0.0230009079,-0.00264333,0.0124380844,-0.012219425,-0.0180070124,0.0042302849,-0.0064868116,0.0317340138,0.0067736172,0.0097632382,-0.0211786786,-0.0008933159,-0.0332654084,-0.0216572179,0.0159984608,-0.0151786588,-0.0193175913,0.003649492,-0.0162879358,0.0120858781,-0.0226824548,0.0039510817,-0.0084304597,0.0126399409,-0.0236745378,0.0241818401,0.0062899976,-0.0035863106,0.0043642228,-0.0020720905,-0.020514882,-0.0281323266,-0.0094203808,-0.0160424786,0.0202772335,0.0034048452,-0.0329755178,0.0274533059,-0.0127414908,-0.0085895562,-0.0068420641,-0.0250941522,0.002812778,0.018595012,0.0328043404,-0.016350897,0.0334681866,-0.0167331698,-0.0161937116,-0.0212212711,-0.013117094,0.0337554596,0.0091095523,-0.0075671406,0.0178452634,-0.0073837142,0.0275599645,-0.011325195]},"517":{"Abstract":"Surface remeshing is a key component in many geometry processing applications. The typical goal consists in finding a mesh that is (1) geometrically faithful to the original geometry, (2) as coarse as possible to obtain a low-complexity representation and (3) free of bad elements that would hamper the desired application (e.g.,\u00a0the minimum interior angle is above an application-dependent threshold). Our algorithm is designed to address all three optimization goals simultaneously by targeting prescribed bounds on approximation error  $\\delta$ , minimal interior angle $\\theta$  and maximum mesh complexity $N$  (number of vertices). The approximation error bound $\\delta$  is a hard constraint, while the other two criteria are modeled as optimization goals to guarantee feasibility. Our optimization framework applies carefully prioritized local operators in order to greedily search for the coarsest mesh with minimal interior angle above $\\theta$  and approximation error bounded by $\\delta$ . Fast runtime is enabled by a local approximation error estimation, while implicit feature preservation is obtained by specifically designed vertex relocation operators. Experiments show that for reasonable angle bounds ( $\\theta \\leq 35^\\circ$ ) our approach delivers high-quality meshes with implicitly preserved features (no tagging required) and better balances between geometric fidelity, mesh complexity and element quality than the state-of-the-art.","Authors":"K. Hu; D. Yan; D. Bommes; P. Alliez; B. Benes","DOI":"10.1109\/TVCG.2016.2632720","Keywords":"Surface remeshing;error-bounded;feature preserving;minimal angle improvement;saliency function","Title":"Error-Bounded and Feature Preserving Surface Remeshing with Minimal Angle Improvement","Keywords_Processed":"feature preserve;surface remeshing;saliency function;error bound;minimal angle improvement","Keyword_Vector":[0.0577409951,-0.0915188262,-0.1047366913,0.0755837232,-0.0697099823,-0.1178266108,0.0585263959,0.0181876113,0.0480018366,-0.0053810296,0.0138184069,0.0007345043,-0.0212099163,-0.0593696111,0.035526895,0.0063502325,-0.0144513247,-0.0021041355,-0.0035906992,0.0141787896,0.0158286232,0.019921966,0.0314818735,0.0001135658,-0.0049349522,0.0002533005,0.0056506478,-0.0104253583,0.0096167899,-0.007902212,0.0186299805,0.0044922835,0.0256636461,-0.0026222261,0.0221358924,0.0019880034,0.0121544201,-0.0213338014,-0.0142265366,-0.0329297056,-0.0108855476,-0.0114984919,0.0015517738,-0.0294793434,-0.0013098204,-0.0020254142,-0.0313747222,-0.0123963454,0.0172936501,-0.0082019102,0.0205660575,0.0004593874],"Abstract_Vector":[0.1671204174,-0.1174332658,-0.0101239334,0.0270372951,-0.0017814117,-0.0189193914,0.0151120274,-0.0051086234,-0.0650714289,0.0798110848,0.0472892521,-0.0704085674,-0.0685845256,0.0422564007,0.0361732585,-0.000333973,-0.0040359977,-0.0074098313,-0.0125213639,0.0279524587,-0.0027216051,0.0360051666,-0.0311063363,-0.0362833455,0.0119392514,0.0406247701,0.0349901223,-0.0217648872,0.0427604749,0.0330509237,-0.0644535409,0.0003710767,-0.0148942725,0.0077557849,0.0173501575,0.0181393518,-0.0017019898,0.050801955,-0.0549807366,-0.0012474062,0.0250329711,0.046949189,-0.0207276395,0.0076935596,-0.0124512113,0.002936937,0.0109050113,0.026633014,-0.0064017531,0.0511562185,0.0087860408,-0.001086381,-0.0000856022,-0.0379830613,0.0372419842,0.0262773646,-0.0114496752,0.025975206,0.0075207555,-0.0500916951,-0.0595579576,0.0008375401,-0.0291578809,0.0205447514,0.0051625472,0.0040421426,0.0195160667,0.0391055715,-0.0328431225,0.0002287619,-0.0103456891,0.0136234188,0.0032156584,-0.0163102324,-0.0052547822,0.0114807467,0.0055494269,0.0053885394,-0.0180090117,-0.0258303711,0.012651813,0.0170265894,-0.0481307883,-0.0112035118,-0.0124033161,-0.049270854,-0.0172168544,0.0005630373,0.0148942774,0.0199998699,0.0113417647,-0.0320887307,0.0373269087,-0.0106212145,-0.0603138784,-0.000115548,-0.017522673,0.0200847083,0.0243729501,0.0078188137,0.0077980192,0.0210092151,0.0196183849,-0.0261163461,-0.0522167136,0.0054768814,0.0014224295,-0.021795377,0.0129528624,0.0009297363,-0.0196008484,0.025574951,0.0170007178,-0.0043300203,0.0004533719,0.0136979924]},"518":{"Abstract":"Wide-baseline street image interpolation is useful but very challenging. Existing approaches either rely on heavyweight 3D reconstruction or computationally intensive deep networks. We present a lightweight and efficient method which uses simple homography computing and refining operators to estimate piecewise smooth homographies between input views. To achieve the goal, we show how to combine homography fitting and homography propagation together based on reliable and unreliable superpixel discrimination. Such a combination, other than using homography fitting only, dramatically increases the accuracy and robustness of the estimated homographies. Then, we integrate the concepts of homography and mesh warping, and propose a novel homography-constrained warping formulation which enforces smoothness between neighboring homographies by utilizing the first-order continuity of the warped mesh. This further eliminates small artifacts of overlapping, stretching, etc. The proposed method is lightweight and flexible, allows wide-baseline interpolation. It improves the state of the art and demonstrates that homography computation suffices for interpolation. Experiments on city and rural datasets validate the efficiency and effectiveness of our method.","Authors":"Y. Nie; Z. Zhang; H. Sun; T. Su; G. Li","DOI":"10.1109\/TVCG.2016.2618878","Keywords":"Image interpolation;street view synthesis;homography propagation;homography-constrained warping","Title":"Homography Propagation and Optimization for Wide-Baseline Street Image Interpolation","Keywords_Processed":"image interpolation;street view synthesis;homography propagation;homography constrain warp","Keyword_Vector":[0.1497039252,-0.1558470339,-0.0996206923,0.1482210492,-0.1253852693,-0.1083831944,0.0508277509,-0.0176107657,0.0159681478,-0.0671319209,0.0129186273,-0.0184540186,0.0017916316,0.1604222824,-0.0465792123,-0.0193769249,0.1580357203,0.0289680853,0.036199612,-0.0891072557,0.0358696444,0.0269723198,-0.0475446048,0.0496782789,-0.0397332074,0.0159925159,-0.0786585878,-0.0412412203,-0.0805519437,0.0846914303,0.0150066718,-0.0004145905,0.1485480142,-0.0263492048,-0.0263992518,-0.014507315,0.0107515252,-0.155682739,0.0287893998,0.0479006753,-0.0595538612,0.0410999757,-0.1212098472,0.1157370989,0.011088209,0.008800528,-0.0165528143,0.0242398972,0.0209714954,0.0422439706,0.0189626628,0.0662974192],"Abstract_Vector":[0.1576551824,0.0000903833,0.1352324851,-0.036850884,0.0193623529,-0.0045988704,0.0941787206,-0.0281429523,-0.0434506514,-0.0076717785,-0.017063194,-0.0080300358,0.0022255822,-0.0236183486,-0.0640293225,0.0487638459,0.0024496442,0.0917839009,-0.0197929165,-0.0804520681,0.0457292725,0.0103415578,-0.0133402495,-0.041939751,-0.0309877578,0.0871136477,0.0471531611,-0.0354514304,0.0580742949,0.002722211,0.0571407122,-0.0079195467,-0.1117038843,0.0060801705,0.028606189,0.0624500535,0.0647234037,-0.0572010813,0.0416854646,-0.084343749,-0.0634953601,0.0276967964,0.0285809527,0.0824436093,-0.0613649211,0.0194291997,0.1300040003,-0.0498211383,0.0325037973,0.0328802625,-0.078968324,-0.0728258298,0.2216595084,-0.0037351257,-0.0451942866,0.1375210104,-0.0326062528,-0.0937083762,-0.1107239,0.1475585715,-0.0648428516,0.020848356,0.010464949,0.0825985651,-0.0203509688,0.0826334032,0.0657562808,-0.1455091093,0.0301801268,-0.0224950264,0.1225567864,0.0124242226,0.0228624221,-0.0064983576,0.0249005073,0.0253751,-0.0690647385,0.0968940563,-0.0078901281,-0.0091700026,-0.013704173,0.013859357,0.0150773577,-0.0033914396,0.0692141404,-0.021070639,0.0214528973,0.0169258587,0.0620843302,-0.0037402606,0.0346426668,-0.002520926,0.052821946,-0.0054172357,-0.0192576762,-0.0045371644,0.0091134154,-0.0029737645,-0.0161161478,0.0261060688,0.0247497977,0.0275717147,-0.0042926321,0.0319453159,-0.0068462384,0.0119789611,0.0015915694,-0.008610858,0.0163821695,-0.0303598906,0.0196320951,-0.0341525112,-0.0237633409,0.0104303788,0.0199203653,0.0109478951]},"519":{"Abstract":"People often have erroneous intuitions about the results of uncertain processes, such as scientific experiments. Many uncertainty visualizations assume considerable statistical knowledge, but have been shown to prompt erroneous conclusions even when users possess this knowledge. Active learning approaches been shown to improve statistical reasoning, but are rarely applied in visualizing uncertainty in scientific reports. We present a controlled study to evaluate the impact of an interactive, graphical uncertainty prediction technique for communicating uncertainty in experiment results. Using our technique, users sketch their prediction of the uncertainty in experimental effects prior to viewing the true sampling distribution from an experiment. We find that having a user graphically predict the possible effects from experiment replications is an effective way to improve one's ability to make predictions about replications of new experiments. Additionally, visualizing uncertainty as a set of discrete outcomes, as opposed to a continuous probability distribution, can improve recall of a sampling distribution from a single experiment. Our work has implications for various applications where it is important to elicit peoples' estimates of probability distributions and to communicate uncertainty effectively.","Authors":"J. Hullman; M. Kay; Y. Kim; S. Shrestha","DOI":"10.1109\/TVCG.2017.2743898","Keywords":"Graphical prediction;interactive uncertainty visualization;replication crisis;probability distribution","Title":"Imagining Replications: Graphical Prediction & Discrete Visualizations Improve Recall & Estimation of Effect Uncertainty","Keywords_Processed":"probability distribution;graphical prediction;replication crisis;interactive uncertainty visualization","Keyword_Vector":[0.0620139353,0.0306225702,0.0047316046,0.0546093614,0.0707722737,-0.0142968578,-0.0252585941,0.0746016756,-0.0504840817,-0.0434547241,-0.0113221891,0.0089832387,0.0142228549,0.0213324426,0.0628138521,0.0101060844,0.0285597372,0.0399789061,0.0138101366,-0.0058938521,0.0156139601,-0.0573938421,0.0847530416,0.0321782015,-0.0329100532,-0.0083775814,0.0251316094,0.0059974686,0.0174701851,-0.0388306429,-0.0102793868,-0.0067899846,0.0003247378,-0.0137143571,0.0171715411,-0.0242672003,0.0071282287,-0.0170395843,0.0227783057,0.0118880469,0.0526147327,-0.0187270849,-0.0570761592,-0.0118910192,-0.003749377,0.0026242687,-0.0205104496,0.0963190495,0.0223478428,0.0008634008,0.0332649402,-0.0187094345],"Abstract_Vector":[0.1504420521,0.0108417568,-0.0472360652,0.0034696228,-0.0313587681,0.014872981,0.0534953318,0.0300053631,0.0271874665,-0.0042118566,-0.0128030594,-0.0157928406,-0.0024222418,-0.0314799824,0.0237765354,-0.0068814091,-0.022188962,0.0523524431,0.0062799351,0.0448037405,0.0168723639,-0.0458112179,0.0333528822,0.0664951145,-0.0349162286,-0.0002163738,-0.0221896179,0.0671331242,0.0207348153,0.0288219035,0.0402262584,0.0013508452,0.0143315879,0.0181228924,0.0024561054,-0.0282045556,0.009572325,-0.0095433416,0.066773619,0.0537297142,0.0240762871,-0.02496597,-0.0245417781,0.0373993827,-0.0044259314,-0.0299900988,0.0064309529,-0.0066456607,-0.0083880695,0.0103683987,-0.035394374,-0.0134135788,-0.0178074257,-0.0068941713,-0.010498951,-0.0477007891,-0.0097095835,-0.0137597406,-0.0502909562,0.0212208139,-0.0177077118,-0.013967991,0.0098924233,0.0744951528,0.0165777197,-0.0225785384,0.0682852037,0.0412752665,0.0506520358,0.0452020053,-0.0001887532,-0.0075360412,-0.0321847432,0.0106298347,0.0108263946,0.0299377353,-0.0553037325,-0.0000332865,-0.0200222986,-0.0313510614,0.0379227969,0.0069213352,-0.0347474323,-0.0004855552,-0.0052576751,0.0110487895,0.0224352919,0.0006452243,-0.0700301445,-0.0563365819,0.0429092627,-0.0034997617,-0.0109167692,-0.0398622477,-0.0207903984,-0.0164580074,-0.0053688197,0.0060105545,-0.0498264442,-0.0135387916,0.0376189669,-0.070392101,0.016930966,-0.0389889855,0.014280236,0.0328225869,-0.0198390163,0.030052683,0.0096796013,0.0035547862,-0.0245169126,0.0206609463,0.0229267671,0.0070750773,0.0613395897,-0.0136680055]},"52":{"Abstract":"In this work, we propose an original scheme for generic content-based retrieval of marker-based motion capture data. It works on motion capture data of arbitrary subject types and arbitrary marker attachment and labelling conventions. Specifically, we propose a novel motion signature to statistically describe both the high-level and the low-level morphological and kinematic characteristics of a motion capture sequence, and conduct the content-based retrieval by computing and ordering the motion signature distance between the query and every item in the database. The distance between two motion signatures is computed by a weighted sum of differences in separate features contained in them. For maximum retrieval performance, we propose a method to pre-learn an optimal set of weights for each type of motion in the database through biased discriminant analysis, and adaptively choose a good set of weights for any given query at the run time. Excellence of the proposed scheme is experimentally demonstrated on various data sets and performance metrics.","Authors":"N. Lv; Z. Jiang; Y. Huang; X. Meng; G. Meenakshisundaram; J. Peng","DOI":"10.1109\/TVCG.2017.2702620","Keywords":"Motion capture;content-based retrieval;minimal motion spanning tree;motion signature;biased discriminant analysis","Title":"Generic Content-Based Retrieval of Marker-Based Motion Capture Data","Keywords_Processed":"content base retrieval;biased discriminant analysis;motion capture;motion signature;minimal motion spanning tree","Keyword_Vector":[0.1484281654,0.0236315592,0.1092773868,-0.0553339112,-0.0778197221,-0.0393801917,-0.0369065293,0.0298967644,0.0332833596,-0.157726074,-0.0207852473,0.0227696098,0.0067704186,-0.0120220147,-0.058317461,-0.0045779561,0.0121106515,-0.0396079597,-0.0022154799,-0.0040751021,-0.0816313185,-0.0295555682,0.0187068618,-0.008538751,-0.0248620707,-0.0255725316,0.0528401091,-0.0506170493,0.0506669873,-0.0733293356,0.0020275149,-0.0044575297,0.015052738,-0.0136716536,0.0337380434,0.0027974703,0.0190097251,-0.0400860646,0.0312616754,-0.008252077,0.0754982041,-0.0597967357,0.0331393579,0.0151731348,-0.0203440787,-0.0106167446,0.0277967287,-0.1066292802,0.0257818966,-0.0955461307,-0.0374500038,-0.0255949081],"Abstract_Vector":[0.1836095462,0.0391080336,0.1907741116,-0.0561170687,0.0538114858,0.0553996144,0.1334038192,-0.1330575248,-0.0030968892,-0.0465819129,0.0238245298,-0.0259482002,-0.0493188648,0.0244086212,-0.0713320585,0.087358658,-0.0729714533,-0.0119381023,-0.0289632662,0.0519984295,0.0333193021,-0.0066068895,0.0285947321,0.0117263736,-0.0429525326,0.0274091677,0.0241096774,-0.0337926511,0.0214811728,-0.0170422022,-0.0015513273,-0.0338790057,0.0106756252,-0.0623367186,-0.0759722005,0.0140083073,0.0244450884,-0.0413143879,0.010989939,-0.0034380788,-0.0365443582,0.0348833642,-0.0141712761,-0.0225050225,0.0083137988,0.0322499588,0.0185884039,-0.0015189592,0.0162601262,0.036072424,-0.0280333602,0.0100886907,-0.0146363855,0.0411408442,0.0238163798,-0.002031749,0.0258851428,-0.0167011185,-0.0448485338,0.0018975634,0.0265015839,-0.0323656685,-0.0182399523,-0.040416547,0.02070759,-0.0373012219,0.0308145688,0.0226209822,-0.0131098124,-0.0118669401,-0.0038731115,-0.0070023917,-0.0104534004,-0.0146002435,-0.0071828961,0.0051235565,0.0547164438,-0.045925232,-0.0018540243,0.0308199617,0.0121626858,0.007942506,-0.0315713049,-0.0126762161,0.0256642449,-0.0101135357,0.0169945834,0.0108635888,-0.0092623002,-0.0454965904,-0.00287812,0.0360881124,-0.0640198566,-0.0399148139,-0.0233916771,-0.0007083277,-0.006746819,0.0299614902,0.0124995633,0.0213882352,0.0171265696,-0.0114746709,-0.0390031155,0.044640979,-0.0123198411,0.0196953608,-0.033763365,-0.0285308961,0.0004896387,-0.0541672317,0.002112424,0.038576449,-0.0175324571,-0.0058034147,0.0044231587,0.0191011193]},"520":{"Abstract":"This paper presents a novel algorithm for assessing the motion stability of a video after stabilization. The assessment works in a non-reference manner that directly measures the intrinsic smoothness of the video motion path. Specifically, the motion path is cast as a curve embedded in the Lie group of homographies, and its smoothness is mathematically characterized by the intrinsic geodesic curvature. A bundle of paths are adopted to handle spatially variant motions through the frames. Then, we compute the weighted curvature for a holistic assessment on the motion stability. Other factors related to video stabilization, e.g., distortion and cropping, are also investigated as supplement. We collect 160 shaky video clips and their stabilized results for verification, and the experimental evidence shows the effectiveness of our algorithm in good correlation with human subjective judgements.","Authors":"L. Zhang; Q. Zheng; H. Huang","DOI":"10.1109\/TVCG.2018.2817209","Keywords":"Stabilization;non-reference;assessment;geodesic curvature","Title":"Intrinsic Motion Stability Assessment for Video Stabilization","Keywords_Processed":"non reference;geodesic curvature;stabilization;assessment","Keyword_Vector":[0.0325797344,-0.0138001037,-0.0207134995,-0.0177478742,-0.0006805146,-0.0268285399,-0.0203149287,-0.0046964868,-0.0304213054,0.0486450998,-0.0406821912,0.0342089489,-0.0210665876,0.0259182837,-0.0453253739,0.0037331147,-0.0196529224,0.0028835344,-0.0143819022,-0.0098693845,0.054514112,-0.008945265,-0.0240086099,0.035057191,-0.0615952579,0.048544263,-0.0154808036,0.0411358785,-0.0013013907,-0.0311243082,-0.0328318607,-0.0584288419,-0.0328209997,-0.0046753197,0.0016378988,-0.028144606,0.1002917069,-0.0347664039,-0.0279866338,-0.0276711942,0.0586398505,0.0410827521,0.0202577604,-0.0310055915,-0.0137695243,-0.0124973738,-0.0157272481,-0.0175423587,-0.0169807123,-0.0086467087,-0.0219015804,0.0514542897],"Abstract_Vector":[0.0997166622,-0.0672219697,0.0067826546,-0.0059380185,-0.0389596578,0.0153467948,0.0094738615,0.0174462839,-0.0244411751,-0.0152154675,-0.0199181318,0.0199552242,0.0424090091,-0.0168317402,0.0205980204,-0.0058979381,-0.0364144167,-0.0047658595,-0.0041203834,-0.0108056337,0.0084030908,-0.0070509632,-0.002032857,-0.0266436295,-0.021725475,0.0176181965,-0.0086468798,-0.0072518706,-0.0419728213,-0.022288482,-0.0277485806,-0.0046925689,0.0162103573,-0.0023458336,0.0022923056,0.016432749,-0.0079475354,0.0035973739,-0.0273830244,0.013886802,-0.0005269832,-0.0232053799,0.0306012683,-0.0599338935,0.0100070229,-0.0282113339,0.013007997,-0.015312552,-0.0150052634,0.0181414292,-0.0017506715,0.0125877195,0.0285101358,-0.0104701121,0.0031627669,0.0031922149,-0.0272909929,-0.001509617,-0.0159198088,0.0057546429,0.0047348298,-0.0161832605,-0.0137600137,-0.0240022159,0.0011259523,-0.0111023262,-0.0246877173,0.0152188249,-0.0017912785,0.0037813605,0.0038868186,-0.0243951281,0.0085576975,-0.001802984,0.010706762,-0.0061497535,-0.0079928326,0.0256253547,-0.0105076829,0.0101737035,0.0046883455,-0.0071967266,0.0001740443,0.0021909641,-0.0095464555,-0.0053010077,0.0022927222,0.0141125485,0.0072879927,-0.0031666924,-0.0004131413,0.0096129778,0.0181670263,-0.0075173689,-0.005222372,-0.0013688158,0.0006580333,0.0096989303,-0.0011129447,0.0033007029,0.0053623925,-0.0069782693,0.0213572144,-0.0041737773,0.0036719429,0.0131652689,-0.0173480692,0.0058879444,-0.0178040872,0.0041635123,-0.0044370326,-0.0012550597,0.0071153462,0.0065568781,0.0050046748,0.02290958]},"521":{"Abstract":"To effectively assess the potential consequences of human interventions in model-driven analytics systems, we establish the concept of speculative execution as a visual analytics paradigm for creating user-steerable preview mechanisms. This paper presents an explainable, mixed-initiative topic modeling framework that integrates speculative execution into the algorithmic decision-making process. Our approach visualizes the model-space of our novel incremental hierarchical topic modeling algorithm, unveiling its inner-workings. We support the active incorporation of the user's domain knowledge in every step through explicit model manipulation interactions. In addition, users can initialize the model with expected topic seeds, the backbone priors. For a more targeted optimization, the modeling process automatically triggers a speculative execution of various optimization strategies, and requests feedback whenever the measured model quality deteriorates. Users compare the proposed optimizations to the current model state and preview their effect on the next model iterations, before applying one of them. This supervised human-in-the-Ioop process targets maximum improvement for minimum feedback and has proven to be effective in three independent studies that confirm topic model quality improvements.","Authors":"M. El-Assady; F. Sperrle; O. Deussen; D. Keim; C. Collins","DOI":"10.1109\/TVCG.2018.2864769","Keywords":"User-Steerable Topic Modeling;Speculative Execution;Mixed-Initiative Visual Analytics;Explainable Machine Learning","Title":"Visual Analytics for Topic Model Optimization based on User-Steerable Speculative Execution","Keywords_Processed":"speculative execution;Mixed Initiative visual analytic;explainable machine Learning;User Steerable topic modeling","Keyword_Vector":[0.0870111906,-0.0057703184,0.0163753879,-0.0432496677,0.0154959249,0.0060238902,-0.0330354274,-0.0155021059,-0.00476805,-0.0358832826,0.0107300928,-0.0074159296,-0.0354219097,0.0144888862,0.0225360457,0.0345335641,-0.015529065,-0.0049808713,0.0115884952,-0.0118700495,0.0365595608,0.0403775454,-0.0636505233,0.0050674458,0.0202158891,0.0352808445,0.0054423739,0.0322873051,0.0454189524,0.0031941501,0.028832542,0.0040881908,0.0224645307,-0.0338817661,-0.0212385933,-0.0162444036,-0.0218882094,-0.050314197,-0.0053119965,0.0133301227,-0.0435195429,0.058391809,0.0602907123,0.0083411102,-0.0273841225,0.0343738918,0.0118279519,0.0445043386,0.0363899441,-0.0027734081,0.0026605314,0.0033910717],"Abstract_Vector":[0.2163696833,-0.04924757,-0.0096728806,-0.0210406076,-0.0238037425,-0.0529445645,0.0471835885,-0.0049305978,0.0047100041,0.0591573503,0.0207548623,-0.0125401746,-0.0028022345,0.010901092,-0.0487930029,-0.0005826454,0.02403928,0.0122764006,-0.0063088636,-0.0600513589,0.0198339233,-0.0001724248,-0.010793581,-0.0064183065,0.0120000353,0.0403678361,-0.0028265013,-0.010213168,-0.0416403644,-0.0322499025,0.0113906515,0.0208995985,-0.0397862596,0.0046402943,0.0156637705,-0.0029353562,-0.0139204478,0.0063295423,0.0537554062,0.0341114569,0.0354127192,0.0171188068,-0.0620193212,0.0739538725,0.0055167168,0.0145808981,-0.053681689,0.0467009223,-0.0221752937,-0.0429898563,0.040356102,0.0445950345,-0.0169173126,-0.0248185649,-0.0031751121,-0.0061687735,0.0507020364,-0.0503189546,-0.0562423632,-0.0047107035,0.0065105959,-0.0053420485,0.0036523375,-0.0133639148,0.0215400592,-0.0012680103,-0.0255877267,-0.0145719654,0.0100666816,-0.0256465912,-0.0025474104,0.0254446606,-0.0112495386,0.003689922,-0.00493196,0.0193163829,0.047171584,-0.011261393,0.008857716,0.0432554833,-0.022440229,0.0154821057,-0.0030901016,-0.0355086323,0.0268273077,-0.0214400591,-0.0087222608,0.0351574971,0.0058127375,0.0149431323,-0.0015478267,0.0105732149,0.0037312136,0.0391094605,0.064121967,-0.0024377682,-0.0006960454,-0.0045239737,0.0115485767,0.0343693362,0.0014864557,-0.0076241121,0.0067928473,0.0356394335,0.0271448345,0.0224800399,0.07459408,-0.0081450774,0.0461499843,0.0430136539,-0.02750606,0.009399067,-0.0212024956,0.0289325516,-0.0146109523,0.0022886747]},"53":{"Abstract":"During asynchronous collaborative analysis, handoff of partial findings is challenging because externalizations produced by analysts may not adequately communicate their investigative process. To address this challenge, we developed techniques to automatically capture and help encode tacit aspects of the investigative process based on an analyst's interactions, and streamline explicit authoring of handoff annotations. We designed our techniques to mediate awareness of analysis coverage, support explicit communication of progress and uncertainty with annotation, and implicit communication through playback of investigation histories. To evaluate our techniques, we developed an interactive visual analysis system, KTGraph, that supports an asynchronous investigative document analysis task. We conducted a two-phase user study to characterize a set of handoff strategies and to compare investigative performance with and without our techniques. The results suggest that our techniques promote the use of more effective handoff strategies, help increase an awareness of prior investigative process and insights, as well as improve final investigative outcomes.","Authors":"J. Zhao; M. Glueck; P. Isenberg; F. Chevalier; A. Khan","DOI":"10.1109\/TVCG.2017.2745279","Keywords":"Collaboration;sensemaking;handoff;handover;structured externalizations;interactive visual analysis","Title":"Supporting Handoff in Asynchronous Collaborative Sensemaking Using Knowledge-Transfer Graphs","Keywords_Processed":"collaboration;handoff;sensemake;handover;interactive visual analysis;structured externalization","Keyword_Vector":[0.139908097,0.0073539954,-0.0201149497,-0.0183841921,0.0506224862,-0.0492604081,-0.0708617308,0.0223016754,-0.1069416324,-0.0467016265,-0.0772280825,0.0204436005,0.0224585263,0.0409205411,0.0761170391,-0.0383818412,0.0446221066,0.15105435,0.012728079,0.0971134564,0.089252638,-0.0009202617,-0.171242646,0.1296840773,0.0425463243,-0.0293934964,-0.1156224014,-0.0276948983,0.0021912306,0.016384675,0.0281844986,0.0314035991,-0.0078391878,0.0478454891,-0.0015000525,-0.0207685688,-0.0956421448,0.0298046314,-0.0416091012,-0.0384673861,-0.0021099578,-0.028934213,-0.0524676646,-0.0650517205,-0.010924524,-0.0352167931,0.0579069385,-0.0849278958,0.0052240858,0.0180453137,0.0270718424,-0.0413880593],"Abstract_Vector":[0.1176413516,-0.0435876139,-0.0024802111,0.0091829948,-0.0458447913,0.0442384974,0.0084448853,0.0053760366,0.0281857145,0.0149208359,-0.0404880874,0.0345220754,-0.0064706942,0.0004384209,-0.0186684598,0.0011097905,-0.0086864128,-0.0203050194,0.0139125896,-0.0239459448,0.0145612453,-0.0342404089,0.0929477004,-0.0204984272,-0.0084803829,-0.0254696002,0.0048026502,-0.0152083189,-0.0107728858,0.0166846396,0.0520572785,-0.0036452093,-0.0394556017,-0.0124821259,0.0359279769,-0.0264472803,-0.0001556536,0.0083839751,-0.0275021086,-0.0043223166,0.0073992077,0.0051729892,0.0218774708,0.0170020547,0.0302901359,-0.0453450547,0.0598607494,0.0479241441,-0.043899305,0.0368374893,0.0051849643,-0.0093184804,0.0051598967,-0.0141302068,0.0113126833,0.0269694233,0.0488649451,0.0255401039,0.0100284267,0.0166814689,0.0345221964,0.0005172528,0.0357778261,0.0128911617,0.0231041859,-0.0171493029,0.0000959349,-0.0036538366,-0.0488761224,0.0668937626,0.0046669985,0.0424457002,-0.0086191921,0.0016091225,0.0321541043,-0.0438998496,0.0180541209,-0.0437500086,0.0178288759,-0.0306893692,0.0071370761,0.042791111,0.0162769981,-0.0145746798,-0.0912069241,0.0215226418,-0.0326733667,-0.0089663845,0.0564654468,0.0294328139,0.0025014209,0.0159264584,0.0055499581,-0.0227313022,0.0091478412,0.0044193458,-0.007174403,0.0089653645,0.0083231926,-0.0328690102,-0.0271824162,-0.0263363967,-0.009948619,0.0075892675,-0.0210733537,-0.0225930253,0.016319497,0.0087215838,0.0114315096,-0.013385701,0.0025483317,-0.0100853676,-0.0123166075,-0.0191555963,0.0235699753,0.0035744918]},"54":{"Abstract":"In the development of graphical algorithms, choosing an appropriate data representation plays a pivotal role. Hence, there is a need for studies that support corresponding decision making. Here, we investigate curvature estimation based on two discrete representations-volume images and triangle meshes-and present a comprehensive cross-comparison. For doing so, four carefully selected geometries, represented as implicit functions, have been discretized to volume images and triangle meshes in different resolutions on a comparable scale. Afterwards, implementations available in open-source libraries (CGAL, DIPimage, libigl, trimesh2, VTK) and our own implementation of a relevant paper [1] were applied to them and the resulting estimations of mean and Gaussian curvature were compared in terms of quality and runtime. Independent of the underlying discrete representation, all estimators generated similar errors, but overall, mesh-based methods allowed for more accurate estimations. We measured a maximum normalized mean absolute error difference of 6.36 percent between the most precise mesh-based method compared to corresponding image-based methods when considering only discretizations of sufficient resolution. In terms of runtime, methods working on triangle meshes were faster when geometries had a small surface density. For geometries with larger surface densities, which is fairly common when considering real data, e.g., in material or medical science, the runtimes for both representations were similar.","Authors":"M. Kronenberger; O. Wirjadi; H. Hagen","DOI":"10.1109\/TVCG.2018.2861007","Keywords":"Differential geometry;mean curvature;Gaussian curvature;curvature estimation;volume image;triangle mesh","Title":"Empirical Comparison of Curvature Estimators on Volume Images and Triangle Meshes","Keywords_Processed":"volume image;mean curvature;curvature estimation;gaussian curvature;triangle mesh;differential geometry","Keyword_Vector":[0.25991319,-0.0959638434,0.0060225772,0.0386736332,0.0851766707,0.0814887262,-0.0881003458,0.0275200855,-0.0635876042,0.0168895793,-0.0848997683,0.1375987118,0.0073691758,-0.0940120206,-0.0502067646,0.0012743014,-0.0094075859,-0.0092488963,-0.0248332055,0.0204151554,-0.0327578503,-0.0486414705,-0.0750341943,-0.0809747564,0.0280286515,-0.030766269,-0.0587480432,0.0711563706,0.0091482645,0.0453293321,0.0650504974,-0.0096019957,-0.087117254,-0.0996766993,-0.0027309337,-0.0166867051,0.189370983,0.0329817013,-0.0343967946,-0.1624468353,-0.0094267098,-0.0553655729,-0.0646395605,0.0229389359,0.018623604,0.0005326404,0.097525129,-0.0131033489,0.0077737242,0.0288718012,-0.1072112634,-0.0486910876],"Abstract_Vector":[0.2423617442,-0.1026532027,-0.0340598825,0.0029002674,0.0460812719,-0.1404609828,-0.0692900527,-0.0563783152,0.0089851759,-0.0351667159,0.0319435291,-0.0852433331,0.0278788915,-0.0076867901,0.0245805201,-0.0168098126,-0.0262752237,-0.0526430302,-0.0109262305,0.0269872338,-0.0141453069,-0.0100635203,-0.0006623324,-0.0532141251,-0.0196769351,0.0453548854,0.0297038587,-0.0874728099,-0.0281809714,0.0142845137,0.0967166671,0.0147535982,0.0051819578,0.0041857354,-0.0630223219,-0.0694504194,0.0987010312,-0.0069668055,-0.0729864596,-0.0515937454,0.122969714,-0.0295645649,-0.0575015807,-0.0894608846,0.0352875302,-0.0125107464,0.063702003,0.0104167566,-0.0196036419,-0.0018014621,0.1501942804,0.0428572418,0.0436771147,-0.0039413701,-0.0783329654,-0.0126948234,-0.102518726,-0.0230444544,-0.025807554,0.0000712536,0.0546130649,-0.0604082672,0.0632839195,-0.0042427198,-0.1030562203,0.0402874512,0.1356445186,-0.0521207011,-0.029358031,-0.0084768317,0.0068109346,0.0142004161,0.0193876408,0.0696036196,0.0098816057,0.0020817959,0.0471174762,-0.0264058489,0.0560193704,0.0125788515,0.0842629946,0.0406195908,-0.0137175494,0.0470229365,-0.0239187407,-0.0021077677,-0.0188116082,-0.0220853252,-0.0648384301,-0.0416258455,0.0039186992,0.0199322227,0.0183412332,-0.0084396454,-0.0339344935,0.0562346139,0.0311748025,0.0296178736,0.0411868328,-0.0156015228,0.0102265947,-0.0242001885,-0.0174199726,-0.0225410737,0.0511722457,-0.0170803191,-0.0352594922,-0.0611350247,-0.037379384,-0.006323416,-0.031026676,0.0187002891,-0.0088729187,0.0072540876,0.0198574047,-0.0139494636]},"55":{"Abstract":"Specular microfacet distributions have been successfully employed by many authors for representing glossiness of materials. They are generally combined with a Lambertian term to account for the colored aspect. These representations make use of the Fresnel reflectance factor at the interface, but the transmission factor at the interface should also be managed. One solution is to employ a multi-layered model with a single layer for the rough interface, which requires a numerical simulation for handling the multiple reflections of light between the substrate and the interface. In this paper, we propose rather to use a representation corresponding to a Fresnel interface lying on a Lambertian substrate, for which the multiple reflections of light between the interface and the substrate can be expressed analytically. With this interfaced Lambertian model, we show how Fresnel transmission affects the material appearance for flat and rough surfaces with isotropic and anisotropic distributions, that produce light backscattering effects. We also propose a methodology for using such materials in any physically based Monte Carlo rendering system, as well as an approximate representation, suitable for GPU applications or measured data fitting. Our approach generalizes several previous models, including flat Lambertian materials as well as specular and Lambertian microfacets. Our results illustrate the wide range of materials that can be rendered with this representation.","Authors":"D. Meneveaux; B. Bringier; E. Tauzia; M. Ribardi\u00e8re; L. Simonot","DOI":"10.1109\/TVCG.2017.2660490","Keywords":"Surface appearance;BRDF;microfacets;importance sampling","Title":"Rendering Rough Opaque Materials with Interfaced Lambertian Microfacets","Keywords_Processed":"surface appearance;microfacet;importance sample;BRDF","Keyword_Vector":[0.1973930147,-0.0548940899,-0.0199856725,0.0810142538,0.026699739,0.1275924915,-0.0404612618,-0.0441429348,-0.0461573017,-0.0757593049,0.0358542328,0.0369986269,0.0204966604,0.0571271748,0.0451503809,-0.0130754812,0.024023588,0.0371241464,0.0650056158,-0.1225521961,0.0412841579,0.1248710109,-0.2046268803,-0.0304798336,0.0971336562,0.0478902552,-0.0501721262,-0.0110485678,0.1058772242,0.0323646909,0.0096022462,0.0128979903,0.0186238046,0.1270444997,-0.0424011358,0.1465069408,0.061888605,-0.0170061091,-0.0711060395,0.01713889,0.084306044,0.1709167131,0.0512662237,-0.0599823086,-0.0793219607,-0.0042007853,0.087993193,0.0885185208,0.1301841944,0.0394960591,0.0008776934,-0.1082578847],"Abstract_Vector":[0.1860117816,0.0091508535,-0.0259690305,0.0254567214,-0.0193323265,0.0048128741,0.0401293996,0.042666924,-0.0143079224,-0.0006435053,-0.004802438,-0.0405212162,0.0045288948,0.0089150839,-0.0067168444,0.0542066811,0.0388866968,0.049208903,0.0138199418,-0.0088828589,0.0354157779,0.0004082248,0.0187013722,-0.0253055767,0.0024309761,0.0578722414,0.0254865123,0.019285185,0.0072643402,0.0025086548,0.0129888675,-0.0106825938,-0.0201081848,0.0641770878,-0.0345563349,0.0539673036,-0.0087016307,0.0299185164,0.0205100206,0.0295877438,-0.0185456477,0.0271968175,-0.0499589318,0.0057853997,0.0242729663,-0.0082352333,0.0155551046,-0.0041109903,0.0114782908,-0.0086560353,0.0331385599,0.0235676987,0.0043436294,0.0338968017,-0.0123304895,0.0259269288,0.0505831187,0.0146586274,-0.0090377906,-0.013184024,0.0557018204,-0.0055535242,0.0380942189,0.0091637089,-0.0200145965,-0.0119554951,-0.0132856578,0.0038061756,0.0109352689,-0.0454232543,0.0002452882,0.045155019,0.0219612314,0.0243616724,-0.0107942902,-0.0011798232,0.0351910957,-0.0054867972,0.0079181849,-0.0024702722,-0.0178990233,0.0152448613,-0.031358459,0.0165760326,-0.0206536418,0.0313065272,-0.0237245448,-0.0173614098,-0.0181852603,-0.0092124378,-0.0339629814,-0.018100293,-0.0004071163,-0.0042329223,-0.002142325,-0.0224504611,0.0169632992,0.0004550695,-0.0237792268,0.0320485717,-0.0212414282,0.0112346965,0.0151455714,0.0259746154,0.0204519934,0.0159438729,0.0087603444,0.0250790248,0.0187646323,-0.0222724516,0.016522991,-0.0042097351,0.0238492909,0.0038265338,-0.0116209532,0.0057577419]},"56":{"Abstract":"With the rapidly growing VR industry, in recent years, more and more attention has been paid for fire sound synthesis. However, previous methods usually ignore the influences of the different solid combustibles, leading to unrealistic sounding results. This paper proposes SSC (sounding solid combustibles), which is a new recording-driven non-premixed flame sound synthesis framework accounting for different solid combustibles. SSC consists of three components: combustion noise, vortex noise and popping sounds. The popping sounds are the keys to distinguish the differences of solid combustibles. To improve the quality of fire sound, we extract the features of popping sounds from the real fire sound examples based on modified Empirical Mode Decomposition (EMD) method. Unlike previous methods, we take both direct combustion noise and vortex noise into account because the fire model is non-premixed flame. In our method, we also greatly resolve the synchronization problem during blending the three components of SSC. Due to the introduction of the popping sounds, it is easy to distinguish the fire sounds of different solid combustibles by our method, with great potential in practical applications such as games, VR system, etc. Various experiments and comparisons are presented to validate our method.","Authors":"Q. Yin; S. Liu","DOI":"10.1109\/TVCG.2016.2642958","Keywords":"Non-premixed fire sound;solid combustibles;direct combustion noise;vortex noise;popping sounds","Title":"Sounding Solid Combustibles: Non-Premixed Flame Sound Synthesis for Different Solid Combustibles","Keywords_Processed":"solid combustible;vortex noise;pop sound;non premixed fire sound;direct combustion noise","Keyword_Vector":[0.096916599,-0.123147991,-0.1261588645,0.0722293171,-0.0834356427,-0.1595048329,0.0514799729,0.0068965335,0.0408688376,-0.0289491676,0.0069991118,0.0121390581,-0.0366403101,-0.0855482754,0.0194616373,0.0080208875,-0.0117756575,-0.000192347,0.0315505106,-0.0082516009,0.0186619492,0.0360332288,-0.0033971672,-0.0043055345,-0.0469482738,0.0424959718,-0.0013222975,-0.0160492366,0.0070392633,0.064128333,-0.0051868316,-0.0574075206,-0.0410860248,0.026668738,0.0423905366,0.0358881896,-0.0197252833,-0.0030064717,-0.0335744985,-0.0463609151,0.0075532906,-0.0138779565,-0.0336808458,-0.0975812015,0.0780819636,-0.0323187033,-0.0764947358,-0.0177162347,0.0378591402,-0.0276374189,0.0630877509,0.0230338473],"Abstract_Vector":[0.1463769627,-0.0622549554,0.029515513,0.0005363637,-0.0079378486,-0.0021296389,-0.0112611341,0.0144046005,-0.0414517785,0.0237821612,-0.0352366137,0.0104322469,-0.01518701,-0.0185102724,0.0019210748,0.0148601147,0.0079290492,-0.0119291219,-0.0147180226,0.0068547421,0.0150506929,-0.0131205227,-0.0272688674,-0.0169507817,-0.0223838268,0.0002646699,-0.0102057569,-0.0049936393,0.014781363,0.0016615352,0.0229365639,0.0173682915,-0.027165678,0.0162580146,-0.0228396698,-0.0381694774,-0.0196271878,-0.0036755488,-0.0091146836,0.0063712861,0.0087508037,0.0071369947,0.0208041929,-0.0045034667,-0.0133067432,0.0109678305,0.0111102447,0.0103042614,0.0089157094,-0.0392014716,-0.0049684041,-0.0003840667,-0.0045376507,-0.0141250598,-0.0231476887,-0.0020921034,-0.0128002416,0.0067139985,0.013260578,-0.001945945,0.0211177542,-0.0114296255,-0.00285427,0.0176022227,-0.0297973962,-0.0493719432,-0.0320836525,-0.0055840531,-0.0225279051,-0.0357612277,0.023818473,-0.0110678456,-0.0078992804,-0.0085759987,-0.0059475975,0.0165053429,0.0258380914,0.0273100205,-0.0063762148,-0.0071999344,0.0032718967,0.0167207768,-0.0027423542,0.0119375468,-0.0035375469,-0.0036231635,-0.0122101279,0.0322331611,-0.0131137304,-0.0057332405,-0.0057251733,0.0153035431,-0.0180909416,0.0015675138,-0.0077708901,-0.0141983053,-0.0153416176,-0.0000498369,-0.0212059521,-0.0386025646,0.020572981,-0.004240716,-0.0261137509,0.0118604382,-0.0062799037,0.0081785867,0.012419863,0.0002405439,-0.0240641502,-0.0031805302,0.0056237635,-0.0168999087,0.0058526813,0.0165260353,0.006305097,-0.0076683721]},"57":{"Abstract":"Before-and-after image pairs show how entities in a given region have evolved over a specific period of time. Satellite images are a major source of such data, that capture how natural phenomena or human activity impact a geographical area. These images are used both for data analysis and to illustrate the resulting findings to diverse audiences. The simple techniques used to display them, including juxtaposing, swapping and monolithic blending, often fail to convey the underlying phenomenon in a meaningful manner. We introduce Baia, a framework to create advanced animated transitions, called animation plans, between before-and-after images. Baia relies on a pixel-based transition model that gives authors much expressive power, while keeping animations for common types of changes easy to create thanks to predefined animation primitives. We describe our model, the associated animation editor, and report on two user studies. In the first study, advanced transitions enabled by Baia were compared to monolithic blending, and perceived as more realistic and better at focusing viewer's attention on a region of interest than the latter. The second study aimed at gathering feedback about the usability of Baia's animation editor.","Authors":"M. Lobo; C. Appert; E. Pietriga","DOI":"10.1109\/TVCG.2018.2796557","Keywords":"Animation;blending;staging;remote sensing images","Title":"Animation Plans for Before-and-After Satellite Images","Keywords_Processed":"remote sensing image;animation;blend;stage","Keyword_Vector":[0.1471285344,-0.0857377545,-0.068655229,0.1031326579,-0.0917644047,-0.0576139227,0.0317098526,0.0022386431,-0.0617765693,-0.0544449047,0.0968706907,-0.0427551127,-0.0450283411,0.182478084,-0.0991375812,-0.0027768322,0.1219246711,0.0179941149,0.0466352775,-0.07799475,0.0007940898,-0.032883021,-0.0540290793,-0.0185504465,0.0366079701,-0.0284531615,-0.011813858,-0.023901111,-0.0028892011,0.0397769639,0.0260361594,0.047790204,0.0001464975,0.0712706997,-0.0240874804,0.0878823779,0.086315563,-0.0047629731,0.0103752531,-0.0089737097,0.035925474,0.007456324,-0.0352924619,-0.0026758237,-0.0438240733,0.0080587176,0.0520877107,-0.0333536169,-0.0096919809,-0.0110593454,-0.0405154126,0.0216163429],"Abstract_Vector":[0.1641412875,-0.0024793205,0.0311614619,-0.0035367207,-0.0187441885,0.026995524,-0.0740469666,0.0971771081,0.0010143566,0.0531115411,0.0705785902,-0.0068796401,-0.0788757981,0.0456388518,-0.1035061918,0.0361887559,-0.0677780933,0.012436254,0.1021428547,-0.0587466775,0.0034812068,0.00344113,0.0170316279,0.1566841467,-0.1636549655,0.1672200556,-0.1235324104,-0.1143226216,0.0270186563,0.0452327811,-0.0170266897,-0.0387895367,0.0180793921,0.0816789026,0.0175265211,0.0826098042,-0.0162140389,0.0184720906,-0.0210359104,-0.0272904959,0.0112128044,-0.0427375096,0.0738895576,-0.0192928922,0.0662694124,-0.0279924519,-0.0833855721,-0.0061710628,-0.0366950102,0.0675821333,0.038874363,-0.0010861208,-0.0082952945,0.0634851574,-0.0416632776,-0.0241022951,-0.0327261988,-0.0122218769,-0.005518846,0.0038338187,-0.022899938,-0.0196683452,-0.0238260268,-0.0138036919,-0.0013024595,0.0245659745,-0.0425732233,-0.017326415,-0.0097473266,-0.0567360784,0.0228801048,-0.0145184986,-0.023469746,-0.036137795,-0.0011747366,0.0184401331,0.0019614249,-0.0058636207,0.0318531767,-0.0051670539,0.0308447378,-0.0420511598,0.0127364033,-0.0077189773,0.0043559589,0.0815410714,0.000081601,0.0089439427,-0.0204918146,-0.0447573394,0.010959236,0.002795051,0.0346491792,0.0369029708,0.0142228868,-0.0253556103,0.001077768,-0.0161355388,-0.0198700292,0.0070655768,-0.028773129,-0.0145442989,0.0323839174,0.0395502178,-0.0633468239,0.0442172965,-0.0069752764,-0.0250309259,0.0170068602,-0.0471997482,-0.0400588843,0.0232990415,0.0057010982,-0.0213814631,0.0232740207,0.0130065924]},"58":{"Abstract":"Lossy texture compression is increasingly used to reduce GPU memory and bandwidth consumption. However, as raised by recent studies, evaluating the quality of compressed textures is a difficult problem. Indeed using Peak Signal-to-Noise Ratio (PSNR) on texture images, like done in most applications, may not be a correct way to proceed. In particular, there is evidence that masking effects apply when the texture image is mapped on a surface and combined with other textures (e.g., affecting geometry or normal). These masking effects have to be taken into account when compressing a set of texture maps, in order to have a real understanding of the visual impact of the compression artifacts on the rendered images. In this work, we present the first psychophysical experiment investigating the perceptual impact of texture compression on rendered images. We explore the influence of compression bit rate, light direction, and diffuse and normal map content on the visual impact of artifacts. The collected data reveal huge masking effects from normal map to diffuse map artifacts and vice versa, and reveal the weakness of PSNR applied on individual textures for evaluating compression quality. The results allow us to also analyze the performance and failures of image quality metrics for predicting the visibility of these artifacts. We finally provide some recommendations for evaluating the quality of texture compression and show a practical application to approximating the distortion measured on a rendered 3D shape.","Authors":"G. Lavou\u00e9; M. Langer; A. Peytavie; P. Poulin","DOI":"10.1109\/TVCG.2018.2805355","Keywords":"Texture compression;psychophysical experiment;image quality assessment;diffuse map;normal map","Title":"A Psychophysical Evaluation of Texture Compression Masking Effects","Keywords_Processed":"image quality assessment;psychophysical experiment;diffuse map;texture compression;normal map","Keyword_Vector":[0.0272473276,-0.0003060741,-0.0094485683,-0.0171710946,0.0056444404,-0.0076414639,0.0096166371,-0.0155825717,-0.0271159831,-0.0182217391,-0.0090479218,-0.0023176716,0.010630695,0.0137055088,-0.0220573858,-0.0082154026,0.0416670692,-0.019943921,-0.002167294,0.0114706479,0.0075307462,0.0183799225,-0.0168291245,0.0096303653,-0.0089696296,0.0661322768,-0.0325967158,0.0323553856,-0.0209139805,0.0936940765,0.0199314792,-0.0109912387,0.0277673718,-0.009740868,-0.047520292,0.0088835654,0.0070740946,0.0343381066,0.0072286218,0.0260506821,0.0289832439,0.0443411789,-0.0481499088,-0.0407717744,-0.0948016264,0.0584532191,-0.075250916,-0.0180596171,0.0142281122,0.0018102877,-0.0874555918,-0.0495670285],"Abstract_Vector":[0.1750865962,-0.0109028102,-0.0086670772,0.0181272288,-0.0414182827,0.0456866423,0.0235383837,0.0029590576,-0.0194115744,-0.0137859213,-0.0169805448,0.0088567891,0.0249797581,0.0348953069,0.038149878,-0.0314844865,-0.0134364694,0.0280736812,-0.0235332781,-0.0267663982,0.0055607057,0.0096140895,0.0648080308,0.0299193439,-0.0209612709,-0.0135654538,-0.0175361059,0.0154961193,-0.0836843936,0.0113480289,-0.0277070855,-0.0256625815,-0.0159099606,-0.0169191906,0.0346754102,-0.0061259697,0.0249235632,-0.0217861671,0.0114907954,0.0185976415,0.0155563587,-0.0014377128,0.048280951,-0.0225693581,0.0232620649,-0.0843105633,0.0102426334,0.064641291,-0.0139386019,0.008311105,-0.0185780478,0.0145687666,0.0014661188,-0.0387605191,0.0261966755,0.0303638089,-0.0141471724,-0.0164215851,0.0306033336,-0.0086321445,0.0117807264,-0.0052182699,0.0813895975,-0.0456667916,0.0108877535,-0.0465379557,0.0177712224,0.0201642692,-0.0218313024,0.0532911778,-0.0255598401,0.0312302668,0.0024895543,0.0127215252,0.0286656053,-0.0425227028,0.0214092393,0.0562763131,0.0226062274,-0.0117553811,-0.0117701866,0.0383256564,0.0432617177,-0.0319554654,-0.0377533486,-0.0111654534,0.0219933492,0.0235469382,0.0680728911,-0.0268170986,-0.0068808278,-0.0158139246,0.0027317709,-0.0009805869,0.0063343702,-0.0260342378,-0.006987877,-0.0040055551,-0.0103309101,0.0276406089,0.0079582456,-0.0371231937,-0.0421283601,-0.0093858712,-0.0366156085,0.0184217576,-0.0187774377,0.0105945878,0.0136783217,0.0028131954,0.0492016845,0.0294653317,-0.008353365,0.0198968922,0.0260760497,-0.0016630077]},"59":{"Abstract":"In this paper we introduce a novel micropolar material model for the simulation of turbulent inviscid fluids. The governing equations are solved by using the concept of Smoothed Particle Hydrodynamics (SPH). As already investigated in previous works, SPH fluid simulations suffer from numerical diffusion which leads to a lower vorticity, a loss in turbulent details and finally in less realistic results. To solve this problem we propose a micropolar fluid model. The micropolar fluid model is a generalization of the classical Navier-Stokes equations, which are typically used in computer graphics to simulate fluids. In contrast to the classical Navier-Stokes model, micropolar fluids have a microstructure and therefore consider the rotational motion of fluid particles. In addition to the linear velocity field these fluids also have a field of microrotation which represents existing vortices and provides a source for new ones. However, classical micropolar materials are viscous and the translational and the rotational motion are coupled in a dissipative way. Since our goal is to simulate turbulent fluids, we introduce a novel modified micropolar material for inviscid fluids with a non-dissipative coupling. Our model can generate realistic turbulences, is linear and angular momentum conserving, can be easily integrated in existing SPH simulation methods and its computational overhead is negligible. Another important visual feature of turbulent liquids is foam. Therefore, we present a post-processing method which considers microrotation in the foam particle generation. It works completely automatic and requires only one user-defined parameter to control the amount of foam.","Authors":"J. Bender; D. Koschier; T. Kugelstadt; M. Weiler","DOI":"10.1109\/TVCG.2018.2832080","Keywords":"Smoothed particle hydrodynamics;micropolar fluids;turbulence;incompressible fluids;foam","Title":"Turbulent Micropolar SPH Fluids with Foam","Keywords_Processed":"micropolar fluid;incompressible fluid;turbulence;foam;smoothed particle hydrodynamic","Keyword_Vector":[0.1883577649,-0.1910472386,-0.1785531385,0.1504766903,0.0112334729,-0.208529997,0.0107415407,0.1076968004,-0.0150503419,0.1020586047,-0.1076074271,0.1405106062,0.0107547932,-0.10744557,0.0511666633,0.001578976,0.0889555167,-0.0127138602,-0.0488673124,0.0382705441,0.0006120063,0.0399426421,0.076549676,-0.0570265341,0.0562162137,-0.0655610054,0.0558370441,-0.1035527207,-0.0036050028,0.0076744313,-0.030330036,0.0773791528,-0.0163873882,-0.0675000748,0.0274005784,0.0281399892,0.0205449457,0.0506711961,0.0280282197,-0.0399987829,-0.0654272357,-0.0159957919,0.0174925873,-0.0249512839,-0.0443370796,0.0454091707,-0.0829950518,-0.0231469892,0.0795665234,0.0051669066,0.0014328852,0.0027525605],"Abstract_Vector":[0.2883670593,-0.1494220883,-0.0184248216,-0.0233375109,-0.0375158152,-0.0982387482,-0.0602145429,-0.038748858,-0.0292620377,-0.0562462136,-0.0015356696,-0.0367498278,0.0028551169,-0.0463620628,0.0140815822,0.0267465876,0.0853245755,-0.0234275567,0.0028692659,-0.0017760722,-0.0617802642,-0.0590805077,-0.0171955121,0.0308997757,-0.0016456726,-0.057497307,0.0205029979,-0.027730389,0.0404699708,0.0256063708,0.0017694007,0.0424739312,0.0981539461,-0.0047590301,-0.0037064595,-0.0455629236,-0.0064975768,-0.0283262454,0.0761094522,-0.0351433392,0.0357634443,-0.0328749131,-0.061375208,0.0470318288,0.0706215889,0.0796244337,0.0222372752,-0.0595793421,0.0584759725,0.0461100428,0.0375454959,-0.0227700471,0.0701722604,0.0473408958,0.0582896529,-0.1234790027,0.0270258958,0.0954633727,-0.0341633553,0.0301801078,-0.0120486455,-0.0199741483,0.1144798953,-0.0159377768,-0.0156674404,0.0144579759,-0.0057989316,0.0047452145,-0.0326254431,0.0342626872,0.0429544862,-0.016011321,-0.031423526,0.0147339047,0.0046531431,0.0714369031,-0.0061383812,0.0128522826,0.0237908079,-0.0489620197,-0.0490879081,-0.008377835,0.0216502703,0.0899869311,0.0008906598,-0.1188871924,-0.0032780899,-0.0392967679,-0.0317789262,-0.0208431012,0.082820605,0.0274898533,-0.0293441327,0.0331502109,-0.0288146974,-0.012450493,-0.0411184098,0.0720793361,-0.0139948508,-0.0165260895,0.0392011407,0.0339620558,-0.0149044835,-0.0188131517,0.0029381509,-0.0504487931,-0.0328280553,-0.0087848906,-0.000675278,-0.0220443573,-0.0647535496,-0.0146769694,0.005192827,0.0106259655,0.0565022033,0.0336668363]},"6":{"Abstract":"Sensemaking is described as the process of comprehension, finding meaning and gaining insight from information, producing new knowledge and informing further action. Understanding the sensemaking process allows building effective visual analytics tools to make sense of large and complex datasets. Currently, it is often a manual and time-consuming undertaking to comprehend this: researchers collect observation data, transcribe screen capture videos and think-aloud recordings, identify recurring patterns, and eventually abstract the sensemaking process into a general model. In this paper, we propose a general approach to facilitate such a qualitative analysis process, and introduce a prototype, SensePath, to demonstrate the application of this approach with a focus on browser-based online sensemaking. The approach is based on a study of a number of qualitative research sessions including observations of users performing sensemaking tasks and post hoc analyses to uncover their sensemaking processes. Based on the study results and a follow-up participatory design session with HCI researchers, we decided to focus on the transcription and coding stages of thematic analysis. SensePath automatically captures user's sensemaking actions, i.e., analytic provenance, and provides multi-linked views to support their further analysis. A number of other requirements elicited from the design session are also implemented in SensePath, such as easy integration with existing qualitative analysis workflow and non-intrusive for participants. The tool was used by an experienced HCI researcher to analyze two sensemaking sessions. The researcher found the tool intuitive and considerably reduced analysis time, allowing better understanding of the sensemaking process.","Authors":"P. H. Nguyen; K. Xu; A. Wheat; B. L. W. Wong; S. Attfield; B. Fields","DOI":"10.1109\/TVCG.2015.2467611","Keywords":"Sensemaking;analytic provenance;transcription;coding;qualitative research;timeline visualization;Sensemaking;analytic provenance;transcription;coding;qualitative research;timeline visualization","Title":"SensePath: Understanding the Sensemaking Process Through Analytic Provenance","Keywords_Processed":"analytic provenance;sensemake;qualitative research;code;transcription;timeline visualization","Keyword_Vector":[0.2090841555,-0.020938812,-0.0030846728,-0.1048008746,0.0714077353,0.0085870076,-0.0428288643,0.077958192,0.1766789664,0.0254169546,0.1354471989,0.1606663783,-0.0928715727,0.1005865808,-0.0395780825,-0.0648762396,-0.0468775103,0.0521872939,0.0781706839,-0.0577088814,0.0318666297,0.0001456238,-0.1203135835,0.1025933357,0.0367488202,-0.07455965,-0.0553904957,0.0084903525,0.0000570667,-0.0060068906,0.069251297,-0.0441142936,-0.0566580082,0.049936444,-0.001023698,0.0738560625,-0.0077115877,-0.0095145263,-0.0078596666,0.0592332553,-0.0211803089,0.0976070861,0.0832453331,-0.0743038658,-0.0577976964,-0.0159431326,0.0334164722,0.0015069821,0.0284212214,0.0702391802,0.0167176599,-0.1268592193],"Abstract_Vector":[0.2066766104,0.0082136731,-0.067153522,-0.0657537446,-0.0337982084,0.0943497054,0.0164468748,0.0253094567,0.0721610391,-0.0267760478,-0.0289218875,-0.0241880601,-0.0522729191,-0.014713345,-0.0609772008,-0.0159115647,0.0301856888,0.071960066,-0.0044766361,0.0147080906,-0.0552664258,0.0301261267,-0.0515728692,-0.0570659629,0.0231964021,-0.0483520064,-0.0266066557,-0.0095593769,-0.0084775858,0.015130931,-0.1008057482,0.019642112,-0.0033655563,-0.0350212195,-0.0557456099,0.0226009663,0.0325621059,-0.0491052952,-0.0094077098,0.0080024198,-0.0083385826,0.0063699182,0.0122172523,0.0079901445,-0.0141119282,-0.0496053316,0.0006823773,0.0247858514,0.0171454205,0.0580612967,0.0274745361,0.0007644139,-0.0259091213,-0.0146315238,-0.0855376271,0.0392818491,0.0001332023,0.0348992016,0.0110540564,-0.0530007267,0.0153337445,-0.0489088079,0.0097167882,-0.0356076595,0.0367743747,0.0246090435,-0.0212740153,0.0198531854,-0.0627769855,-0.0289744488,0.0409541422,0.0263173667,-0.0265406133,0.0672021937,0.0116097843,0.003034071,-0.0358901116,0.0092239756,0.0070885579,-0.029824813,0.0395534246,-0.0190141408,0.0107772625,-0.0074208159,0.0142464559,0.0095518672,-0.0015266363,0.0078891665,-0.0042892145,-0.0119179605,0.0297992423,0.0159663361,0.0117391359,-0.0137579954,-0.0241148083,0.0070036431,0.0203753898,0.0068812445,0.0029604187,0.0636625977,-0.0122253053,0.0086787783,0.0146908417,-0.0141735215,0.005782937,-0.0056796003,-0.0065295436,-0.0213385311,-0.0443486731,-0.0182630363,-0.017543467,0.0506131789,-0.02445735,0.0049885514,0.0109537963,-0.0361345231]},"60":{"Abstract":"In order to effectively infer correct labels from noisy crowdsourced annotations, learning-from-crowds models have introduced expert validation. However, little research has been done on facilitating the validation procedure. In this paper, we propose an interactive method to assist experts in verifying uncertain instance labels and unreliable workers. Given the instance labels and worker reliability inferred from a learning-from-crowds model, candidate instances and workers are selected for expert validation. The influence of verified results is propagated to relevant instances and workers through the learning-from-crowds model. To facilitate the validation of annotations, we have developed a confusion visualization to indicate the confusing classes for further exploration, a constrained projection method to show the uncertain labels in context, and a scatter-plot-based visualization to illustrate worker reliability. The three visualizations are tightly integrated with the learning-from-crowds model to provide an iterative and progressive environment for data validation. Two case studies were conducted that demonstrate our approach offers an efficient method for validating and improving crowdsourced annotations.","Authors":"S. Liu; C. Chen; Y. Lu; F. Ouyang; B. Wang","DOI":"10.1109\/TVCG.2018.2864843","Keywords":"Crowdsourcing;learning-from-crowds;interactive visualization;focus + context","Title":"An Interactive Method to Improve Crowdsourced Annotations","Keywords_Processed":"learn from crowd;focus context;crowdsource;interactive visualization","Keyword_Vector":[0.1589714732,0.056455598,0.3383809279,0.0804121578,-0.1618065176,-0.1288646866,0.0910679121,0.1243665303,-0.2985104372,0.2081012816,0.2924811871,-0.0413824137,-0.14329232,-0.0117889519,0.0225269727,-0.0672318512,0.0411086995,-0.041170447,-0.0507041444,-0.0078561654,0.0302607516,0.0135271455,0.0084557218,-0.1030851696,0.0297336329,-0.0325869804,-0.0758838782,0.0427672712,-0.0413244607,0.0445679819,0.0530091724,-0.0122540319,-0.0070199133,0.0048693105,-0.0106595606,0.0131015339,0.0036180291,0.0696744918,-0.0351471537,0.0286558718,-0.0878175585,0.0523576972,0.016527578,-0.0512097754,-0.0078383107,0.0646677047,0.0581207719,-0.0028331795,0.0962650924,0.0516285478,-0.0272684919,-0.005629404],"Abstract_Vector":[0.19390173,0.1585771956,0.2441669011,0.0260968812,-0.0793461466,-0.0022559527,-0.1655217036,0.0600681261,-0.0350981591,0.1439250603,0.084396291,-0.0291866062,0.0822868821,0.0057411468,0.0086943774,0.0094986558,0.0399626789,0.008571981,-0.0214966643,0.0527716173,-0.0160466169,-0.0191611134,-0.0031075682,-0.0562320176,-0.0020488487,-0.0258481116,-0.0157864387,0.0687786581,0.0033607651,-0.0581632153,0.0433844864,0.0086209847,-0.0192886139,-0.02238318,-0.004327869,-0.0159578646,-0.0342556019,-0.0326665823,0.0364774941,-0.0311702223,-0.0282640562,0.0247494254,-0.0091345433,-0.0119660393,-0.0168225406,0.0047342035,0.0656713571,-0.0248186463,-0.0028553634,-0.0009130044,0.0396437811,0.042838066,-0.0254027118,-0.0124489225,-0.0715699483,0.0270659216,0.0090597491,-0.0564693758,0.0151824242,-0.047486365,-0.0362391311,-0.0104757214,-0.0118338829,0.0029237574,0.0736996292,-0.0021698485,-0.0321085931,0.0441445033,-0.0280412177,-0.0497454182,0.022669561,-0.0735091557,-0.0031438637,0.0067688048,-0.0421193463,-0.0022627688,-0.0077699519,-0.0123759046,0.0035536848,0.0171282761,-0.0055233475,0.000906046,0.0094227422,-0.0000322425,-0.0343094364,-0.0096226545,0.0132095059,0.0766131556,0.0511999616,-0.0372034329,0.0107603359,-0.0341581218,-0.0142918827,0.0762687107,0.0297158804,-0.0234011139,-0.001930295,-0.0032284155,-0.010106659,-0.0242379177,-0.05267302,-0.0504860316,-0.0258982065,0.0198560431,0.0237470291,-0.0156573981,0.0078196918,0.0184374123,0.0143119931,-0.0339698654,-0.0177641257,0.0197619561,0.0224417671,-0.0053210264,0.039229214,-0.0220391861]},"61":{"Abstract":"Graphics convey numerical information very efficiently, but rely on a different set of mental processes than tabular displays. Here, we present a study relating demographic characteristics and visual skills to perception of graphical lineups. We conclude that lineups are essentially a classification test in a visual domain, and that performance on the lineup protocol is associated with general aptitude, rather than specific tasks such as card rotation and spatial manipulation. We also examine the possibility that specific graphical tasks may be associated with certain visual skills and conclude that more research is necessary to understand which visual skills are required in order to understand certain plot types.","Authors":"S. VanderPlas; H. Hofmann","DOI":"10.1109\/TVCG.2015.2469125","Keywords":"Data visualization,;Perception;Statistical graphics;Statistical computing;Data visualization;Perception;Statistical graphics;Statistical computing","Title":"Spatial Reasoning and Data Displays","Keywords_Processed":"statistical computing;perception;statistical graphic;datum visualization","Keyword_Vector":[0.2929642422,-0.118647178,0.0322300023,0.0734702446,0.0360189784,0.2278939963,-0.0870061369,0.031142395,-0.0181162226,-0.0481318548,-0.0093869529,-0.0270529263,0.0253066612,-0.0815993302,-0.0197550003,0.0121338259,-0.0325243314,0.0265959524,-0.0494999999,-0.0016894784,-0.0970103436,-0.0129985952,0.0251693329,-0.0095980825,0.0229141561,-0.0223688993,-0.0189794762,0.0448193756,0.0204688997,-0.041611765,0.0242085992,0.0922226109,-0.0911645939,-0.1048246402,0.1021003965,-0.0438728322,-0.0497159872,0.0127153774,-0.0715227655,-0.0508324834,-0.0335142246,0.215871184,0.0081473209,-0.0398016283,-0.0316273138,0.0387646537,0.0263715443,-0.0144018423,-0.0158204158,-0.0418986642,-0.0222557773,-0.016617834],"Abstract_Vector":[0.2955583601,-0.0550220403,0.0329585422,-0.0476650211,0.0162993291,-0.0882054778,0.0149919564,-0.0173437997,0.0385039925,0.0242355039,0.0003124721,-0.0284546676,-0.0319686504,-0.002372629,0.0085180027,-0.0172536664,0.062358722,0.106762987,0.0024835268,-0.0822728657,0.0478994759,-0.1175355475,0.0036067149,-0.0042892703,0.0906120225,0.0341402396,0.0005403791,0.0857187937,0.0218727037,-0.0658662637,0.0560650384,0.0395347093,-0.0430435502,0.0153576816,0.009714887,-0.0342600692,-0.028396648,0.0713670603,-0.017756781,-0.0687813763,0.0269552861,-0.0499165973,0.0319312354,-0.0302306101,0.0110134454,-0.0650420585,-0.108031377,-0.0090665068,-0.0124979779,0.0475783383,0.0302395707,0.0446364115,-0.0490392487,0.0057910603,-0.0422319807,-0.0568613914,0.0574963124,-0.0746486897,-0.0269081641,-0.0066265005,-0.0083667714,-0.0084636849,-0.0120299356,-0.0067266438,0.0552395816,-0.0712620313,0.0287110877,-0.0369659111,0.1073503242,0.0102443772,0.0023444124,0.025009575,-0.0506569797,0.0720634942,-0.0337801067,0.019804213,-0.0542200359,0.0261160348,-0.0529664222,0.0194768507,-0.0279536697,0.0573843908,0.0123261682,-0.0006414737,0.0485115715,0.0584916776,-0.0432422098,-0.0029353592,0.0181097279,-0.0074520329,-0.0080319094,0.0166604468,0.0402269262,-0.0024466735,-0.0031172932,0.0073129096,-0.0010827757,-0.011408233,0.001053755,0.0422483747,-0.0030730521,-0.0457054488,-0.0103183774,-0.0237181605,0.0047473449,0.0311910575,-0.0119801613,-0.0257899902,-0.0384120097,0.0265649872,-0.0087107525,-0.0277001492,-0.0134649505,-0.0086748295,-0.044048529,0.0114364903]},"62":{"Abstract":"Many visualizations, including word clouds, cartographic labels, and word trees, encode data within the sizes of fonts. While font size can be an intuitive dimension for the viewer, using it as an encoding can introduce factors that may bias the perception of the underlying values. Viewers might conflate the size of a word's font with a word's length, the number of letters it contains, or with the larger or smaller heights of particular characters (`o' versus `p' versus `b'). We present a collection of empirical studies showing that such factors-which are irrelevant to the encoded values-can indeed influence comparative judgements of font size, though less than conventional wisdom might suggest. We highlight the largest potential biases, and describe a strategy to mitigate them.","Authors":"E. Alexander; C. Chang; M. Shimabukuro; S. Franconeri; C. Collins; M. Gleicher","DOI":"10.1109\/TVCG.2017.2723397","Keywords":"Text and document data;cognitive and perceptual skill;quantitative evaluation","Title":"Perceptual Biases in Font Size as a Data Encoding","Keywords_Processed":"cognitive and perceptual skill;text and document datum;quantitative evaluation","Keyword_Vector":[0.1206783385,-0.101012562,-0.0487237357,-0.1676361777,0.0590157404,-0.0544437167,-0.1242156455,-0.0433127709,-0.0661422094,0.1094848918,0.0330735461,-0.1468713358,0.0376258199,0.0153933616,0.1181054404,-0.0211161197,0.0952056627,-0.0255675604,0.1607107985,-0.0575516258,-0.0778795811,0.0785796898,0.1474953466,0.1004264138,-0.1113543332,-0.0721202847,-0.0264652548,-0.0512968904,0.0943926034,-0.0687610568,-0.0269951066,-0.1043733557,0.0763960719,0.0431109577,-0.0729889927,0.007551092,-0.0288513485,0.0286014421,0.007233722,-0.0442372359,0.0146154655,0.0028244007,0.0117489366,0.013801337,-0.0083025744,0.0348529233,0.0314237069,0.0397990934,-0.011465446,0.0108052537,-0.079555001,-0.0048493538],"Abstract_Vector":[0.1932593354,-0.0519431519,-0.0234325373,-0.0068737955,-0.0496641012,0.0951973553,-0.0239401709,0.0150035531,0.1218287553,-0.0158910675,-0.0758683154,0.0421317471,-0.080824984,0.0070275023,-0.002910807,-0.0536225942,0.0369128974,0.051954848,0.0156641107,-0.0420741428,-0.0040302144,-0.0094554156,0.0467060309,-0.0187938837,-0.007578575,-0.0671968941,0.0156204771,0.0090367612,-0.0298893634,-0.0532249767,0.0153139003,-0.0384058914,0.0484914939,0.0058186172,-0.0388627247,-0.0295280106,-0.0060306183,-0.0416998119,0.0113195516,-0.0625005317,0.0374367107,0.0127672688,0.0640882043,-0.0065291618,-0.052269755,-0.0017558983,-0.0032024048,0.0082743488,0.0185827777,-0.0012541937,-0.0213047395,-0.0420555887,-0.0034759569,0.0081746455,-0.0267287615,0.0180001072,0.0029296927,0.0368537754,0.0559753244,-0.0080049532,0.0037124661,-0.0209870457,0.0663110809,-0.0225495788,-0.0060294927,-0.0120803711,0.047633599,-0.0043730408,-0.046312506,-0.0171261615,0.0494690167,0.1069246338,-0.0067421988,0.0134778615,-0.0154177045,-0.0154491834,0.0455642385,0.0533751283,-0.019538231,-0.0203348285,-0.0203572952,0.0443816763,-0.0142785223,-0.0332367159,0.0382553203,-0.0118509119,0.0118264056,0.0875853723,-0.0458790487,0.0143297559,0.0048857655,-0.0061550238,-0.0451652965,0.0546949571,-0.029062139,0.0200955163,-0.028777551,-0.0055563181,-0.013112275,0.0876929102,-0.0024688289,-0.0334268548,0.0268237497,-0.010454274,-0.0284628061,-0.0330272297,0.0217741926,0.0083489689,0.0403551907,0.0620831117,-0.0275462118,0.0713777838,-0.0421139947,-0.0208325525,0.1029821175,0.0259348939]},"63":{"Abstract":"Building Information Modeling (BIM) provides an integrated 3D environment to manage large-scale engineering projects. The Architecture, Engineering and Construction (AEC) industry explores 4D visualizations over these datasets for virtual construction planning. However, existing solutions lack adequate visual mechanisms to inspect the underlying schedule and make inconsistencies readily apparent. The goal of this paper is to apply best practices of information visualization to improve 4D analysis of construction plans. We first present a review of previous work that identifies common use cases and limitations. We then consulted with AEC professionals to specify the main design requirements for such applications. These guided the development of CasCADe, a novel 4D visualization system where task sequencing and spatio-temporal simultaneity are immediately apparent. This unique framework enables the combination of diverse analytical features to create an information-rich analysis environment. We also describe how engineering collaborators used CasCADe to review the real-world construction plans of an Oil & Gas process plant. The system made evident schedule uncertainties, identified work-space conflicts and helped analyze other constructability issues. The results and contributions of this paper suggest new avenues for future research in information visualization for the AEC industry.","Authors":"P. Ivson; D. Nascimento; W. Celes; S. D. Barbosa","DOI":"10.1109\/TVCG.2017.2745105","Keywords":"Visualization in physical sciences and engineering;design studies;integrating spatial and non-spatial data visualization;task and requirements analysis","Title":"CasCADe: A Novel 4D Visualization System for Virtual Construction Planning","Keywords_Processed":"visualization in physical science and engineering;integrate spatial and non spatial datum visualization;design study;task and requirement analysis","Keyword_Vector":[0.0404909808,0.0175757078,0.094833779,0.035299529,-0.0578563555,-0.0343576385,-0.0020637836,-0.0546662103,0.0178680993,-0.0643558408,-0.0454570647,-0.0070247193,-0.0777847701,-0.0164890014,-0.0618136872,0.0249838783,0.1670929635,-0.07909434,0.0472839731,-0.0471080647,0.0705829992,0.0020341126,0.0359977304,0.0131178898,-0.0570770572,-0.0944307634,-0.0498082247,0.015657296,-0.071355007,0.0018432421,0.0427979757,0.0462935256,0.0372336164,0.0357624276,-0.0813904263,-0.0264691943,0.027662135,0.0051661336,-0.0034522184,0.0447525026,-0.0263240828,0.0461747608,0.0270559252,-0.0543967886,0.0653044772,0.0418497753,0.0038355193,-0.024172127,0.077831576,0.0083119543,0.0438417177,0.0191197586],"Abstract_Vector":[0.1617988104,0.084505588,0.2202457387,-0.0758247493,0.0902312088,0.0157495987,0.1836977699,-0.074564621,0.0272143922,-0.0336343759,-0.0592246469,-0.0120703667,-0.0456583145,-0.0458348852,-0.0289302978,0.0612674829,-0.0678272011,0.0526721282,0.0094166305,-0.0372430007,-0.0007881805,-0.0490370946,0.0381585734,-0.0408947699,-0.0230166017,0.052012106,-0.030211757,-0.0074328205,0.0338589221,0.0092362243,0.0728390675,-0.022558754,-0.0707747783,0.0220048559,0.0757534682,0.0812123332,0.1139002688,-0.0768642074,0.0140155545,-0.0718514495,-0.0386218874,0.0075437277,0.0594711586,0.0482088192,-0.0379948228,0.0745988767,0.1356009692,-0.0011859397,0.0541554246,0.0281127365,-0.0391935324,-0.0665670061,0.159224555,0.017764564,0.0129803547,0.0981874106,-0.0186780547,-0.0322475678,0.0048174703,0.0988430465,-0.0779205281,0.0302379303,-0.027277747,0.0360879842,-0.0501166262,0.0515075318,0.0848859569,-0.0700497999,0.0218086782,0.0428773586,0.1052792738,-0.0279122135,0.07339883,0.0103019662,0.0293916741,-0.0296447678,-0.0698526193,0.0897582802,0.0107057392,-0.0005115309,0.0163446062,-0.0200858217,0.0052152987,-0.0161420812,0.0241046574,-0.0386545909,-0.0075245595,0.0277389777,0.0709052642,-0.0155741718,0.0591006151,-0.0053943386,0.0051228384,0.0398899828,0.0222253641,-0.0249374522,0.0001236298,0.0157888625,0.0004504368,0.0374085577,0.0101031334,0.0392027624,-0.023020512,-0.0172563593,0.0076644879,0.041225262,-0.0089663298,-0.0101421638,0.0139066944,-0.0086647802,-0.0001020672,-0.0475513626,-0.0141165533,0.0176421209,0.0152451765,-0.0095533389]},"64":{"Abstract":"We present a novel visual representation and interface named the matrix of isosurface similarity maps (MISM) for effective exploration of large time-varying multivariate volumetric data sets. MISM synthesizes three types of similarity maps (i.e., self, temporal, and variable similarity maps) to capture the essential relationships among isosurfaces of different variables and time steps. Additionally, it serves as the main visual mapping and navigation tool for examining the vast number of isosurfaces and exploring the underlying time-varying multivariate data set. We present temporal clustering, variable grouping, and interactive filtering to reduce the huge exploration space of MISM. In conjunction with the isovalue and isosurface views, MISM allows users to identify important isosurfaces or isosurface pairs and compare them over space, time, and value range. More importantly, we introduce path recommendation that suggests, animates, and compares traversal paths for effectively exploring MISM under varied criteria and at different levels-of-detail. A silhouette-based method is applied to render multiple surfaces of interest in a visually succinct manner. We demonstrate the effectiveness of our approach with case studies of several time-varying multivariate data sets and an ensemble data set, and evaluate our work with two domain experts.","Authors":"J. Tao; M. Imre; C. Wang; N. V. Chawla; H. Guo; G. Sever; S. H. Kim","DOI":"10.1109\/TVCG.2018.2864808","Keywords":"Time-varying multivariate data visualization;isosurface;similarity map;visual interface;path recommendation","Title":"Exploring Time-Varying Multivariate Volume Data Using Matrix of Isosurface Similarity Maps","Keywords_Processed":"similarity map;isosurface;path recommendation;time vary multivariate datum visualization;visual interface","Keyword_Vector":[0.1486626402,-0.0655138812,-0.0025473464,-0.0369978736,-0.0035515462,-0.0180763335,-0.0588717936,-0.0122151577,-0.0462492022,0.0300568721,-0.0461709547,0.1277073333,0.0457151914,-0.0960557424,0.0257308246,-0.0346550073,0.0956936783,-0.0035502467,0.0317461658,0.0592831431,-0.0591118924,-0.0042367096,-0.0851172676,0.0069775088,0.0808647273,0.01420276,0.0347124349,-0.0202917031,-0.0604644413,0.0710634659,0.0841647438,-0.0047942118,0.0464473422,-0.1042036074,-0.0275756439,0.0363717795,0.0577373996,-0.0329447267,0.0542818927,0.0184737189,-0.0489390345,0.0728154117,-0.0976594304,0.1294628464,-0.0273075227,0.0382635592,0.0026955969,0.0739721892,-0.1223824028,0.001320283,-0.0516261934,0.0395463248],"Abstract_Vector":[0.2319354842,-0.0408681699,0.0311945613,-0.050871505,0.011358025,0.0144557342,-0.0150010801,0.0292149497,0.0912718928,0.0504195653,-0.0075562383,-0.026545432,-0.0481156946,0.0458774133,-0.033222426,-0.0058289101,-0.0033780153,0.0026934345,0.0709690417,-0.0205706044,0.0023406486,-0.0353237707,0.0418733444,0.0561833634,-0.0523574426,0.0424049193,-0.0858954633,0.0242931091,-0.0013225038,0.0320908797,0.0559580527,-0.0417243023,-0.0154342331,0.0834930715,0.044132035,0.0623918106,0.0190399737,-0.0073198613,-0.0274204074,-0.0435642274,0.0463743018,-0.0048288119,-0.0368872246,0.0284392515,0.0432796876,0.0207693326,0.0086006183,0.01649951,0.0482290702,-0.0087108259,-0.0092956031,0.0484045323,0.017591769,-0.0561255494,0.0255875982,-0.0092478964,-0.0183854597,0.0312852065,0.0373707973,0.0198743231,-0.0916264697,-0.0095662828,0.0002570948,0.0063010673,0.0320682149,0.0021742579,0.0067990242,-0.0052748944,-0.037113268,-0.0395612934,-0.0179937353,-0.0977796488,-0.0142833105,0.0002293541,-0.0074829512,0.0051925409,0.0106997524,-0.0051741684,0.0386012963,0.0049762362,0.0692054182,0.014310816,-0.0041001093,0.0103281848,-0.0087774779,-0.0209330647,-0.0172167539,-0.0364513407,-0.0144606498,0.011874122,0.0279287679,0.0060560068,0.0262462067,0.022612177,-0.0143406467,0.051332217,-0.0522434086,-0.0176682581,-0.0876771468,0.0263489776,-0.0287831253,0.0073192146,0.0316464897,-0.0112357978,0.0420268372,-0.0074240785,0.044742023,0.0362898316,0.0353663639,0.0237967895,-0.0153815226,0.0175773993,0.0074280906,0.0316006258,0.0212693965,-0.0597288031]},"65":{"Abstract":"In this design study, we present a visualization technique that segments patients' histories instead of treating them as raw event sequences, aggregates the segments using criteria such as the whole history or treatment combinations, and then visualizes the aggregated segments as static dashboards that are arranged in a dashboard network to show longitudinal changes. The static dashboards were developed in nine iterations, to show 15 important attributes from the patients' histories. The final design was evaluated with five non-experts, five visualization experts and four medical experts, who successfully used it to gain an overview of a 2,000 patient dataset, and to make observations about longitudinal changes and differences between two cohorts. The research represents a step-change in the detail of large-scale data that may be successfully visualized using dashboards, and provides guidance about how the approach may be generalized.","Authors":"J. Bernard; D. Sessler; J. Kohlhammer; R. A. Ruddle","DOI":"10.1109\/TVCG.2018.2803829","Keywords":"Information visualization;visual analytics;multivariate data visualization;electronic health care records;medical data analysis;prostate cancer disease;design study;user study;evaluation;static dashboard;dashboard network","Title":"Using Dashboard Networks to Visualize Multiple Patient Histories: A Design Study on Post-Operative Prostate Cancer","Keywords_Processed":"user study;prostate cancer disease;electronic health care record;multivariate datum visualization;design study;dashboard network;evaluation;medical datum analysis;static dashboard;visual analytic;information visualization","Keyword_Vector":[0.0839396213,0.0705337218,-0.0525789109,-0.0274983055,0.010952643,-0.0190566935,-0.023879848,-0.0187324237,-0.0577324492,0.0139844332,0.0160164583,0.0468395959,0.0094231842,0.017586607,0.0629267315,0.0340519993,-0.01328071,-0.0600749777,0.1054392902,-0.0937775434,0.0333222013,0.0369656249,0.0260453799,0.0608362854,-0.0252238735,-0.0667541037,0.1406558002,-0.0033138974,0.0104646815,-0.0299787555,-0.0709967466,0.0091478118,0.0124505251,-0.0367207561,0.0229747608,0.0127438834,-0.0500450841,0.0172911168,-0.023912402,-0.0136235235,-0.0451282221,-0.0113053952,0.0401185158,0.0032358897,-0.0024724737,0.0170348743,-0.0113823316,0.013819491,0.0034892968,0.0200795012,-0.0193076243,0.0020016417],"Abstract_Vector":[0.1677053389,0.0674498544,-0.0894419405,-0.023050112,0.0014425407,0.0652811353,0.0420518363,0.0217011517,0.020208857,0.009879275,-0.0064342134,-0.0021526941,-0.0385631314,-0.0577550727,-0.0172256765,0.0306529436,0.0143995712,-0.0038387008,0.0005787865,0.0069166011,-0.0155846949,-0.0077655324,-0.0211569428,0.0123311061,0.0166171402,-0.0310340426,-0.000308635,-0.0182337066,-0.002475103,0.0159025695,-0.003036157,-0.0038824562,-0.0260886962,-0.019446413,0.017863768,0.0046583182,0.0031844686,-0.0289557904,-0.0044269898,-0.0080764165,0.0282906597,0.0322208611,0.0361054567,0.005975897,-0.0365582396,-0.0103715478,0.0174398927,-0.0223829765,0.0145999226,-0.0212880629,0.0122231965,0.0186612013,0.0175547871,0.005817952,0.0209445947,-0.0496400573,0.0520387394,-0.0164520567,-0.0307064615,-0.0002415025,-0.0055517398,0.0274375648,-0.0110580173,-0.0020761103,-0.0105090593,-0.0017333161,-0.0172207412,0.0273241002,-0.0360512217,-0.0530306281,-0.0361000328,0.0335629761,0.0185776712,0.0530520767,-0.0120680902,0.0093135458,-0.0286646296,0.0194048453,-0.0020661751,-0.0020287758,-0.0122764868,-0.0016532107,-0.0131793118,0.0171857552,0.0227006862,0.0205118228,-0.0377346727,-0.0206503686,-0.0250403835,-0.0042891955,0.0219735816,-0.037218843,-0.0546045788,-0.0309220391,0.0196136896,-0.0137326961,-0.0038943992,-0.0041723208,-0.0136946599,0.0133470542,-0.0003676618,-0.0022661107,-0.0233977025,-0.0256027916,-0.0271177665,-0.0355175762,0.0280588606,-0.0009731043,-0.0242201075,-0.0152806942,-0.0232987575,0.0463808565,-0.0116840157,-0.0044219949,0.0068253537,0.0159044248]},"66":{"Abstract":"Labeling data instances is an important task in machine learning and visual analytics. Both fields provide a broad set of labeling strategies, whereby machine learning (and in particular active learning) follows a rather model-centered approach and visual analytics employs rather user-centered approaches (visual-interactive labeling). Both approaches have individual strengths and weaknesses. In this work, we conduct an experiment with three parts to assess and compare the performance of these different labeling strategies. In our study, we (1) identify different visual labeling strategies for user-centered labeling, (2) investigate strengths and weaknesses of labeling strategies for different labeling tasks and task complexities, and (3) shed light on the effect of using different visual encodings to guide the visual-interactive labeling process. We further compare labeling of single versus multiple instances at a time, and quantify the impact on efficiency. We systematically compare the performance of visual interactive labeling with that of active learning. Our main findings are that visual-interactive labeling can outperform active learning, given the condition that dimension reduction separates well the class distributions. Moreover, using dimension reduction in combination with additional visual encodings that expose the internal state of the learning model turns out to improve the performance of visual-interactive labeling.","Authors":"J. Bernard; M. Hutter; M. Zeppelzauer; D. Fellner; M. Sedlmair","DOI":"10.1109\/TVCG.2017.2744818","Keywords":"Labeling;Visual-Interactive Labeling;Information Visualization;Visual Analytics;Active Learning;Machine Learning;Classification;Evaluation;Experiment;Dimensionality Reduction","Title":"Comparing Visual-Interactive Labeling with Active Learning: An Experimental Study","Keywords_Processed":"labeling;visual analytic;classification;experiment;Machine Learning;evaluation;dimensionality Reduction;visual interactive labeling;Active Learning;information visualization","Keyword_Vector":[0.0876413111,-0.0388011129,-0.0005403672,-0.0615228975,0.0206398837,-0.0295354943,-0.0369987081,-0.0109971308,-0.0301368409,-0.0084600601,-0.0232334307,0.0392555951,-0.0236693142,0.0069009494,-0.0332705481,0.028256237,-0.103112608,0.0065214527,-0.0156508637,-0.020331218,0.0115346445,-0.053690853,0.0397310907,-0.0411173925,-0.0381681387,0.0086428534,-0.0125914876,0.0665843324,-0.0177046057,-0.0051781855,0.0535435957,0.0705354654,0.0558994705,0.091872844,-0.0499403434,0.0825470871,-0.1060607505,0.0208819263,0.038681096,-0.062665043,-0.016066285,0.0425886655,-0.0484140987,0.0056237984,0.000705908,-0.0361632387,0.0110610237,0.0184429256,-0.0187681411,-0.0126868205,0.0260311978,0.0281854865],"Abstract_Vector":[0.1719587413,-0.082346928,-0.0108960186,0.0781719202,0.0704625975,0.0167471977,-0.0495307025,-0.0250571141,0.11012938,-0.022542064,-0.0283348787,0.1116875163,-0.0177395873,-0.0106249831,0.094992738,0.0143687993,-0.0545767082,0.0060219519,-0.0758341862,-0.0254385273,-0.018248008,-0.0167143732,-0.0038243017,0.0514280938,0.0017229347,-0.0271373219,0.0109343819,-0.0400584085,0.0112889694,-0.0811472545,0.0725655783,-0.0395931834,0.0059587754,0.0268299769,-0.0002970589,-0.0530179403,-0.0009770998,0.0853363473,-0.0236912654,-0.0457888561,-0.0115698982,0.0375576908,-0.0441585519,0.0092319938,0.0083424839,0.0261774103,0.0274946448,-0.0024122559,-0.0050016922,0.0885610108,-0.0619374307,0.0443639329,-0.0486161406,0.0134297283,-0.0107892276,-0.0252464206,0.0239305829,0.0104408113,-0.0520192563,0.0077017548,-0.0454933754,-0.0086827739,-0.0022500122,0.0266491896,-0.063056314,0.0065450819,-0.0455093921,0.0069171464,0.013030882,-0.0285055928,0.0015354779,0.0012504472,-0.0033922787,-0.0021369306,-0.0142304535,0.0044437992,0.0093141649,0.0015771196,0.0077116549,-0.0260048465,0.0146411032,-0.0444445598,-0.0394815385,0.0052159482,-0.0043308575,-0.0022193324,0.0701959402,0.0098808893,-0.0055553537,0.0018611327,-0.0140259744,-0.0539320435,-0.0094793403,-0.0150299227,0.0234848755,-0.0106077035,0.0199517491,-0.0031140549,0.0350913532,-0.0009815677,0.0081753767,0.0000821305,0.0075799805,-0.0188767518,0.0027488601,-0.0294247612,0.0079174728,0.0043662278,-0.0044647359,0.0367022268,-0.0005704331,0.0096709942,-0.0199266446,0.0006375915,-0.0011542319,-0.0464260816]},"67":{"Abstract":"Data ensembles are often used to infer statistics to be used for a summary display of an uncertain prediction. In a spatial context, these summary displays have the drawback that when uncertainty is encoded via a spatial spread, display glyph area increases in size with prediction uncertainty. This increase can be easily confounded with an increase in the size, strength or other attribute of the phenomenon being presented. We argue that by directly displaying a carefully chosen subset of a prediction ensemble, so that uncertainty is conveyed implicitly, such misinterpretations can be avoided. Since such a display does not require uncertainty annotation, an information channel remains available for encoding additional information about the prediction. We demonstrate these points in the context of hurricane prediction visualizations, showing how we avoid occlusion of selected ensemble elements while preserving the spatial statistics of the original ensemble, and how an explicit encoding of uncertainty can also be constructed from such a selection. We conclude with the results of a cognitive experiment demonstrating that the approach can be used to construct storm prediction displays that significantly reduce the confounding of uncertainty with storm size, and thus improve viewers' ability to estimate potential for storm damage.","Authors":"L. Liu; A. P. Boone; I. T. Ruginski; L. Padilla; M. Hegarty; S. H. Creem-Regehr; W. B. Thompson; C. Yuksel; D. H. House","DOI":"10.1109\/TVCG.2016.2607204","Keywords":"Implicit uncertainty presentation;ensembles;ensemble visualization;sampling;uncertainty;hurricane prediction","Title":"Uncertainty Visualization by Representative Sampling from Prediction Ensembles","Keywords_Processed":"ensemble visualization;implicit uncertainty presentation;ensemble;sample;hurricane prediction;uncertainty","Keyword_Vector":[0.1287786593,-0.1416874368,-0.1532099722,0.153422075,-0.0981902555,-0.0889571016,0.0589166702,-0.0139066304,0.0249195681,-0.0350075466,0.0636787857,-0.0430574587,0.0391826632,0.135279167,-0.0715318845,-0.0496950758,0.0316193437,0.0703944005,0.0417261098,-0.0612600478,0.0094743844,-0.0313991209,-0.0137120283,0.0227831749,0.0045720799,0.0252573856,0.0143344637,-0.0474750842,-0.0216379989,0.060105042,0.0082986487,-0.0380953286,0.0116532214,0.0605094495,0.0133865278,0.0202067286,0.0415749661,0.0058303757,-0.0038251718,-0.0304968356,0.0053902518,-0.0386236186,-0.0313201909,0.0503649051,0.0840718981,-0.00717582,0.0245687173,-0.0678890649,-0.0419575735,-0.0181946306,0.0027386505,-0.0006485765],"Abstract_Vector":[0.2658483632,-0.1183165009,-0.0053510586,-0.0020130306,0.0080800178,-0.0802537197,0.0198776684,0.0338450305,-0.006958731,0.0184394933,0.0022346093,-0.0165120119,-0.0244654271,-0.0385917314,-0.0035816199,-0.0074390729,-0.0230930934,-0.0197529282,0.0451278967,0.0295445255,0.0473838073,0.005337718,0.0396558264,0.0200929303,0.0034683449,0.0045787948,-0.0492125097,0.0126573383,-0.0429934455,-0.0469708985,0.0256727614,0.0190030373,-0.0219678321,-0.027852659,-0.0299495927,0.0139081602,-0.0182268698,-0.0261130542,0.0352548756,0.028226247,-0.0395650797,0.0397528543,-0.0264603514,-0.0336387644,0.0388522243,-0.0341271457,0.0016148367,0.0221518559,-0.0174868309,0.0190314335,0.0277588267,-0.0118661379,0.0788168938,0.0380444131,0.0408875932,-0.0009716244,0.0118951869,0.0573113299,0.0288436221,-0.0079057639,-0.0485856644,0.0163460515,-0.0459568378,0.0154450657,0.0316792838,-0.0026861884,0.0335582784,0.0166505154,-0.0633696089,-0.0215508586,-0.0477865634,0.0060280329,-0.0396376544,-0.0072082568,0.0151950446,0.0655500278,0.0495244055,0.0301355145,0.0092517603,0.025400598,0.0299183272,0.0079394453,0.0417512768,-0.0419945086,0.0784470496,0.0142648356,-0.0171045654,-0.0011709437,-0.0889852137,0.0033329704,-0.0342483251,-0.0155606563,-0.0364253908,0.0108422539,0.06654215,0.0101155195,0.0158100821,-0.0567426649,0.0293909598,-0.0065402461,0.0111849137,-0.0022748911,0.0087008506,0.0075314086,-0.0328920234,0.0663507494,0.0124157641,0.0174487353,-0.0085414938,-0.0278289144,-0.0125372848,-0.029331171,-0.0550900217,-0.0048447716,-0.0355097252,0.0182966006]},"68":{"Abstract":"Prior research into network layout has focused on fast heuristic techniques for layout of large networks, or complex multi-stage pipelines for higher quality layout of small graphs. Improvements to these pipeline techniques, especially for orthogonal-style layout, are difficult and practical results have been slight in recent years. Yet, as discussed in this paper, there remain significant issues in the quality of the layouts produced by these techniques, even for quite small networks. This is especially true when layout with additional grouping constraints is required. The first contribution of this paper is to investigate an ultra-compact, grid-like network layout aesthetic that is motivated by the grid arrangements that are used almost universally by designers in typographical layout. Since the time when these heuristic and pipeline-based graph-layout methods were conceived, generic technologies (MIP, CP and SAT) for solving combinatorial and mixed-integer optimization problems have improved massively. The second contribution of this paper is to reassess whether these techniques can be used for high-quality layout of small graphs. While they are fast enough for graphs of up to 50 nodes we found these methods do not scale up. Our third contribution is a large-neighborhood search meta-heuristic approach that is scalable to larger networks.","Authors":"V. Yoghourdjian; T. Dwyer; G. Gange; S. Kieffer; K. Klein; K. Marriott","DOI":"10.1109\/TVCG.2015.2467251","Keywords":"Network visualization;graph drawing;power graph;optimization;large-neighborhood search;Network visualization;graph drawing;power graph;optimization;large-neighborhood search","Title":"High-Quality Ultra-Compact Grid Layout of Grouped Networks","Keywords_Processed":"graph drawing;power graph;network visualization;large neighborhood search;optimization","Keyword_Vector":[0.2931720447,0.0195339607,0.3577095843,-0.0351153779,-0.1683797953,-0.0832916564,-0.0229252955,0.0267813121,0.0496120455,-0.0981628744,-0.0427649767,-0.0105904306,0.0631845525,-0.0390201436,-0.0631017257,0.0460005727,0.0440350579,0.0360896813,0.051351305,-0.0158917385,-0.0321471883,0.0206133965,-0.0219958078,-0.0368695098,-0.0785020672,0.0426417233,-0.0340043554,0.0173082408,-0.1035451109,0.0326108564,0.0771793233,-0.0104676192,0.0172500944,0.0023623273,0.0045669856,0.0361451252,-0.0450743518,-0.0326947356,-0.0256049774,0.0265096108,-0.0426775281,-0.0290292353,0.153763577,-0.0255209979,-0.0656904183,0.1276741507,-0.0778018348,-0.045168307,0.0399225496,0.0628526841,0.0198195814,-0.0948894376],"Abstract_Vector":[0.086365268,0.0172865232,0.1104720788,-0.0285249642,0.0502025949,0.0101815735,0.0667565427,-0.0316891628,-0.0148220194,-0.0102910493,-0.027081337,0.004585466,-0.0128541409,-0.0228398789,-0.0123100755,0.057098287,-0.029234883,0.0557962413,-0.0119606381,0.0130665835,0.0262319173,-0.0212109061,0.0148781293,-0.0489719636,-0.029839995,0.026621002,0.0238007416,-0.0024268307,0.0322390368,0.0215364369,0.0381142177,-0.0106068923,-0.0344285687,-0.0011023526,-0.017046759,0.0420826218,0.0370085276,-0.0630307219,0.0139202564,-0.0395491971,-0.0202722039,0.0239339485,0.0503307066,0.0498822149,-0.0276055374,-0.0043774473,0.0803509761,-0.011875438,0.0298977668,0.0455441393,-0.0916210892,-0.0370578913,0.1137827273,0.0265905637,0.0135182659,0.043137618,-0.025381476,-0.0510422905,-0.0390705465,0.0697055576,-0.0445391417,0.0248292512,0.0069132279,0.0544244962,-0.0392613017,0.0382795353,0.0313565244,-0.090539669,0.0088564197,-0.0212631557,0.0718279957,-0.0106164714,0.0197844218,0.0060965323,0.0165062648,-0.0155559881,-0.0158006457,0.0589138965,-0.004467479,0.0137869531,-0.005727237,-0.019092894,-0.0177316536,-0.0149673054,0.0126513561,-0.0278544092,-0.0135743715,0.0088047196,0.0366113047,-0.0214604367,0.0223135479,0.0020739415,0.0000331447,0.0280830771,0.0121714389,0.0077616224,-0.0182701257,-0.0214175696,-0.0154130998,0.0342283872,0.0164261472,-0.0043446999,-0.0150417817,0.0203086479,0.0070155371,0.0118401338,0.0071151942,-0.0009549708,0.0088599172,-0.0227974345,-0.0014574912,-0.0241544398,-0.0117694489,0.0143701346,0.0240357628,0.0141963694]},"69":{"Abstract":"Virtual reality systems are widely believed to be the next major computing platform. There are, however, some barriers to adoption that must be addressed, such as that of motion sickness - which can lead to undesirable symptoms including postural instability, headaches, and nausea. Motion sickness in virtual reality occurs as a result of moving visual stimuli that cause users to perceive self-motion while they remain stationary in the real world. There are several contributing factors to both this perception of motion and the subsequent onset of sickness, including field of view, motion velocity, and stimulus depth. We verify first that differences in vection due to relative stimulus depth remain correlated with sickness. Then, we build a dataset of stereoscopic 3D videos and their corresponding sickness ratings in order to quantify their nauseogenicity, which we make available for future use. Using this dataset, we train a machine learning algorithm on hand-crafted features (quantifying speed, direction, and depth as functions of time) from each video, learning the contributions of these various features to the sickness ratings. Our predictor generally outperforms a na\u00efve estimate, but is ultimately limited by the size of the dataset. However, our result is promising and opens the door to future work with more extensive datasets. This and further advances in this space have the potential to alleviate developer and end user concerns about motion sickness in the increasingly commonplace virtual world.","Authors":"N. Padmanaban; T. Ruban; V. Sitzmann; A. M. Norcia; G. Wetzstein","DOI":"10.1109\/TVCG.2018.2793560","Keywords":"Virtual reality;simulator sickness;vection;machine learning","Title":"Towards a Machine-Learning Approach for Sickness Prediction in 360\u00b0 Stereoscopic Videos","Keywords_Processed":"machine learning;virtual reality;vection;simulator sickness","Keyword_Vector":[0.0370340958,0.0186929522,0.0068314927,0.0129310286,0.0149168206,-0.0404703482,-0.0142604017,-0.0435062291,-0.0131093298,-0.015621889,0.0188798068,0.0285588682,0.0243531904,0.0015588325,0.0389845902,-0.012887573,0.0607875615,0.0232153314,0.0515098014,0.0147103501,0.0162723021,-0.0617292205,0.0807733834,-0.0346898802,-0.0202594882,-0.0166046209,-0.0017885086,0.0653525078,0.067693523,-0.0051252589,-0.0129999729,-0.1103471634,-0.0507710547,0.0050914246,0.0356794807,0.0089026682,0.0102940439,0.0089936497,0.0392116417,0.0170607653,0.0134244521,0.0307658024,-0.0484623839,0.029915085,-0.0532512584,-0.0346760069,-0.0213573077,-0.0064935169,0.014518786,0.0381199445,0.0715306745,0.0155250825],"Abstract_Vector":[0.1378779597,0.0766546636,-0.013736117,-0.018697904,-0.0027430755,0.0302886331,-0.0488349838,-0.0278715299,0.0259936065,-0.0069913062,-0.0435581539,-0.0412960437,0.0262817585,0.0602655449,0.0531630253,0.0359442313,-0.0134889515,0.0055751698,0.0354376158,-0.0000637881,0.0317521952,0.0495632771,0.0360871234,0.0046691171,-0.0625264978,-0.0019427455,-0.0577751042,0.1035921747,-0.0546168465,0.0570135971,-0.0032696219,0.001761764,0.0022941277,0.0514846342,-0.0035628234,0.0729097348,-0.0159164168,-0.0023210464,-0.0038764603,-0.0072653996,0.0083980249,0.0024152788,-0.020836238,-0.0377433352,0.0319545064,0.0482382002,0.0265631449,0.0026688993,0.0498240758,-0.0242430505,0.0271822209,0.101707491,0.0191456013,-0.0627809248,-0.0514570026,-0.0232833989,0.0125246016,-0.005031542,-0.0066162197,-0.0057160715,-0.0069845912,0.0016886077,-0.0010967515,0.0360031199,0.0173003852,0.0195201776,-0.0168898424,0.0113579515,-0.0139707598,0.000456851,0.0108112078,-0.0082820843,0.0113608457,-0.0468732911,-0.0042896404,-0.0283256272,-0.0251139037,0.0230299772,-0.0208262574,0.0160060151,-0.0537757098,0.0115551343,0.0068553302,-0.0228035384,-0.0387135414,-0.0391399663,-0.0040269989,0.0255389232,0.0242087561,0.0124591318,-0.0288763184,-0.03315738,0.0118016215,0.0071755056,0.0326089258,0.012850992,-0.0205756646,0.0126378934,-0.0132744244,0.0104395144,0.0182646625,-0.0134956306,-0.0349836037,0.0134340677,-0.0075081394,-0.0450383103,-0.0221723216,0.0115379994,0.0164409538,0.0091955191,0.0253113663,-0.0356141965,-0.0028433798,-0.0019923172,-0.0010202025,0.017011152]},"7":{"Abstract":"We present a novel, data-driven eye-head coordination model that can be used for realtime gaze prediction for immersive HMD-based applications without any external hardware or eye tracker. Our model (SGaze) is computed by generating a large dataset that corresponds to different users navigating in virtual worlds with different lighting conditions. We perform statistical analysis on the recorded data and observe a linear correlation between gaze positions and head rotation angular velocities. We also find that there exists a latency between eye movements and head movements. SGaze can work as a software-based realtime gaze predictor and we formulate a time related function between head movement and eye movement and use that for realtime gaze position prediction. We demonstrate the benefits of SGaze for gaze-contingent rendering and evaluate the results with a user study.","Authors":"Z. Hu; C. Zhang; S. Li; G. Wang; D. Manocha","DOI":"10.1109\/TVCG.2019.2899187","Keywords":"Eye-head coordination;gaze prediction;Pearsons correlation coefficient;eye tracking;saliency","Title":"SGaze: A Data-Driven Eye-Head Coordination Model for Realtime Gaze Prediction","Keywords_Processed":"eye tracking;Eye head coordination;gaze prediction;pearson correlation coefficient;saliency","Keyword_Vector":[0.2134145953,-0.2104231511,-0.1856877054,0.052400928,-0.1158264647,-0.2421682753,-0.0154210011,0.0078753602,-0.0088087569,0.0744525687,-0.0586402271,0.2298536604,0.004907394,-0.1697348562,0.0540433763,0.0492873836,0.1308127436,-0.0215927008,0.0008215482,0.12267559,-0.1116509138,-0.0890866156,-0.0960033525,-0.027207067,0.0774557298,0.0525258396,0.0392300233,-0.0326374968,0.0159714925,0.0500333211,-0.0644150754,0.031900411,0.0121487496,-0.0173292362,-0.0423482323,-0.0519297116,-0.0436565115,0.0227764974,0.0161540025,-0.0363364233,-0.0514480506,-0.0534993652,0.0009683687,0.0286537618,0.0544431626,-0.0051264952,0.0126169645,-0.0164668348,-0.0306061692,-0.0268547432,0.0864650805,-0.0501088164],"Abstract_Vector":[0.1053500152,-0.0408005391,-0.0064473839,0.0116062996,-0.0565616722,0.013847274,0.0221773429,0.0225495344,-0.0406998976,-0.0146466258,0.0017776011,-0.0012472206,-0.0212726612,-0.0202714447,-0.007239665,0.0296076994,0.0343020464,0.0283488234,-0.0178859941,-0.0326190453,0.0442847654,0.0218472917,0.0099806051,0.0214514793,-0.0177623364,0.0016710331,0.0399169257,-0.000729807,0.0052002601,-0.0028372521,-0.0314907809,0.0048585956,-0.0037323198,0.0146317525,-0.0058434582,-0.010469805,-0.0021463691,-0.0232096272,0.0259834573,-0.0140796423,-0.0137694832,-0.0138569221,-0.0157429455,-0.0080519034,-0.004159347,0.0370162822,0.0041046613,-0.0033321146,0.0021097182,-0.0230359043,-0.0132919301,0.013836867,-0.0142483405,0.0090950059,0.0087148809,-0.0155089945,-0.0005919919,-0.0182596052,0.0096884705,-0.0061898747,0.020991729,0.0148949018,-0.0226072434,-0.0044009061,-0.0220377577,0.001948032,0.0449074506,0.0045638892,-0.0068151289,-0.003150557,-0.0174927146,-0.0035107476,-0.0221976494,-0.0313097309,-0.006511888,0.0004749701,-0.0085511781,0.0384124789,0.0174883611,-0.0163482113,0.0067381586,0.0060622498,0.0172275193,0.0104520395,-0.0197276557,0.0021166194,-0.0176685847,0.0011478107,-0.0255071079,-0.0158048917,0.024920151,-0.022514672,0.0091096542,0.0091602894,-0.0001886021,0.0137094882,0.0109412077,-0.0050133645,0.0040557099,0.0078659466,0.0010067291,-0.0031902883,0.0098648841,0.0131522454,0.0036590157,-0.0262894432,0.0207259477,-0.0165304225,0.0269200621,0.0148141269,-0.0160620319,-0.007077132,0.0119162798,0.0380096999,0.0090913093,-0.0181527859]},"70":{"Abstract":"The heterogeneity and complexity of multivariate characteristics poses a unique challenge to visual exploration of multivariate scientific data sets, as it requires investigating the usually hidden associations between different variables and specific scalar values to understand the data's multi-faceted properties. In this paper, we present a novel association analysis method that guides visual exploration of scalar-level associations in the multivariate context. We model the directional interactions between scalars of different variables as information flows based on association rules. We introduce the concepts of informativeness and uniqueness to describe how information flows between scalars of different variables and how they are associated with each other in the multivariate domain. Based on scalar-level associations represented by a probabilistic association graph, we propose the Multi-Scalar Informativeness-Uniqueness (MSIU) algorithm to evaluate the informativeness and uniqueness of scalars. We present an exploration framework with multiple interactive views to explore the scalars of interest with confident associations in the multivariate spatial domain, and provide guidelines for visual exploration using our framework. We demonstrate the effectiveness and usefulness of our approach through case studies using three representative multivariate scientific data sets.","Authors":"X. Liu; H. Shen","DOI":"10.1109\/TVCG.2015.2467431","Keywords":"Multivariate data;association analysis;visual exploration;multiple views","Title":"Association Analysis for Visual Exploration of Multivariate Scientific Data Sets","Keywords_Processed":"association analysis;multiple view;multivariate datum;visual exploration","Keyword_Vector":[0.181874911,-0.0950001066,-0.0039986893,0.0816797225,-0.0200238102,0.2111190328,-0.0788141668,-0.0674834298,-0.0359917959,-0.0351653154,0.020377591,0.0174510242,0.0646804431,-0.0925383078,-0.0099723849,-0.0782558174,-0.0528573761,-0.0124451859,-0.0178434704,-0.0697298421,-0.0598139256,-0.0020409862,0.0072664102,0.0003934599,-0.0132231092,-0.0207871101,-0.0387360108,0.0545516791,0.0091043625,0.0395624599,-0.0198279167,-0.0577614249,-0.0634311201,-0.0134246051,0.0289121096,0.0096799474,-0.0452839053,0.0572737775,-0.0762966875,0.0321286485,0.0957518817,-0.0317845626,-0.0532846868,0.0423652163,0.0569776418,-0.06684912,-0.0424959098,-0.0136716245,0.089881305,-0.0262349662,0.0710253063,0.0562692223],"Abstract_Vector":[0.1639391518,-0.0563844397,-0.0028872086,0.038828683,0.0049900191,0.0087573417,0.0129735215,-0.0235108627,-0.0149950102,0.0235769132,0.0445436009,-0.0544838543,-0.0807054806,0.014978586,0.0347881675,-0.0211894992,-0.0047111175,0.0002096036,-0.0168295535,-0.0318300915,-0.0198419129,0.0118427741,-0.0118257706,-0.0113213997,-0.0345309057,-0.0097282287,0.02338026,0.0284169593,-0.0096069178,0.0165342539,0.0151837069,0.0605185412,-0.0395887593,0.0178899776,-0.0029404757,0.0452468636,0.0190064443,-0.0095125221,0.040532967,-0.0072801627,-0.0223110327,-0.0164826394,0.0399652812,-0.0268102113,-0.0080680771,0.0227731501,-0.0340659657,0.0545849002,0.0205216435,0.0010850466,-0.0274437398,0.0381822038,-0.0081536002,-0.0459323305,0.0057364579,-0.0785841151,0.0244803425,0.0334002636,0.0160174827,0.0441019882,0.0039980722,-0.0404665435,-0.0066075984,0.0276656837,-0.01692105,0.0200758309,0.0147607685,0.0028373626,0.0260296369,0.0127559054,0.0101803888,0.0324007716,0.0115701891,0.0057859195,-0.0254680478,0.00517113,-0.0207593626,0.0069015701,0.0161109923,0.0301961784,-0.0372899886,-0.0040240098,-0.0126024608,-0.0083899252,-0.0104660892,-0.0009416666,-0.0165115086,-0.0283457369,-0.0007918059,0.0214322733,-0.0308918444,-0.024907583,0.0200582788,0.0310189796,-0.0236991086,-0.0165733138,-0.0239696734,-0.0096186026,-0.0441919004,0.0008420221,-0.0333085176,0.0019490315,0.0005616347,-0.0205219283,0.0045451458,-0.012154165,-0.0134814434,0.0235323526,0.020974264,0.0112088489,-0.0047713615,0.0240512123,0.0011629089,0.0147275201,-0.0112460122,-0.0291744551]},"71":{"Abstract":"We present an improved stress majorization method that incorporates various constraints, including directional constraints without the necessity of solving a constraint optimization problem. This is achieved by reformulating the stress function to impose constraints on both the edge vectors and lengths instead of just on the edge lengths (node distances). This is a unified framework for both constrained and unconstrained graph visualizations, where we can model most existing layout constraints, as well as develop new ones such as the star shapes and cluster separation constraints within stress majorization. This improvement also allows us to parallelize computation with an efficient GPU conjugant gradient solver, which yields fast and stable solutions, even for large graphs. As a result, we allow the constraint-based exploration of large graphs with 10K nodes - an approach which previous methods cannot support.","Authors":"Y. Wang; Y. Wang; Y. Sun; L. Zhu; K. Lu; C. Fu; M. Sedlmair; O. Deussen; B. Chen","DOI":"10.1109\/TVCG.2017.2745919","Keywords":"Graph visualization;stress majorization;constraints","Title":"Revisiting Stress Majorization as a Unified Framework for Interactive Constrained Graph Visualization","Keywords_Processed":"graph visualization;constraint;stress majorization","Keyword_Vector":[0.0173803624,0.010025545,0.0001669619,0.0023751485,-0.0099730836,-0.000673647,-0.0026620593,0.0059949265,-0.0189218694,-0.0120111068,0.0088470861,0.0016089998,-0.0202448041,-0.0042986564,-0.0110384565,-0.0104446189,0.0107464464,-0.0196849157,0.0094860963,-0.0223572835,0.0229961928,-0.0124082664,-0.0050934108,-0.0111295735,-0.0048016842,0.0126992707,-0.0026911062,-0.0100024475,-0.0072613101,0.0021701551,-0.010019402,-0.0051894391,-0.0148774823,0.0281187049,-0.0073640298,0.0082466385,-0.0090727331,-0.0101947139,-0.0190772429,0.0125951686,-0.016141687,0.0100197286,0.0070093938,-0.008797627,0.0251454537,-0.0057548224,-0.0061377119,0.0339974457,-0.023379101,-0.0103538331,0.0026657092,-0.0152910601],"Abstract_Vector":[0.1345621026,0.0943294138,-0.055760692,-0.0061339172,-0.0409482396,0.0587955555,0.0509231154,0.0116804563,0.0597776939,0.0752624886,0.0340881582,0.0276313965,-0.0187448785,-0.014983821,0.0035999523,-0.0036220738,0.0567877219,-0.0154893075,-0.0489596006,-0.0552154555,0.0148028174,-0.0362400006,0.000698623,-0.0090153754,0.0215116468,0.0243123504,0.0193455397,-0.029963842,-0.0204630413,0.0201042184,-0.0000417682,-0.0286029766,0.0381483373,-0.0241756047,-0.0036551713,0.0141530847,-0.0157092486,-0.0257311015,-0.013531846,0.0475537509,0.0721008406,0.0171848396,0.0063686613,0.0559227335,-0.0006803875,-0.0123003385,0.0087628181,-0.0024731807,0.0095040429,-0.0127783702,0.0206228405,0.0349215033,0.003530917,0.0012046514,0.0063566512,-0.020818886,-0.0212559072,0.0082543144,-0.0295145958,-0.0071260177,0.0073854911,-0.0031743348,-0.031879926,0.0089031379,0.0082467631,0.0091638657,-0.0165036381,0.0055929947,-0.0259469576,-0.0006595679,-0.021250611,0.0504629364,-0.0039023612,-0.0283626117,0.025890132,0.0005796642,-0.0382608093,0.0163115291,-0.0056327351,-0.0200216525,0.0176890358,-0.007142295,-0.0306006864,0.026589366,0.0168435081,-0.0034617554,0.0059904899,-0.0069719681,-0.0095946581,0.0335104322,-0.0118527222,0.0099875472,0.0043380493,-0.0034576935,0.0054908621,-0.0419332413,0.0114013268,0.0064650411,0.0023835606,-0.0016244594,0.0476842737,0.017259611,0.0100197769,-0.0010247374,0.0113479183,0.0119964152,0.0294563737,-0.0020624753,-0.0130969992,0.012325544,0.0245015622,-0.0465579451,0.005231858,0.0006293666,0.005668203,-0.0075518941]},"72":{"Abstract":"Traditional methods in graphics to simulate liquid-air dynamics under different scenarios usually employ separate approaches with sophisticated interface tracking\/reconstruction techniques. In this paper, we propose a novel unified approach which is easy and effective to produce a variety of liquid-air interface phenomena. These phenomena, such as complex surface splashes, bubble interactions, as well as surface tension effects, can co-exist in one single simulation, and are created within the same computational framework. Such a framework is unique in that it is free from any complicated interface tracking\/reconstruction procedures. Our approach is developed from the two-phase lattice Boltzmann method with the mean field model, which provides a unified framework for interface dynamics but is numerically unstable under turbulent conditions. Considering the drawbacks of the existing approaches, we propose techniques to suppress oscillations for significant stability enhancement, as well as derive a new subgrid-scale model to further improve stability, faithfully preserving liquid-air interface details without excessive diffusion by taking into account the density variation. The whole framework is highly parallel, enabling very efficient implementation. Comparisons with the related approaches show superiority on stable simulations with detail preservation and multiphase phenomena simultaneously involved. A set of animation results demonstrate the effectiveness of our method.","Authors":"Y. Guo; X. Liu; X. Xu","DOI":"10.1109\/TVCG.2016.2532335","Keywords":"Flow simulation;two-phase lattice Boltzmann method;interface flow","Title":"A Unified Detail-Preserving Liquid Simulation by Two-Phase Lattice Boltzmann Modeling","Keywords_Processed":"interface flow;flow simulation;two phase lattice Boltzmann method","Keyword_Vector":[0.0724772654,0.0397391357,0.0373759438,0.0146416395,-0.0007536293,0.0003589793,-0.0116345237,-0.0014863192,-0.0383756319,-0.0162371405,0.0039785333,0.027862795,0.0518216598,-0.0085280595,0.0147769082,-0.0053885888,-0.045471045,-0.0430576066,0.0303942151,-0.0204527827,0.0412719334,-0.0530523794,0.0111649751,-0.0533342369,0.0491162033,-0.0209119061,-0.022737541,0.0349394514,-0.0210286802,0.0090275235,0.0997344419,-0.0085921878,-0.0512131006,0.0121688396,0.0207722545,0.0319333032,0.2048209267,0.044226139,-0.0096205749,-0.0538352953,-0.072915215,-0.0537694106,-0.0345779101,0.0118254492,-0.0201991415,0.034478289,0.0128132724,0.0753204712,-0.0725689914,0.0280345919,0.041669144,-0.009084981],"Abstract_Vector":[0.1979396616,0.1707428444,-0.0974726021,-0.0731111836,0.0550912225,0.0955666657,0.0829708732,0.0677700824,0.012348562,0.0121708879,0.0310467726,-0.0135024705,-0.0255077155,-0.1029056494,0.0122327113,-0.0272294584,-0.0385005544,-0.0859094011,-0.0095260042,-0.0796692232,0.0210298391,-0.0970333108,-0.0258932401,-0.0501153802,-0.0093406838,0.0362711896,-0.0037215979,0.027770176,0.0209518454,0.0002728627,-0.0196388974,-0.0689103332,-0.0737939884,0.0100152879,-0.0637822316,0.1132794223,0.0981647508,-0.0440684006,0.000913279,-0.0858941147,0.0578354471,0.1312726483,-0.0389394022,0.057427603,-0.0013692133,0.0319021392,-0.0015419516,-0.0469951733,-0.055527304,-0.0102805163,0.0611279621,0.0399546412,0.0057419927,0.049813342,-0.0838283299,-0.0264810263,-0.0408152252,-0.0302049119,-0.1261307174,-0.0479838167,-0.0224771998,0.0238808224,0.0383832153,-0.0471135777,0.0475715233,0.0091035981,-0.0587851141,0.0322732834,-0.0281340012,-0.0108724494,-0.0713712855,0.0342594823,-0.030981933,-0.0483091411,-0.031886726,0.0074396214,0.0527197013,-0.0468887382,0.0100595628,-0.0230281282,-0.0453607945,-0.0801774152,0.0160733476,0.0391668433,0.0254174219,-0.0220786304,0.0085842164,0.0701403517,-0.0092871465,0.0294023064,-0.0332769765,0.0332228544,-0.0088473273,-0.0472263578,0.0111951388,0.0817319447,-0.0070349775,0.0185197279,0.0007421762,-0.022202586,0.0370002269,0.00135081,-0.0106316886,-0.03571378,-0.0402452501,-0.015154609,0.0024972044,-0.0185615553,0.0088780467,0.0437717354,0.0183544083,0.0201922934,0.002793521,-0.0173994257,0.0175770917,-0.0053099413]},"73":{"Abstract":"Urban data is massive, heterogeneous, and spatio-temporal, posing a substantial challenge for visualization and analysis. In this paper, we design and implement a novel visual analytics approach, Visual Analyzer for Urban Data (VAUD), that supports the visualization, querying, and exploration of urban data. Our approach allows for cross-domain correlation from multiple data sources by leveraging spatial-temporal and social inter-connectedness features. Through our approach, the analyst is able to select, filter, aggregate across multiple data sources and extract information that would be hidden to a single data subset. To illustrate the effectiveness of our approach, we provide case studies on a real urban dataset that contains the cyber-, physical-, and social- information of 14 million citizens over 22 days.","Authors":"W. Chen; Z. Huang; F. Wu; M. Zhu; H. Guan; R. Maciejewski","DOI":"10.1109\/TVCG.2017.2758362","Keywords":"Urban data;visual analysis;visual reasoning;heterogeneous;spatio-temporal","Title":"VAUD: A Visual Analysis Approach for Exploring Spatio-Temporal Urban Data","Keywords_Processed":"spatio temporal;heterogeneous;urban datum;visual reasoning;visual analysis","Keyword_Vector":[0.0385150979,0.0153994863,0.0411844749,0.0331646949,-0.0126232637,-0.0110845885,0.0004686004,-0.0356550314,-0.0099033006,-0.0184946432,-0.0182943411,-0.0048638797,-0.0404603378,-0.0080050764,-0.0082061203,-0.0198675011,0.046731023,-0.061990115,0.0114612705,-0.0158782813,0.0435957009,-0.0104760982,0.0007213244,0.018799025,0.0065519726,-0.0371187952,0.0208133583,0.0514326004,-0.026611017,-0.0167956778,-0.0160818183,0.0364710972,0.0411836785,0.0110430836,-0.0198812756,-0.0583053502,0.0627778392,0.0357947098,-0.0240045611,0.0607339209,-0.0184585072,0.0234363323,-0.01951572,-0.0490048857,0.0423527779,0.0004031759,-0.0089046599,-0.0138720168,-0.0180737876,0.0047123531,0.0453959517,0.0038938749],"Abstract_Vector":[0.1723991748,0.1035605195,0.0633172823,0.0379720239,-0.0212584468,-0.0431328575,-0.099625686,-0.0652608769,-0.0633452157,-0.0208936506,-0.0859384786,-0.0221506294,-0.0795820488,-0.0614676834,0.0191172237,-0.0332631369,0.0682891603,-0.0394347313,0.0724691925,-0.0373730968,-0.0929612754,-0.0901911925,0.0089399367,-0.0194902844,-0.0091146465,0.028609848,-0.0021839233,-0.030973403,-0.000376471,-0.0689145804,-0.0434266319,0.0696800572,0.1066799365,0.0173804819,-0.0638599256,-0.0427668667,0.0007721181,0.0711355977,0.0751850533,-0.0310150186,-0.0086708324,-0.0913318224,0.0234005465,-0.0054101327,-0.0803334499,-0.0076488934,0.0969241596,-0.0245000016,0.0280074988,-0.0288929517,0.0073010255,-0.0177238623,-0.0488088247,-0.0004043144,0.0843590603,0.031994861,-0.0398352369,0.0234433471,-0.0337665264,0.026483649,-0.0160481966,0.0080161675,0.0465159745,0.0099975428,0.0136219202,0.0046525609,-0.041787496,0.004359029,0.0213059431,-0.0219121701,-0.0000421093,-0.0068572687,0.0019190956,0.0037180272,0.0184104119,-0.0441159339,-0.0222289793,-0.02028113,-0.0148326629,-0.0339005125,0.0160375755,-0.0191199749,-0.0099576802,0.0468711882,-0.0240373776,0.0181876643,-0.0121704516,-0.0147458084,0.0029346058,-0.0246116903,-0.0530849037,0.0213793515,-0.0288266599,-0.0072145749,0.0035170354,0.0149019659,-0.0011855077,-0.0029140666,-0.0160454314,-0.0380331197,0.0566256733,-0.0043392844,-0.0079650133,-0.0266063732,0.0029001341,0.0403844509,-0.0161516146,0.032290983,-0.008686083,-0.0172607319,0.0250436414,0.0083178798,0.0058551546,0.0065517186,0.0530601337,-0.0117178264]},"74":{"Abstract":"Traditional fisheye views for exploring large graphs introduce substantial distortions that often lead to a decreased readability of paths and other interesting structures. To overcome these problems, we propose a framework for structure-aware fisheye views. Using edge orientations as constraints for graph layout optimization allows us not only to reduce spatial and temporal distortions during fisheye zooms, but also to improve the readability of the graph structure. Furthermore, the framework enables us to optimize fisheye lenses towards specific tasks and design a family of new lenses: polyfocal, cluster, and path lenses. A GPU implementation lets us process large graphs with up to 15,000 nodes at interactive rates. A comprehensive evaluation, a user study, and two case studies demonstrate that our structure-aware fisheye views improve layout readability and user performance.","Authors":"Y. Wang; Y. Wang; H. Zhang; Y. Sun; C. Fu; M. Sedlmair; B. Chen; O. Deussen","DOI":"10.1109\/TVCG.2018.2864911","Keywords":"Graph Visualization;Focus-Context Technique;Structure-aware Zoom;Graph Layout Technique","Title":"Structure-aware Fisheye Views for Efficient Large Graph Exploration","Keywords_Processed":"structure aware zoom;Graph Layout technique;Focus Context technique;Graph visualization","Keyword_Vector":[0.2319219005,-0.1169909842,0.0414988424,-0.2065018551,0.0560797445,-0.0283219169,-0.0811319143,-0.0024965802,0.0421973125,0.005158089,0.0297548874,-0.1717102249,-0.0169427978,-0.0200554657,0.0264598687,-0.0064350276,-0.0178390822,-0.0130707967,-0.0116272197,0.0519909693,-0.0193905093,-0.039400128,-0.0215598283,-0.005285914,0.0046799421,0.0122289277,-0.0049349523,0.0163349458,0.00817536,-0.0007986685,0.0027497634,-0.0106757361,0.0138146168,0.0016501647,-0.0232942825,-0.0020387593,0.0242044779,0.0067035902,-0.0127106947,-0.0109482815,-0.0043647184,0.0109298307,0.0149544977,-0.0098148109,-0.0037223945,-0.0029697279,0.0381655156,0.016047384,0.0145253154,0.0299452708,-0.0372817642,0.0139416513],"Abstract_Vector":[0.1287628174,-0.0688783237,0.0324590274,0.0034693159,-0.0539603462,0.044068148,-0.0176164177,0.0122688579,0.0101245282,-0.0276502843,-0.0381724798,0.0642070354,0.0248988261,-0.0083071193,0.0021343477,-0.0330134822,-0.0792695924,-0.0508049725,0.0014041616,-0.0336644821,0.0272016607,0.0044173923,-0.0051632448,-0.0158978511,0.0250110379,0.0437565441,0.0101143706,0.0199888224,0.0101017911,-0.0055024906,-0.0099872567,-0.0309919718,0.0410828788,0.0119792912,0.0173937676,-0.0091381021,-0.0045751931,0.0274401275,-0.012448525,0.0193350799,0.0015682828,0.0211739929,0.0433717655,-0.0419712707,-0.0353478673,-0.0092666463,0.0132350177,0.0029266576,0.0218307128,0.0114335195,0.0083463425,-0.0308393849,0.0212182666,-0.0228419123,0.0306387351,-0.0352790687,-0.0051377742,-0.0064865772,-0.0257565583,-0.0183415467,0.0046800341,-0.0288475583,0.0059861006,-0.0131710909,0.0040816347,-0.032035112,0.0386626159,-0.0179936339,-0.0178628705,0.0124072542,-0.0049797118,0.0034366408,-0.0221660459,-0.0143814404,0.0046217852,0.0031540182,-0.001394807,-0.0029308471,-0.0262126037,-0.0177404583,0.0055638695,0.0082865028,-0.002648796,0.0146376405,0.0193308109,-0.0119150365,0.0354836716,0.0193270646,-0.0179728502,-0.0055711048,-0.0286138032,0.0237222562,0.0238694923,0.0103035176,0.0136287304,-0.0007621648,-0.0178514711,-0.008366936,-0.029630819,0.0085485678,-0.0167700137,-0.0186705035,-0.0229035082,-0.011491059,-0.0109598736,-0.0135666688,0.0000403382,0.0262762368,-0.0032095912,-0.0181432159,0.0382766457,0.0089460846,0.0062816855,-0.0463901797,-0.0134081576,0.0106120069]},"75":{"Abstract":"Scalar topology in the form of Morse theory has provided computational tools that analyze and visualize data from scientific and engineering tasks. Contracting isocontours to single points encapsulates variations in isocontour connectivity in the Reeb graph. For multivariate data, isocontours generalize to fibers-inverse images of points in the range, and this area is therefore known as fiber topology. However, fiber topology is less fully developed than Morse theory, and current efforts rely on manual visualizations. This paper presents how to accelerate and semi-automate this task through an interface for visualizing fiber singularities of multivariate functions R3\u2192R2. This interface exploits existing conventions of fiber topology, but also introduces a 3D view based on the extension of Reeb graphs to Reeb spaces. Using the Joint Contour Net, a quantized approximation of the Reeb space, this accelerates topological visualization and permits online perturbation to reduce or remove degeneracies in functions under study. Validation of the interface is performed by assessing whether the interface supports the mathematical workflow both of experts and of less experienced mathematicians.","Authors":"D. Sakurai; O. Saeki; H. Carr; H. Wu; T. Yamamoto; D. Duke; S. Takahashi","DOI":"10.1109\/TVCG.2015.2467433","Keywords":"Singular fibers;fiber topology;mathematical visualization;design study;Singular fibers;fiber topology;mathematical visualization;design study","Title":"Interactive Visualization for Singular Fibers of Functions f : R3 \u2192 R2","Keywords_Processed":"singular fiber;design study;mathematical visualization;fiber topology","Keyword_Vector":[0.0800356773,-0.019795578,0.0224147378,-0.0217617149,0.0010843506,0.005048341,-0.027617715,-0.0225421573,-0.0081408745,-0.0456821894,0.0122786395,0.0077831166,-0.0366113053,0.0012970069,0.0101212827,0.0193397303,0.0097751591,0.0311844005,0.0061486093,0.0036180799,0.0221287784,0.0298706821,-0.0552703982,0.014345122,0.0136343839,0.0375983106,-0.0137164812,0.0263872915,0.0406554626,-0.0014345021,0.0113046539,0.0305971859,-0.0048195098,-0.0320279443,0.0062274915,-0.0084428549,-0.0244832942,-0.0363573014,-0.029476456,0.0060768684,-0.0299145883,0.0776127101,0.0153410883,0.0073022522,-0.0034269391,0.015459599,0.0262284784,0.0275202997,0.0331007516,-0.0395973095,-0.0063186657,0.046637918],"Abstract_Vector":[0.2378879156,-0.1203008918,-0.005643352,-0.0352565853,-0.033786945,-0.0442818712,0.047188592,0.0034165306,0.0359279,0.0554651652,0.0416652853,-0.0635898306,0.0246963109,0.0174890655,-0.0521187036,0.0044265537,-0.0011902572,0.0723249025,0.0255272431,-0.0087820573,0.0150552799,-0.0762338394,-0.0067962889,0.0138155456,0.0469633534,0.0391035848,-0.0830628165,0.0494724669,-0.0081517793,-0.0568862944,0.0402763079,-0.0022163677,-0.0493915046,0.0156796275,0.0217590826,0.0491283743,0.0028612964,0.090832597,0.0002819199,-0.0438733567,-0.0560927612,0.0736463064,-0.0326235341,0.0773357915,0.0346669476,-0.0676851049,-0.0441464878,0.0794495609,-0.0264950702,-0.040388989,0.0402422551,0.0365289096,0.0982074295,-0.0554547188,0.0648056987,0.0145590738,0.097670532,0.0821606663,0.0143192201,-0.0286447163,0.1171579482,-0.0534708009,0.1171303737,0.0527571494,-0.0294327213,0.051117234,0.0146499244,-0.0501187551,0.0399418947,-0.0720323753,0.0135664818,-0.0605782539,-0.0552882902,0.0052346444,0.0028227285,-0.0238370405,-0.0473566162,-0.019153819,-0.0013954878,0.0412347729,0.0333430538,-0.0360563307,-0.0803820069,0.0076175431,-0.0457877426,-0.0293486315,-0.0331771521,-0.0342173076,0.0374254098,0.0524119992,0.090744054,0.0277153952,0.0257678414,0.0189647455,-0.0071442984,0.0051282311,-0.000628966,-0.0365981911,0.001279681,-0.0265431343,-0.0275594129,-0.0287811223,0.0755492374,-0.0333923041,0.0014243999,0.0009416814,0.0036787022,0.0224941201,0.0022959192,0.049774633,0.0506815995,0.0414471849,-0.0088189937,0.0463202922,0.0235209657,-0.0285473421]},"76":{"Abstract":"The large-scale structure of the universe is comprised of virialized blob-like clusters, linear filaments, sheet-like walls and huge near empty three-dimensional voids. Characterizing the large scale universe is essential to our understanding of the formation and evolution of galaxies. The density range of clusters, walls and voids are relatively well separated, when compared to filaments, which span a relatively larger range. The large scale filamentary network thus forms an intricate part of the cosmic web. In this paper, we describe Felix, a topology based framework for visual exploration of filaments in the cosmic web. The filamentary structure is represented by the ascending manifold geometry of the 2-saddles in the Morse-Smale complex of the density field. We generate a hierarchy of Morse-Smale complexes and query for filaments based on the density ranges at the end points of the filaments. The query is processed efficiently over the entire hierarchical Morse-Smale complex, allowing for interactive visualization. We apply Felix to computer simulations based on the heuristic Voronoi kinematic model and the standard ACDM cosmology, and demonstrate its usefulness through two case studies. First, we extract cosmic filaments within and across cluster like regions in Voronoi kinematic simulation datasets. We demonstrate that we produce similar results to existing structure finders. Second, we extract different classes of filaments based on their density characteristics from the ACDM simulation datasets. Filaments that form the spine of the cosmic web, which exist in high density regions in the current epoch, are isolated using Felix. Also, filaments present in void-like regions are isolated and visualized. These filamentary structures are often over shadowed by higher density range filaments and are not easily characterizable and extractable using other filament extraction methodologies.","Authors":"N. Shivashankar; P. Pranav; V. Natarajan; R. v. d. Weygaert; E. G. P. Bos; S. Rieder","DOI":"10.1109\/TVCG.2015.2452919","Keywords":"Morse-Smale complexes;tessellations;cosmology theory;cosmic web;large-scale structure of the universe;Morse-Smale complexes;tessellations;cosmology theory;cosmic web;large-scale structure of the universe","Title":"Felix: A Topology Based Framework for Visual Exploration of Cosmic Filaments","Keywords_Processed":"tessellation;cosmology theory;large scale structure of the universe;cosmic web;Morse Smale complex","Keyword_Vector":[0.1500812028,-0.0638124257,0.0324108309,-0.1037200289,0.0243632341,-0.0654896334,-0.0802820257,-0.0557931037,-0.0119355632,0.0181842159,-0.0372259757,-0.0508633364,-0.0798499709,0.0130248842,0.003920077,0.0381419626,0.1293108861,-0.067283042,0.0517860625,0.0218304519,0.0117011627,-0.0918118994,0.0120188145,0.0132125649,-0.0094734822,0.014016723,-0.0278606371,0.0525533159,-0.0886698673,0.0621128521,-0.005085402,0.0003902585,0.0046986389,0.0286820936,-0.0200832343,-0.0137433691,-0.0889421655,-0.0425383411,0.0119518887,-0.0086866663,0.0046473702,-0.0259479931,-0.0410738601,0.0258839056,0.0638068427,0.0289179252,0.0906799465,-0.0502657937,-0.0667309575,0.0252835573,0.0652000283,-0.0199473983],"Abstract_Vector":[0.2180206119,-0.025903695,-0.0067890188,-0.0439859637,-0.0430698985,0.0823264551,-0.0723403386,0.0245239936,0.0353819542,-0.1005394792,-0.0675197666,0.0553042611,-0.0321693354,0.0704837999,-0.0044410613,-0.1190185572,-0.0878780484,0.0104516737,-0.0995670867,-0.008349901,0.0036160378,0.074121196,-0.081629803,-0.0291577602,0.0360824258,-0.0022315072,0.0157595469,-0.0400469899,0.0391235676,0.0767926471,0.0670056746,0.0344529669,0.0662703415,0.0460422943,-0.0270537045,0.0653106437,-0.064053016,-0.0733249868,-0.0494014984,-0.0712744538,-0.1052081706,-0.0965687683,0.0755647592,0.0335118977,-0.011064676,0.0168384786,0.044271144,0.0724693572,-0.0320105276,-0.063870005,0.0437580674,-0.0365058833,-0.0248171005,0.0342737412,0.0553738692,0.0269431923,0.0171877018,0.0030435662,-0.0729777934,-0.0362982883,-0.0386046996,0.025359058,0.0131964375,0.0061814587,-0.0290404606,-0.0086461272,0.008715405,-0.0001843,-0.0257629459,-0.029387012,-0.0030276204,0.0048140915,-0.0406361455,0.003415328,0.0014150064,-0.0022932041,-0.0163003236,0.0016679071,0.005256467,-0.0110448176,-0.02265202,-0.0122902081,0.0165227825,0.0080662923,0.0122401746,0.0384235448,0.0316481738,-0.0008277233,-0.024030359,0.0240086976,-0.0034606124,0.0193778023,-0.0027708705,-0.0184155368,0.0392994057,-0.0361927359,0.0262792659,0.03487424,0.0219245872,-0.003420562,-0.0360171209,-0.0044599359,0.0401912249,0.009192371,0.0386406184,-0.0109809091,-0.019407732,-0.0468219809,0.0000495716,-0.0076402867,0.0187085979,0.0236494598,-0.0162310146,-0.0032297089,0.0223278525,-0.0174635718]},"77":{"Abstract":"Effective analysis of features in time-varying data is essential in numerous scientific applications. Feature extraction and tracking are two important tasks scientists rely upon to get insights about the dynamic nature of the large scale time-varying data. However, often the complexity of the scientific phenomena only allows scientists to vaguely define their feature of interest. Furthermore, such features can have varying motion patterns and dynamic evolution over time. As a result, automatic extraction and tracking of features becomes a non-trivial task. In this work, we investigate these issues and propose a distribution driven approach which allows us to construct novel algorithms for reliable feature extraction and tracking with high confidence in the absence of accurate feature definition. We exploit two key properties of an object, motion and similarity to the target feature, and fuse the information gained from them to generate a robust feature-aware classification field at every time step. Tracking of features is done using such classified fields which enhances the accuracy and robustness of the proposed algorithm. The efficacy of our method is demonstrated by successfully applying it on several scientific data sets containing a wide range of dynamic time-varying features.","Authors":"S. Dutta; H. Shen","DOI":"10.1109\/TVCG.2015.2467436","Keywords":"Gaussian mixture model (GMM);Incremental learning;Feature extraction and tracking;Time-varying data analysis;Gaussian mixture model (GMM);Incremental learning;Feature extraction and tracking;Time-varying data analysis","Title":"Distribution Driven Extraction and Tracking of Features for Time-varying Data Analysis","Keywords_Processed":"gaussian mixture model GMM;time vary datum analysis;feature extraction and tracking;incremental learning","Keyword_Vector":[0.1893919969,-0.0016814477,0.2129060612,0.0216380536,-0.1008296891,-0.1193087785,-0.0077820552,0.0543615406,-0.263902788,0.1794981648,0.2140641767,0.1166791307,-0.109126526,-0.0317331427,0.1039557087,-0.0896157329,0.0354631247,-0.0694073858,-0.115989646,0.0728162778,-0.1027076914,-0.0327539425,-0.0576434762,-0.1128513902,0.0957340008,0.039139902,-0.0317609313,-0.0447729617,-0.0033184082,0.0508002016,-0.0180293584,0.0278523403,-0.0502665265,-0.0218673119,-0.0422085204,0.0020167561,-0.0058387983,0.0314863993,0.0114748498,0.0149315894,-0.0646709982,0.0053663691,0.0178783027,0.0300703359,-0.0121656304,0.0132906625,0.0044506533,-0.0470084307,0.0525163415,0.0891437345,0.0041849927,0.0010450437],"Abstract_Vector":[0.2371947464,0.177824278,0.2472782808,0.0129913682,-0.0863604861,-0.008656218,-0.1825260015,0.1368843101,-0.0090026252,0.1349922812,0.1086470832,0.0030815432,0.0398202285,-0.0752972845,-0.0093458321,0.0152209265,-0.0371468381,0.0227384478,-0.0852848671,0.0478971,0.0518654602,-0.0135593602,0.0393662506,-0.0367064511,-0.0148271829,-0.0475876737,-0.0869298695,-0.0339729033,-0.0101883617,0.0294145762,-0.0196884512,-0.0037514903,0.0279498132,0.0022745915,0.0220081639,-0.0158434321,-0.0341648804,-0.0425217295,0.01190354,0.0150572312,-0.060653995,-0.0052669081,0.0567734866,-0.0042394008,0.0401794197,-0.0446480252,-0.0403087293,0.0097407638,0.015387126,0.0735746008,0.0111313281,0.0256950676,0.0166983807,0.0337418443,-0.0243637866,-0.0063028313,0.0035161248,-0.0240861773,0.0164549692,0.0236745473,-0.0684553621,-0.0048296148,-0.046142519,0.018639884,0.0019746718,0.0034638854,0.0358890302,0.0326787725,-0.0269347938,-0.0858312039,0.0120275429,-0.0036375054,0.0042289994,-0.0036924715,-0.0287641932,-0.0216481211,-0.0152294744,-0.034276894,0.0388469741,0.0053011559,0.0376645735,0.0191202198,0.0058817415,0.0209786237,-0.0021882512,0.0450269166,0.0063020216,-0.0197181937,0.0271414556,-0.0151722024,-0.0403454725,-0.0063253842,-0.0278209246,0.0378898383,-0.0045040405,0.0055650263,-0.0113283638,-0.0544525515,-0.031171242,-0.0100153728,-0.0211483144,-0.0287023038,-0.0091203924,0.0091677698,-0.0260904676,0.0100970905,0.0331573194,-0.032440491,-0.0198505809,0.0018369735,0.0022393523,0.0267226952,0.0148810375,-0.0154545796,-0.0042219538,0.0426240932]},"78":{"Abstract":"This paper presents a spatial reduction framework for simulating nonlinear deformable objects interactively. This reduced model is built using a small number of overlapping quadratic domains as we notice that incorporating high-order degrees of freedom (DOFs) is important for the simulation quality. Departing from existing multi-domain methods in graphics, our method interprets deformed shapes as blended quadratic transformations from nearby domains. Doing so avoids expensive safeguards against the domain coupling and improves the numerical robustness under large deformations. We present an algorithm that efficiently computes weight functions for reduced DOFs in a physics-aware manner. Inspired by the well-known multi-weight enveloping technique, our framework also allows subspace tweaking based on a few representative deformation poses. Such elastic weighting mechanism significantly extends the expressivity of the reduced model with light-weight computational efforts. Our simulator is versatile and can be well interfaced with many existing techniques. It also supports local DOF adaption to incorporate novel deformations (i.e., induced by the collision). The proposed algorithm complements state-of-the-art model reduction and domain decomposition methods by seeking for good trade-offs among animation quality, numerical robustness, pre-computation complexity and simulation efficiency from an alternative perspective.","Authors":"R. Luo; W. Xu; H. Wang; K. Zhou; Y. Yang","DOI":"10.1109\/TVCG.2017.2783335","Keywords":"Quadratic deformation;FEM;model reduction;domain decomposition;weight function","Title":"Physics-Based Quadratic Deformation Using Elastic Weighting","Keywords_Processed":"model reduction;FEM;quadratic deformation;domain decomposition;weight function","Keyword_Vector":[0.0845547351,-0.0200335508,0.0070052992,-0.0279553449,0.0268089357,-0.029922619,-0.0359298932,-0.0125175976,-0.0443951731,0.0193714542,-0.0314639195,0.0403963528,-0.0254635302,0.0222448771,-0.0673231398,-0.0224409342,-0.0714077309,0.0073016902,-0.039903632,0.0234299733,0.0331643751,0.0155698402,-0.009347666,0.0329496533,-0.0079683998,0.0596151119,0.0205321974,0.06195186,-0.0101476724,-0.0206082893,-0.0655422281,-0.0502214042,-0.0276975239,-0.0185646655,-0.0026745234,-0.0190023441,0.0538070542,-0.0373383492,-0.0387345971,-0.0452198046,0.0554065542,0.0151047541,-0.0302027228,0.0011690164,0.0042169176,-0.0086772957,0.0068466241,0.0314805629,0.0076669006,-0.0051228461,-0.0221109459,0.0185235138],"Abstract_Vector":[0.1798533434,-0.0446585003,0.0274358837,0.0057120827,-0.0540444315,0.0290729656,-0.0576929468,-0.0188865327,0.0236106489,-0.0148076005,-0.0273177292,0.0758142333,-0.0582733303,0.0360268384,0.055057808,-0.0236559392,-0.0316179943,-0.0264359928,-0.0108606495,-0.0192262445,0.0275011044,-0.0667283812,-0.0216517454,-0.0597048024,-0.0357118407,0.0152457631,0.0055122711,-0.0085700116,-0.0017039609,-0.0210520478,0.0593724703,0.0036916319,0.0216701318,0.0278365647,-0.0850946596,-0.0043968054,0.0812117093,0.0054294234,0.0253766787,0.0125000843,0.021274206,-0.0041329314,-0.0123616981,-0.0100859915,-0.0546977427,-0.0301489737,-0.0071886183,-0.0021843881,-0.0014403777,0.0140623615,-0.0289591051,-0.0385011738,-0.0160842096,-0.0136383725,0.0090576524,0.0089422075,0.0376305871,0.011702141,-0.0120587961,-0.0135145405,0.0139251372,0.055871837,0.0074487223,-0.0240269939,-0.0044181313,-0.0395660533,0.0279922462,0.0022318307,0.0349851748,0.0640398361,-0.0080054258,0.0180330592,-0.0220102573,-0.0451052738,0.02190888,0.0020747808,0.0057970179,0.0047053881,0.0166996669,-0.0034406038,-0.0020213771,-0.0244963837,0.0057518714,0.0234906196,-0.0009576468,-0.0123469926,0.0184621436,-0.0302548471,-0.0010839284,0.0066505727,-0.0217200567,-0.0166921656,0.0353615382,0.0132253047,0.0188194794,-0.0617499198,-0.0251865966,-0.0066448289,-0.0069979415,0.0103491314,0.0359729742,-0.0405778596,-0.0132577622,-0.0197055296,0.0127081425,-0.0228483417,0.0026993409,0.0119424576,-0.0073131179,0.0265074171,0.0055286482,0.0050323296,0.0118182239,-0.0186600371,0.0164804296,-0.0002177964]},"79":{"Abstract":"We present a novel method to compute anisotropic shading for direct volume rendering to improve the perception of the orientation and shape of surface-like structures. We determine the scale-aware anisotropy of a shading point by analyzing its ambient region. We sample adjacent points with similar scalar values to perform a principal component analysis by computing the eigenvectors and eigenvalues of the covariance matrix. In particular, we estimate the tangent directions, which serve as the tangent frame for anisotropic bidirectional reflectance distribution functions. Moreover, we exploit the ratio of the eigenvalues to measure the magnitude of the anisotropy at each shading point. Altogether, this allows us to model a data-driven, smooth transition from isotropic to strongly anisotropic volume shading. In this way, the shape of volumetric features can be enhanced significantly by aligning specular highlights along the principal direction of anisotropy. Our algorithm is independent of the transfer function, which allows us to compute all shading parameters once and store them with the data set. We integrated our method in a GPU-based volume renderer, which offers interactive control of the transfer function, light source positions, and viewpoint. Our results demonstrate the benefit of anisotropic shading for visualization to achieve data-driven local illumination for improved perception compared to isotropic shading.","Authors":"M. Ament; C. Dachsbacher","DOI":"10.1109\/TVCG.2015.2467963","Keywords":"Direct volume rendering;volume illumination;anisotropic shading;Direct volume rendering;volume illumination;anisotropic shading","Title":"Anisotropic Ambient Volume Shading","Keywords_Processed":"anisotropic shading;direct volume render;volume illumination","Keyword_Vector":[0.2550717374,-0.10240708,0.067297809,0.1084657466,-0.0534011335,0.2382231377,-0.0914680422,-0.0414855398,0.0332052485,0.0460591698,0.0280292546,0.0159922461,0.0520536978,-0.0046185832,0.0336887036,0.1124273535,0.0110837685,-0.0730578697,0.0093099259,0.1534180057,0.0108995277,0.1318688488,0.0354429017,0.0323076481,-0.0259150728,0.0758418203,0.0249450937,0.0645414947,-0.0473437341,0.0207864266,-0.0049583841,-0.0341862116,-0.1159951023,0.0645492842,0.0600019441,0.0487092802,-0.0058668772,0.1216998816,0.0180252815,-0.0220333104,0.0001400351,0.1106492222,-0.0717173245,-0.1225255922,0.0002348734,0.026905569,0.0387685269,0.0177230238,-0.0357939339,-0.1573953101,0.0758835085,0.1016705732],"Abstract_Vector":[0.3302291879,-0.1251522144,0.0010794604,-0.0365069003,0.0340128122,-0.1245270711,0.0008850538,-0.0649439227,0.1505634625,0.0511547308,0.0162910893,0.0549976293,0.0169783818,0.0697840568,0.017409999,-0.0174316745,0.0188682548,-0.0321426277,0.0347316505,-0.0994488845,-0.0215487106,-0.0239024113,-0.0164450824,0.0496432543,0.0512051715,-0.0450723099,0.0293887726,0.0828669247,-0.0932937539,-0.025170993,-0.0192363817,-0.061324356,0.0400611543,-0.0473580822,-0.048292434,-0.012032229,-0.0295374367,-0.0598796126,-0.0514491706,0.055232658,0.0063742971,-0.0163614989,0.0797382318,0.0176137107,0.0056530712,-0.0159352577,-0.0188803099,-0.0767642615,-0.0297003141,-0.0379937903,-0.0310131656,0.0160612515,0.0255977992,0.0009817632,0.0275082509,0.019251064,0.034128943,-0.0883666846,0.020170323,-0.0036165425,-0.0102624135,0.085984062,0.0111324474,-0.0150769949,0.0170317662,-0.0429680425,-0.037204568,0.0075179135,0.0196151101,0.0367784233,0.076357654,-0.0155838982,0.0132044907,0.0173231669,-0.0168425153,0.0504437896,-0.0278146158,-0.0385891663,-0.0523425221,-0.0353613186,0.0160952343,-0.0176089836,-0.0336637542,-0.0121599704,0.0116867455,0.0257287173,-0.0956517679,-0.027158403,-0.0170656062,0.0511096673,-0.0106217688,0.0332746099,0.0028484841,0.0177933157,-0.0295558026,-0.0319861637,-0.0455703894,-0.0602567698,-0.0077867638,-0.0203543911,0.0054201272,-0.0170201524,0.0336676408,-0.0029544204,0.0121429742,-0.0731717695,-0.0248425148,-0.0312713389,0.0493317773,-0.0274454057,-0.0027588865,-0.0255625583,0.0171081328,0.0105875774,-0.0240718719,-0.0236247718]},"8":{"Abstract":"Shader lamps can augment physical objects with projected virtual replications using a camera-projector system, provided that the physical and virtual object are well registered to each other. Precise registration and tracking has been a cumbersome and intrusive process in the past. In this paper, we present a new method for tracking complex-shaped physical objects interactively. In contrast to previous approaches our system is mobile and makes solely use of the projection of the virtual replication to track the physical object and \u201cstick\u201d the projection to it. Our method consists of two stages, a fast pose initialization based on structured light patterns and a non-intrusive frame-by-frame tracking based on features detected in the projection. During the tracking phase, a radiometrically corrected virtual camera view based on the current pose prediction is rendered and compared to the captured image. Matched features are triangulated providing a sparse set of surface points that is robustly aligned to the virtual model. The alignment transformation serves as an input for the new pose prediction. Detailed experiments including the evaluation of the overlay accuracy show that our approach can accurately and robustly track complex objects at interactive rates.","Authors":"C. Resch; P. Keitler; G. Klinker","DOI":"10.1109\/TVCG.2015.2450934","Keywords":"Computer Vision;Augmented Reality.;Computer vision;augmented reality","Title":"Sticky Projections-A Model-Based Approach to Interactive Shader Lamps Tracking","Keywords_Processed":";augmented reality;computer vision","Keyword_Vector":[0.271699453,-0.1275082812,0.0709225564,-0.0373559633,-0.0101122886,0.1862422889,-0.0747511232,-0.0624798639,0.0534974959,0.0828631887,0.0166425903,-0.0901250869,-0.0161115412,-0.0359010647,0.088991048,0.1112654203,-0.0109142688,-0.0331236245,-0.1028528576,0.0685984944,-0.0028750459,-0.0136231202,0.0352560475,0.0906802218,-0.0318693509,0.0302950879,-0.0186975003,-0.0093082103,-0.0290142807,0.0591281315,-0.0991330026,-0.0775930266,0.0065981575,0.0619983055,-0.0197909545,0.0085937127,0.0058992937,0.0073733188,-0.0016642586,0.0041730243,0.0192553685,-0.0043417165,-0.0332008992,0.0202288713,-0.023766905,-0.0422557519,-0.0021688014,-0.0056648076,0.0328414573,0.0142642081,-0.0165604577,0.0172896926],"Abstract_Vector":[0.2382112811,0.024227812,0.093912023,-0.0011628442,0.0192893026,-0.0605815831,0.031079171,-0.0521298537,0.0164793343,-0.0378859982,-0.0099068327,-0.0516587982,-0.064762771,-0.1039674919,0.0073837357,-0.0567988719,-0.0211723584,-0.0600838387,0.0154911543,-0.0306153209,-0.0233057709,-0.0719223858,0.0552248629,-0.0113402671,-0.013542406,-0.0153699078,0.044975473,-0.0433512294,-0.0666520611,0.0074398645,-0.110093204,0.0199519775,0.0618948604,0.0425929547,-0.0418766038,0.0529801422,-0.058037301,0.0224258948,0.0042845101,-0.0023901863,0.0159870718,-0.0598581727,0.0138246335,0.0452973619,-0.0052605553,0.0061733482,0.0488641059,-0.0060451419,-0.0000522517,-0.0204867908,-0.0343847863,0.0060211051,-0.0344143777,-0.0184377448,0.012168131,-0.0039638995,0.054005398,0.0084899182,-0.0277307786,-0.0189829029,-0.0295056808,-0.0114287289,0.0119837306,-0.0332596545,0.0058084093,-0.0193773524,-0.0189332984,-0.0215354898,0.0073765475,0.0751858014,0.0071043362,-0.0386529274,-0.0471283884,0.0239805139,-0.0067939562,-0.0375817954,-0.0133684161,-0.0099952634,-0.0054903964,-0.0678024568,-0.0226547401,0.0026583793,0.0139776245,-0.0230842788,-0.0008582102,-0.0305495159,-0.0121768296,0.0038821368,-0.0211060488,-0.0091870402,-0.0042603917,-0.0343057387,0.0177466176,-0.0182565758,-0.0296048856,0.037006194,0.0285908987,-0.0029475681,0.0327627205,-0.0365898587,0.0650357296,0.0038222928,-0.0582019737,-0.0116599724,-0.0960491164,0.0150840766,-0.0277672709,-0.0084486134,-0.0248231859,0.0238059895,-0.0244461172,0.0503000898,-0.0147677749,-0.0426605135,-0.0063221068,-0.0097873646]},"80":{"Abstract":"Despite significant advances in the analysis and visualization of unsteady flow, the interpretation of it's behavior still remains a challenge. In this work, we focus on the linear correlation and non-linear dependency of different physical attributes of unsteady flows to aid their study from a new perspective. Specifically, we extend the existing spatial correlation quantification, i.e. the Local Correlation Coefficient (LCC), to the spatio-temporal domain to study the correlation of attribute-pairs from both the Eulerian and Lagrangian views. To study the dependency among attributes, which need not be linear, we extend and compute the mutual information (MI) among attributes over time. To help visualize and interpret the derived correlation and dependency among attributes associated with a particle, we encode the correlation and dependency values on individual pathlines. Finally, to utilize the correlation and MI computation results to identify regions with interesting flow behavior, we propose a segmentation strategy of the flow domain based on the ranking of the strength of the attributes relations. We have applied our correlation and dependency metrics to a number of 2D and 3D unsteady flows with varying spatio-temporal kernel sizes to demonstrate and assess their effectiveness.","Authors":"M. Berenjkoub; R. O. Monico; R. S. Laramee; G. Chen","DOI":"10.1109\/TVCG.2018.2864817","Keywords":"Unsteady flow;correlation study;mutual information","Title":"Visual Analysis of Spatia-temporal Relations of Pairwise Attributes in Unsteady Flow","Keywords_Processed":"unsteady flow;mutual information;correlation study","Keyword_Vector":[0.0918047867,-0.0346960903,-0.0000945739,-0.0595619664,0.0071479069,-0.0258939631,-0.0383114111,-0.0093367795,-0.0403640057,0.0384616453,-0.05682993,0.0462197892,-0.0192408326,0.0202651505,-0.0583314378,0.0304931403,-0.024250532,0.0104929271,-0.042031269,-0.0124901332,0.0487546568,0.0004855951,-0.0389294329,0.027626403,-0.0532739195,0.0571588348,-0.0139440581,0.0485651791,0.0097535252,-0.027111864,-0.0372322674,-0.0538171252,-0.0520555978,-0.0162815678,0.0009221287,-0.0291492096,0.1210803001,-0.0550442652,-0.0447223591,-0.0199837154,0.0432042652,0.046362078,0.020964611,-0.0538475368,-0.0279876061,-0.0047236487,0.0281012246,-0.0346739394,-0.0377627459,-0.0219625195,-0.0286312854,0.0652603309],"Abstract_Vector":[0.1349680355,-0.0814274459,-0.0005045942,0.0059368017,-0.0280797838,-0.008052796,-0.0004829802,-0.0002590464,-0.0115539542,-0.0018211526,-0.0374768678,0.0290338168,0.041471177,-0.0479972586,0.0407236111,-0.0355833239,-0.028961357,-0.0267332336,-0.0314222508,-0.0120546764,-0.0360593208,-0.0273085636,0.0865937372,-0.0516130604,-0.0354715339,0.0101041744,-0.0089736235,-0.0031452987,-0.0621748569,-0.051006986,-0.0544039887,-0.0067885918,0.0700054616,0.0404304534,-0.0149597791,0.0414992935,-0.0437088567,0.0208183176,-0.0451386447,0.0098965124,-0.0065703205,-0.0273092502,0.0602206507,-0.0609288207,0.0088776064,-0.0401069286,0.0312186639,-0.0049839827,0.0113927582,0.0171348261,-0.0076952396,0.0050298461,0.0355176012,0.0201760221,0.0085774063,0.0330625165,-0.0525796388,-0.0254730323,0.0047446125,0.0204418525,0.0079848454,-0.0283359752,0.0292203831,-0.0477142905,-0.029949071,-0.0014076958,0.0139673573,0.0061233695,0.0008834252,-0.0016616725,0.0062349685,0.0033336512,-0.0206877625,0.040314607,0.02164224,-0.0014294979,0.0277424929,-0.0033160934,-0.0041594081,-0.0101777952,-0.0084900319,-0.0014253198,-0.0242709521,0.0141987041,0.0747357182,0.0044521124,0.024254558,0.0378902907,-0.0179496607,-0.0082900167,-0.0235342442,-0.0045099458,-0.0371960111,-0.0113206007,0.0151776827,0.0146348416,-0.0276407099,-0.0071310115,-0.0304970636,0.0282623603,-0.0043046509,0.0271040181,0.0161020891,-0.0093914617,-0.0103349976,-0.0023293441,-0.0158027929,-0.0079570172,0.013345106,0.0198252837,-0.0041608533,-0.0000446343,0.0017616719,0.0005390032,0.0104902248,-0.0042516779]},"81":{"Abstract":"We present a visualization approach for the analysis of CO2 bubble-induced attenuation in porous rock formations. As a basis for this, we introduce customized techniques to extract CO2 bubbles and their surrounding porous structure from X-ray computed tomography data (XCT) measurements. To understand how the structure of porous media influences the occurrence and the shape of formed bubbles, we automatically classify and relate them in terms of morphology and geometric features, and further directly support searching for promising porous structures. To allow for the meaningful direct visual comparison of bubbles and their structures, we propose a customized registration technique considering the bubble shape as well as its points of contact with the porous media surface. With our quantitative extraction of geometric bubble features, we further support the analysis as well as the creation of a physical model. We demonstrate that our approach was successfully used to answer several research questions in the domain, and discuss its high practical relevance to identify critical seismic characteristics of fluid-saturated rock that govern its capability to store CO2.","Authors":"H. Zhang; S. Frey; H. Steeb; D. Uribe; T. Ertl; W. Wang","DOI":"10.1109\/TVCG.2018.2864506","Keywords":"3D volume rendering;bubble visualization;porous media","Title":"Visualization of Bubble Formation in Porous Media","Keywords_Processed":"porous medium;3d volume render;bubble visualization","Keyword_Vector":[0.266980561,0.2833527427,-0.0812549143,0.0040280569,-0.0813807768,0.0318901373,0.1295622857,0.016926097,-0.0353642763,0.0036286464,-0.178265376,-0.0825749473,-0.1563687863,-0.0145526929,-0.1245036687,-0.0274396714,0.2001895978,-0.1626357224,-0.0043148849,-0.0111825445,0.0002114712,0.0275370376,0.060199201,0.1334414991,0.0592759464,0.0236154922,-0.0072327985,0.1857321307,0.0923676301,-0.0051354333,0.1088293783,0.0766362826,0.0004012309,0.0461238441,-0.0386964771,-0.0598157245,0.0522624397,-0.0028896791,0.0677113295,0.0809463977,-0.0810026474,-0.0070056362,-0.0207011187,0.0009973898,0.0225134605,-0.0514745435,0.0197815001,-0.0037014773,0.0528110369,0.029140238,0.0498880094,-0.0499327047],"Abstract_Vector":[0.207504815,0.0137984399,-0.0340025892,-0.0758442593,0.0122337236,0.0132243546,-0.0733280928,-0.0575268667,-0.0731095861,0.0271842694,-0.0743197306,0.0184449372,-0.0543852457,0.0577117126,-0.0249383668,0.0375525015,0.029048918,0.0179880179,-0.0561693348,-0.021232678,-0.0609599211,-0.0375396502,-0.0402709064,-0.0624723419,-0.009669104,0.0666624147,-0.0468452087,-0.0055922169,0.0243551043,-0.0568142668,-0.0008525444,0.0525213934,0.0079765463,-0.0231324114,-0.0382104298,-0.0276117984,0.0749891957,0.0286127223,0.0351451177,0.033931129,0.0079791354,-0.0602874268,0.0445463301,-0.0982870284,-0.0226631814,-0.0014923369,0.0241637888,-0.0117332095,0.0141397654,-0.0056959588,0.010038364,0.0009878474,-0.0177751962,0.0128963252,0.0489584946,0.0088691772,0.0203912424,-0.030494498,-0.0290888603,-0.003258504,0.0368164584,0.0069790873,-0.0139671912,0.0053276485,0.0080302451,0.0024057672,-0.0610205193,0.0276711921,-0.0490772483,-0.0108966522,-0.0236869288,-0.020785601,-0.0219660311,0.0111654795,-0.0244379748,-0.0523309007,-0.0261966966,-0.0133104149,-0.0349551488,-0.0056125909,-0.0103358526,0.0286144228,0.0097395367,0.0263288672,0.0004787756,0.0052256526,-0.0096655457,0.0063686698,-0.0293685561,-0.0206394007,-0.0006549622,0.0061145692,0.007774443,0.0280962311,-0.0016572661,0.0041064802,0.004980052,-0.0303570851,-0.0292377227,0.0033550675,-0.0140987348,0.0009433869,0.0159182895,-0.0040681742,-0.0223720684,0.008890065,0.0050411829,-0.0255447861,0.0150155833,-0.0049005236,-0.0194525522,-0.0214756186,-0.0013197688,-0.0068386992,0.0163863455,-0.0217085522]},"82":{"Abstract":"Introducing motion into existing static paintings is becoming a field that is gaining momentum. This effort facilitates keeping artworks current and translating them to different forms for diverse audiences. Chinese ink paintings and Japanese Sumies are well recognized in Western cultures, yet not easily practiced due to the years of training required. We are motivated to develop an interactive system for artists, non-artists, Asians, and non-Asians to enjoy the unique style of Chinese paintings. In this paper, our focus is on replacing static water flow scenes with animations. We include flow patterns, surface ripples, and water wakes which are challenging not only artistically but also algorithmically. We develop a data-driven system that procedurally computes a flow field based on stroke properties extracted from the painting, and animate water flows artistically and stylishly. Technically, our system first extracts water-flow-portraying strokes using their locations, oscillation frequencies, brush patterns, and ink densities. We construct an initial flow pattern by analyzing stroke structures, ink dispersion densities, and placement densities. We cluster extracted strokes as stroke pattern groups to further convey the spirit of the original painting. Then, the system automatically computes a flow field according to the initial flow patterns, water boundaries, and flow obstacles. Finally, our system dynamically generates and animates extracted stroke pattern groups with the constructed field for controllable smoothness and temporal coherence. The users can interactively place the extracted stroke patterns through our adapted Poisson-based composition onto other paintings for water flow animation. In conclusion, our system can visually transform a static Chinese painting to an interactive walk-through with seamless and vivid stroke-based flow animations in its original dynamic spirits without flickering artifacts.","Authors":"Y. Lai; B. Chen; K. Chen; W. Si; C. Yao; E. Zhang","DOI":"10.1109\/TVCG.2016.2622269","Keywords":"NPR;data-driven;chinese ink painting;tensor field smoothing;naiver-stokes equations","Title":"Data-Driven NPR Illustrations of Natural Flows in Chinese Painting","Keywords_Processed":"tensor field smooth;datum drive;NPR;naiver stoke equation;chinese ink painting","Keyword_Vector":[0.0627366405,0.0307928979,0.0711327004,0.0104432704,-0.0356638744,-0.0712152433,0.0183176212,0.0202278901,-0.1023454388,0.0502473449,0.0601424676,-0.0033903402,0.0429670494,0.0223156231,0.0648233084,-0.0596415011,-0.0071276519,0.0573252957,0.0147768183,0.0217976431,0.0274742686,0.025083647,-0.0371350934,0.0682980935,0.0417013121,0.0164507235,-0.005444878,0.0030901566,-0.0131618423,-0.0128748941,0.0410747754,0.0572194827,-0.0325776971,0.0199757688,-0.0134615975,-0.0085584387,-0.0005581435,0.032530527,-0.0656680721,-0.0699706652,0.0717126087,-0.0468108616,-0.0383816373,0.0442210081,0.0032926638,-0.0295023199,-0.0029807074,-0.0258966288,-0.0111775099,-0.0019969985,0.0056503588,0.0571628975],"Abstract_Vector":[0.1888822547,0.0685994183,0.09994212,-0.0073145993,-0.0047791071,0.0554433157,-0.1070484245,0.0596676726,0.0039322479,-0.0158439623,-0.0644465716,0.0374258274,-0.0396045565,-0.016389897,-0.0189249828,-0.0496351168,-0.0401355846,0.0212877536,-0.0455849292,-0.0282499634,-0.0394640564,0.0335729636,-0.0887791541,0.048242646,0.0223063981,-0.0318968003,0.0014438818,0.010388456,0.0524656496,0.0531773104,0.0392642746,0.0364994372,-0.0220900362,0.0268687327,-0.0062998957,0.018288503,-0.0471158086,-0.0789438849,-0.0005486685,-0.0014401717,-0.0254309083,0.0155495064,0.0755093898,-0.0217286282,-0.0280296242,-0.0292912479,0.0302692815,0.0512976925,-0.0265359407,0.0087980202,0.0581661554,-0.0372253986,-0.0304295818,0.0585159014,-0.0232651069,0.01370262,0.0433376235,-0.0492582854,-0.0151883295,-0.0524731332,-0.0066505572,-0.0136172124,0.003872763,-0.0196668735,0.0210530452,-0.0281717193,-0.0150558844,0.0006985221,0.0234672332,-0.0047544809,-0.0206564032,-0.0710319077,-0.009068992,0.087151381,-0.0263100217,0.0297154168,-0.0107458623,-0.0084502521,0.0029942796,0.0349210665,0.0080583844,-0.0508506738,-0.0258315661,0.0490893332,0.0075093829,-0.0554972678,-0.0322438651,0.0139080907,0.0473741162,-0.0006563667,-0.0026800429,-0.0764216718,-0.0442835688,0.0268096733,0.0576722537,-0.0753085683,0.0018176853,0.0246856934,0.0646258912,-0.0439786074,-0.0153145045,0.0037607264,-0.018058044,0.0124014487,0.0015870651,-0.0134490633,0.0422873038,-0.0143277688,0.0197618175,-0.0516139282,0.0486360149,0.0027576504,0.0013938154,0.0365212195,0.0038061257,-0.0581259229]},"83":{"Abstract":"Collecting sensor data results in large temporal data sets which need to be visualized, analyzed, and presented. One-dimensional time-series charts are used, but these present problems when screen resolution is small in comparison to the data. This can result in severe over-plotting, giving rise for the requirement to provide effective rendering and methods to allow interaction with the detailed data. Common solutions can be categorized as multi-scale representations, frequency based, and lens based interaction techniques. In this paper, we comparatively evaluate existing methods, such as Stack Zoom [15] and ChronoLenses [38], giving a graphical overview of each and classifying their ability to explore and interact with data. We propose new visualizations and other extensions to the existing approaches. We undertake and report an empirical study and a field study using these techniques.","Authors":"J. Walker; R. Borgo; M. W. Jones","DOI":"10.1109\/TVCG.2015.2467751","Keywords":"Time-series Exploration;Focus+Context;Lens;Interaction Techniques;Time-series Exploration;Focus+Context;Lens;Interaction Techniques","Title":"TimeNotes: A Study on Effective Chart Visualization and Interaction Techniques for Time-Series Data","Keywords_Processed":"Focus Context;time series Exploration;interaction technique;Lens","Keyword_Vector":[0.1179884218,-0.0425659196,-0.0098411197,-0.0335861996,0.0192421664,-0.0625898871,-0.0154238232,-0.0130793486,-0.0290201821,-0.0198896902,-0.0519679438,0.0608074742,-0.0295902813,-0.0129816642,-0.0830347004,0.0070080519,-0.0743955263,0.0237039765,-0.0464481442,-0.0160811688,0.0309790338,0.0328149662,-0.0390979292,-0.0024272465,-0.061404605,0.0244113848,-0.0376506668,0.0945954086,-0.0140041166,0.0165567979,0.0339530861,-0.0488439747,-0.1154407471,-0.0541462702,0.0181467693,-0.0917202113,0.0669417792,0.0643057793,-0.0369978365,0.007146092,0.0315811153,-0.0667065035,-0.005069831,0.1131624771,0.031900966,-0.027020967,0.087822443,0.0323507333,-0.0842413494,0.011260361,-0.0805868835,-0.0369717828],"Abstract_Vector":[0.2292152317,-0.1530463809,0.0044131274,-0.0033958848,-0.1431615581,0.064641397,0.0428990947,0.0483770683,-0.0867539033,-0.0432458857,-0.0440210802,0.0527390882,0.0876567907,-0.0524750573,0.0199173364,-0.0171870472,-0.0465163164,0.0327332575,-0.0263650343,-0.0290681161,0.0239076433,-0.0183887349,0.0089788339,-0.0090106057,-0.0583842928,0.0061940179,-0.008402792,-0.0397158599,-0.0328966455,-0.0249000868,-0.0214063088,0.0043838129,0.0678055981,0.0262554583,-0.0217761031,0.0436874821,0.0266809134,-0.0240197576,-0.0012869477,0.0893981274,0.0013077891,-0.013990909,0.0648020351,-0.0843851672,0.0184110672,-0.0203416187,0.009688928,0.041522994,0.0002308519,0.0392165495,0.0041476682,0.0218674887,0.0471607504,-0.0008710405,0.0164729911,0.0149397132,-0.0232737531,-0.0413829906,-0.0003234441,-0.0164810569,-0.0011653161,-0.061238873,0.0041939418,-0.0566866544,-0.0357062575,-0.0095316375,0.0091565277,-0.0460315632,0.014589539,0.0186460999,-0.007154113,-0.0092804296,0.0063776459,0.021297255,0.0126440295,0.012063867,-0.0395250223,0.0092760401,0.0137913099,0.0215284302,0.0014869298,0.0227060047,-0.0202690241,-0.0112429852,0.0166201035,-0.029961458,0.0235745151,0.0343869785,-0.0782240142,-0.0030118238,-0.0092311592,-0.0118099021,-0.0277785452,-0.0017049527,0.0050539996,0.0108188093,0.0214458092,-0.0300898522,-0.0240103926,0.0248419686,0.0307529005,0.0021596074,0.0103987454,-0.0036690274,0.0087093813,0.0109877296,-0.0256521585,0.0072473752,-0.0202782167,-0.0066030835,-0.0048961222,0.0093217545,0.0193073297,-0.0364167126,0.0409734378,-0.0553267251]},"84":{"Abstract":"In this paper, we present Gaia Sky, a free and open-source multiplatform 3D Universe system, developed since 2014 in the Data Processing and Analysis Consortium framework of ESA's Gaia mission. Gaia's data release 2 represents the largest catalog of the stars of our Galaxy, comprising 1.3 billion star positions, with parallaxes, proper motions, magnitudes, and colors. In this mission, Gaia Sky is the central tool for off-the-shelf visualization of these data, and for aiding production of outreach material. With its capabilities to effectively handle these data, to enable seamless navigation along the high dynamic range of distances, and at the same time to provide advanced visualization techniques including relativistic aberration and gravitational wave effects, currently no actively maintained cross-platform, modern, and open alternative exists.","Authors":"A. Sagrist\u00e0; S. Jordan; T. M\u00fcller; F. Sadlo","DOI":"10.1109\/TVCG.2018.2864508","Keywords":"Astronomy visualization;3D Universe software;star catalog rendering;Gaia mission","Title":"Gaia Sky: Navigating the Gaia Catalog","Keywords_Processed":"3d Universe software;gaia mission;astronomy visualization;star catalog render","Keyword_Vector":[0.0230388788,-0.0082716754,-0.0083893954,0.0027106158,0.0146280656,-0.0022628559,-0.0221116347,-0.0090070124,-0.0279133032,0.0006441826,0.0301309512,0.0251052635,0.0086429042,0.045498895,0.0079998595,0.0212201854,0.0256487,0.011927614,0.0836276159,0.0398450164,-0.0046431331,0.0665697036,0.0377323864,-0.0089328206,-0.0477496078,0.0438510965,0.0020275972,-0.0196428908,0.0242006259,-0.0739543567,0.0177565129,-0.0301541625,0.0189944162,-0.0082158907,0.0169250497,-0.0111223608,-0.0287153518,-0.0040073486,0.0091854359,-0.0030396541,0.0096133809,-0.0032628738,0.0237157833,-0.0139301341,0.0191968485,-0.012704149,-0.0074069115,0.0052828906,0.0071409762,0.0119673752,-0.0102401153,-0.0082710025],"Abstract_Vector":[0.1735214165,0.1099219139,0.0162284021,0.0615422319,-0.1909336711,0.0999697981,-0.0173457438,-0.0206565884,0.0777603992,0.0388042826,-0.0041571693,-0.0057109396,-0.0177178357,0.0106656048,-0.026621078,-0.0191103201,0.0758321402,0.0787288716,-0.0313851457,-0.0156448284,-0.0085354062,0.0333866573,0.1261150015,-0.020799811,-0.0437364814,-0.0496290583,0.0171866223,0.0026054731,0.0355655005,0.0202406071,0.0037567145,-0.0199412627,-0.0116361855,0.0008033705,-0.0309914284,-0.0114530832,0.0048428949,-0.0262178023,-0.0160082887,-0.0059820959,0.0715626367,0.0204800034,-0.0009864707,0.0056717665,-0.0284529797,0.018932785,-0.0177544171,0.0114944905,-0.0091383481,0.0155754212,0.0408831957,0.0195575271,-0.0010139191,0.0145243406,-0.0268437131,0.0104468083,-0.0346082191,0.0230478017,0.0004690984,-0.0276800696,-0.0413029243,-0.0045350933,0.0236395149,0.0012938441,0.0014918538,0.0272499048,0.0115683756,0.0098741867,-0.0399213551,-0.0349687096,0.0625733741,-0.0062731895,-0.0186294646,-0.0045951591,0.0178723513,0.0124013712,0.0139021955,0.0554846885,-0.0094481475,-0.0165220207,0.0568481729,-0.0206308213,-0.008458075,-0.0433497916,-0.0257860373,0.0049036038,-0.0191054967,-0.0331162823,-0.017068328,0.0085434658,0.0158459864,0.0297999954,-0.0358858687,0.0340854537,0.003826956,0.0001930791,0.0279736899,-0.018469019,0.0133126249,0.0106276651,0.0237612962,0.0083984336,0.0075467511,-0.0065509875,-0.03462083,0.028564604,-0.0183672947,0.0423741519,0.017179324,-0.0251761631,0.028177883,0.0175097664,-0.0501247049,0.0081323495,0.0129252485,-0.0012380926]},"85":{"Abstract":"Analyzing large, multivariate graphs is an important problem in many domains, yet such graphs are challenging to visualize. In this paper, we introduce a novel, scalable, tree-table multivariate graph visualization technique, which makes many tasks related to multivariate graph analysis easier to achieve. The core principle we follow is to selectively query for nodes or subgraphs of interest and visualize these subgraphs as a spanning tree of the graph. The tree is laid out linearly, which enables us to juxtapose the nodes with a table visualization where diverse attributes can be shown. We also use this table as an adjacency matrix, so that the resulting technique is a hybrid node-link\/adjacency matrix technique. We implement this concept in Juniper and complement it with a set of interaction techniques that enable analysts to dynamically grow, restructure, and aggregate the tree, as well as change the layout or show paths between nodes. We demonstrate the utility of our tool in usage scenarios for different multivariate networks: a bipartite network of scholars, papers, and citation metrics and a multitype network of story characters, places, books, etc.","Authors":"C. Nobre; M. Streit; A. Lex","DOI":"10.1109\/TVCG.2018.2865149","Keywords":"Multivariate graphs;networks;tree-based graph visualization;adjacency matrix;spanning trees;visualization","Title":"Juniper: A Tree+Table Approach to Multivariate Graph Visualization","Keywords_Processed":"adjacency matrix;visualization;tree base graph visualization;network;span tree;multivariate graph","Keyword_Vector":[0.119390903,-0.0989426599,-0.063362039,0.0124052181,-0.0302364046,-0.0614102852,0.0125397899,-0.0055805515,0.0210824182,-0.0320702356,0.0308482503,-0.0511856205,0.0093314099,-0.0289524518,0.0055887204,-0.0027858881,-0.0151825142,0.0232912189,0.0447868683,-0.0136530785,-0.0226390094,0.0498024498,0.0091062085,0.0046530135,0.0114487211,-0.0059127124,0.0019190291,-0.0204334002,0.0284268941,-0.038601971,0.0609266521,-0.0230833312,-0.0305970177,-0.0524647463,0.0327893619,0.0189924651,0.0337593257,-0.0028124125,0.0156069744,-0.0307310935,-0.0234528397,0.0053677349,-0.0216103525,-0.0044474225,0.0115606876,-0.1168016173,-0.0411333558,-0.0052630588,-0.0602272588,0.0185346755,0.1361563728,-0.0892122992],"Abstract_Vector":[0.1708312488,-0.11803233,0.0152207436,0.0101003603,-0.0533990725,0.0134180779,0.0197515124,0.0247330402,-0.0293749983,-0.0150623279,-0.0146599968,0.0104345179,-0.0030300699,-0.0066038475,-0.0003454544,-0.0047461005,-0.0199684479,-0.0500692795,0.0063114354,-0.015942731,0.0423520071,0.0165767509,-0.014456395,-0.0161332265,-0.0163766249,-0.0185017933,0.0363201524,-0.0106471267,-0.0293463844,-0.0101499125,-0.0289276472,0.010580239,-0.0387430935,0.000249116,-0.0116777133,0.0597535259,-0.01439605,0.0032690217,0.023998716,-0.0093914462,0.032322507,-0.0271167114,0.0195771529,0.0115138819,-0.0488633299,0.0062375823,-0.039083411,-0.0240572383,-0.0003878835,0.0120166029,0.0216200654,0.0019038702,-0.0080982313,-0.0355368934,-0.0251587129,-0.0207571464,0.0076461593,0.003239416,-0.0142228276,-0.0116443037,0.0028369221,-0.0141662179,0.0049084797,-0.010132515,-0.0098695067,-0.0369209375,0.0158844707,-0.0094450535,0.0490165039,-0.017330891,0.024306283,0.0156042854,-0.0076956057,-0.0427350214,-0.0128050528,-0.0032249712,0.0112787012,0.0292726929,0.0075564167,-0.0006588788,0.0018011629,-0.0077490705,-0.0011178264,-0.0185029419,-0.029662602,0.0213056683,0.0015598668,-0.0100970649,0.003003952,-0.051794371,-0.015069659,0.0254619613,-0.0128680229,-0.0204864602,-0.0034002397,-0.0154128944,-0.0467124529,-0.0060537573,-0.0063924185,-0.018649733,-0.0053531529,-0.0114385558,-0.0237963557,0.0238322932,-0.033338149,0.0151524427,-0.0133715237,0.012138088,-0.0092372387,-0.0042336546,-0.0095925171,-0.0342264104,0.0130587755,-0.0010257341,-0.00536589,-0.0058044646]},"86":{"Abstract":"RadViz and star coordinates are two of the most popular projection-based multivariate visualization techniques that arrange variables in radial layouts. Formally, the main difference between them consists of a nonlinear normalization step inherent in RadViz. In this paper we show that, although RadViz can be useful when analyzing sparse data, in general this design choice limits its applicability and introduces several drawbacks for exploratory data analysis. In particular, we observe that the normalization step introduces nonlinear distortions, can encumber outlier detection, prevents associating the plots with useful linear mappings, and impedes estimating original data attributes accurately. In addition, users have greater flexibility when choosing different layouts and views of the data in star coordinates. Therefore, we suggest that analysts and researchers should carefully consider whether RadViz's normalization step is beneficial regarding the data sets' characteristics and analysis tasks.","Authors":"M. Rubio-S\u00e1nchez; L. Raya; F. D\u00edaz; A. Sanchez","DOI":"10.1109\/TVCG.2015.2467324","Keywords":"RadViz;Star coordinates;Exploratory data analysis;Cluster analysis;Classication;Outlier detection;RadViz;Star coordinates;Exploratory data analysis;Cluster analysis;Classification;Outlier detection","Title":"A comparative study between RadViz and Star Coordinates","Keywords_Processed":"RadViz;exploratory datum analysis;classification;star coordinate;outli detection;classication;cluster analysis","Keyword_Vector":[0.0204525477,-0.0049251725,-0.0007322877,-0.0009244788,-0.0065360579,-0.0043611156,0.0120108782,0.0045111112,-0.0031816365,0.0074180637,-0.0047849485,0.0010229968,-0.0170793269,0.0195764132,-0.0055133491,0.006080496,0.0139707092,-0.0295838915,0.0055339643,-0.0052830814,0.0103118612,-0.0208159966,-0.0178776527,0.0420535582,-0.0220431253,-0.0019837253,-0.0011635175,0.0089339172,0.0073417256,-0.0254361865,-0.0080470418,-0.0146208683,-0.0340648807,0.0149482294,0.0106977531,0.015753543,0.0563781355,-0.0181441445,-0.0193374009,0.0222003576,0.0364095533,0.0360714912,0.0290269466,-0.0151316689,-0.0123298824,0.0006335906,-0.004447982,-0.023412098,-0.0594360226,-0.0165105244,-0.0136468369,0.0871437632],"Abstract_Vector":[0.1282171926,0.0341941241,0.0573764432,-0.044499637,0.0114450079,0.031759947,0.0652511171,-0.0238498967,0.0403060137,-0.0207152225,-0.0014705132,0.002426501,-0.0398024094,-0.0061973648,-0.0223054326,0.0119509121,-0.0763158216,0.0492058555,-0.0016932405,-0.0401824589,0.0033099046,-0.0532467722,0.0254042222,0.006303636,0.0518601927,-0.0113097165,-0.0349714929,0.0298691926,0.0092137007,-0.0171705899,0.0109598011,0.0439725332,0.0014270677,0.0524725922,0.0710442136,-0.008559948,0.0434653879,-0.002530956,-0.0205192549,-0.0213380677,0.0121922212,-0.0223554438,0.0261136818,0.0033110943,0.0112046139,0.0348183662,0.0111685705,-0.0127033627,0.0132901369,-0.0438942533,0.0614887732,0.013878454,-0.010828442,0.0083109542,-0.0063207527,0.0033172273,0.0055249605,-0.0083294722,0.0682012697,-0.0076115087,-0.0133211681,0.0007007292,0.0079850754,-0.0253568794,0.0086929805,-0.0204090593,0.0151581465,0.0190931376,-0.007321545,0.0452960594,0.0131439437,-0.0128992605,0.0435849578,-0.0208232614,0.0146948329,0.0105326593,0.0035277618,0.0107226215,0.0071225449,0.0046558746,0.0158636253,-0.0042741617,-0.0106791842,-0.0514008841,-0.0329979634,-0.015966834,0.0217309196,0.0129317915,-0.0088785322,0.0143056434,0.015322996,0.0163089036,0.001234484,-0.0179022246,0.004777923,-0.0211790354,-0.0204226047,-0.0013900187,0.0021360421,0.0041048459,0.025303973,-0.009154985,0.0016253902,-0.0046283367,-0.025418556,0.0001510936,-0.0245769503,-0.0204259667,-0.0022188924,-0.0233269284,0.0059588535,-0.0234268919,-0.0071433391,-0.0009457491,0.023821964,-0.003651673]},"87":{"Abstract":"We present a new method to efficiently generate a set of morphologically diverse and inspiring virtual trees through hierarchical topology-preserving blending, aiming to facilitate designers' creativity production. By maintaining the topological consistency of the tree branches, sequences of similar yet different trees and novel intermediate trees with encouragingly interesting structures are generated by performing inner-species and cross-species blending, respectively. Hierarchical fuzzy correspondences are automatically established between two or multiple trees based on the multi-scale topology tree representations. Fundamental blending tasks including morph, grow and wilt are introduced and organized into a tree-structured blending scheduler, which not only introduces the randomness into the blending procedure but also wisely schedules the tasks to generate topology-aware blending sequences, contributing to a variety of resulting trees that exhibit diversities in both geometry and topology. Most significantly, multiple batches of blending can be executed in parallel, resulting in a rapid creation of a large repository of diverse trees.","Authors":"Y. Wang; X. Xue; X. Jin; Z. Deng","DOI":"10.1109\/TVCG.2016.2636187","Keywords":"Creative modeling;tree modeling;hierarchical topology preservation;and shape blending","Title":"Creative Virtual Tree Modeling Through Hierarchical Topology-Preserving Blending","Keywords_Processed":"tree modeling;hierarchical topology preservation;creative modeling;and shape blending","Keyword_Vector":[0.2598900348,-0.1913153526,-0.0346919893,-0.0372744727,0.0442466674,0.1973033807,0.1016559864,-0.098187175,-0.1296256093,-0.0053224296,-0.0640828435,0.0517075084,0.0633729303,-0.0332371514,-0.078898267,-0.0148620521,-0.096779342,0.0714384558,-0.1154288156,-0.1387670576,0.030270814,0.0266446598,0.0609530371,-0.0920666181,0.0226961024,0.0013396488,-0.020697146,0.0817455787,-0.0678785926,-0.0739062874,-0.0580111549,-0.0657018347,-0.048301011,-0.1105572751,-0.0440570997,-0.108442531,0.068802198,-0.0669986897,-0.071388248,-0.0289570401,0.0454173163,-0.0020361605,0.0732591632,0.0469345425,0.0123163884,-0.0047495459,0.0658626947,0.0175024311,0.1305532701,-0.0182092463,0.1088162541,0.0251381837],"Abstract_Vector":[0.1846004098,-0.1176790745,0.0106751516,-0.0101109211,-0.068354635,0.0045485133,0.0160447382,0.0398033363,-0.0346680731,0.00298016,-0.0087705214,0.0325961812,0.029381104,-0.0185178711,0.0223858566,-0.0224023262,-0.0590502304,-0.0020475608,-0.0159067353,-0.0045682492,0.0243304906,-0.0117279021,-0.0028080223,-0.0306835396,-0.0335221641,0.0537837248,-0.0086273562,0.0092822025,-0.0439646516,-0.0320695332,0.0042577146,-0.0191648243,0.0189765764,0.0029033811,-0.0139654555,-0.0039415949,-0.0090806206,0.0013301352,-0.0252502489,-0.0152001281,-0.0101115137,0.0392126216,0.0364356062,-0.0285065508,-0.0004542323,-0.0300754277,0.0023211342,-0.0170794101,-0.0134771192,0.0440225283,0.0250450964,-0.0020996662,0.0520915645,-0.0293376893,0.0285841147,-0.0224013296,-0.0190864354,0.0312193734,-0.0244644145,-0.0162211402,-0.002692146,-0.0112921464,-0.0405618679,-0.0101462012,-0.0371354545,0.0061741989,-0.0597222801,0.01459677,-0.0481308509,0.0120313912,0.0044018611,-0.018740755,0.0027625354,0.0091929331,0.0393877187,-0.0048802118,0.0129226042,0.0289686144,-0.0159894655,0.007098632,-0.0047312291,0.0016708028,0.0095087304,0.0057334476,-0.0034334881,0.0006429849,0.0209732669,0.0151637885,-0.0303380157,-0.0154856549,-0.0379196835,0.003969372,-0.0077982617,-0.036328528,0.0052615225,0.0139243444,0.0080425307,-0.0333193401,0.0167211093,-0.0029578239,0.0081902891,-0.0034949747,0.0099896524,0.0002552642,-0.0237148177,0.0216274166,0.0027334669,0.0414987213,-0.0253348926,-0.0139287406,-0.034765982,-0.0136254385,-0.0319266056,-0.0078098785,-0.0260764691,-0.0026042514]},"88":{"Abstract":"We discuss touch-based navigation of 3D visualizations in a combined monoscopic and stereoscopic viewing environment. We identify a set of interaction modes, and a workflow that helps users transition between these modes to improve their interaction experience. In our discussion we analyze, in particular, the control-display space mapping between the different reference frames of the stereoscopic and monoscopic displays. We show how this mapping supports interactive data exploration, but may also lead to conflicts between the stereoscopic and monoscopic views due to users' movement in space; we resolve these problems through synchronization. To support our discussion, we present results from an exploratory observational evaluation with domain experts in fluid mechanics and structural biology. These experts explored domain-specific datasets using variations of a system that embodies the interaction modes and workflows; we report on their interactions and qualitative feedback on the system and its workflow.","Authors":"D. L\u00f3pez; L. Oehlberg; C. Doger; T. Isenberg","DOI":"10.1109\/TVCG.2015.2440233","Keywords":"Visualization of 3D data;human-computer interaction;expert interaction;direct-touch input;mobile displays;stereoscopic environments;VR;AR;conceptual model of interaction;interaction reference frame mapping;observational study;Visualization of 3D data;human-computer interaction;expert interaction;direct-touch input;mobile displays;stereoscopic environments;VR;AR;conceptual model of interaction;interaction reference frame mapping;observational study","Title":"Towards An Understanding of Mobile Touch Navigation in a Stereoscopic Viewing Environment for 3D Data Exploration","Keywords_Processed":"visualization of 3d datum;VR;human computer interaction;stereoscopic environment;direct touch input;conceptual model of interaction;observational study;mobile display;ar;interaction reference frame mapping;expert interaction","Keyword_Vector":[0.3005569541,0.2889973326,-0.1951107275,-0.035651905,-0.1075019514,0.1543882564,0.0399606444,-0.0032129157,-0.0223056649,0.0032757111,-0.0370025197,-0.0353194342,0.0173052636,-0.0560633923,-0.055252462,-0.0415358768,-0.0395145888,0.0036620388,-0.0188987907,0.0631276447,-0.0486615433,0.0241825218,0.0331356083,-0.0328330945,0.0001479095,0.0180976345,-0.0728805632,-0.0490555075,-0.0303137993,0.0507095846,0.0167179487,-0.0190149873,0.0354445493,-0.0259646709,-0.0315672169,0.0467149831,0.0568491402,-0.0159788429,0.0142081004,0.0408464598,-0.0443655004,0.020682673,-0.0735680555,0.0277465088,-0.003107922,0.0402791671,0.0184632291,-0.0244251632,0.0117673163,0.036323098,-0.0271160234,-0.0088558539],"Abstract_Vector":[0.2785750493,0.0357251804,-0.1342487969,-0.139991574,-0.0035299448,0.0745259328,-0.0341723757,-0.0338344487,-0.0989716335,-0.0133039284,-0.0100220135,0.0274792383,0.1076969794,0.0035651251,-0.0035816352,0.024684818,-0.0372386294,-0.0520449377,-0.0286494004,-0.0060757962,-0.0509692564,-0.0170454351,-0.0230479188,0.0313731282,0.0275115074,-0.0176989384,0.0639280095,-0.015659135,0.0440562137,-0.0143150582,-0.0278183907,-0.0595115807,0.0360790087,0.0000715698,-0.0158645432,0.034495025,-0.0328010722,-0.0004942692,-0.0071911683,0.0495359394,-0.0378237042,-0.0062828793,0.0434756722,-0.0358152468,-0.0005351219,-0.0412732079,-0.020293911,-0.0380976391,-0.0002454804,-0.0062823423,-0.0267784406,0.0086082112,-0.0021114613,-0.0009634602,-0.051190256,-0.0097370098,0.0416222932,-0.008187335,0.0512783128,0.0415097921,-0.0241079347,0.0312554371,0.0432135348,-0.001027238,0.0363824405,0.0104374443,0.0165964034,-0.0187367514,0.0344771784,0.0028555086,-0.0266550117,0.0300257964,0.0161144442,-0.0008892154,0.0262652297,0.0135865492,-0.0287381167,0.0328745503,0.0311376103,0.013809117,-0.0022303905,0.0139213923,-0.0249321397,0.0356253793,-0.0103557649,0.0246315524,0.015046232,-0.0074163836,-0.0196994159,-0.0161426803,0.0299030964,0.0294180418,0.0451299276,-0.0292518012,-0.0016609423,-0.046472881,0.0237876085,0.0021251784,-0.0177818718,0.0317267797,0.0201504623,-0.0004371867,0.0155150998,-0.0081302029,-0.0048168472,-0.0137724449,-0.01546122,-0.0140095698,-0.0396771811,-0.0033402644,0.0060722065,0.0292950785,-0.0262346417,0.010034166,0.0219504983,-0.006255097]},"89":{"Abstract":"Multi-scale virtual environments contain geometric details ranging over several orders of magnitude and typically employ out-of-core rendering techniques. When displayed in virtual reality systems this entails using a 7 degree-of-freedom (DOF) view model where view scale is a separate 7th DOF in addition to 6DOF view pose. Dynamic adjustment of this and other view parameters become very important to usability. In this paper, we evaluate how two adjustment techniques interact with uni- and bi-manual 7DOF navigation in DesktopVR and a CAVE. The travel task has two stages, an initial targeted zoom and a detailed geometric inspection. The results show benefits of the auto-adjustments on completion time and stereo fusion issues, but only in certain circumstances. Peculiar view configuration examples show the difficulty of creating robust adjustment rules.","Authors":"I. Cho; J. Li; Z. Wartell","DOI":"10.1109\/TVCG.2017.2668405","Keywords":"Multi-scale virtual environment;3D user interface;7DOF user interaction;virtual reality;view adjustment","Title":"Multi-Scale 7DOF View Adjustment","Keywords_Processed":"virtual reality;3d user interface;view adjustment;7dof user interaction;multi scale virtual environment","Keyword_Vector":[0.0977783138,-0.047886331,-0.0186012366,0.0563300114,0.0983382145,-0.0034722364,-0.0599104694,0.0987790088,-0.0112808978,0.0456284886,0.000898045,0.1158413401,0.0277604886,0.0659334708,0.0348765027,0.0504044488,0.0584468341,-0.0939763951,0.1554536674,0.1843085625,0.039201843,0.0461249562,0.0560252168,-0.085512933,0.0482155086,0.0819041276,-0.0155919075,-0.0570498436,0.0012455235,-0.0970853846,0.0073052364,0.0067193812,-0.019235795,-0.0574971288,-0.0135073654,-0.0323711673,-0.0489886266,0.068332865,0.0500094781,0.0370629083,0.0166965502,-0.0081706614,0.0123368229,0.0099458878,-0.010077818,-0.0345595303,-0.0496066297,-0.0028179217,0.0516617127,0.0244685025,-0.0210226435,-0.0020576364],"Abstract_Vector":[0.2433446623,-0.1739290222,-0.0257479058,0.0071460309,-0.0319581153,0.0239060073,-0.003893195,-0.0004929313,-0.0102265003,-0.0076448158,-0.0416500186,0.0113411882,0.0603606571,-0.0281546155,0.0239462922,0.1130816258,-0.0067268356,0.0059160322,0.009493987,0.0152225423,-0.0320767211,-0.0226485274,-0.0161958961,0.0391001944,0.0195354729,-0.081414161,-0.0037131111,0.0215796324,-0.0137331815,0.0138554488,-0.0109815653,0.0179681958,-0.0008473038,-0.0344645698,0.0490706565,0.0072276831,0.0184813689,0.0168104581,-0.0368215442,-0.0292494923,0.0079445698,-0.0360608426,0.0608397672,-0.0179796178,0.01782805,-0.0139206269,0.0019649043,-0.0413997644,-0.0150559348,0.0065081134,-0.0046467827,0.0520423951,-0.0267954555,0.0625400404,0.0061838846,0.0269481737,0.0308968658,-0.0097557198,-0.0517960286,0.009420463,0.0341904338,0.0545737631,0.0173927581,-0.0013647467,-0.0166519099,0.0187650286,-0.0011156055,-0.0220469412,-0.0273539601,0.0054713161,0.013234322,0.0172244355,0.025961007,0.0950120035,0.0056044236,-0.0284863197,-0.0506048272,0.0057746031,0.0062220979,-0.0268611229,0.0443143606,0.0015228694,0.0413852823,-0.0000783615,-0.0084997011,0.0601163476,-0.0210971422,0.0319326898,-0.0128808009,0.0161003391,-0.0207634944,-0.0240548164,-0.0320347597,-0.0075887333,-0.0149196633,0.0091757514,0.0099782427,-0.0271062071,-0.0270522939,0.0004988444,-0.0177931232,-0.0019466067,0.0159380897,0.04497906,-0.0487479746,-0.0067348426,0.0077007594,-0.0263543828,0.0126502372,-0.0020108263,0.0049743201,0.0074143717,-0.0137051209,-0.024130374,0.0176566998,0.0068242667]},"9":{"Abstract":"Kinetic approaches, i.e., methods based on the lattice Boltzmann equations, have long been recognized as an appealing alternative for solving incompressible Navier-Stokes equations in computational fluid dynamics. However, such approaches have not been widely adopted in graphics mainly due to the underlying inaccuracy, instability and inflexibility. In this paper, we try to tackle these problems in order to make kinetic approaches practical for graphical applications. To achieve more accurate and stable simulations, we propose to employ the non-orthogonal central-moment-relaxation model, where we develop a novel adaptive relaxation method to retain both stability and accuracy in turbulent flows. To achieve flexibility, we propose a novel continuous-scale formulation that enables samples at arbitrary resolutions to easily communicate with each other in a more continuous sense and with loose geometrical constraints, which allows efficient and adaptive sample construction to better match the physical scale. Such a capability directly leads to an automatic sample construction which generates static and dynamic scales at initialization and during simulation, respectively. This effectively makes our method suitable for simulating turbulent flows with arbitrary geometrical boundaries. Our simulation results with applications to smoke animations show the benefits of our method, with comparisons for justification and verification.","Authors":"W. Li; K. Bai; X. Liu","DOI":"10.1109\/TVCG.2018.2859931","Keywords":"multi-resolution fluid simulation;lattice Boltzmann model;adaptive refinement","Title":"Continuous-Scale Kinetic Fluid Simulation","Keywords_Processed":"multi resolution fluid simulation;lattice Boltzmann model;adaptive refinement","Keyword_Vector":[0.0935343858,-0.0007474815,-0.0188323776,-0.0415636755,0.0240485945,-0.0620015492,-0.082437104,-0.0637064882,-0.1042901964,-0.0093764669,-0.0323937859,0.0976655208,0.0423119591,-0.0004730125,0.0630044534,-0.0003041715,0.174609686,0.1056336054,0.0439136533,0.0675846204,0.058357981,0.0375831367,-0.1441955383,0.0927773285,0.0551148247,0.0948998087,-0.1030012528,0.0339949834,-0.0277492127,0.1215625351,-0.0433859544,0.0372363248,0.0789244511,-0.0603563257,-0.0349298226,-0.0679915495,-0.0743236977,-0.0779415973,-0.0705860781,-0.0006666752,0.0196775784,-0.0193768874,-0.1100384706,0.0599264446,0.0025754028,-0.0583833716,0.0290763996,-0.0355129629,-0.023621794,-0.0362588803,0.0109329284,-0.0198980668],"Abstract_Vector":[0.1541706781,0.0532430456,-0.0101728123,0.0528217845,0.0089944537,0.0060140362,0.0270030198,0.0159984442,0.0120865521,0.0298851818,-0.0173852795,0.0655683884,-0.0305429031,-0.0318365054,0.0170826077,0.0480504453,-0.0166215526,0.0464123884,-0.0483539898,-0.0694666755,0.0127648995,0.0188881017,-0.0078704555,0.0540239236,-0.0298233514,0.0037931536,0.0400047435,-0.0037202921,0.0012463383,0.0889767594,0.028854802,-0.0276329277,-0.0083593253,-0.0526889824,-0.0015831611,-0.0118984387,-0.0152602994,0.0061073676,0.0062383329,0.04996983,0.0186883012,-0.0134669719,-0.0032296398,-0.0035074777,-0.0222783059,-0.0220815527,0.0356888652,0.027106903,0.0155780146,-0.0506533656,-0.00387189,-0.0442920382,0.0212335039,-0.0355411433,-0.0022682565,-0.0293083789,0.0362814794,0.0112020832,0.0009959195,-0.0017339401,-0.0000266554,0.0000458591,0.0214474945,0.0268873278,-0.0244358951,-0.0078030795,0.0048745679,-0.0091656335,0.0264110459,0.0206847095,0.0019515152,0.0110942973,0.0094658353,0.0543861134,-0.0214825917,-0.0284029428,0.0069785472,0.0079923034,-0.0077261352,0.0136575476,-0.0126305992,0.0345729588,0.0469220145,-0.0244421725,-0.0219871391,0.0031046101,0.0358968881,0.0104883896,0.0166869077,0.0180194509,-0.0395464186,-0.0089380767,-0.0279762948,-0.0269171028,0.0022118797,0.0086573989,0.0328578317,0.0161198032,-0.0046093851,0.0046030771,-0.0169026263,-0.0121826017,-0.0007199015,-0.0120003644,-0.0095293574,-0.0088288606,-0.0020885646,-0.01472637,0.0156293272,0.0085871085,0.0159695361,-0.0089887566,0.028957809,-0.0051698578,0.0030814367,-0.0123361743]},"90":{"Abstract":"We present a novel method to generate plausible diffraction effects for interactive sound propagation in dynamic scenes. Our approach precomputes a diffraction kernel for each dynamic object in the scene and combines them with interactive ray tracing algorithms at runtime. A diffraction kernel encapsulates the sound interaction behavior of individual objects in the free field and we present a new source placement algorithm to significantly accelerate the precomputation. Our overall propagation algorithm can handle highly-tessellated or smooth objects undergoing rigid motion. We have evaluated our algorithm's performance on different scenarios with multiple moving objects and demonstrate the benefits over prior interactive geometric sound propagation methods. We also performed a user study to evaluate the perceived smoothness of the diffracted field and found that the auditory perception using our approach is comparable to that of a wave-based sound propagation method.","Authors":"A. Rungta; C. Schissler; N. Rewkowski; R. Mehra; D. Manocha","DOI":"10.1109\/TVCG.2018.2794098","Keywords":"sound propagation;diffraction;dynamic environments;spatial presence","Title":"Diffraction Kernels for Interactive Sound Propagation in Dynamic Environments","Keywords_Processed":"spatial presence;sound propagation;diffraction;dynamic environment","Keyword_Vector":[0.1194509037,-0.1057125993,-0.1267452718,0.0951064527,-0.0456562059,-0.1155626266,0.0578261479,-0.0143275181,-0.0260030025,-0.0105239528,0.0040637149,-0.0130170073,-0.0078539801,0.1716275952,-0.0564245531,-0.0308628566,0.0074046677,-0.0140059582,-0.0019569923,0.0274991243,-0.0155753274,-0.0608150916,-0.0597352664,0.0553953092,0.0259349198,-0.1199861569,-0.0114611225,-0.0574609728,-0.0052030588,-0.0391493844,0.0780254383,-0.0880709322,-0.041740362,0.0870099892,0.122793642,0.0210216447,-0.0343032372,-0.0705392242,-0.0226710761,0.1025354643,-0.0298188483,-0.0326623056,-0.0889546124,0.0271205709,-0.0305491879,0.0299598064,-0.0179235668,0.0018922623,0.0397830402,-0.0078463142,0.0155005433,-0.0074658019],"Abstract_Vector":[0.1966812674,-0.0603799397,-0.0170818982,0.0781123222,0.0488970364,0.0166654631,0.0045019475,-0.0214852251,-0.0267531959,0.0195963143,-0.03201269,0.0965322315,0.0135198152,-0.0373216301,-0.0180025668,0.0806676157,0.0234310971,0.0670968222,0.0658155835,0.0180637775,0.1116286362,0.0681221753,0.0704213646,-0.0252424402,0.0836839939,0.081940928,-0.070550717,-0.0553085389,-0.0666736632,-0.0324907143,-0.041382716,-0.0180003882,-0.0820408695,0.0481057827,-0.1254703894,-0.1087120765,-0.01602225,-0.0746158044,0.0840614469,0.0167893848,-0.0013598536,-0.0114087889,0.1134242612,-0.0443560328,0.0444451335,0.1283038349,0.0027953145,-0.026413186,-0.0548427542,-0.059551257,0.0358824812,0.0188778274,-0.0082092544,0.0210525928,0.0190869963,0.0304428362,-0.0092177602,0.039459006,-0.002735857,0.0098405856,0.0027509655,-0.0519371472,0.0022281837,0.0007442426,0.0142810352,0.0673154069,-0.0351454869,0.014706453,0.0385984862,0.0701369401,-0.004296887,0.0150048612,-0.0418029085,-0.0330302546,0.0121532047,0.0000434813,-0.0344835839,-0.0075082414,-0.0032777554,0.0180065456,0.0236034917,0.0246984293,-0.0273852304,-0.0265120468,0.0461907342,0.0202739135,0.0302593627,0.0197491352,0.0351361322,-0.0430904307,0.0509068713,-0.0431222844,0.0132765718,0.0265229189,-0.0046034974,-0.013738738,-0.0129730765,-0.0401043832,0.0149350022,0.0056248794,0.0161806276,-0.0143327759,0.0155745418,-0.0189581867,0.0047667566,-0.0614540363,-0.0059258717,-0.0394593552,-0.0093012628,0.0125417911,0.0251994729,0.0380425961,0.0095032925,0.0159477116,0.0117403649,0.0096970435]},"91":{"Abstract":"The analysis and visualization of nucleic acids (RNA and DNA) is playing an increasingly important role due to their fundamental importance for all forms of life and the growing number of known 3D structures of such molecules. The great complexity of these structures, in particular, those of RNA, demands interactive visualization to get deeper insights into the relationship between the 2D secondary structure motifs and their 3D tertiary structures. Over the last decades, a lot of research in molecular visualization has focused on the visual exploration of protein structures while nucleic acids have only been marginally addressed. In contrast to proteins, which are composed of amino acids, the ingredients of nucleic acids are nucleotides. They form structuring patterns that differ from those of proteins and, hence, also require different visualization and exploration techniques. In order to support interactive exploration of nucleic acids, the computation of secondary structure motifs as well as their visualization in 2D and 3D must be fast. Therefore, in this paper, we focus on the performance of both the computation and visualization of nucleic acid structure. We present a ray casting-based visualization of RNA and DNA secondary and tertiary structures, which enables for the first time real-time visualization of even large molecular dynamics trajectories. Furthermore, we provide a detailed description of all important aspects to visualize nucleic acid secondary and tertiary structures. With this, we close an important gap in molecular visualization.","Authors":"N. Lindow; D. Baum; M. Leborgne; H. Hege","DOI":"10.1109\/TVCG.2018.2864507","Keywords":"Ribonucleic acids;DNA;RNA;secondary & tertiary structures;interactive rendering;ray casting;brushing & linking","Title":"Interactive Visualization of RNA and DNA Structures","Keywords_Processed":"dna;ray cast;brush linking;interactive render;ribonucleic acid;RNA;secondary tertiary structure","Keyword_Vector":[0.0953093008,0.0192070074,-0.039505869,-0.0022183945,0.0508413441,-0.0255438776,-0.0703687903,0.0506257383,-0.133596446,-0.0287223429,-0.093201019,0.0055213555,0.0765686746,0.0197871962,0.1445089467,-0.0607539493,0.1424792605,0.1999739582,0.0124184709,0.055309518,0.115552877,-0.0130724259,-0.1244564749,0.1000697269,0.0418162411,0.0365996687,-0.0623027589,-0.0182097071,0.0108811526,0.0540590701,0.0087802924,0.0979463565,0.0510526796,0.0565873742,-0.0048850905,-0.0980339274,-0.1124851549,0.0706587419,-0.0492511078,-0.0799992122,0.0419127756,-0.0428346104,-0.0584188689,-0.0648574045,-0.0116115468,-0.0269257058,0.0254061944,-0.137695437,-0.0211979755,0.0310548735,0.0469314416,-0.0471716557],"Abstract_Vector":[0.1978931234,0.1216134673,0.0703910292,0.0807670933,-0.1320810633,0.0080863781,0.0056773387,-0.0467960714,0.0125896617,0.0333415896,0.0575209362,0.035013563,-0.0144645349,0.0262987878,-0.0284754619,-0.0101032366,-0.0062123207,0.0056369819,-0.0271219287,-0.0639039789,0.0350881812,-0.0170822174,0.021576687,0.0373754584,0.0230596923,-0.0337130601,0.0324078098,-0.0004972712,0.0096998488,-0.0407285845,-0.001491213,-0.0138005374,-0.0034596724,0.0111620564,0.0145061362,-0.0115148211,-0.0255189003,-0.0258990128,0.0037572928,0.0004814795,0.0055636773,-0.0065186487,0.0247032558,0.0271197528,-0.0183864285,-0.0429487571,0.0235925956,0.039135382,-0.0014877145,0.0091789178,-0.015807552,-0.0185068449,0.0053064525,-0.0021912232,0.0134576714,0.0258547364,0.0437823046,-0.0039728691,0.0179169601,0.018327548,0.0135229378,-0.0139465911,0.0397663921,-0.016811344,-0.0059185569,-0.010827619,0.02211428,0.0054604509,-0.0309370431,0.030125039,-0.0115298937,0.0252107267,0.0106531015,-0.037572459,0.003096833,-0.010968429,0.0163688934,-0.0230886749,0.0274561075,-0.0125894324,0.0196842222,0.0105461729,0.0461897819,-0.0012943903,-0.0301792305,-0.0129207561,-0.0191634899,0.0078213727,0.0489532123,-0.0130089359,0.0389598064,-0.0222772505,0.0000004838,-0.0344971674,0.000624294,0.0140626686,0.0039434009,-0.0149408607,0.005871051,-0.0394747627,-0.0336811538,-0.0350626122,0.0137583644,0.0297771088,-0.0087416802,-0.0163891246,0.0069041794,0.0004813728,-0.0068997658,-0.0345607171,-0.0160676598,-0.0243005478,0.0146318539,0.0256511395,0.0412630602,-0.0287995363]},"92":{"Abstract":"We propose to generate novel animations from a set of elementary examples of video-based surface motion capture, under user-specified constraints. 4D surface capture animation is motivated by the increasing demand from media production for highly realistic 3D content. To this aim, data driven strategies that consider video-based information can produce animation with real shapes, kinematics and appearances. Our animations rely on the combination and the interpolation of textured 3D mesh data, which requires examining two aspects: (1) Shape geometry and (2) appearance. First, we propose an animation synthesis structure for the shape geometry, the Essential graph, that outperforms standard Motion graphs in optimality with respect to quantitative criteria, and we extend optimized interpolated transition algorithms to mesh data. Second, we propose a compact view-independent representation for the shape appearance. This representation encodes subject appearance changes due to viewpoint and illumination, and due to inaccuracies in geometric modelling independently. Besides providing compact representations, such decompositions allow for additional applications such as interpolation for animation.","Authors":"A. Boukhayma; E. Boyer","DOI":"10.1109\/TVCG.2018.2831233","Keywords":"Character animation;3D video;multiview reconstruction;video-based animation;4D modeling;4D performance capture","Title":"Surface Motion Capture Animation Synthesis","Keywords_Processed":"4d modeling;character animation;multiview reconstruction;3d video;4d performance capture;video base animation","Keyword_Vector":[0.0649686863,0.0365035763,0.1252463168,0.0810708321,0.0001290507,-0.063372723,0.0453652174,-0.0478412578,0.0031704736,0.1293373117,0.0000272203,-0.0378744985,0.0267109578,-0.01716396,-0.0430911966,-0.0071320229,-0.0215572162,0.0583499044,0.047707465,0.0065639572,-0.034350641,-0.014780809,-0.0245234125,0.0662078784,0.0213950835,0.0246378163,0.0390150919,0.0090543163,-0.0334587706,-0.0832760919,0.0014966404,0.0059956351,0.0629939333,-0.0213522343,-0.0461530006,0.0607884614,0.0031027662,-0.0109410029,0.012672301,0.0747256203,0.0208877933,0.0526527962,-0.0094256941,-0.0293448884,0.1217865457,0.0770396722,0.03200241,0.0016549802,-0.0044039718,0.0882218331,0.089159538,0.0031830674],"Abstract_Vector":[0.1864047211,0.0627507563,0.0788429893,-0.002479518,0.0160544089,-0.0222476771,0.0133293337,0.0559066277,-0.0124766342,0.0240712154,-0.0850349054,-0.0044094992,0.0075310604,-0.0249956031,0.0341675459,0.0256387283,0.0791542318,0.0029185499,-0.0066185745,-0.041082214,0.043226093,-0.0070122846,0.006953202,-0.0324608616,0.0062277195,-0.0362349476,-0.0238237114,0.0612218305,0.0263221785,0.0619818459,0.0191097205,-0.0006019098,-0.0513426133,0.0378054884,-0.0117282328,-0.0484754454,-0.028097217,0.004336139,0.0097100794,0.0115066741,-0.0265118301,0.0216092566,0.0028161322,-0.045126126,0.0312904911,0.0266286358,-0.0344495127,0.0415480272,-0.0081433344,0.0156422684,-0.0490342154,0.0147078547,-0.0344509288,0.0022628904,0.0279163155,0.0259990094,0.0164182383,-0.0415843123,0.0226822065,0.0523891995,0.0140697722,0.0097046361,-0.0856406454,0.0200904751,0.0011529606,0.0019835936,-0.0072913081,-0.0385638302,-0.0252501542,-0.0210521134,0.0309185085,-0.0165233223,0.0134918989,-0.0258466298,-0.0034165428,-0.010246444,0.0050207397,0.0610989653,0.021781386,-0.0242159678,-0.0015266929,0.0448960339,-0.0013436554,0.0184101714,0.0280871346,-0.0001483466,0.0017741508,0.0257448845,-0.053068321,-0.0126498063,-0.0006571393,0.0078525783,-0.0139993063,-0.0057558553,-0.0049031534,0.0432800821,-0.0378277381,-0.0215463964,0.0465593173,-0.0155921853,-0.0243767459,-0.042461621,0.0327270233,-0.0097326065,-0.0279642674,0.0040641218,0.0166742113,-0.0085051821,0.0227783312,0.0156845319,0.0041441182,0.0174469456,-0.0140871607,0.0004916066,-0.0055335546,-0.0717535814]},"93":{"Abstract":"We present design and impact studies of visual feedback for virtual grasping. The studies suggest new or updated guidelines for feedback. Recent grasping techniques incorporate visual cues to help resolve undesirable visual or performance artifacts encountered after real fingers enter a virtual object. Prior guidelines about such visuals are based largely on other interaction types and provide inconsistent and potentially-misleading information when applied to grasping. We address this with a two-stage study. In the first stage, users adjusted parameters of various feedback types, including some novel aspects, to identify promising settings and to give insight into preferences regarding the parameters. In the next stage, the tuned feedback techniques were evaluated in terms of objective performance (finger penetration, release time, and precision) and subjective rankings (visual quality, perceived behavior impact, and overall preference). Additionally, subjects commented on the techniques while reviewing them in a final session. Performance wise, the most promising techniques directly reveal penetrating hand configuration in some way. Subjectively, subjects appreciated visual cues about interpenetration or grasp force, and color changes are most promising. The results enable selection of the best cues based on understanding the relevant tradeoffs and reasonable parameter values. The results also provide a needed basis for more focused studies of specific visual cues and for choosing conditions in comparisons to other feedback modes, such as haptic, audio, or multimodal. Considering results, we propose that 3D interaction guidelines must be updated to capture the importance of interpenetration cues, possible performance benefits of direct representations, and tradeoffs involved in cue selection.","Authors":"M. Prachyabrued; C. W. Borst","DOI":"10.1109\/TVCG.2015.2456917","Keywords":"Virtual grasping;virtual reality;visual feedback;Virtual grasping;virtual reality;visual feedback","Title":"Design and Evaluation of Visual Interpenetration Cues in Virtual Grasping","Keywords_Processed":"visual feedback;virtual reality;virtual grasping","Keyword_Vector":[0.1214085963,-0.0983784232,-0.083881856,0.11141141,-0.0245916264,0.1091804952,-0.0319939138,-0.014847675,-0.0719977302,0.0280194733,-0.0140861909,0.049745437,0.0755239948,0.1052605664,-0.0357945337,-0.0532899555,0.0644676271,-0.02792686,0.0277172955,-0.008755896,-0.0326890386,-0.1501521504,-0.0237314904,-0.0433640847,0.0233784003,-0.0153024254,0.0130827314,-0.0482827949,-0.0195820856,-0.0027772062,-0.0187649303,0.0037223052,-0.0286394541,-0.0293325769,-0.0445422421,-0.0174549012,-0.0430789032,0.0141323804,-0.0210218505,-0.0044797932,-0.0410730453,-0.0428486442,-0.0421910753,-0.001891144,-0.0254150273,0.0383622168,-0.0440883051,0.0112817212,0.0692840728,0.0471777345,-0.0503784617,0.0661266552],"Abstract_Vector":[0.1785330391,-0.0866877573,-0.0124806843,-0.0284311574,-0.0011672286,-0.0891135118,-0.0505620127,-0.0079656092,-0.0174986127,-0.0293466413,0.029783247,-0.0585867667,-0.0398159778,-0.0110182762,-0.027670584,0.0033882124,-0.0041582664,0.0057188988,-0.0213718569,0.0015336795,0.0374742169,-0.0130246044,-0.0112167708,-0.0469650873,-0.0598483722,-0.0170070369,-0.0097591575,0.0209968095,0.0273541393,-0.0141279455,0.006346953,0.0044436863,-0.0422949227,-0.0110931919,0.0402737229,0.0016842287,0.0063386981,0.0001356326,0.0731895551,0.0356231505,0.0098136352,-0.0424504114,-0.013811125,0.0367474258,0.0661992049,0.0095753554,0.0112522059,-0.0439822701,0.0349163118,-0.0002062076,0.0421097279,-0.0052243688,0.0229059252,-0.016361207,0.0459967491,-0.1272645645,-0.0090085946,0.0430461186,0.0017778477,0.0363964474,-0.0683053597,-0.0466350963,0.0302134804,0.0456349057,0.0762804858,0.085056154,0.0929585394,0.0087184385,-0.0234739526,0.0118370899,0.032838463,-0.0078908699,-0.0597614366,0.0424435539,0.0219909408,-0.0196303076,0.018607649,-0.035326235,0.029688624,-0.0832869354,-0.0279077083,-0.0020857976,0.0026114285,0.0259858147,0.0203144315,-0.0408177729,0.0240803884,-0.0097289478,-0.0625126415,-0.0433669241,-0.0104014374,0.0259108035,-0.0216026207,-0.0285283832,-0.000143745,-0.0251033595,-0.0277624235,0.0295339367,0.0209340871,-0.0217502553,0.0372044812,-0.0186052527,-0.007473395,0.0119181047,0.0237871385,-0.0172587684,-0.0220738637,-0.0438762861,-0.0186667765,0.0014010339,-0.0145170869,0.0091085355,-0.0177834945,-0.0019432737,-0.0189611514,-0.0351522496]},"94":{"Abstract":"Recent success in deep learning has generated immense interest among practitioners and students, inspiring many to learn about this new technology. While visual and interactive approaches have been successfully developed to help people more easily learn deep learning, most existing tools focus on simpler models. In this work, we present GAN Lab, the first interactive visualization tool designed for non-experts to learn and experiment with Generative Adversarial Networks (GANs), a popular class of complex deep learning models. With GAN Lab, users can interactively train generative models and visualize the dynamic training process's intermediate results. GAN Lab tightly integrates an model overview graph that summarizes GAN's structure, and a layered distributions view that helps users interpret the interplay between submodels. GAN Lab introduces new interactive experimentation features for learning complex deep learning models, such as step-by-step training at multiple levels of abstraction for understanding intricate training dynamics. Implemented using TensorFlow.js, GAN Lab is accessible to anyone via modern web browsers, without the need for installation or specialized hardware, overcoming a major practical challenge in deploying interactive tools for deep learning.","Authors":"M. Kahng; N. Thorat; D. H. Chau; F. B. Vi\u00e9gas; M. Wattenberg","DOI":"10.1109\/TVCG.2018.2864500","Keywords":"Deep learning;information visualization;visual analytics;generative adversarial networks;machine learning;interactive experimentation;explorable explanations","Title":"GAN Lab: Understanding Complex Deep Generative Models using Interactive Visual Experimentation","Keywords_Processed":"interactive experimentation;generative adversarial network;deep learning;machine learning;explorable explanation;visual analytic;information visualization","Keyword_Vector":[0.0737690753,-0.0152838266,0.0064637364,-0.0325675959,0.032375863,-0.0356033162,-0.0539068644,0.0189940004,-0.0630302082,-0.0445904977,0.0098372569,0.05878515,-0.022306038,0.0126327077,0.0374304302,0.087067491,-0.0369408825,0.001066345,0.1508631501,-0.0114532934,-0.0217963848,-0.0468855856,0.1085442135,0.0471043997,-0.009771314,0.0743531906,-0.0463073044,-0.0082022157,-0.0187696828,-0.0283328638,0.0500212416,-0.0184558535,0.0029958931,0.0156304562,-0.0281514781,-0.0058147951,-0.0963812616,-0.0356769658,0.0167950695,-0.0178843277,-0.0227714657,0.0209025942,-0.0329702967,-0.0619576391,-0.0049540487,-0.0111200749,0.0887663484,0.0671880557,-0.0381956321,0.0149141771,0.0002848619,0.0489961282],"Abstract_Vector":[0.1863946835,-0.0756367943,0.0205866438,-0.0046862554,-0.0259559886,0.0517669469,-0.0059777014,0.0256411466,0.0813128059,0.0138262603,-0.087175288,0.0770465292,-0.0259075017,0.0348394315,0.0562455433,-0.0148977023,-0.0120955267,-0.0510067121,-0.0101878993,0.0098868536,-0.0107861497,-0.0309162146,0.0302226278,-0.0023661636,0.0006576657,-0.0307312738,0.0021240054,-0.0036493452,0.0308820724,0.0674333008,0.0404559463,0.0473754921,-0.0213024901,-0.0169948274,-0.0110802901,-0.0528429939,0.0265556674,0.0022046769,0.0149610857,-0.0553892934,-0.0399502121,0.1121993522,0.0410041895,0.0215360721,-0.0564892427,0.0499000817,0.0076399381,0.0586268842,-0.0073279264,0.0054179432,0.0005760469,0.020149777,0.0521101284,0.0217990249,0.0409956165,-0.0040799135,0.0241583287,-0.0172351918,0.0277397752,-0.0568757936,0.0145025247,-0.0190796983,-0.0167079949,-0.0117606027,-0.0408418296,-0.021457067,0.0410066064,-0.0587391257,-0.0209262131,0.0138230162,-0.0499872493,-0.0225033564,0.0351894554,0.0144487363,-0.0407515332,0.0460406913,0.0457023294,-0.0166072665,0.0203469157,-0.0335600302,-0.019929623,-0.018014142,0.0389312085,-0.0477803468,-0.0114594447,0.0558881831,0.0131264882,0.0407563555,0.0456409458,-0.0039691448,-0.0107051163,-0.0161525264,0.015745068,0.013642058,-0.0316020365,0.0092213733,0.0101140739,-0.0009481665,0.0158593486,-0.0609217392,0.0313037257,-0.0185854787,-0.0198467187,-0.0543593789,0.0192402242,0.0073605178,-0.036231755,-0.0338483264,-0.0116528209,0.0248831471,-0.0148259809,-0.0277260107,0.030937987,-0.0042919222,0.0461844123,0.0130492549]},"95":{"Abstract":"The forensic investigation of communication datasets which contain unstructured text, social network information, and metadata is a complex task that is becoming more important due to the immense amount of data being collected. Currently there are limited approaches that allow an investigator to explore the network, text and metadata in a unified manner. We developed Beagle as a forensic tool for email datasets that allows investigators to flexibly form complex queries in order to discover important information in email data. Beagle was successfully deployed at a security firm which had a large email dataset that was difficult to properly investigate. We discuss our experience developing Beagle as well as the lessons we learned applying visual analytic techniques to a difficult real-world problem.","Authors":"J. Koven; C. Felix; H. Siadati; M. Jakobsson; E. Bertini","DOI":"10.1109\/TVCG.2018.2865023","Keywords":"Visual Analytics;Email Investigation;Email Forensics","Title":"Lessons Learned Developing a Visual Analytics Solution for Investigative Analysis of Scamming Activities","Keywords_Processed":"email investigation;email forensic;visual analytic","Keyword_Vector":[0.1335532892,-0.1965623238,-0.203154499,0.1770796639,-0.1490354584,-0.1741885543,0.103461286,0.0091954135,0.0624421464,-0.0289544898,0.0560051055,-0.0252430921,0.007327258,0.106064374,-0.0204565719,0.0147254525,0.0397148369,0.0225714384,0.001385536,-0.0157357273,0.00455777,-0.0105506519,0.034148841,-0.0305376797,-0.0011950328,0.0105279032,0.0017241891,-0.0387974201,-0.0051965938,-0.0029138157,0.0069433171,0.0134576964,0.0458470799,-0.0237178042,0.0213426891,-0.0005410256,0.0192376281,-0.0133030227,-0.0181267244,-0.0617252487,-0.0426118146,-0.0315499585,-0.016529294,-0.0347843216,0.0078747795,0.0050369331,-0.0625062196,-0.0045140979,-0.0068796338,-0.0074757573,0.0222138465,0.0071935498],"Abstract_Vector":[0.2579870863,-0.1707479114,0.0218742648,0.0239983097,-0.0508988293,-0.0266925556,-0.0021266764,0.0030079466,-0.0652002827,0.0206517176,-0.0210148247,-0.019208225,-0.078631817,-0.0346284328,-0.0253716207,-0.0209677913,-0.0271742839,-0.087470883,-0.0154303484,0.0463999702,0.0689895708,0.0252864534,0.0068785749,0.0032060652,-0.0179368488,0.1098658596,0.0016096373,0.0244550149,-0.0328026554,-0.0131507263,0.0611906755,-0.0554969552,0.0049556924,-0.0208193896,-0.0063656898,-0.0904837903,0.0505165138,0.0064545534,-0.0188680659,-0.0304929561,-0.0133053157,0.0087610018,-0.0773270956,-0.0198461876,0.0442819621,-0.0337261929,0.0833004381,0.029792791,-0.0998213036,0.005265539,0.0284749051,0.0692111642,-0.0046084209,0.0199533692,-0.0072916901,0.003673778,-0.0143359724,-0.017065616,0.066487119,0.032024641,-0.0180855366,0.0114905084,-0.00311332,-0.0006465566,0.0273588083,0.0195245521,0.0155272845,-0.0024391676,-0.0140626454,-0.022157886,-0.0104610474,-0.0032196738,0.0087054421,0.0307251805,-0.0198874004,-0.0372639257,-0.0307148736,-0.0037234287,0.0123729864,-0.0185288674,0.0217869659,-0.0393966125,0.0102177636,0.0276462096,0.003014342,0.0252157837,0.0178148573,-0.0011490184,-0.0614951704,0.036298434,0.0026845038,-0.005489584,0.0108725555,-0.0092187265,0.0387464966,0.0401039548,0.0135412941,-0.0139993639,0.0460441216,-0.0093278099,-0.0565330964,-0.0155905273,-0.0115312169,0.0163968823,-0.0398917046,-0.0273888299,0.02241079,0.0143084944,-0.0239684652,-0.0383463212,-0.0647820569,-0.0374536126,-0.0445997962,-0.0207664158,-0.0254221417,-0.0113281224]},"96":{"Abstract":"Superfluidity is a special state of matter exhibiting macroscopic quantum phenomena and acting like a fluid with zero viscosity. In such a state, superfluid vortices exist as phase singularities of the model equation with unique distributions. This paper presents novel techniques to aid the visual understanding of superfluid vortices based on the state-of-the-art non-linear Klein-Gordon equation, which evolves a complex scalar field, giving rise to special vortex lattice\/ring structures with dynamic vortex formation, reconnection, and Kelvin waves, etc. By formulating a numerical model with theoretical physicists in superfluid research, we obtain high-quality superfluid flow data sets without noise-like waves, suitable for vortex visualization. By further exploring superfluid vortex properties, we develop a new vortex identification and visualization method: a novel mechanism with velocity circulation to overcome phase singularity and an orthogonal-plane strategy to avoid ambiguity. Hence, our visualizations can help reveal various superfluid vortex structures and enable domain experts for related visual analysis, such as the steady vortex lattice\/ring structures, dynamic vortex string interactions with reconnections and energy radiations, where the famous Kelvin waves and decaying vortex tangle were clearly observed. These visualizations have assisted physicists to verify the superfluid model, and further explore its dynamic behavior more intuitively.","Authors":"Y. Guo; X. Liu; C. Xiong; X. Xu; C. Fu","DOI":"10.1109\/TVCG.2017.2719684","Keywords":"Superfluid dynamics;vortex structure;visual analysis","Title":"Towards High-Quality Visualization of Superfluid Vortices","Keywords_Processed":"visual analysis;superfluid dynamic;vortex structure","Keyword_Vector":[0.0506418823,0.0137003687,0.000680191,0.0094884079,0.0155845521,0.004853062,-0.0086142611,-0.0399604571,-0.0168939044,-0.0314388246,0.0223130245,0.0014739362,-0.05603993,-0.0064679347,0.0623231189,0.0717799955,-0.0529670419,-0.0842151499,0.0836430289,-0.0677414213,0.1247548292,0.0005390638,-0.1388140715,0.0168765735,-0.0022943721,-0.0137365982,0.0800150145,0.0274448075,0.0948139084,-0.0371822147,-0.1088791388,0.0065755266,0.0026710027,0.0146157088,-0.0127075714,-0.0223995211,0.0034729617,0.0343408766,-0.0387371308,0.021070708,-0.0713270585,-0.0351051127,-0.0676455224,0.0000550355,0.0338036848,0.0781171999,0.0076569168,-0.036338302,-0.0450862215,0.0143431337,-0.0035068418,-0.0390500618],"Abstract_Vector":[0.1251015812,-0.0059108248,0.0153179746,0.02713156,-0.0088782388,0.0091786818,0.0192280547,0.0237117339,0.000021494,-0.0149649337,0.0114888318,0.0027947431,-0.0179131801,-0.008280745,-0.0022265234,0.0163538734,0.0248032813,0.0118248342,0.0031812441,-0.0239105913,0.0483090262,-0.0228085366,0.0284648833,-0.0167424163,0.0102676943,0.0208858438,0.0221254502,-0.0205252695,-0.0207944492,0.0063210966,0.0039344207,0.0317399435,-0.0295144958,0.0117458521,0.0134907353,0.0157264484,-0.0028696168,0.0298274789,-0.030669043,0.0040287326,0.0270366838,0.0039773024,-0.0129762145,-0.0136478036,0.0229663958,-0.0170217148,0.0205545562,0.0290537297,-0.001510431,-0.0244157503,-0.000165977,-0.027084914,0.000818953,0.0010362674,0.0051945076,0.0143988929,0.0463148285,-0.0296335147,-0.006039934,0.0049960153,0.0386217284,0.0035445746,-0.0022655876,0.0006527338,0.0299483744,-0.0554583673,-0.0239104564,-0.0141450685,-0.0060361009,0.0247775158,0.0135189658,-0.0059738356,-0.0556282887,0.0153550411,0.0633516661,-0.0402390364,0.0049186573,-0.0367549237,0.0240304633,-0.0118920483,-0.0177899647,-0.0075470782,0.0052502253,-0.0400485965,0.0053983642,-0.0052949832,-0.0101403524,-0.0243175572,-0.0090088931,-0.0228388448,-0.0067488202,0.0052594676,-0.0190085296,0.0033029343,-0.0041068409,0.0290728723,-0.0263643113,-0.0147435137,0.0156323438,0.0003825088,-0.041482968,0.0351968862,-0.0346027272,-0.0004374668,0.0002023232,0.0147329911,-0.0265353537,-0.0097126102,0.0162107001,-0.0242363278,0.0092422724,0.0203730031,0.019729728,-0.0175159506,-0.0174877116,0.0107509327]},"97":{"Abstract":"Recent publications and art performances demonstrate amazing results using projection mapping. To our knowledge, there exists no multi-projection system that can project onto non-rigid target geometries. This constrains the applicability and quality for live performances with multiple spectators. Given the cost and complexity of current systems, we present a low-cost easy-to-use markerless non-rigid face multi-projection system. It is based on a non-rigid, dense face tracker and a real-time multi-projection solver adapted to imprecise tracking, geometry and calibration. Using this novel system we produce compelling results with only consumer-grade hardware.","Authors":"C. Siegl; V. Lange; M. Stamminger; F. Bauer; J. Thies","DOI":"10.1109\/TVCG.2017.2734428","Keywords":"Face Projection;Mixed Reality;Multi-Projection Mapping;Non-Rigid Face Tracking","Title":"FaceForge: Markerless Non-Rigid Face Multi-Projection Mapping","Keywords_Processed":"Non rigid face tracking;Multi Projection mapping;mixed reality;face projection","Keyword_Vector":[0.0107230533,-0.0045508963,0.0000471349,-0.0053443563,0.0041126209,-0.0150840205,-0.0020619293,-0.0020984852,-0.0062483403,0.0008112685,0.0060241754,0.0044301186,-0.0020356147,0.0105899891,-0.0058058859,-0.0070572852,-0.0021366034,-0.0078000781,0.0186943337,0.0037947394,0.0095957162,0.0163168006,-0.0113008439,0.0043848376,0.0038187503,0.0004001175,-0.0013340161,-0.0087200912,0.007498623,0.0042478229,-0.0062900936,-0.0173528617,-0.0125827115,0.0085127609,-0.0068385485,0.0070711005,0.0038096128,-0.000993966,-0.0032760873,0.0048421059,0.0102752736,0.020798955,0.0020163694,-0.019635534,0.0119624294,-0.0204051319,0.0022858662,0.0005280195,0.0117329332,-0.0030669984,0.0056344531,0.0256452797],"Abstract_Vector":[0.1381839301,0.0568558848,-0.0167652154,0.0206955895,-0.0326775881,0.0439044204,0.0276237082,0.0301477819,0.0037476787,0.0311515819,0.0081820889,-0.005676673,-0.0454050126,0.0023591929,0.0350074025,0.01156944,0.0077049219,0.0911192245,-0.0188638729,-0.068952115,0.0184858558,-0.0004957037,0.0237423894,0.0164047305,-0.0397488356,0.0041592577,0.0130622733,0.039287797,0.0090414329,0.0542439002,0.0278962774,0.0020421664,-0.0316764062,-0.0550686702,0.0155197048,-0.0240979052,-0.011792709,0.021696709,-0.0239827517,-0.0389175408,0.0697419155,0.0304081365,-0.0051101323,0.0116856134,0.0232998896,0.035348982,-0.0026643696,-0.0202678696,0.0266157664,-0.0307089027,0.0414877382,0.0114823162,-0.0029846675,-0.0385830249,0.0024354836,0.0406644573,-0.037492228,-0.0036011645,-0.0108110154,-0.0468426454,-0.0077976113,-0.020121968,0.0378360639,-0.0116355156,-0.0229178235,-0.0249034834,-0.0168819414,0.0039256212,-0.0024215618,-0.0212738609,0.0340489268,0.0111405312,0.0263227799,-0.0120849049,0.0163149912,-0.0655620018,0.0163680905,0.0505479429,-0.0308032427,0.0071588583,-0.0086338056,-0.0141150639,-0.0143245648,-0.019516188,-0.0239442138,0.0271582792,-0.013508209,-0.0335949086,-0.0160908774,-0.0094627851,-0.0038865329,0.0090155487,-0.0224224947,0.0091980828,-0.0155004192,-0.0570261452,0.043118866,-0.0456981926,-0.0377322513,0.009174343,0.0405269493,-0.0125581116,0.0429122792,-0.0026645321,-0.0770010981,0.0109835322,0.0016071941,0.0305422004,0.00725848,-0.0030736455,-0.0019171283,-0.061247868,-0.0633560047,0.0166998473,0.0219090894,0.0176159532]},"98":{"Abstract":"Dashboards are one of the most common use cases for data visualization, and their design and contexts of use are considerably different from exploratory visualization tools. In this paper, we look at the broad scope of how dashboards are used in practice through an analysis of dashboard examples and documentation about their use. We systematically review the literature surrounding dashboard use, construct a design space for dashboards, and identify major dashboard types. We characterize dashboards by their design goals, levels of interaction, and the practices around them. Our framework and literature review suggest a number of fruitful research directions to better support dashboard design, implementation and use.","Authors":"A. Sarikaya; M. Correll; L. Bartram; M. Tory; D. Fisher","DOI":"10.1109\/TVCG.2018.2864903","Keywords":"Dashboards;literature review;survey;design space;open coding","Title":"What Do We Talk About When We Talk About Dashboards?","Keywords_Processed":"survey;design space;literature review;open coding;dashboard","Keyword_Vector":[0.1216367857,-0.0750115443,-0.0928410812,0.0859471194,-0.0533254978,-0.1442640232,0.0112030782,0.0414268076,-0.0829316778,-0.0661732254,0.0427373526,0.1175219771,-0.0116678822,-0.0143367414,0.1059825036,0.1124676783,0.0854706132,0.0423037499,0.2517704261,-0.0603003764,-0.1096514581,0.0975029514,0.1724614106,0.1126041542,-0.0838402734,-0.0687292789,-0.048224061,-0.0269115571,0.1100238083,-0.1377032256,-0.0448093415,-0.0990702288,0.144441446,0.0539908769,-0.0246636374,0.0934195911,0.090831338,-0.0339391615,-0.0078740051,-0.0678005108,0.044642031,0.0964658961,-0.0180364983,0.0270688274,0.0508827933,0.0002058663,-0.0213249319,0.0335961584,-0.0591549196,0.1118945193,0.0174362428,0.0432548025],"Abstract_Vector":[0.1117249121,-0.0279595583,0.0009534237,0.0049995225,-0.0607726444,0.0372123386,0.0241867922,0.0122531999,0.0247588495,0.0252623741,-0.0130472295,0.0156597389,0.0065509956,-0.0122427283,0.0077890037,0.0003837241,0.0140983089,0.0358313524,-0.0015458089,0.0392652426,-0.0128252065,-0.0344080665,0.0194512443,0.0035904823,0.0036352434,-0.0045428428,-0.0199533601,0.0217360446,0.0303333478,0.0163057669,0.0334688435,0.0216982237,0.0186882556,0.0219476624,0.0074649142,0.0060354529,0.0063592564,-0.0083402316,-0.0119684295,0.0176333572,0.0007776637,0.0294850076,0.0139496278,-0.0016926491,0.0333646138,0.0177800145,-0.0130080362,0.0400806277,0.0011499813,-0.0096008679,-0.0286169019,-0.043981383,-0.0025874213,-0.0053966815,0.032847782,-0.000709724,0.0021463741,0.0142786061,-0.0049902943,-0.0004726098,0.0105965926,0.0187334218,0.0151450599,0.0067845287,-0.0135787385,-0.0175339147,-0.0045697613,-0.0384588561,0.0087483316,0.0190598246,0.0229026748,0.0174460956,0.0149900816,0.0022599728,0.0309315114,-0.0295104352,0.0176199407,-0.0087885549,0.0183531764,-0.0192365608,-0.0216895088,0.0223539132,0.0109821461,-0.0018724718,-0.0215582494,-0.0113119149,0.005127045,-0.000536445,0.0196398551,-0.0250868963,0.0091621365,-0.019944751,-0.0067276543,-0.0193490842,-0.0065811957,0.0195394634,0.0152249554,-0.0071705521,-0.028994018,-0.0293182745,-0.0093614201,-0.0083321569,0.0037025609,-0.0143187433,-0.0185177434,0.0011449405,0.0103255479,0.0053412926,-0.0093228666,-0.0117972709,-0.0012315049,-0.0208181435,0.0423561302,-0.0143249094,0.0336818797,0.0001367475]},"99":{"Abstract":"Today molecular simulations produce complex data sets capturing the interactions of molecules in detail. Due to the complexity of this time-varying data, advanced visualization techniques are required to support its visual analysis. Current molecular visualization techniques utilize ambient occlusion as a global illumination approximation to improve spatial comprehension. Besides these shadow-like effects, interreflections are also known to improve the spatial comprehension of complex geometric structures. Unfortunately, the inherent computational complexity of interreflections would forbid interactive exploration, which is mandatory in many scenarios dealing with static and time-varying data. In this paper, we introduce a novel analytic approach for capturing interreflections of molecular structures in real-time. By exploiting the knowledge of the underlying space filling representations, we are able to reduce the required parameters and can thus apply symbolic regression to obtain an analytic expression for interreflections. We show how to obtain the data required for the symbolic regression analysis, and how to exploit our analytic solution to enhance interactive molecular visualizations.","Authors":"R. Sk\u00e5nberg; P. V\u00e1zquez; V. Guallar; T. Ropinski","DOI":"10.1109\/TVCG.2015.2467293","Keywords":"Molecular visualization;diffuse interreflections;ambient occlusion;Molecular visualization;diffuse interreflections;ambient occlusion","Title":"Real-Time Molecular Visualization Supporting Diffuse Interreflections and Ambient Occlusion","Keywords_Processed":"diffuse interreflection;molecular visualization;ambient occlusion","Keyword_Vector":[0.038574824,0.0443587897,-0.0204630929,-0.0095504019,-0.0038600564,-0.0100575751,-0.0155715501,-0.0276686573,-0.0319663412,-0.0023259331,-0.0041196127,-0.0236050749,0.0535831512,0.0111282854,0.0493446974,-0.0122361227,0.012740872,0.0732628167,-0.0055750878,0.034563081,0.0715197936,-0.0016191365,-0.0248604966,0.063644352,0.0414008104,-0.0072136003,-0.0602416687,-0.022122166,0.0047834112,-0.0132489948,0.0057265261,0.0499009034,0.0011034869,0.0191235928,0.0213048275,-0.0120719401,-0.0222814396,0.0320885686,-0.0245375351,-0.046677281,0.0265589751,-0.017470332,-0.0430755835,-0.0101750836,-0.0117544779,-0.0000449674,0.0322517882,-0.0472585082,0.0161718248,0.0251207355,0.0355367124,-0.0279450916],"Abstract_Vector":[0.1692493105,0.1147978699,-0.0714777803,0.0348050635,-0.0216225558,-0.0128979342,0.0843555638,0.0248661257,0.0599296719,0.0692600552,0.0226489628,0.0072222443,-0.0287831376,-0.0211233279,0.0278356079,-0.0461216274,-0.0126811346,-0.0042466322,-0.0248541088,0.0093113621,0.0206152982,-0.0422111384,-0.0067048892,0.0246923438,0.011235745,0.004118654,-0.0236724073,-0.0325206158,-0.0317586144,0.0187711664,-0.0201039662,-0.0010074596,0.0075955564,0.00906673,0.0021610027,-0.0349270215,-0.0116679324,0.0152818472,0.0299927972,0.0073353498,0.0161169802,0.038081246,-0.0146804849,0.0164379821,-0.0007266383,-0.0072031467,0.0110449622,0.0115288624,0.0484564006,-0.0227358237,0.0105333757,-0.0154074325,-0.0330153913,-0.0148562056,0.0113283327,-0.0280539208,0.001008406,-0.0272121871,-0.0213328714,0.006104224,0.0099691865,0.0083643458,-0.0017545105,-0.0068731164,-0.0069451273,0.0158623273,-0.011875862,0.019831392,0.0017285301,0.0057333279,0.0113437506,-0.0420535669,-0.021232679,-0.0158876008,0.0229818538,0.0085647196,-0.0202904801,0.0037034658,0.039245064,0.0039074387,0.0119771155,-0.0123769698,0.0406069337,0.0107516264,0.0432220555,-0.0289677388,-0.0442675829,0.0116922141,0.0154739488,0.0125343053,0.0012113134,0.0155412327,-0.0281406017,0.0149798029,0.0342770104,-0.0015300466,-0.0511207411,-0.0169386222,0.0294470404,-0.0159094926,-0.0120627234,-0.0150581246,-0.012595618,0.0002959825,-0.0264014137,-0.0208185276,-0.0371538852,0.042475991,0.0017070575,-0.0363861359,-0.0060090788,0.0227257338,-0.0301981784,0.0061025347,-0.007075297,0.0054170451]}}